diff --git a/src/mpid/ch4/netmod/ucx/ucx/.gitlab-ci.yml b/src/mpid/ch4/netmod/ucx/ucx/.gitlab-ci.yml
new file mode 100644
index 000000000..ba1209350
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/.gitlab-ci.yml
@@ -0,0 +1,14 @@
+build:
+  only:
+  - tags
+  script:
+  - "./autogen.sh"
+  - "./contrib/configure-release --disable-numa"
+  - make -j
+  - make dist
+  - 'export upload_url=$(curl -s -H "Authorization: token $github_token" "https://api.github.com/repos/openucx/ucx/releases" | python -c "import sys,os,json; d=json.load(sys.stdin); tag=os.environ.get(\"CI_COMMIT_TAG\"); rel = [r for r in d if r[\"tag_name\"] == tag]; url = rel[0][\"upload_url\"] if rel else \"\"; print url" | grep -oP "https\S+assets")'
+  - echo $upload_url
+  - 'export tar_name=$(ls *.tar.gz)'
+  - echo $tar_name
+  - 'curl -s -H "Authorization: token $github_token" -H "Content-Type: application/zip" --data-binary @"$tar_name" "${upload_url}?name=${tar_name}&label=${tar_name}"'
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/AUTHORS b/src/mpid/ch4/netmod/ucx/ucx/AUTHORS
index 9c4b28f28..77d052761 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/AUTHORS
+++ b/src/mpid/ch4/netmod/ucx/ucx/AUTHORS
@@ -1,15 +1,23 @@
+Akshay Venkatesh
 Alexander Margolin <alexma@mellanox.com>
 Alexander Mikheev <alexm@mellanox.com>
 Alina Sklarevich <alinas@mellanox.com>
+Andrey Maslennikov
+Artem Polyakov
+Artemy Kovalyov
 Aurelien Bouteiller <bouteill@icl.utk.edu>
+Devendar Bureddy
 Elad Persiko <eladpe@mellanox.com>
 Eugene Voronov <eugene@mellanox.com>
 Evgeny Leksikov <evgenylek@mellanox.com>
+Gilles Gouaillardet
 Graham Lopez <lopezmg@ornl.gov>
 Guy Shattah <sguy@mellanox.com>
 Howard Pritchard <howardp@lanl.gov>
 Igor Ivanov <igori@mellanox.com>
 Ilya Nelkenbaum
+Jeff Daily
+Khaled Hamidouche
 Manjunath Gorentla Venkata <manjugv@ornl.gov>
 Matthew Baker <bakermb@ornl.gov>
 Mike Dubman <miked@mellanox.com>
@@ -18,15 +26,18 @@ Nathan Hjelm <hjelmn@lanl.gov>
 Netanel Yosephian <netanelyo@mellanox.com>
 Pavel Shamis <pavel.shamis@arm.com>
 Sasha Kotchubievsky <sashakot@mellanox.com>
+Sergey Oblomov
 Sergey Shalnov
+Serguei Sagalovitch
 Stephen Richmond <srichmond@dancer.icl.utk.edu>
 Swen Boehm <boehms@ornl.gov>
-Tony Curtis <tonyc@uh.edu>
+Tony Curtis <anthony.curtis@stonybrook.edu>
 Xin Zhao <xinz@mellanox.com>
 Yossi Itigin <yosefe@mellanox.com>
 
-In addition we would like to acknowledge the following members of UCX community for
-their participation in annual face-to-face meeting, design discussions, and code reviews:
+In addition we would like to acknowledge the following members of UCX community
+for their participation in annual face-to-face meeting, design discussions, and
+code reviews:
 
 Amith Mamidala
 Barney Maccabe
diff --git a/src/mpid/ch4/netmod/ucx/ucx/CONTRIBUTING.md b/src/mpid/ch4/netmod/ucx/ucx/CONTRIBUTING.md
index 791e70445..140810400 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/CONTRIBUTING.md
+++ b/src/mpid/ch4/netmod/ucx/ucx/CONTRIBUTING.md
@@ -1,7 +1,7 @@
 
 1. Please sign the [UCX contributors agreement](http://www.openucx.org/license).
 
-1. Please follow the [code style](https://github.com/openucx/ucx/blob/master/doc/CodeStyle) and [logging style](https://github.com/openucx/ucx/blob/master/doc/LoggingStyle).
+1. Please follow the [code style](https://github.com/openucx/ucx/blob/master/doc/CodeStyle.md) and [logging style](https://github.com/openucx/ucx/blob/master/doc/LoggingStyle.md).
 
 1. Make sure automatic tests pass.
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/NEWS b/src/mpid/ch4/netmod/ucx/ucx/NEWS
index a2750ac9c..fa6864e1b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/NEWS
+++ b/src/mpid/ch4/netmod/ucx/ucx/NEWS
@@ -1,12 +1,92 @@
 #
-## Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
+## Copyright (C) Mellanox Technologies Ltd. 2001-2019.  ALL RIGHTS RESERVED.
 ## Copyright (C) UT-Battelle, LLC. 2014-2015. ALL RIGHTS RESERVED.
-## Copyright (C) ARM Ltd. 2017.  ALL RIGHTS RESERVED.
+## Copyright (C) ARM Ltd. 2017-2019.  ALL RIGHTS RESERVED.
 ##
 ## See file LICENSE for terms.
 ##
 #
 
+## 1.5.0 (February 14, 2019)
+Features:
+- New emulation mode enabling full UCX functionality (Atomic, Put, Get)
+  over TCP and RDMA-CORE interconnects which don't implement full RDMA semantics
+- Non-blocking API for all one-sided operations. All blocking communication APIs marked
+  as deprecated
+- New client/server connection establishment API, which allows connected handover between workers
+- Support for rdma-core direct-verbs (DEVX) and DC with mlx5 transports
+- GPU - Support for stream API and receive side pipelining
+- Malloc hooks using binary instrumentation instead of symbol override
+- Statistics for UCT tag API
+- GPU-to-Infiniband HCA affinity support based on locality/distance (PCIe)
+
+Bugfixes:
+- Fix overflow in RC/DC flush operations
+- Update description in SPEC file and README
+- Fix RoCE source port for dc_mlx5 flow control
+- Improve ucx_info help message
+- Fix segfault in UCP, due to int truncation in count_one_bits()
+- Multiple other bugfixes (full list on github)
+
+Tested configurations:
+- InfiniBand: MLNX_OFED 4.4-4.5, distribution inbox drivers, rdma-core
+- CUDA: gdrcopy 1.2, cuda 9.1.85
+- XPMEM: 2.6.2
+- KNEM: 1.1.2
+
+## 1.4.0 (October 23, 2018)
+
+Features:
+- Improved support for installation with latest ROCm
+- Improved support for latest rdma-core
+- Adding support for CUDA IPC for intra-node GPU
+- Added support for CUDA memory allocation cache for mem-type detection
+- Added support for latest Mellanox devices
+- Added support for Nvidia GPU managed memory
+- Added support for multiple connections between the same pair of workers
+- Added support large worker address for client/server connection establishment
+  and INADDR_ANY
+- Added support for bitwise atomics operations
+
+Bugfixes:
+- Performance fixes for rendezvous protocol
+- Memory hook fixes
+- Clang support fixes
+- Self tl multi-rail fix
+- Thread safety fixes in IB/RDMA transport
+- Compilation fixes with upstream rdma-core
+- Multiple minor bugfixes (full list on github)
+- Segfault fix for a code generated by armclang compiler
+- UCP memory-domain index fix for zero-copy active messages
+
+Tested configurations:
+- InfiniBand: MLNX_OFED 4.2-4.4, distribution inbox drivers, rdma-core
+- CUDA: gdrcopy 1.2, cuda 9.1.85
+- XPMEM: 2.6.2
+- KNEM: 1.1.2
+
+Known issues:
+  #2919 - Segfault in CUDA support when KNEM not present and CMA is active
+  intra-node RMA transport. As a workaround user can disable CMA support at
+  compile time: --disable-cma. Alternatively user can remove CMA from UCX_TLS
+  list, for example: UCX_TLS=mm,rc,cuda_copy,cuda_ipc,gdr_copy.
+
+## 1.3.1 (August 20, 2018)
+
+Bugfixes:
+- Prevent potential out-of-order sending in shared memory active messages
+- CUDA: Include cudamem.h in source tarball, pass cudaFree memory size
+- Registration cache: fix large range lookup, handle shmat(REMAP)/mmap(FIXED)
+- Limit IB CQE size for specific ARM boards
+- RPM: explicitly set gcc-c++ as requirement
+- Multiple bugfixes (full list on github)
+
+Tested configurations:
+- InfiniBand: MLNX_OFED 4.2, inbox OFED drivers.
+- CUDA: gdrcopy 1.2, cuda 9.1.85
+- XPMEM: 2.6.2
+- KNEM: 1.1.2
+
 ## 1.3.0 (February 15, 2018)
 
 Features:
@@ -28,17 +108,19 @@ Features:
 - Add support for external epoll fd and edge-triggered events
 - Added registration cache for knem
 - Initial support for Java bindings
+
 Bugfixes:
-- Multiple bugfixes (full list on githib)
+- Multiple bugfixes (full list on github)
+
 Tested configurations:
 - InfiniBand: MLNX_OFED 4.2, inbox OFED drivers.
 - CUDA: gdrcopy 1.2, cuda 9.1.85
 - XPMEM: 2.6.2
 - KNEM: 1.1.2
+
 Known issues:
   #2047 - UCP: ucp_do_am_bcopy_multi drops data on UCS_ERROR_NO_RESOURCE
   #2047 - failure in ud/uct_flush_test.am_zcopy_flush_ep_nb/1
-  #2025 - Data corruption on some ARM machines
   #1977 - failure in shm/test_ucp_rma.blocking_small/0
   #1926 - Timeout in mpi_test_suite with HW TM
   #1920 - transport retry count exceeded in many-to-one tests
@@ -79,6 +161,7 @@ Supported platforms
     for community evaluation and has not been tested in context of this release
   - Cray Gemini and Aries
   - Architectures: x86_64, ARMv8 (64bit), Power64
+
 Features:
   - Added support for InfiniBand DC and UD transports, including accelerated verbs for Mellanox devices
   - Full support for PGAS/SHMEM interfaces, blocking and non-blocking APIs
@@ -91,8 +174,10 @@ Features:
   - Support for ARMv8 64bit architecture
   - A new API for efficient memory polling
   - Support for malloc-hooks and memory registration caching
+
 Bugfixes:
   - Multiple bugfixes improving overall stability of the library
+
 Known issues:
   #1604 - Failure in ud/test_ud_slow_timer.retransmit1/1 with valgrind bug
   #1588 - Fix reading cpuinfo timebase for ppc bug portability training
@@ -111,21 +196,14 @@ Known issues:
 
 ## 1.1.0 (September 1, 2015)
 
-Workarounds:
 Features:
   - Added support for AM based on FIFO in `mm` shared memory transport
   - Added support for UCT `knem` shared memory transport (http://knem.gforge.inria.fr)
   - Added support for UCT `mm/xpmem` shared memory transport (https://github.com/hjelmn/xpmem)
 
-
-Bugfixes:
-Known issues:
-
-
 ## 1.0.0 (July 22, 2015)
 
 Features:
-
   - Added support for UCT `cma` shared memory transport (Cross-Memory Attatch)
   - Added support for UCT `mm` shared memory transport with mmap/sysv APIs
   - Added support for UCT `rc` transport based on Infiniband/RC with verbs
diff --git a/src/mpid/ch4/netmod/ucx/ucx/PULL_REQUEST_TEMPLATE.md b/src/mpid/ch4/netmod/ucx/ucx/PULL_REQUEST_TEMPLATE.md
new file mode 100644
index 000000000..80318aecb
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/PULL_REQUEST_TEMPLATE.md
@@ -0,0 +1,10 @@
+## What
+_Describe what this PR is doing._ 
+
+## Why ?
+_Justification for the PR. If there is existing issue/bug please reference. For
+bug fixes why and what can be merged in a single item._
+
+## How ?
+_It is optional but for complex PRs please provide information about the design,
+architecture, approach, etc.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/README b/src/mpid/ch4/netmod/ucx/ucx/README
index 7ffcdf5cc..b5a1c721d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/README
+++ b/src/mpid/ch4/netmod/ucx/ucx/README
@@ -1,17 +1,48 @@
-UCX is a communication library implementing high-performance messaging for MPI/PGAS frameworks
+UCX stands for Unified Communication X. UCX provides an optimized communication
+layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric
+applications. UCX utilizes high-speed networks, such as RDMA (InfiniBand, RoCE,
+etc), Cray Gemini or Aries, for inter-node communication. If no such network is
+available, TCP is used instead. UCX supports efficient transfer of data in
+either main memory (RAM) or GPU memory (through CUDA and ROCm libraries).
+In addition, UCX provides efficient intra-node communication, by leveraging the
+following shared memory mechanisms: posix, sysv, cma, knem, and xpmem.
+
+### Running internal unit tests
+```sh
+$ ./autogen.sh
+$ ./contrib/configure-devel
+$ make
+$ make -C test/gtest test
+```
+
+### Running UCX hello-world
+Compile:
 
-### Quick start
 ```sh
-$ export PATH=$PATH:$MPI_HOME/bin  
 $ ./autogen.sh  
-$ ./contrib/configure-release --prefix=$PWD/install --with-mpi  
-$ make -j8 install  
-$ salloc -N2 --ntasks-per-node=1 mpirun --display-map \
-      $PWD/install/bin/ucx_perftest -d mlx5_1:1 -x rc_mlx5 -c 12 -t put_lat  
+$ ./contrib/configure-release --prefix=$PWD/install
+$ make
+```
+Start server:
+
+```sh
+$ ./src/tools/perf/ucx_perftest -c 0
+```
+
+Connect client:
+
+```sh
+$ ./src/tools/perf/ucx_perftest <server-hostname> -t tag_lat -c 0
 ```
 
-### UCX layout
-- UCX - Unified Communication X
-- UCP - UCX Protocol
-- UCT - UCX Transport
-- UCS - UCX Services
+### UCX high-level layout
+UCX contains four main components:
+- UCP - UCX Protocol. Implements high-level abstractions such as tag-matching,
+  streams, connection negotiation and establishment, multi-rail, and handling 
+  different memory types.
+- UCT - UCX Transport. Implements low-level communication primitives such as
+  active messages, remote memory access, and atomic operations.
+- UCS - UCX Services. A collection of data structures, algorithms, and system
+  utilities for common use.
+- UCM - UCX Memory. Intercepts memory allocation and release events, used by
+  memory registration cache.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/pom.xml b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/pom.xml
index 59d12b39a..abebf4c80 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/pom.xml
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/pom.xml
@@ -18,11 +18,21 @@
     <ucs.lib.path>${ucx.src.dir}/ucs/.libs</ucs.lib.path>
     <uct.lib.path>${ucx.src.dir}/uct/.libs</uct.lib.path>
     <ucp.lib.path>${ucx.src.dir}/ucp/.libs</ucp.lib.path>
+    <junit.version>4.12</junit.version>
     <sources>**/jucx/**</sources>
     <testSources>**/dummy/**</testSources>
     <skipCopy>false</skipCopy>
   </properties>
 
+  <dependencies>
+    <dependency>
+      <groupId>junit</groupId>
+      <artifactId>junit</artifactId>
+      <version>${junit.version}</version>
+      <scope>test</scope>
+    </dependency>
+  </dependencies>
+
   <build>
     <testResources>
       <testResource>
@@ -62,7 +72,7 @@
               <testExcludes>
                 <exclude>${testSources}</exclude>
               </testExcludes>
-            </configuration> 
+            </configuration>
             <goals>
               <goal>testCompile</goal>
             </goals>
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Bridge.java b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Bridge.java
index cd9876b92..5e52ed616 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Bridge.java
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Bridge.java
@@ -4,6 +4,8 @@
  */
 package org.ucx.jucx;
 
+import org.ucx.jucx.Worker.CompletionQueue;
+
 public class Bridge {
     private static final String UCM  = "libucm.so";
     private static final String UCS  = "libucs.so";
@@ -18,4 +20,20 @@ public class Bridge {
         LoadLibrary.loadLibrary(UCP);   // UCP library
         LoadLibrary.loadLibrary(JUCX);  // JUCP native library
     }
+
+    private static native long createWorkerNative(int maxCompletions,
+                                                  CompletionQueue compQueue,
+                                                  Worker worker);
+
+    static long createWorker(final int maxCompletions,
+                             final CompletionQueue compQueue,
+                             final Worker worker) {
+        return createWorkerNative(maxCompletions, compQueue, worker);
+    }
+
+    private static native void releaseWorkerNative(long workerNativeId);
+
+    static void releaseWorker(final Worker worker) {
+        releaseWorkerNative(worker.getNativeId());
+    }
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/LoadLibrary.java b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/LoadLibrary.java
index 745473329..350a63f4f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/LoadLibrary.java
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/LoadLibrary.java
@@ -44,7 +44,7 @@ public class LoadLibrary {
                 System.load(filename);
             } catch (UnsatisfiedLinkError e) {
                 errorMessage = "Native code library failed to load: "
-                        + resourceName;
+                               + resourceName;
             }
 
             file.deleteOnExit();
@@ -75,7 +75,7 @@ public class LoadLibrary {
         }
 
         File file = new File(tempDir,
-                new File(resourceURL.getPath()).getName());
+                             new File(resourceURL.getPath()).getName());
         FileOutputStream os = null;
         try {
             os = new FileOutputStream(file);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Worker.java b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Worker.java
new file mode 100644
index 000000000..b4f032b29
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/java/org/ucx/jucx/Worker.java
@@ -0,0 +1,132 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+package org.ucx.jucx;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+
+/**
+ * Worker is the object representing a local communication resource such as a
+ * network interface or host channel adapter port.
+ */
+@SuppressWarnings("unused")
+public class Worker implements Closeable {
+    public static final int MAX_QUEUED_EVENTS = (1 << 20);
+
+    private long            nativeId;
+    private CompletionQueue compQueue;
+    private byte[]          workerAddress;
+    private Callback        callback;
+    private int             maxEvents;
+    private boolean         closed;
+
+    /**
+     * Creates a new Worker.
+     *
+     * @param cb
+     *            Implementation of Worker.Callback interface
+     *
+     * @param maxEvents
+     *            Number of maximum queued completed events
+     *
+     * @throws IllegalArgumentException
+     *             In case cb == null or maxEvents <= 0 or
+     *             maxEvents > MAX_QUEUED_EVENTS
+     *
+     * @throws UnsatisfiedLinkError
+     *             In case an error while loading native libraries
+     *
+     * @throws IOException
+     *             In case native Worker creation failed
+     */
+    public Worker(Callback cb, int maxEvents) throws IOException {
+        if (cb == null || maxEvents <= 0 || maxEvents > MAX_QUEUED_EVENTS) {
+            throw new IllegalArgumentException();
+        }
+
+        // An error occurred while loading native libraries
+        if (LoadLibrary.errorMessage != null) {
+            throw new UnsatisfiedLinkError(LoadLibrary.errorMessage);
+        }
+
+        this.callback       = cb;
+        this.closed         = false;
+        this.maxEvents      = maxEvents;
+        this.workerAddress  = null;
+        this.compQueue      = new CompletionQueue(); // Shared queue wrapper
+        this.nativeId       = Bridge.createWorker(maxEvents, compQueue, this);
+        if (nativeId == 0) {
+            throw new IOException("Failed to create Worker");
+        }
+
+        // align Java side shared queue operations endianness to be as
+        // allocated in native (C) code, in nativeCreateWorker()
+        this.compQueue.setEndianness();
+    }
+
+    /**
+     * Getter for native pointer as long.
+     *
+     * @return long integer representing native pointer
+     */
+    long getNativeId() {
+        return nativeId;
+    }
+
+    /**
+     * Getter for workerAddress as a byte array.
+     *
+     * @return clone of address (for safety reasons)
+     */
+    public byte[] getWorkerAddress() {
+        return workerAddress.clone();
+    }
+
+    /**
+     * Frees all resources associated with this Worker.</br>
+     * Worker should not be used after calling this method.
+     */
+    @Override
+    public void close() {
+        closed = true;
+        Bridge.releaseWorker(this);
+    }
+
+    /**
+     * Called when object is garbage collected. Frees native allocated
+     * resources.
+     */
+    @Override
+    protected void finalize() throws Throwable {
+        if (!closed) {
+            close();
+        }
+    }
+
+
+    /**
+     * Wrapper object for shared buffer between Java and native code.
+     */
+    class CompletionQueue {
+        ByteBuffer completionBuff = null;
+
+        private void setEndianness() {
+            completionBuff.order(ByteOrder.nativeOrder());
+        }
+    }
+
+
+    /**
+     * The Callback interface must be implemented in-order to create a
+     * Worker.</br>
+     * Worker will invoke the implemented method whenever a request is
+     * completed.
+     */
+    public static interface Callback {
+        // Handlers to implement will be added when data path is added
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/Makefile.am
index 049de80f8..7abe96fa3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/Makefile.am
@@ -7,9 +7,9 @@ lib_LTLIBRARIES = libjucx.la
 libjucx_la_CPPFLAGS = -I$(JDK)/include -I$(JDK)/include/linux \
                       -I$(topdir)/src
 
-libjucx_la_SOURCES = bridge.cc request_util.cc
+libjucx_la_SOURCES = bridge.cc worker.cc request_util.cc context.cc
 
-libjucx_la_CXXFLAGS = -fPIC -DPIC -Werror -std=c++0x
+libjucx_la_CXXFLAGS = -fPIC -DPIC -Werror -std=c++11
 
 libjucx_la_LIBADD = $(topdir)/src/ucs/libucs.la \
                     $(topdir)/src/uct/libuct.la \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.cc b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.cc
index b3049fdaf..be1f84a48 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.cc
@@ -4,44 +4,63 @@
  */
 
 #include "bridge.h"
-#include "request_util.h"
+#include "worker.h"
 
 #include <ucp/api/ucp.h>
-#include "ucs/time/time.h"
 
 #include <cstring>
 #include <iostream>
 
 
-#define ERR_EXIT(_msg, _ret)  do {                    \
+#define ERR_EXIT(_msg, _ret)  do {                   \
                                 print_error(_msg);   \
                                 return _ret;         \
-                            } while(0)
+                              } while(0)
 
+#define ERR_JUMP(_msg, _label)  do {                    \
+                                    print_error(_msg);  \
+                                    goto _label;        \
+                                } while(0)
 
-static JavaVM *cached_jvm;
-static jfieldID field_buff = NULL;
-static jfieldID field_addr = NULL;
+static JavaVM   *cached_jvm             = NULL;
+static jfieldID field_comp_queue        = NULL;
+static jfieldID field_worker_addr_arr   = NULL;
 
-static ucp_context_h cached_ctx = NULL;
+static context cached_ctx;
 
 
 static void print_error(const char* error);
 
-// Create context when JNI first loads
-static ucs_status_t create_context();
 
 extern "C" JNIEXPORT jint JNICALL JNI_OnLoad(JavaVM *jvm, void* reserved) {
     cached_jvm = jvm;
     JNIEnv *env;
 
     if (jvm->GetEnv((void **) &env, JNI_VERSION_1_8)) {
-        ERR_EXIT("JNI version 1.8 or higher required.", JNI_ERR);
+        ERR_EXIT("JNI version 1.8 or higher required", JNI_ERR);
     }
 
-    ucs_status_t status = create_context();
-    if (status != UCS_OK) {
-        ERR_EXIT("Failed to create UCP context.", JNI_ERR);
+    jclass queue_cls_data = env->FindClass("org/ucx/jucx/Worker$CompletionQueue");
+    if (queue_cls_data == NULL) {
+        ERR_EXIT("java org/ucx/jucx/Worker$CompletionQueue class was NOT found",
+                 JNI_ERR);
+    }
+
+    field_comp_queue = env->GetFieldID(queue_cls_data, "completionBuff",
+                                       "Ljava/nio/ByteBuffer;");
+    if (field_comp_queue == NULL) {
+        ERR_EXIT("could not get completionBuff's field id", JNI_ERR);
+    }
+
+    jclass worker_cls_data = env->FindClass("org/ucx/jucx/Worker");
+    if (worker_cls_data == NULL) {
+        ERR_EXIT("java org/ucx/jucx/Worker class was NOT found", JNI_ERR);
+    }
+
+    field_worker_addr_arr = env->GetFieldID(worker_cls_data,
+                                            "workerAddress", "[B");
+    if (field_worker_addr_arr == NULL) {
+        ERR_EXIT("could not get workerAddress' field id", JNI_ERR);
     }
 
     return JNI_VERSION_1_8;
@@ -51,33 +70,77 @@ static void print_error(const char* error_msg) {
     fprintf(stderr, "[ERROR] JUCX - %s: %s\n", __FILE__, error_msg);
 }
 
-static ucs_status_t create_context() {
-    if (cached_ctx) {
-        return UCS_OK;
+JNIEXPORT jlong JNICALL
+Java_org_ucx_jucx_Bridge_createWorkerNative(JNIEnv *env, jclass cls,
+                                            jint max_comp, jobject comp_queue,
+                                            jobject jworker) {
+    ucp_worker_params_t worker_params = { 0 };
+    worker* worker_ptr = NULL;
+    ucs_status_t status;
+    jobject jbyte_buff;
+    uint32_t cap = (uint32_t) max_comp;
+    ucp_address_t* local_addr;
+    size_t local_addr_len;
+    jbyteArray jaddr_arr;
+    jlong* addr_ptr;
+    jbyte* local_addr_wrap;
+
+    worker_params.field_mask    = UCP_WORKER_PARAM_FIELD_THREAD_MODE;
+    worker_params.thread_mode   = UCS_THREAD_MODE_SINGLE;
+
+    try {
+        worker_ptr = new worker(&cached_ctx, cap, worker_params);
+    } catch (const std::bad_alloc& ex) {
+        ERR_JUMP("Failed to initialize ucp native worker", err);
     }
 
-    ucp_params_t ucp_params = { 0 };
-    ucp_config_t *config;
-    ucs_status_t status;
-    ucp_context_h ucp_context;
+    status = worker_ptr->extract_worker_address(&local_addr, local_addr_len);
+    if (!local_addr) {
+        ERR_JUMP("Failed to get ucp worker native address", err_worker);
+    }
+
+    local_addr_wrap = new jbyte[local_addr_len];
+    if (!local_addr_wrap) {
+        ERR_JUMP("Allocation failure", err_local_addr);
+    }
+    memcpy(local_addr_wrap, local_addr, local_addr_len);
 
-    status = ucp_config_read(NULL, NULL, &config);
-    if (status != UCS_OK) {
-        return status;
+    jaddr_arr = env->NewByteArray(local_addr_len);
+    if (!jaddr_arr) {
+        ERR_JUMP("Failed to create Java byte[] object", err_worker_addr);
     }
 
-    ucp_params.features     = UCP_FEATURE_TAG;
-    ucp_params.field_mask   = UCP_PARAM_FIELD_FEATURES      |
-                              UCP_PARAM_FIELD_REQUEST_INIT  |
-                              UCP_PARAM_FIELD_REQUEST_SIZE;
+    env->SetByteArrayRegion(jaddr_arr, 0, local_addr_len, local_addr_wrap);
+    delete[] local_addr_wrap;
+    worker_ptr->release_worker_address(local_addr);
 
-    ucp_params.request_size = sizeof(jucx_request);
-    ucp_params.request_init = request_util::request_handler::request_init;
+    // Set the Java workerAddress field
+    env->SetObjectField(jworker, field_worker_addr_arr, jaddr_arr);
 
-    status = ucp_init(&ucp_params, config, &ucp_context);
-    ucp_config_release(config);
+    jbyte_buff = env->NewDirectByteBuffer(worker_ptr->get_event_queue(), cap);
+    if (!jbyte_buff) {
+        env->ExceptionClear();
+        ERR_JUMP("Failed to create Java ByteBuffer object", err_worker);
+    }
 
-    cached_ctx = ucp_context;
+    // Set the completion queue field
+    env->SetObjectField(comp_queue, field_comp_queue, jbyte_buff);
+
+    return (native_ptr) worker_ptr;
+
+err_worker_addr:
+    delete[] local_addr_wrap;
+err_local_addr:
+    worker_ptr->release_worker_address(local_addr);
+err_worker:
+    delete worker_ptr;
+err:
+    return 0;
+}
 
-    return status;
+JNIEXPORT void JNICALL
+Java_org_ucx_jucx_Bridge_releaseWorkerNative(JNIEnv *env, jclass cls,
+                                             jlong worker_id) {
+    worker* worker_ptr = (worker*) worker_id;
+    delete worker_ptr;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.h b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.h
index ac3fd2f39..7ac8d5304 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/bridge.h
@@ -11,7 +11,23 @@
 typedef uintptr_t native_ptr;
 
 extern "C" {
-    // JNI methods proto-types goes here. Will be added later
+
+/*
+ * Class:     org_ucx_jucx_Bridge
+ * Method:    createWorkerNative
+ * Signature: (ILorg/ucx/jucx/Worker/CompletionQueue;Lorg/ucx/jucx/Worker;)J
+ */
+JNIEXPORT jlong JNICALL Java_org_ucx_jucx_Bridge_createWorkerNative
+  (JNIEnv *, jclass, jint, jobject, jobject);
+
+
+/*
+ * Class:     org_ucx_jucx_Bridge
+ * Method:    releaseWorkerNative
+ * Signature: (J)V
+ */
+JNIEXPORT void JNICALL Java_org_ucx_jucx_Bridge_releaseWorkerNative
+  (JNIEnv *, jclass, jlong);
 }
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.cc b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.cc
new file mode 100644
index 000000000..4f2323c03
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.cc
@@ -0,0 +1,72 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+#include "context.h"
+#include "request_util.h"
+
+ucs_status_t context::ref_context() {
+    ucs_status_t status = UCS_OK;
+    {   // Lock before checking context and updating reference counter
+        std::lock_guard<std::mutex> lk(ref_lock);
+        if (ucp_context == nullptr) {
+            status = create_context();
+        }
+
+        if (status == UCS_OK) {
+            ++ref_count;
+        }
+    }   // Unlock
+
+    return status;
+}
+
+void context::deref_context() {
+    std::lock_guard<std::mutex> lk(ref_lock);
+    if (--ref_count == 0) { // All workers released
+        release_context();
+    }
+}
+
+context::~context() {
+    if (ucp_context) {
+        ucp_cleanup(ucp_context);
+    }
+}
+
+ucp_context_h context::get_ucp_context() {
+    return ucp_context;
+}
+
+ucs_status_t context::create_context() {
+    ucp_params_t ucp_params = { 0 };
+    ucp_config_t *config;
+    ucs_status_t status;
+
+    status = ucp_config_read(nullptr, nullptr, &config);
+    if (status != UCS_OK) {
+        return status;
+    }
+
+    uint64_t features   =   UCP_FEATURE_TAG;
+    uint64_t field_mask =   UCP_PARAM_FIELD_FEATURES        |
+                            UCP_PARAM_FIELD_REQUEST_INIT    |
+                            UCP_PARAM_FIELD_REQUEST_SIZE;
+
+    ucp_params.features     = features;
+    ucp_params.field_mask   = field_mask;
+    ucp_params.request_size = sizeof(jucx_request);
+    ucp_params.request_init = request_util::request_handler::request_init;
+
+    status = ucp_init(&ucp_params, config, &ucp_context);
+    ucp_config_release(config);
+
+    return status;
+}
+
+void context::release_context() {
+    if (ucp_context) {
+        ucp_cleanup(ucp_context);
+    }
+    ucp_context = nullptr;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.h b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.h
new file mode 100644
index 000000000..4011b702f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/context.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+#ifndef CONTEXT_H_
+#define CONTEXT_H_
+
+#include <ucp/api/ucp.h>
+
+#include <mutex>
+
+
+/**
+ * Context wrapper - allocates and releases ucp context
+ */
+class context {
+public:
+    context() : ucp_context(nullptr), ref_count(0) {}
+
+    ucs_status_t ref_context();
+
+    void deref_context();
+
+    ~context();
+
+    ucp_context_h get_ucp_context();
+
+private:
+    ucp_context_h   ucp_context;
+    size_t          ref_count;
+    std::mutex      ref_lock;
+
+    ucs_status_t create_context();
+
+    void release_context();
+};
+
+
+#endif /* CONTEXT_H_ */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.cc b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.cc
index aa62f1a95..03d78a878 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.cc
@@ -3,9 +3,10 @@
  * See file LICENSE for terms.
  */
 #include "request_util.h"
+#include "worker.h"
 
 void request_util::request_handler::request_init(void *request) {
     jucx_request* req   = (jucx_request*) request;
 
-    // Currently no initialization needed
+    req->request_worker = NULL;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.h b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.h
index 3bd1888bb..76e9fe17f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/request_util.h
@@ -10,14 +10,16 @@
 
 #include <cstdint>
 
+// forward declaration
+class worker;
+
 /**
  * class to hold the request struct and request handlers
  */
 class request_util {
 public:
     struct request_t {
-        // User defined request
-        // Data members will be added later
+        worker*     request_worker;
     };
 
     class request_handler {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.cc b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.cc
new file mode 100644
index 000000000..0f63b2342
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.cc
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+#include "worker.h"
+
+#include <new> // bad_alloc exception
+
+worker::worker(context* ctx, uint32_t cap, ucp_worker_params_t params) :
+               jucx_context(ctx),  ucp_worker(nullptr),
+               queue_size(cap),    event_queue(nullptr) {
+    ucs_status_t status = jucx_context->ref_context();
+    if (status != UCS_OK) {
+        throw std::bad_alloc{};
+    }
+
+    status = ucp_worker_create(jucx_context->get_ucp_context(),
+                               &params, &ucp_worker);
+    if (status != UCS_OK) {
+        jucx_context->deref_context();
+        throw std::bad_alloc{};
+    }
+    event_queue = new char[queue_size];
+}
+
+worker::~worker() {
+    delete[] event_queue;
+    ucp_worker_destroy(ucp_worker);
+    jucx_context->deref_context();
+}
+
+ucs_status_t worker::extract_worker_address(ucp_address_t** worker_address,
+                                            size_t& address_length) {
+    return ucp_worker_get_address(ucp_worker, worker_address, &address_length);
+}
+
+void worker::release_worker_address(ucp_address_t* worker_address) {
+    ucp_worker_release_address(ucp_worker, worker_address);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.h b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.h
new file mode 100644
index 000000000..d941c08ab
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/main/native/worker.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+#ifndef SRC_WORKER_H_
+#define SRC_WORKER_H_
+
+#include "context.h"
+
+#include <ucp/api/ucp.h>
+
+#include <cstddef>
+
+class worker {
+public:
+    worker(context* ctx, uint32_t cap, ucp_worker_params_t params);
+
+    ~worker();
+
+    ucs_status_t extract_worker_address(ucp_address_t** worker_address,
+                                        size_t& address_length);
+
+    void release_worker_address(ucp_address_t* worker_address);
+
+    char *get_event_queue() const {
+        return event_queue;
+    }
+
+    ucp_worker_h get_ucp_worker() const {
+        return ucp_worker;
+    }
+
+private:
+    context*        jucx_context;
+    ucp_worker_h    ucp_worker;
+    uint32_t        queue_size;
+    char*           event_queue;
+};
+
+#endif /* SRC_WORKER_H_ */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/test/java/org/ucx/jucx/WorkerTest.java b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/test/java/org/ucx/jucx/WorkerTest.java
new file mode 100644
index 000000000..1c909e9d5
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/bindings/java/src/test/java/org/ucx/jucx/WorkerTest.java
@@ -0,0 +1,97 @@
+package org.ucx.jucx;
+
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.junit.Before;
+import org.junit.Test;
+import org.ucx.jucx.Worker.Callback;
+
+/**
+ * Worker unit tests class
+ */
+public class WorkerTest {
+    private Callback cb;
+
+    @Before
+    public void initCallback() {
+        cb = new Callback() {};
+    }
+
+    @Test
+    public void testMultipleWorkersInitialization() {
+        int numOfWorkers = 10;
+        int maxEvents = 128;
+        Worker[] workers = new Worker[numOfWorkers];
+
+        for (int i = 0; i < numOfWorkers; i++) {
+            try {
+                workers[i] = new Worker(cb, maxEvents);
+            } catch (IOException e) {
+                fail(e.getMessage());
+            }
+        }
+
+        for (int i = 0; i < workers.length; i++) {
+            workers[i].close();
+        }
+    }
+
+    @Test
+    public void testWorkerFieldsAndGetters() {
+        int maxEvents = 128;
+        Worker worker = null;
+        long nativeId = -1;
+        byte[] workerAddress = null;
+
+        try {
+            worker = new Worker(cb, maxEvents);
+            nativeId = worker.getNativeId();
+            workerAddress = worker.getWorkerAddress();
+        } catch (IOException e) {
+            fail(e.getMessage());
+        }
+
+        assertTrue("Worker fields initialization failed",
+                   nativeId > 0 &&
+                   workerAddress != null &&
+                   workerAddress.length > 0);
+
+        worker.close();
+    }
+
+    @SuppressWarnings({ "resource", "unused" })
+    @Test(expected = IllegalArgumentException.class)
+    public void testWorkerCbNullInitialization() {
+        // Random legal event queue size
+        int maxEvents = new Random().nextInt(Worker.MAX_QUEUED_EVENTS) + 1;
+        try {
+            Worker worker = new Worker(null, maxEvents);
+        } catch (IOException e) {
+            fail(e.getMessage());
+        }
+    }
+
+    @SuppressWarnings({ "resource", "unused" })
+    @Test(expected = IllegalArgumentException.class)
+    public void testWorkerQueueSizeNonPositiveInitialization() {
+        try {
+            Worker worker = new Worker(cb, 0);
+        } catch (IOException e) {
+            fail(e.getMessage());
+        }
+    }
+
+    @SuppressWarnings({ "resource", "unused" })
+    @Test(expected = IllegalArgumentException.class)
+    public void testWorkerQueueSizeExceedingInitialization() {
+        try {
+            Worker worker = new Worker(cb, Worker.MAX_QUEUED_EVENTS + 1);
+        } catch (IOException e) {
+            fail(e.getMessage());
+        }
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/compiler.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/compiler.m4
index e5ce14ed2..9982feadf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/compiler.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/compiler.m4
@@ -1,6 +1,7 @@
 #
 # Copyright (C) Mellanox Technologies Ltd. 2001-2014.  ALL RIGHTS RESERVED.
 # Copyright (c) UT-Battelle, LLC. 2017. ALL RIGHTS RESERVED.
+# Copyright (C) ARM Ltd. 2016-2018.  ALL RIGHTS RESERVED.
 # See file LICENSE for terms.
 #
 
@@ -84,15 +85,69 @@ AC_DEFUN([COMPILER_OPTION],
     AS_IF([test "x$with_$1" != "xno"],
           [SAVE_CFLAGS="$CFLAGS"
            CFLAGS="$BASE_CFLAGS $CFLAGS $3"
-           AC_MSG_CHECKING([$2])
+           AC_MSG_CHECKING([$3])
            CHECK_CROSS_COMP([AC_LANG_SOURCE([$5])],
                             [AC_MSG_RESULT([yes])
-                             OPT_CFLAGS="$OPT_CFLAGS|$1"],
+			     # TODO: Add CPU UARCH detector and validator in UCX init.
+			     # As for now we will avoid passing this information to
+			     # library.
+			     AS_IF([test "x$1" != "xmcpu" -a "x$1" != "xmarch"],
+                             [OPT_CFLAGS="$OPT_CFLAGS|$1"],[])],
                             [AC_MSG_RESULT([no])])
            CFLAGS="$SAVE_CFLAGS"])
 ])
 
 
+#
+# Check platform uarch and apply micro-architecture specific optimizations
+#
+AC_DEFUN([DETECT_UARCH],
+[
+    cpuimpl=`grep 'CPU implementer' /proc/cpuinfo 2> /dev/null | cut -d: -f2 | tr -d " " | head -n 1`
+    cpuarch=`grep 'CPU architecture' /proc/cpuinfo 2> /dev/null | cut -d: -f2 | tr -d " " | head -n 1`
+    cpuvar=`grep 'CPU variant' /proc/cpuinfo 2> /dev/null | cut -d: -f2 | tr -d " " | head -n 1`
+    cpupart=`grep 'CPU part' /proc/cpuinfo 2> /dev/null | cut -d: -f2 | tr -d " " | head -n 1`
+   
+    ax_cpu=""
+    ax_arch=""
+    
+    AC_MSG_NOTICE(Detected CPU implementation: ${cpuimpl})
+    AC_MSG_NOTICE(Detected CPU arhitecture: ${cpuarch})
+    AC_MSG_NOTICE(Detected CPU variant: ${cpuvar})
+    AC_MSG_NOTICE(Detected CPU part: ${cpupart})
+   
+    case $cpuimpl in
+      0x42) case $cpupart in
+        0x516 | 0x0516)
+          AC_DEFINE([HAVE_AARCH64_THUNDERX2], 1, [Cavium ThunderX2])
+          ax_cpu="thunderx2t99"
+          ax_arch="armv8.1-a+lse" ;;
+        0xaf | 0x0af)
+          AC_DEFINE([HAVE_AARCH64_THUNDERX2], 1, [Cavium ThunderX2])
+          ax_cpu="thunderx2t99"
+          ax_arch="armv8.1-a+lse" ;;
+        esac
+        ;;
+      0x43) case $cpupart in
+        0x516 | 0x0516)
+          AC_DEFINE([HAVE_AARCH64_THUNDERX2], 1, [Cavium ThunderX2])
+          ax_cpu="thunderx2t99"
+          ax_arch="armv8.1-a+lse" ;;
+        0xaf | 0x0af)
+          AC_DEFINE([HAVE_AARCH64_THUNDERX2], 1, [Cavium ThunderX2])
+          ax_cpu="thunderx2t99"
+          ax_arch="armv8.1-a+lse" ;;
+        0xa1 | 0x0a1)
+          AC_DEFINE([HAVE_AARCH64_THUNDERX1], 1, [Cavium ThunderX1])
+          ax_cpu="thunderxt88" ;;
+        esac
+        ;;
+      *) ax_cpu="native"
+         ;;
+    esac 
+])
+
+
 #
 # CHECK_DEPRECATED_DECL_FLAG (flag, variable)
 #
@@ -165,6 +220,25 @@ AS_IF([test "x$with_avx" != xyes],
       ])
 
 
+DETECT_UARCH()
+
+#
+# CPU tuning
+#
+AS_IF([test "x$ax_cpu" != "x"],
+      [COMPILER_OPTION([mcpu], [CPU Model], [-mcpu=$ax_cpu], [$enable_optimizations],
+		 [int main() { return 0;}])
+      ])
+
+# 
+# Architecture tuning
+# 
+AS_IF([test "x$ax_arch" != "x"],
+      [COMPILER_OPTION([march], [architecture tuning], [-march=$ax_arch], [$enable_optimizations],
+		 [int main() { return 0;}])
+      ])
+
+
 #
 # Check for compiler attribute which disables optimizations per-function.
 #
@@ -172,6 +246,25 @@ CHECK_SPECIFIC_ATTRIBUTE([optimize], [NOOPTIMIZE],
                          [int foo (int arg) __attribute__ ((optimize("O0")));])
 
 
+#
+# Check for C++11 support
+#
+AC_MSG_CHECKING([c++11 support])
+AC_LANG_PUSH([C++])
+SAVE_CXXFLAGS="$CXXFLAGS"
+CXX11FLAGS="-std=c++11"
+CXXFLAGS="$CXXFLAGS $CXX11FLAGS"
+AC_COMPILE_IFELSE([AC_LANG_SOURCE([[int main() { return 0; } ]])],
+                  [AC_MSG_RESULT([yes])
+                   AC_SUBST([CXX11FLAGS])
+                   cxx11_happy=yes],
+                  [AC_MSG_RESULT([no])
+                   cxx11_happy=no])
+CXXFLAGS="$SAVE_CXXFLAGS"
+AC_LANG_POP
+AM_CONDITIONAL([HAVE_CXX11], [test "x$cxx11_happy" != xno])
+
+
 #
 # Set C++ optimization/debug flags to be the same as for C
 #
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/cray_ugni.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/cray_ugni.m4
index cbc03d5c3..97b9c832c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/cray_ugni.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/cray_ugni.m4
@@ -8,8 +8,8 @@ cray_ugni_supported=no
 AC_ARG_WITH([ugni],
         [AC_HELP_STRING([--with-ugni(=DIR)],
             [Build Cray UGNI support, adding DIR/include, DIR/lib, and DIR/lib64 to the search path for headers and libraries])],
-        [with_ugni=forced],
-        [with_ugni=yes])
+        [],
+        [with_ugni=default])
 
 AS_IF([test "x$with_ugni" != "xno"], 
         [PKG_CHECK_MODULES([CRAY_UGNI], [cray-ugni cray-pmi], 
@@ -17,7 +17,7 @@ AS_IF([test "x$with_ugni" != "xno"],
                            cray_ugni_supported=yes
                            AC_DEFINE([HAVE_TL_UGNI], [1],
                                  [Define if UGNI transport exists.])],
-                           [AS_IF([test "x$with_ugni" == "xforced"],
+                           [AS_IF([test "x$with_ugni" != "xdefault"],
                                   [AC_MSG_WARN([UGNI support was requested but cray-ugni and cray-pmi packages can't be found])
                                    AC_MSG_ERROR([Cannot continue])],[])]
                            )])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ib.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ib.m4
index 59cc567dd..e46b80c5c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ib.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ib.m4
@@ -56,12 +56,15 @@ AC_ARG_WITH([cm],
 
 
 #
-# mlx5 bare-metal support
+# mlx5 DV support
 #
-AC_ARG_WITH([mlx5-hw],
-            [AC_HELP_STRING([--with-mlx5-hw], [Compile with mlx5 bare-metal support])],
+AC_ARG_WITH([mlx5-dv],
+            [AC_HELP_STRING([--with-mlx5-dv], [Compile with mlx5 Direct Verbs
+                support. Direct Verbs (DV) support provides additional
+                acceleration capabilities that are not available in a
+                regular mode.])],
             [],
-            [with_mlx5_hw=yes])
+            [with_mlx5_dv=yes])
 
 
 #
@@ -73,6 +76,15 @@ AC_ARG_WITH([ib-hw-tm],
             [with_ib_hw_tm=yes])
 
 
+#
+# DM Support
+#
+AC_ARG_WITH([dm],
+            [AC_HELP_STRING([--with-dm], [Compile with Device Memory support])],
+            [],
+            [with_dm=yes])
+
+
 #
 # Check basic IB support: User wanted at least one IB transport, and we found
 # verbs header file and library.
@@ -133,40 +145,100 @@ AS_IF([test "x$with_ib" == xyes],
            verbs_exp=yes],
            [verbs_exp=no])
 
-       AS_IF([test "x$with_mlx5_hw" != xno],
-             [AC_CHECK_HEADERS([infiniband/mlx5_hw.h],
-                               [with_mlx5_hw=yes],
-                               [with_mlx5_hw=no])])
-
-       AC_CHECK_DECLS([ibv_mlx5_exp_get_qp_info,
-                       ibv_mlx5_exp_get_cq_info,
-                       ibv_mlx5_exp_get_srq_info,
-                       ibv_mlx5_exp_update_cq_ci,
-                       MLX5_WQE_CTRL_SOLICITED],
-                      [], [], [[#include <infiniband/mlx5_hw.h>]])
-
-       # Disable mlx5_hw if the driver does not provide BF locking information
-       AS_IF([test "x$ac_cv_have_decl_ibv_mlx5_exp_get_qp_info" == "xyes"],
-             [AC_CHECK_MEMBERS([struct ibv_mlx5_qp_info.bf.need_lock],
+       AC_CHECK_MEMBERS([struct ibv_exp_device_attr.exp_device_cap_flags,
+                         struct ibv_exp_device_attr.odp_caps,
+                         struct ibv_exp_device_attr.odp_caps.per_transport_caps.dc_odp_caps,
+                         struct ibv_exp_device_attr.odp_mr_max_size,
+                         struct ibv_exp_qp_init_attr.max_inl_recv,
+                         struct ibv_async_event.element.dct],
+                        [], [], [[#include <infiniband/verbs_exp.h>]])
+
+       AC_CHECK_DECLS([IBV_CREATE_CQ_ATTR_IGNORE_OVERRUN],
+                      [have_cq_io=yes], [], [[#include <infiniband/verbs.h>]])
+
+       AC_CHECK_DECLS([IBV_EXP_CQ_IGNORE_OVERRUN],
+                      [have_cq_io=yes], [], [[#include <infiniband/verbs_exp.h>]])
+
+       AS_IF([test "x$with_mlx5_dv" != xno], [
+               AC_MSG_NOTICE([Checking for legacy bare-metal support])
+               AC_CHECK_HEADERS([infiniband/mlx5_hw.h],
+                               [with_mlx5_hw=yes
+                                mlx5_include=mlx5_hw.h
+                       AC_CHECK_DECLS([
+                           ibv_mlx5_exp_get_qp_info,
+                           ibv_mlx5_exp_get_cq_info,
+                           ibv_mlx5_exp_get_srq_info,
+                           ibv_mlx5_exp_update_cq_ci,
+                           MLX5_WQE_CTRL_SOLICITED],
+                                  [], [], [[#include <infiniband/mlx5_hw.h>]])
+                       AC_CHECK_MEMBERS([struct mlx5_srq.cmd_qp],
+                                  [], [with_ib_hw_tm=no],
+                                      [[#include <infiniband/mlx5_hw.h>]])
+                       AC_CHECK_MEMBERS([struct mlx5_ah.ibv_ah],
+                                  [has_get_av=yes], [],
+                                      [[#include <infiniband/mlx5_hw.h>]])
+                       AC_CHECK_MEMBERS([struct ibv_mlx5_qp_info.bf.need_lock],
                                [],
                                [AC_MSG_WARN([Cannot use mlx5 QP because it assumes dedicated BF])
                                 AC_MSG_WARN([Please upgrade MellanoxOFED to 3.0 or above])
                                 with_mlx5_hw=no],
-                               [[#include <infiniband/mlx5_hw.h>]])],
-             [])
+                               [[#include <infiniband/mlx5_hw.h>]])
+                       AC_CHECK_DECLS([
+                           IBV_EXP_QP_INIT_ATTR_RES_DOMAIN,
+                           IBV_EXP_RES_DOMAIN_THREAD_MODEL,
+                           ibv_exp_create_res_domain,
+                           ibv_exp_destroy_res_domain],
+                                [AC_DEFINE([HAVE_IBV_EXP_RES_DOMAIN], 1, [IB resource domain])
+                                 has_res_domain=yes], [], [[#include <infiniband/verbs_exp.h>]])
+                               ], [with_mlx5_hw=no])
+
+              AC_MSG_NOTICE([Checking for DV bare-metal support])
+
+              AC_CHECK_LIB([mlx5-rdmav2], [mlx5dv_query_device],
+                                    [AC_SUBST(LIB_MLX5, [-lmlx5-rdmav2])],[
+              AC_CHECK_LIB([mlx5], [mlx5dv_query_device],
+                                    [AC_SUBST(LIB_MLX5, [-lmlx5])],
+                                    [with_mlx5_dv=no], [-libverbs])], [-libverbs])
+
+              AS_IF([test "x$with_mlx5_dv" != xno], [
+                       AC_CHECK_HEADERS([infiniband/mlx5dv.h],
+                               [with_mlx5_hw=yes
+                                with_mlx5_dv=yes
+                                mlx5_include=mlx5dv.h], [], [ ])])
+
+              AS_IF([test "x$with_mlx5_dv" == xyes -a "x$have_cq_io" == xyes ], [
+                       AC_CHECK_DECLS([
+                           mlx5dv_init_obj],
+                                  [], [], [[#include <infiniband/mlx5dv.h>]])
+                       AC_CHECK_MEMBERS([struct mlx5dv_cq.cq_uar],
+                                  [], [], [[#include <infiniband/mlx5dv.h>]])
+                       AC_CHECK_DECLS([MLX5DV_OBJ_AH], [has_get_av=yes],
+                                      [], [[#include <infiniband/mlx5dv.h>]])
+                       AC_CHECK_DECLS([MLX5DV_DCTYPE_DCT],
+                                  [have_dc_dv=yes], [], [[#include <infiniband/mlx5dv.h>]])
+                       AC_CHECK_DECLS([ibv_alloc_td],
+                                  [has_res_domain=yes], [], [[#include <infiniband/verbs.h>]])])
+
+              AC_CHECK_DECLS([ibv_alloc_td],
+                      [has_res_domain=yes], [], [[#include <infiniband/verbs.h>]])])
+
+       AS_IF([test "x$has_res_domain" == xyes -a "x$have_cq_io" == xyes ], [], [
+               with_mlx5_hw=no])
 
        AS_IF([test "x$with_mlx5_hw" == xyes],
              [AC_MSG_NOTICE([Compiling with mlx5 bare-metal support])
-              AC_DEFINE([HAVE_MLX5_HW], 1, [mlx5 bare-metal support])],
-             [])
+              AC_DEFINE([HAVE_MLX5_HW], 1, [mlx5 bare-metal support])
+              AS_IF([test "x$has_get_av" == xyes],
+                 [AC_DEFINE([HAVE_MLX5_HW_UD], 1, [mlx5 UD bare-metal support])
+                  AC_DEFINE([HAVE_MLX5_HW_DC], 1, [mlx5 DC bare-metal support])], [])], [])
 
        AC_CHECK_DECLS([IBV_LINK_LAYER_INFINIBAND,
                        IBV_LINK_LAYER_ETHERNET,
-                       IBV_EVENT_GID_CHANGE],
+                       IBV_EVENT_GID_CHANGE,
+                       ibv_create_qp_ex],
                       [], [], [[#include <infiniband/verbs.h>]])
 
-       AC_CHECK_DECLS([IBV_EXP_CQ_IGNORE_OVERRUN,
-                       IBV_EXP_ACCESS_ALLOCATE_MR,
+       AC_CHECK_DECLS([IBV_EXP_ACCESS_ALLOCATE_MR,
                        IBV_EXP_ACCESS_ON_DEMAND,
                        IBV_EXP_DEVICE_MR_ALLOCATE,
                        IBV_EXP_WR_NOP,
@@ -180,7 +252,8 @@ AS_IF([test "x$with_ib" == xyes],
                        ibv_exp_create_qp,
                        ibv_exp_prefetch_mr,
                        ibv_exp_create_srq,
-                       ibv_exp_setenv],
+                       ibv_exp_setenv,
+                       ibv_exp_query_gid_attr],
                       [], [], [[#include <infiniband/verbs_exp.h>]])
 
        AC_CHECK_DECLS([ibv_exp_post_send,
@@ -192,8 +265,11 @@ AS_IF([test "x$with_ib" == xyes],
                       [have_ext_atomics=no],
                       [[#include <infiniband/verbs_exp.h>]])
 
+       AC_CHECK_DECLS(IBV_EXP_DEVICE_ATTR_RESERVED_2, [], [],
+                      [[#include <infiniband/verbs_exp.h>]])
+
        # UMR support
-       AC_CHECK_DECLS(IBV_EXP_WR_UMR_FILL,
+       AC_CHECK_DECLS(IBV_EXP_MR_INDIRECT_KLMS,
                      [AC_DEFINE([HAVE_EXP_UMR], 1, [IB UMR support])],
                      [],
                      [[#include <infiniband/verbs.h>]])
@@ -208,8 +284,8 @@ AS_IF([test "x$with_ib" == xyes],
                         [],
                         [[#include <infiniband/verbs.h>]])
 
-       AC_CHECK_DECLS(IBV_EXP_MR_INDIRECT_KLMS,
-                     [AC_DEFINE([HAVE_EXP_UMR_NEW_API], 1, [IB UMR new API])],
+       AC_CHECK_DECLS(IBV_EXP_MR_FIXED_BUFFER_SIZE,
+                     [AC_DEFINE([HAVE_EXP_UMR_KSM], 1, [IB UMR KSM support])],
                      [],
                      [[#include <infiniband/verbs.h>]])
 
@@ -226,27 +302,23 @@ AS_IF([test "x$with_ib" == xyes],
        AC_CHECK_DECLS(IBV_EXP_ODP_SUPPORT_IMPLICIT, [], [],
                       [[#include <infiniband/verbs.h>]])
 
-       AC_CHECK_MEMBERS([struct ibv_exp_device_attr.exp_device_cap_flags,
-                         struct ibv_exp_device_attr.odp_caps,
-                         struct ibv_exp_device_attr.odp_caps.per_transport_caps.dc_odp_caps,
-                         struct ibv_exp_device_attr.odp_mr_max_size,
-                         struct ibv_exp_qp_init_attr.max_inl_recv,
-                         struct ibv_async_event.element.dct],
-                        [], [], [[#include <infiniband/verbs_exp.h>]])
-
        AC_CHECK_MEMBERS([struct mlx5_wqe_av.base,
                          struct mlx5_grh_av.rmac],
-                        [], [], [[#include <infiniband/mlx5_hw.h>]])
+                        [], [], [[#include <infiniband/$mlx5_include>]])
 
        AC_DEFINE([HAVE_IB], 1, [IB support])
 
-       AS_IF([test "x$with_dc" != xno],
-           [AC_CHECK_DECLS(IBV_EXP_QPT_DC_INI, [], [with_dc=no], [[#include <infiniband/verbs.h>]])
-           AC_CHECK_MEMBERS([struct ibv_exp_dct_init_attr.inline_size], [] , [with_dc=no], [[#include <infiniband/verbs.h>]])
-           ])
-       AS_IF([test "x$with_dc" != xno],
-           [AC_DEFINE([HAVE_TL_DC], 1, [DC transport support])
-           transports="${transports},dc"])
+       AC_CHECK_DECLS([IBV_EXP_QPT_DC_INI],
+                [have_dc_exp=yes], [], [[#include <infiniband/verbs.h>]])
+
+       AS_IF([test "x$with_dc" != xno -a \( "x$have_dc_exp" = xyes -o "x$have_dc_dv" = xyes \)], [
+           AC_DEFINE([HAVE_TL_DC], 1, [DC transport support])
+           transports="${transports},dc"
+           AS_IF([test -n "$have_dc_dv"],
+                 [AC_DEFINE([HAVE_DC_DV], 1, [DC DV support])], [
+           AS_IF([test -n "$have_dc_exp"],
+                 [AC_DEFINE([HAVE_DC_EXP], 1, [DC EXP support])])])],
+           [with_dc=no])
 
        AS_IF([test "x$with_rc" != xno],
            [AC_DEFINE([HAVE_TL_RC], 1, [RC transport support])
@@ -279,6 +351,21 @@ AS_IF([test "x$with_ib" == xyes],
                              [], [#include <infiniband/verbs_exp.h>])
            ])
 
+       # Device Memory support
+       AS_IF([test "x$with_dm" != xno],
+           [AC_TRY_COMPILE([#include <infiniband/verbs_exp.h>],
+               [
+                   struct ibv_exp_dm ibv_dm;
+                   struct ibv_exp_alloc_dm_attr dm_attr;
+                   void* a1 = ibv_exp_alloc_dm;
+                   void* a2 = ibv_exp_reg_mr;
+                   void* a3 = ibv_dereg_mr;
+                   void* a4 = ibv_exp_free_dm;
+               ],
+               [AC_DEFINE([HAVE_IBV_EXP_DM], 1, [Device Memory support])],
+               [])
+           ])
+
        AC_CHECK_DECLS([ibv_cmd_modify_qp],
                       [], [], [[#include <infiniband/driver.h>]])
 
@@ -297,6 +384,7 @@ AS_IF([test "x$with_ib" == xyes],
         with_rc=no
         with_ud=no
         with_mlx5_hw=no
+        with_mlx5_dv=no
         with_ib_hw_tm=no
     ])
 
@@ -307,7 +395,12 @@ AS_IF([test "x$with_ib" == xyes],
 AM_CONDITIONAL([HAVE_IB],      [test "x$with_ib" != xno])
 AM_CONDITIONAL([HAVE_TL_RC],   [test "x$with_rc" != xno])
 AM_CONDITIONAL([HAVE_TL_DC],   [test "x$with_dc" != xno])
+AM_CONDITIONAL([HAVE_DC_DV],   [test -n "$have_dc_dv"])
+AM_CONDITIONAL([HAVE_DC_EXP],  [test -n "$have_dc_exp"])
 AM_CONDITIONAL([HAVE_TL_UD],   [test "x$with_ud" != xno])
 AM_CONDITIONAL([HAVE_TL_CM],   [test "x$with_cm" != xno])
 AM_CONDITIONAL([HAVE_MLX5_HW], [test "x$with_mlx5_hw" != xno])
+AM_CONDITIONAL([HAVE_MLX5_DV], [test "x$with_mlx5_dv" != xno])
+AM_CONDITIONAL([HAVE_MLX5_HW_UD], [test "x$with_mlx5_hw" != xno -a "x$has_get_av" != xno])
+AM_CONDITIONAL([HAVE_MLX5_HW_DC], [test "x$with_mlx5_hw" != xno -a "x$has_get_av" != xno -a "x$have_dc" != xno])
 AM_CONDITIONAL([HAVE_IBV_EX_HW_TM], [test "x$with_ib_hw_tm"  != xno])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/java.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/java.m4
index 121716999..dc7c20674 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/java.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/java.m4
@@ -9,6 +9,7 @@
 #
 #
 java_happy="no"
+mvn_args="-Dsources=\"**/dummy/**\" -DtestSources=\"**/jucx/**\" -Dmaven.test.skip=true -DskipCopy=true"
 AC_ARG_WITH([java],
             [AC_HELP_STRING([--with-java=(PATH)],
                             [Compile Java UCX (default is guess).])
@@ -21,8 +22,8 @@ AS_IF([test "x$with_java" != xno],
        AS_IF([test "x${MVNBIN}" == "xyes" -a "x${JAVABIN}" == "xyes"],
              [
               AC_MSG_CHECKING([mvn plugins and dependencies availability])
-              AC_SUBST([MVNAVAIL], [$(cd bindings/java && mvn -Dsources="**/dummy/**" -DtestSources="**/jucx/**" -DskipCopy="true" -q install >/dev/null && \
-                                                          mvn -Dsources="**/dummy/**" -DtestSources="**/jucx/**" -DskipCopy="true" -q clean   >/dev/null && \
+              AC_SUBST([MVNAVAIL], [$(cd bindings/java && mvn $(echo "${mvn_args}") install >/dev/null && \
+                                                          mvn $(echo "${mvn_args}") clean   >/dev/null && \
                                                           echo yes || echo no)])
               AC_MSG_RESULT([${MVNAVAIL}])
               AS_IF([test -n "$with_java" -a "x$with_java" != "xyes" -a "x$with_java" != "xguess"],
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/knem.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/knem.m4
index c748cf151..0cb894bd7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/knem.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/knem.m4
@@ -7,22 +7,32 @@ AC_ARG_WITH([knem],
            [], [with_knem=guess])
 
 AS_IF([test "x$with_knem" != xno],
-      [AS_IF([test ! -d $with_knem], 
-             [
-              AC_MSG_NOTICE([KNEM path was not found, guessing ...])
-              with_knem=$(${PKG_CONFIG} --variable=prefix knem || find /opt/knem* -name knem_io.h |xargs dirname |sed -e s,/include,,g)
-              ],
-              [:])
-       AC_CHECK_HEADER([$with_knem/include/knem_io.h],
-                       [BASE_CFLAGS="$BASE_CFLAGS -I$with_knem/include"
-                        BASE_CPPFLAGS="$BASE_CPPFLAGS -I$with_knem/include"
-                        AC_DEFINE([HAVE_KNEM], [1], [Enable the use of KNEM])
-                        transports="${transports},knem"
-                        knem_happy="yes"],
-                       [AC_MSG_WARN([KNEM requested but not found])
-                        AC_DEFINE([HAVE_KNEM], [0], [Disable the use of KNEM])])],
-      [AC_MSG_WARN([KNEM was explicitly disabled])
-       AC_DEFINE([HAVE_KNEM], [0], [Disable the use of KNEM])]
+    [AS_IF([test "x$with_knem" == xguess -o "x$with_knem" == xyes -o "x$with_knem" == x],
+           [AC_MSG_NOTICE([KNEM path was not found, guessing ...])
+            ucx_check_knem_include_dir=$(pkg-config --cflags knem)],
+           [ucx_check_knem_include_dir=-I$with_knem/include])
+
+     save_CPPFLAGS="$CPPFLAGS"
+
+     CPPFLAGS="$ucx_check_knem_include_dir $CPPFLAGS"
+
+     AC_CHECK_DECL([KNEM_CMD_GET_INFO],
+                   [BASE_CFLAGS="$BASE_CFLAGS $ucx_check_knem_include_dir"
+                    BASE_CPPFLAGS="$BASE_CPPFLAGS $ucx_check_knem_include_dir"
+                    AC_DEFINE([HAVE_KNEM], [1], [Enable the use of KNEM])
+                    transports="${transports},knem"
+                    knem_happy="yes"],
+                   [AS_IF([test "x$with_knem" != xguess],
+                          [AC_MSG_ERROR([KNEM requested but required file (knem_io.h) could not be found])
+                           AC_DEFINE([HAVE_KNEM], [0], [Disable the use of KNEM])],
+                          [AC_MSG_WARN([KNEM requested but required file (knem_io.h) could not be found])])], 
+                   [[#include <knem_io.h>]])
+
+     CPPFLAGS="$save_CPPFLAGS"
+
+    ],
+    [AC_MSG_WARN([KNEM was explicitly disabled])
+     AC_DEFINE([HAVE_KNEM], [0], [Disable the use of KNEM])]
 )
 
 AM_CONDITIONAL([HAVE_KNEM], [test "x$knem_happy" != xno])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/rocm.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/rocm.m4
index 6aa9f8bf5..ee4e93c85 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/rocm.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/rocm.m4
@@ -1,47 +1,93 @@
 #
-# Copyright (C) Advanced Micro Devices, Inc. 2016 - 2017. ALL RIGHTS RESERVED.
+# Copyright (C) Advanced Micro Devices, Inc. 2016 - 2018. ALL RIGHTS RESERVED.
 # See file LICENSE for terms.
 #
 
+# ROCM_PARSE_FLAGS(ARG, VAR_LIBS, VAR_LDFLAGS, VAR_CPPFLAGS)
+# ----------------------------------------------------------
+# Parse whitespace-separated ARG into appropriate LIBS, LDFLAGS, and
+# CPPFLAGS variables.
+AC_DEFUN([ROCM_PARSE_FLAGS],
+[for arg in $$1 ; do
+    AS_CASE([$arg],
+        [yes],               [],
+        [no],                [],
+        [-l*|*.a|*.so],      [$2="$$2 $arg"],
+        [-L*|-WL*|-Wl*],     [$3="$$3 $arg"],
+        [-I*],               [$4="$$4 $arg"],
+        [*lib|*lib/|*lib64|*lib64/],[AS_IF([test -d $arg], [$3="$$3 -L$arg"],
+                                 [AC_MSG_WARN([$arg of $1 not parsed])])],
+        [*include|*include/],[AS_IF([test -d $arg], [$4="$$4 -I$arg"],
+                                 [AC_MSG_WARN([$arg of $1 not parsed])])],
+        [AC_MSG_WARN([$arg of $1 not parsed])])
+done])
+
 #
 # Check for ROCm  support
 #
-rocm_happy="no"
-
 AC_ARG_WITH([rocm],
-           [AS_HELP_STRING([--with-rocm=(DIR)], [Enable the use of ROCm (default is autodetect).])],
-           [], [with_rocm=guess])
+    [AS_HELP_STRING([--with-rocm=(DIR)],
+        [Enable the use of ROCm (default is autodetect).])],
+    [],
+    [with_rocm=guess])
 
+rocm_happy=no
 AS_IF([test "x$with_rocm" != "xno"],
-
-      [AS_IF([test "x$with_rocm" == "x" || test "x$with_rocm" == "xguess" || test "x$with_rocm" == "xyes"],
-             [
-              AC_MSG_NOTICE([ROCm path was not specified. Guessing ...])
-              with_rocm=/opt/rocm
-              ],
-              [:])
-        AC_CHECK_HEADERS([$with_rocm/include/hsa/hsa_ext_amd.h],
-                       [AC_CHECK_DECLS([hsaKmtProcessVMRead,hsaKmtProcessVMWrite],
-                           [rocm_happy="yes"],
-                           [AC_MSG_WARN([ROCm without CMA support was detected. Disable.])
-                            rocm_happy="no"],
-                            [#include <$with_rocm/include/libhsakmt/hsakmt.h>])
-                           AS_IF([test "x$rocm_happy" == "xyes"],
-                            [AC_DEFINE([HAVE_ROCM], 1, [Enable ROCm support])
-                             transports="${transports},rocm"
-                            AC_SUBST(ROCM_CPPFLAGS, "-I$with_rocm/include/hsa -I$with_rocm/include/libhsakmt -DHAVE_ROCM=1")
-                            AC_SUBST(ROCM_CFLAGS, "-I$with_rocm/include/hsa -I$with_rocm/include/libhsakmt -DHAVE_ROCM=1")
-                            AC_SUBST(ROCM_LDFLAGS, "-lhsa-runtime64 -L$with_rocm/lib")
-                            CFLAGS="$CFLAGS $ROCM_CFLAGS"
-                            CPPFLAGS="$CPPFLAGS $ROCM_CPPFLAGS"
-                            LDFLAGS="$LDFLAGS $ROCM_LDFLAGS"],
-                        [])],
-                       [AC_MSG_WARN([ROCm not found])
-                        AC_DEFINE([HAVE_ROCM], [0], [Disable the use of ROCm])])],
-      [AC_MSG_WARN([ROCm was explicitly disabled])
-      AC_DEFINE([HAVE_ROCM], [0], [Disable the use of ROCm])]
+    [AS_CASE(["x$with_rocm"],
+        [x|xguess|xyes],
+            [AC_MSG_NOTICE([ROCm path was not specified. Guessing ...])
+             with_rocm=/opt/rocm
+             ROCM_CPPFLAGS="-I$with_rocm/libhsakmt/include/libhsakmt -I$with_rocm/include/hsa -I$with_rocm/include"
+             ROCM_LDFLAGS="-L$with_rocm/hsa/lib -L$with_rocm/lib"
+             ROCM_LIBS="-lhsa-runtime64"],
+        [x/*],
+            [AC_MSG_NOTICE([ROCm path given as $with_rocm ...])
+             ROCM_CPPFLAGS="-I$with_rocm/libhsakmt/include/libhsakmt -I$with_rocm/include/hsa -I$with_rocm/include"
+             ROCM_LDFLAGS="-L$with_rocm/hsa/lib -L$with_rocm/lib"
+             ROCM_LIBS="-lhsa-runtime64"],
+        [AC_MSG_NOTICE([ROCm flags given ...])
+         ROCM_PARSE_FLAGS([with_rocm],
+                          [ROCM_LIBS], [ROCM_LDFLAGS], [ROCM_CPPFLAGS])])
+    SAVE_CPPFLAGS="$CPPFLAGS"
+    SAVE_LDFLAGS="$LDFLAGS"
+    SAVE_LIBS="$LIBS"
+    CPPFLAGS="$ROCM_CPPFLAGS $CPPFLAGS"
+    LDFLAGS="$ROCM_LDFLAGS $LDFLAGS"
+    LIBS="$ROCM_LIBS $LIBS"
+    rocm_happy=yes
+    AS_IF([test "x$rocm_happy" = xyes],
+          [AC_CHECK_HEADERS([hsa.h], [rocm_happy=yes], [rocm_happy=no])])
+    AS_IF([test "x$rocm_happy" = xyes],
+          [AC_CHECK_HEADERS([hsa_ext_amd.h], [rocm_happy=yes], [rocm_happy=no])])
+    AS_IF([test "x$rocm_happy" = xyes],
+          [AC_CHECK_HEADERS([hsakmt.h], [rocm_happy=yes], [rocm_happy=no])])
+    AS_IF([test "x$rocm_happy" = xyes],
+          [AC_CHECK_DECLS([hsaKmtProcessVMRead,hsaKmtProcessVMWrite],
+              [rocm_happy=yes],
+              [rocm_happy=no
+               AC_MSG_WARN([ROCm without CMA support was detected. Disable.])],
+              [#include <hsakmt.h>])])
+    AS_IF([test "x$rocm_happy" = xyes],
+          [AC_SEARCH_LIBS([hsa_init], [hsa-runtime64])
+           AS_CASE(["x$ac_cv_search_hsa_init"],
+               [xnone*], [],
+               [xno], [rocm_happy=no],
+               [x-l*], [ROCM_LIBS="$ac_cv_search_hsa_init $ROCM_LIBS"])])
+    AS_IF([test "x$rocm_happy" == "xyes"],
+          [AC_DEFINE([HAVE_ROCM], [1], [Set to 1 to enable ROCm support])
+           transports="${transports},rocm"
+           AC_SUBST([ROCM_CPPFLAGS])
+           AC_SUBST([ROCM_LDFLAGS])
+           AC_SUBST([ROCM_LIBS])],
+          [AC_DEFINE([HAVE_ROCM], [0], [Set to 1 to enable ROCm support])
+           AC_MSG_WARN([ROCm not found])])
+    CPPFLAGS="$SAVE_CPPFLAGS"
+    LDFLAGS="$SAVE_LDFLAGS"
+    LIBS="$SAVE_LIBS"
+    ],
+    [AC_DEFINE([HAVE_ROCM], [0], [Set to 1 to enable ROCm support])
+     AC_MSG_WARN([ROCm was explicitly disabled])]
 )
 
-
 AM_CONDITIONAL([HAVE_ROCM], [test "x$rocm_happy" != xno])
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/sysdep.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/sysdep.m4
index aa5d5fec5..03ea5ff0f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/sysdep.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/sysdep.m4
@@ -131,3 +131,14 @@ CHECK_CROSS_COMP([AC_LANG_SOURCE([#include <malloc.h>
                   AC_MSG_WARN([malloc hooks are not supported])]
                 )
 CFLAGS=$SAVE_CFLAGS
+
+
+#
+# Check for capability.h header (usually comes from libcap-devel package) and
+# make sure it defines the types we need
+#
+AC_CHECK_HEADERS([sys/capability.h],
+                 [AC_CHECK_TYPES([cap_user_header_t, cap_user_data_t], [],
+                                 [AC_DEFINE([HAVE_SYS_CAPABILITY_H], [0], [Linux capability API support])],
+                                 [[#include <sys/capability.h>]])]
+                 )
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucm.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucm.m4
index fd3750d56..541bdb132 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucm.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucm.m4
@@ -47,3 +47,63 @@ AC_CHECK_FUNCS([malloc_get_state malloc_set_state],
                [],
                [#include <stdlib.h>])
 
+
+#
+# Madvise flags
+#
+AC_CHECK_DECLS([MADV_FREE,
+                MADV_REMOVE,
+                POSIX_MADV_DONTNEED],
+               [],
+               [],
+               [#include <sys/mman.h>])
+
+
+# BISTRO hooks infrastructure
+#
+# SYS_xxx macro
+#
+mmap_hooks_happy=yes
+AC_CHECK_DECLS([SYS_mmap,
+                SYS_munmap,
+                SYS_mremap,
+                SYS_brk,
+                SYS_madvise],
+               [],
+               [mmap_hooks_happy=no], dnl mmap syscalls are not defined
+               [#include <sys/syscall.h>])
+
+shm_hooks_happy=yes
+AC_CHECK_DECLS([SYS_shmat,
+                SYS_shmdt],
+               [],
+               [shm_hooks_happy=no],
+               [#include <sys/syscall.h>])
+
+ipc_hooks_happy=yes
+AC_CHECK_DECLS([SYS_ipc],
+               [],
+               [ipc_hooks_happy=no],
+               [#include <sys/syscall.h>])
+
+AS_IF([test "x$mmap_hooks_happy" == "xyes"],
+      AS_IF([test "x$ipc_hooks_happy" == "xyes" -o "x$shm_hooks_happy" == "xyes"],
+            [bistro_hooks_happy=yes]))
+
+AS_IF([test "x$bistro_hooks_happy" == "xyes"],
+      [AC_DEFINE([UCM_BISTRO_HOOKS], [1], [Enable BISTRO hooks])],
+      [AC_DEFINE([UCM_BISTRO_HOOKS], [0], [Enable BISTRO hooks])
+       AC_MSG_WARN([Some of required syscalls could not be found])
+       AC_MSG_WARN([BISTRO mmap hook mode is disabled])])
+
+AC_CHECK_FUNCS([__curbrk], [], [], [])
+
+#
+# tcmalloc library - for testing only
+#
+SAVE_LDFLAGS="$LDFLAGS"
+AC_CHECK_LIB([tcmalloc], [tc_malloc],
+             [have_tcmalloc=yes
+              TCMALLOC_LIB="-ltcmalloc"],
+             [have_tcmalloc=no])
+AM_CONDITIONAL([HAVE_TCMALLOC],[test "x$have_tcmalloc" = "xyes"])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucs.m4 b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucs.m4
index 450ec16fc..13d842b80 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucs.m4
+++ b/src/mpid/ch4/netmod/ucx/ucx/config/m4/ucs.m4
@@ -207,3 +207,6 @@ case ${host} in
     # HW timer is supported for all other architectures
     AC_DEFINE([HAVE_HW_TIMER], [1], [high-resolution hardware timer disabled])
 esac
+
+AC_CHECK_FUNCS([__clear_cache], [], [])
+AC_CHECK_FUNCS([__aarch64_sync_cache_range], [], [])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/configure.ac b/src/mpid/ch4/netmod/ucx/ucx/configure.ac
index 6c60ecfe1..ba01f7ef7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/configure.ac
+++ b/src/mpid/ch4/netmod/ucx/ucx/configure.ac
@@ -9,7 +9,7 @@
 AC_PREREQ([2.63])
 
 define([ucx_ver_major], 1)
-define([ucx_ver_minor], 3)
+define([ucx_ver_minor], 5)
 define([ucx_ver_patch], 0)
 define([ts], esyscmd([sh -c "date +%Y%m%d%H%M%S"]))
 
@@ -74,6 +74,7 @@ AC_PROG_LIBTOOL
 AC_HEADER_STDC
 LT_LIB_M
 AC_C_RESTRICT
+AC_FUNC_STRERROR_R
 
 m4_include([config/m4/pkg.m4])
 PKG_PROG_PKG_CONFIG
@@ -117,8 +118,13 @@ AS_IF([test "x$with_docs_only" == xyes],
      AM_CONDITIONAL([HAVE_MEMTRACK], [false])
      AM_CONDITIONAL([HAVE_IB], [false])
      AM_CONDITIONAL([HAVE_MLX5_HW], [false])
+     AM_CONDITIONAL([HAVE_MLX5_HW_UD], [false])
+     AM_CONDITIONAL([HAVE_MLX5_HW_DC], [false])
+     AM_CONDITIONAL([HAVE_MLX5_DV], [false])
      AM_CONDITIONAL([HAVE_TL_RC], [false])
      AM_CONDITIONAL([HAVE_TL_DC], [false])
+     AM_CONDITIONAL([HAVE_DC_DV], [false])
+     AM_CONDITIONAL([HAVE_DC_EXP], [false])
      AM_CONDITIONAL([HAVE_TL_UD], [false])
      AM_CONDITIONAL([HAVE_TL_CM], [false])
      AM_CONDITIONAL([HAVE_IBV_EX_HW_TM], [false])
@@ -136,6 +142,9 @@ AS_IF([test "x$with_docs_only" == xyes],
      AM_CONDITIONAL([HAVE_PROFILING], [false])
      AM_CONDITIONAL([HAVE_UCM_PTMALLOC286], [false])
      AM_CONDITIONAL([HAVE_JAVA], [false])
+     AM_CONDITIONAL([HAVE_CXX11], [false])
+     AM_CONDITIONAL([HAVE_TCMALLOC], [false])
+     AM_CONDITIONAL([HAVE_EXAMPLES], [false])
     ],
     [
      AM_CONDITIONAL([DOCS_ONLY], [false])
@@ -227,6 +236,15 @@ AS_IF([test "x$with_docs_only" == xyes],
      #
      AC_SUBST([VALGRIND_LIBPATH], [${valgrind_libpath}])
 
+     #
+     # Enable examples build
+     #
+     AC_ARG_ENABLE([examples],
+                   [AC_HELP_STRING([--enable-examples],
+                                   [Enable examples build])],
+                   [AM_CONDITIONAL([HAVE_EXAMPLES], [test "x$enable_examples" == "xyes"])],
+                   [AM_CONDITIONAL([HAVE_EXAMPLES], [false])])
+
     ]) # Docs only
 
 
@@ -248,6 +266,7 @@ AC_MSG_NOTICE([Supported transports: $transports])
 # Final output
 #
 AC_CONFIG_FILES([Makefile
+                 doc/doxygen/header.tex
                  src/uct/api/version.h
                  ])
 AS_IF([test "x$with_docs_only" == xyes], [], [
@@ -272,8 +291,10 @@ AC_CONFIG_FILES([
                  src/tools/perf/Makefile
                  src/tools/profile/Makefile
                  test/apps/Makefile
+                 test/apps/sockaddr/Makefile
                  test/examples/Makefile
                  test/gtest/Makefile
+                 test/gtest/ucm/test_dlopen/Makefile
                  test/mpi/Makefile
                  bindings/java/src/main/native/Makefile
                  ])
diff --git a/src/mpid/ch4/netmod/ucx/ucx/contrib/test_jenkins.sh b/src/mpid/ch4/netmod/ucx/ucx/contrib/test_jenkins.sh
index be57d6c9d..2f085887b 100755
--- a/src/mpid/ch4/netmod/ucx/ucx/contrib/test_jenkins.sh
+++ b/src/mpid/ch4/netmod/ucx/ucx/contrib/test_jenkins.sh
@@ -86,6 +86,18 @@ module_load() {
 	fi
 }
 
+#
+# try load cuda modules if nvidia driver is installed
+#
+try_load_cuda_env() {
+	num_gpus=0
+	if [ -f "/proc/driver/nvidia/version" ]; then
+		module_load dev/cuda || true
+		module_load dev/gdrcopy || true
+		num_gpus=$(nvidia-smi -L | wc -l)
+	fi
+}
+
 #
 # Check whether this test should do a task with given index,
 # according to the parallel test execution parameters.
@@ -169,6 +181,11 @@ prepare() {
 #
 build_docs() {
 	echo " ==== Build docs only ===="
+	# Try load newer doxygen if native is older than 1.8.11
+	if ! (echo "1.8.11"; doxygen --version) | sort -CV
+	then
+		module_load tools/doxygen-1.8.11 || true
+	fi
 	../configure --prefix=$ucx_inst --with-docs-only
 	$MAKE clean
 	$MAKE docs
@@ -208,15 +225,22 @@ build_release_pkg() {
 	$MAKE distcheck
 
 	# Show UCX info
-	./src/tools/info/ucx_info -f -c -v -y -d -b -p -w -e -uart
+	./src/tools/info/ucx_info -s -f -c -v -y -d -b -p -w -e -uart
 
-	set +e
-	out=$(rpm -q rpm 2>/dev/null)
-	rc=$?
-	set -e
-	rpm_based=yes
-	if [[ $rc != 0 || "$out" == *"not installed"* ]]; then
+	if [ -f /etc/redhat-release -o -f /etc/fedora-release ]; then
+		rpm_based=yes
+	elif [ `cat /etc/os-release | grep -i "ubuntu\|mint"|wc -l` -gt 0 ]; then
 		rpm_based=no
+	else
+		# try rpm tool to detect distro
+		set +e
+		out=$(rpm -q rpm 2>/dev/null)
+		rc=$?
+		set -e
+		rpm_based=yes
+		if [[ $rc != 0 || "$out" == *"not installed"* ]]; then
+			rpm_based=no
+		fi
 	fi
 
 	if [[ "$rpm_based" == "no" && -x /usr/bin/dpkg-buildpackage ]]; then
@@ -227,6 +251,16 @@ build_release_pkg() {
 		../contrib/buildrpm.sh -s -b
 	fi
 
+	# check that UCX version is present in spec file
+	cd ${WORKSPACE}
+	# extract version from configure.ac and convert to MAJOR.MINOR.PATCH representation
+	version=$(grep -P "define\S+ucx_ver" configure.ac | awk '{print $2}' | sed 's,),,' | xargs echo | tr ' ' '.')
+	if ! grep -q "$version" ucx.spec.in; then
+		echo "Current UCX version ($version) is not present in ucx.spec.in changelog"
+		exit 1
+	fi
+	cd -
+
 	$MAKE distclean
 }
 
@@ -235,19 +269,19 @@ build_release_pkg() {
 #
 build_icc() {
 	echo 1..1 > build_icc.tap
-	if module_load intel/ics
+	if module_load intel/ics && icc -v
 	then
 		echo "==== Build with Intel compiler ===="
 		../contrib/configure-devel --prefix=$ucx_inst CC=icc CXX=icpc
 		$MAKE clean
 		$MAKE
 		$MAKE distclean
-		module unload intel/ics
 		echo "ok 1 - build successful " >> build_icc.tap
 	else
 		echo "==== Not building with Intel compiler ===="
 		echo "ok 1 - # SKIP because Coverity not installed" >> build_icc.tap
 	fi
+	module unload intel/ics
 }
 
 #
@@ -255,7 +289,7 @@ build_icc() {
 #
 build_debug() {
 	echo "==== Build with --enable-debug option ===="
-	../contrib/configure-devel --prefix=$ucx_inst --enable-debug
+	../contrib/configure-devel --prefix=$ucx_inst --enable-debug --enable-examples
 	$MAKE clean
 	$MAKE
 	$MAKE distclean
@@ -271,7 +305,7 @@ build_cuda() {
     then
         if module_load dev/gdrcopy
         then
-            echo "==== Build with enable cuda ===="
+            echo "==== Build with enable cuda, gdr_copy ===="
             ../contrib/configure-devel --prefix=$ucx_inst --with-cuda --with-gdrcopy
             $MAKE clean
             $MAKE
@@ -282,12 +316,14 @@ build_cuda() {
             $MAKE
             $MAKE distclean
             module unload dev/gdrcopy
-        else
-            ../contrib/configure-devel --prefix=$ucx_inst --with-cuda
-            $MAKE clean
-            $MAKE
-            $MAKE distclean
         fi
+
+        echo "==== Build with enable cuda, w/o gdr_copy ===="
+        ../contrib/configure-devel --prefix=$ucx_inst --with-cuda --without-gdrcopy
+        $MAKE clean
+        $MAKE
+        $MAKE distclean
+
         module unload dev/cuda
         echo "ok 1 - build successful " >> build_cuda.tap
     else
@@ -307,6 +343,8 @@ build_clang() {
 		../contrib/configure-devel --prefix=$ucx_inst CC=clang CXX=clang++
 		$MAKE clean
 		$MAKE
+		$MAKE install
+		UCX_HANDLE_ERRORS=bt,freeze UCX_LOG_LEVEL_TRIGGER=ERROR $ucx_inst/bin/ucx_info -d
 		$MAKE distclean
 		echo "ok 1 - build successful " >> build_clang.tap
 	else
@@ -315,6 +353,27 @@ build_clang() {
 	fi
 }
 
+#
+# Build with gcc-latest module
+#
+build_gcc_latest() {
+	echo 1..1 > build_gcc_latest.tap
+	if module_load dev/gcc-latest
+	then
+		echo "==== Build with GCC compiler ($(gcc --version|head -1)) ===="
+		../contrib/configure-devel --prefix=$ucx_inst
+		$MAKE clean
+		$MAKE
+		$MAKE install
+		UCX_HANDLE_ERRORS=bt,freeze UCX_LOG_LEVEL_TRIGGER=ERROR $ucx_inst/bin/ucx_info -d
+		$MAKE distclean
+		echo "ok 1 - build successful " >> build_gcc_latest.tap
+	else
+		echo "==== Not building with latest gcc compiler ===="
+		echo "ok 1 - # SKIP because dev/gcc-latest module is not available" >> build_gcc_latest.tap
+	fi
+}
+
 #
 # Build with armclang compiler
 #
@@ -326,8 +385,11 @@ build_armclang() {
         ../contrib/configure-devel --prefix=$ucx_inst CC=armclang CXX=armclang++
         $MAKE clean
         $MAKE
+        $MAKE install
+        UCX_HANDLE_ERRORS=bt,freeze UCX_LOG_LEVEL_TRIGGER=ERROR $ucx_inst/bin/ucx_info -d
         $MAKE distclean
         echo "ok 1 - build successful " >> build_armclang.tap
+        module unload arm-compiler/latest
     else
         echo "==== Not building with armclang compiler ===="
         echo "ok 1 - # SKIP because armclang not installed" >> build_armclang.tap
@@ -356,8 +418,8 @@ run_hello() {
 	if [ ! -x ${test_name} ]
 	then
 		gcc -o ${test_name} ${ucx_inst}/share/ucx/examples/${test_name}.c \
-		-l${api} -lucs -I${ucx_inst}/include -L${ucx_inst}/lib \
-		-Wl,-rpath=${ucx_inst}/lib
+		    -l${api} -lucs -I${ucx_inst}/include -L${ucx_inst}/lib \
+		    -Wl,-rpath=${ucx_inst}/lib
 	fi
 
 	# set smaller timeouts so the test will complete faster
@@ -374,7 +436,7 @@ run_hello() {
 	./${test_name} ${test_args} -p ${tcp_port} &
 	hw_server_pid=$!
 
-	sleep 5
+	sleep 15
 
 	# temporary disable 
 	if [[ ${test_args} == *"-e"* ]]
@@ -405,6 +467,11 @@ run_hello() {
 # Compile and run UCP hello world example
 #
 run_ucp_hello() {
+	if ./src/tools/info/ucx_info -e -u twe|grep ERROR
+	then
+		return # skip if cannot create ucp ep
+	fi
+
 	for test_mode in -w -f -b -e
 	do
 		echo "==== Running UCP hello world with mode ${test_mode} ===="
@@ -417,17 +484,87 @@ run_ucp_hello() {
 # Compile and run UCT hello world example
 #
 run_uct_hello() {
-	for ucx_dev in $(get_active_ib_devices)
+	for send_func in -i -b -z
 	do
-		for send_func in -i -b -z
+		for ucx_dev in $(get_active_ib_devices)
 		do
-			echo "==== Running UCT hello world server on ${ucx_dev} with sending ${send_func} ===="
-			run_hello uct  -d ${ucx_dev} -t "rc"
+			echo "==== Running UCT hello world server on rc/${ucx_dev} with sending ${send_func} ===="
+			run_hello uct  -d ${ucx_dev} -t "rc" ${send_func}
+		done
+		for ucx_dev in $(ip addr | awk '/state UP/ {print $2}' | sed s/://)
+		do
+			echo "==== Running UCT hello world server on tcp/${ucx_dev} with sending ${send_func} ===="
+			run_hello uct -d ${ucx_dev} -t "tcp" ${send_func}
 		done
 	done
 	rm -f ./uct_hello_world
 }
 
+run_client_server() {
+
+    test_name=ucp_client_server
+
+    if [ ! -x ${test_name} ]
+    then
+        gcc -o ${test_name} ${ucx_inst}/share/ucx/examples/${test_name}.c \
+            -lucp -lucs -I${ucx_inst}/include -L${ucx_inst}/lib \
+            -Wl,-rpath=${ucx_inst}/lib
+    fi
+
+    iface=`ibdev2netdev | grep Up | awk '{print $5}' | head -1`
+    if [ -n "$iface" ]
+    then
+        server_ip=`ip addr show ${iface} | awk '/inet /{print $2}' | awk -F '/' '{print $1}'`
+    fi
+
+    if [ -z "$server_ip" ]
+    then
+        # if there is no inet (IPv4) address, bail
+        return
+    fi
+
+    ibdev=`ibdev2netdev | grep $iface | awk '{print $1}'`
+    node_guid=`cat /sys/class/infiniband/$ibdev/node_guid`
+    if [ $node_guid == "0000:0000:0000:0000" ]
+    then
+        return
+    fi
+
+    server_port=$((10000 + EXECUTOR_NUMBER))
+
+    # run server side
+    ./${test_name} -p ${server_port} &
+    hw_server_pid=$!
+
+    sleep 15
+
+    # need to be ran in background to reflect application PID in $!
+    ./${test_name} -a ${server_ip} -p ${server_port} &
+    hw_client_pid=$!
+
+    wait ${hw_client_pid}
+    kill -9 ${hw_server_pid}
+}
+
+run_ucp_client_server() {
+
+    if [ ! -r /dev/infiniband/rdma_cm  ]
+    then
+        return
+    fi
+
+    ret=`which ibdev2netdev`
+    if [ -z "$ret" ]
+    then
+        return
+    fi
+
+    echo "==== Running UCP client-server  ===="
+    run_client_server
+
+    rm -f ./ucp_client_server
+}
+
 #
 # Run UCX performance test with MPI
 #
@@ -465,9 +602,15 @@ run_ucx_perftest_mpi() {
 	# run cuda tests
 	if (lsmod | grep -q "nv_peer_mem") && (lsmod | grep -q "gdrdrv")
 	then
+		export CUDA_VISIBLE_DEVICES=$(($worker%$num_gpus)),$(($(($worker+1))%$num_gpus))
 		cat $ucx_inst_ptest/test_types | grep cuda | sort -R > $ucx_inst_ptest/test_types_short
 		echo "==== Running ucx_perf with cuda memory===="
-		$MPIRUN -np 2 -x UCX_TLS=rc,cuda_copy,gdr_copy $AFFINITY $UCX_PERFTEST
+		$MPIRUN -np 2 -x UCX_TLS=rc,cuda_copy,gdr_copy -x UCX_MEMTYPE_CACHE=y $AFFINITY $UCX_PERFTEST
+		$MPIRUN -np 2 -x UCX_TLS=rc,cuda_copy,gdr_copy -x UCX_MEMTYPE_CACHE=n $AFFINITY $UCX_PERFTEST
+		$MPIRUN -np 2 -x UCX_TLS=rc,cuda_copy $AFFINITY $UCX_PERFTEST
+		$MPIRUN -np 2 -x UCX_TLS=self,mm,cma,cuda_copy $AFFINITY $UCX_PERFTEST
+		$MPIRUN -np 2 $AFFINITY $UCX_PERFTEST
+		unset CUDA_VISIBLE_DEVICES
 	fi
 }
 
@@ -475,7 +618,7 @@ run_ucx_perftest_mpi() {
 # Test malloc hooks with mpi
 #
 test_malloc_hooks_mpi() {
-	for tname in malloc_hooks external_events flag_no_install
+	for tname in malloc_hooks malloc_hooks_unmapped external_events flag_no_install
 	do
 		echo "==== Running memory hook (${tname}) on MPI ===="
 		$MPIRUN -np 1 $AFFINITY ./test/mpi/test_memhooks -t $tname
@@ -492,26 +635,23 @@ test_malloc_hooks_mpi() {
 #
 run_mpi_tests() {
 	echo "1..2" > mpi_tests.tap
-
-	#load cuda modules if available
-	module_load dev/cuda || true
-	module_load dev/gdrcopy || true
-
 	if module_load hpcx-gcc
 	then
+		# Prevent our tests from using UCX libraries from hpcx module by prepending
+		# our local library path first
+		export LD_LIBRARY_PATH=${ucx_inst}/lib:$LD_LIBRARY_PATH
+
 		../contrib/configure-release --prefix=$ucx_inst --with-mpi # TODO check in -devel mode as well
 		$MAKE clean
 		$MAKE install
 		$MAKE installcheck # check whether installation is valid (it compiles examples at least)
 
-		# Prevent our tests from using installed UCX libraries
-		export LD_LIBRARY_PATH=${ucx_inst}/lib:$LD_LIBRARY_PATH
-
 		MPIRUN="mpirun \
 				-x UCX_ERROR_SIGNALS \
 				-x UCX_HANDLE_ERRORS \
 				-mca pml ob1 \
-				-mca btl vader,self \
+				-mca btl tcp,self \
+				-mca btl_tcp_if_include lo \
 				-mca coll ^hcoll,ml"
 
 		run_ucx_perftest_mpi
@@ -533,22 +673,58 @@ run_mpi_tests() {
 # Test profiling infrastructure
 #
 test_profiling() {
-	echo "==== Running profiling test ===="
-	UCX_PROFILE_MODE=log UCX_PROFILE_FILE=ucx_jenkins.prof ./test/apps/test_profiling
+	echo "==== Running profiling example  ===="
+
+	# configure release mode, application profiling should work
+	../contrib/configure-release --prefix=$ucx_inst
+	$MAKE clean
+	$MAKE
+
+	# compile the profiling example code
+	gcc -o ucx_profiling ${ucx_inst}/share/ucx/examples/ucx_profiling.c \
+		-lm -lucs -I${ucx_inst}/include -L${ucx_inst}/lib -Wl,-rpath=${ucx_inst}/lib
+
+	UCX_PROFILE_MODE=log UCX_PROFILE_FILE=ucx_jenkins.prof ./ucx_profiling
 
 	UCX_READ_PROFILE=${ucx_inst}/bin/ucx_read_profile
 	$UCX_READ_PROFILE -r ucx_jenkins.prof | grep "printf" -C 20
 	$UCX_READ_PROFILE -r ucx_jenkins.prof | grep -q "calc_pi"
 	$UCX_READ_PROFILE -r ucx_jenkins.prof | grep -q "print_pi"
+}
+
+test_dlopen() {
+	../contrib/configure-release --prefix=$ucx_inst
+	$MAKE clean
+	$MAKE
 
 	echo "==== Running dlopen test ===="
-	strace ./test/apps/test_profiling &> strace.log
+	strace ./ucx_profiling &> strace.log
 	! grep '^socket' strace.log
+}
+
+test_memtrack() {
+	../contrib/configure-devel --prefix=$ucx_inst
+	$MAKE clean
+	$MAKE
 
 	echo "==== Running memtrack test ===="
 	UCX_MEMTRACK_DEST=stdout ./test/gtest/gtest --gtest_filter=test_memtrack.sanity
 }
 
+test_unused_env_var() {
+	# We must create a UCP worker to get the warning about unused variables
+	echo "==== Running ucx_info env vars test ===="
+	UCX_IB_PORTS=mlx5_0:1 ./src/tools/info/ucx_info -epw -u t | grep "unused" | grep -q "UCX_IB_PORTS"
+}
+
+test_malloc_hook() {
+	echo "==== Running malloc hooks test ===="
+	if [ -x ./test/apps/test_tcmalloc ]
+	then
+		./test/apps/test_tcmalloc
+	fi
+}
+
 #
 # Run Coverity and report errors
 #
@@ -567,7 +743,11 @@ run_coverity() {
 		rc=$(($rc+$nerrors))
 
 		index_html=$(cd $cov_build && find . -name index.html | cut -c 3-)
-		cov_url="$WS_URL/$cov_build_id/${index_html}"
+		if [ -z "$BUILD_URL" ]; then
+			cov_url="${WS_URL}/${cov_build_id}/${index_html}"
+		else
+			cov_url="${BUILD_URL}/artifact/${cov_build_id}/${index_html}"
+		fi
 		rm -f jenkins_sidelinks.txt
 		if [ $nerrors -gt 0 ]; then
 			cov-format-errors --dir $cov_build --emacs-style
@@ -589,14 +769,12 @@ run_coverity() {
 
 #
 # Run the test suite (gtest)
+# Arguments: <compiler-name> [configure-flags]
 #
 run_gtest() {
-
-	#load cuda modules if available
-	module_load dev/cuda || true
-	module_load dev/gdrcopy || true
-
-	../contrib/configure-devel --prefix=$ucx_inst
+	compiler_name=$1
+	shift
+	../contrib/configure-devel --prefix=$ucx_inst $@
 	$MAKE clean
 	$MAKE
 
@@ -607,6 +785,10 @@ run_gtest() {
 	export GTEST_TAP=2
 	export GTEST_REPORT_DIR=$WORKSPACE/reports/tap
 
+	if [ $num_gpus -gt 0 ]; then
+		export CUDA_VISIBLE_DEVICES=$(($worker%$num_gpus))
+	fi
+
 	GTEST_EXTRA_ARGS=""
 	if [ "$JENKINS_TEST_PERF" == 1 ]
 	then
@@ -617,11 +799,11 @@ run_gtest() {
 
 	mkdir -p $GTEST_REPORT_DIR
 
-	echo "==== Running unit tests ===="
+	echo "==== Running unit tests, $compiler_name compiler ===="
 	$AFFINITY $TIMEOUT make -C test/gtest test
 	(cd test/gtest && rename .tap _gtest.tap *.tap && mv *.tap $GTEST_REPORT_DIR)
 
-	echo "==== Running malloc hooks mallopt() test ===="
+	echo "==== Running malloc hooks mallopt() test, $compiler_name compiler ===="
 	# gtest returns with non zero exit code if there were no
 	# tests to run. As a workaround run a single test on every
 	# shard.
@@ -635,7 +817,7 @@ run_gtest() {
 		make -C test/gtest test
 	(cd test/gtest && rename .tap _mallopt_gtest.tap malloc_hook_cplusplus.tap && mv *.tap $GTEST_REPORT_DIR)
 
-	echo "==== Running malloc hooks mmap_ptrs test with MMAP_THRESHOLD=16384 ===="
+	echo "==== Running malloc hooks mmap_ptrs test with MMAP_THRESHOLD=16384, $compiler_name compiler ===="
 	$AFFINITY $TIMEOUT \
 		env MALLOC_MMAP_THRESHOLD_=16384 \
 		GTEST_SHARD_INDEX=0 \
@@ -646,7 +828,7 @@ run_gtest() {
 
 	if ! [[ $(uname -m) =~ "aarch" ]] && ! [[ $(uname -m) =~ "ppc" ]]
 	then
-		echo "==== Running valgrind tests ===="
+		echo "==== Running valgrind tests, $compiler_name compiler ===="
 
 		# Load newer valgrind if naative is older than 3.10
 		if ! (echo "valgrind-3.10.0"; valgrind --version) | sort -CV
@@ -654,17 +836,34 @@ run_gtest() {
 			module load tools/valgrind-latest
 		fi
 
-		export VALGRIND_EXTRA_ARGS="--xml=yes --xml-file=valgrind.xml --child-silent-after-fork=yes"
+		export VALGRIND_EXTRA_ARGS="--xml=yes --xml-file=valgrind.xml --child-silent-after-fork=yes --gen-suppressions=all"
 		$AFFINITY $TIMEOUT_VALGRIND make -C test/gtest test_valgrind
 		(cd test/gtest && rename .tap _vg.tap *.tap && mv *.tap $GTEST_REPORT_DIR)
 		module unload tools/valgrind-latest
 	else
-		echo "==== Not running valgrind tests ===="
+		echo "==== Not running valgrind tests with $compiler_name compiler ===="
 		echo "1..1"                                          > vg_skipped.tap
 		echo "ok 1 - # SKIP because running on $(uname -m)" >> vg_skipped.tap
 	fi
 }
 
+run_gtest_default() {
+	run_gtest "default"
+}
+
+run_gtest_armclang() {
+	if module_load arm-compiler/arm-hpc-compiler && armclang -v
+	then
+		run_gtest "armclang" CC=armclang CXX=armclang++
+	else
+		echo "==== Not running with armclang compiler ===="
+		echo "1..1"                                          > armclang_skipped.tap
+		echo "ok 1 - # SKIP because armclang not found"     >> armclang_skipped.tap
+	fi
+	module unload arm-compiler/arm-hpc-compiler
+}
+
+
 #
 # Run the test suite (gtest) in release configuration
 #
@@ -715,6 +914,7 @@ run_tests() {
 	do_distributed_task 2 4 build_cuda
 	do_distributed_task 3 4 build_clang
 	do_distributed_task 0 4 build_armclang
+	do_distributed_task 1 4 build_gcc_latest
 
 	# all are running mpi tests
 	run_mpi_tests
@@ -727,16 +927,23 @@ run_tests() {
 
 	do_distributed_task 1 4 run_ucp_hello
 	do_distributed_task 2 4 run_uct_hello
+	do_distributed_task 1 4 run_ucp_client_server
 	do_distributed_task 3 4 test_profiling
+	do_distributed_task 3 4 test_dlopen
+	do_distributed_task 3 4 test_memtrack
+	do_distributed_task 0 4 test_unused_env_var
+	do_distributed_task 1 3 test_malloc_hook
 
 	# all are running gtest
-	run_gtest
+	run_gtest_default
+	run_gtest_armclang
 
 	do_distributed_task 3 4 run_coverity
 	do_distributed_task 0 4 run_gtest_release
 }
 
 prepare
+try_load_cuda_env
 do_distributed_task 0 4 build_docs
 do_distributed_task 0 4 build_disable_numa
 do_distributed_task 1 4 build_no_verbs
diff --git a/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/README b/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/README
index 7e092e0fb..47d88adeb 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/README
+++ b/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/README
@@ -1,3 +1,3 @@
 This is an example of the "batch" configuration files for ucx_perftest.
 The files are passed as an input parameter to the ucx_pertest benchmark:
-ucx_perftest --batch msg_pow2 --batch test_types --batch transports <...>
+ucx_perftest -b msg_pow2 -b test_types -b transports <...>
diff --git a/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/test_types b/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/test_types
index 36f1dd7af..a89ca1d67 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/test_types
+++ b/src/mpid/ch4/netmod/ucx/ucx/contrib/ucx_perftest_config/test_types
@@ -32,7 +32,7 @@ ucp_iov_iov_tag_bw          -t tag_bw  -D iov,iov
 ucp_contig_contig_tag_bw    -t tag_bw  -D contig,contig
 #IOV with RNDV is not yet supported
 #ucp_contig_iov_tag_bw       -t tag_bw  -D contig,iov
-ucp_sync_tag_lat            -t tag_lat_sync
+ucp_sync_tag_lat            -t tag_sync_lat
 ucp_unexp_tag_lat           -t tag_lat -U
 ucp_wild_tag_lat            -t tag_lat -C
 ucp_contig_stream_bw        -t stream_bw  -r recv_data
@@ -42,4 +42,8 @@ ucp_contig_stream_lat       -t stream_lat -r recv
 #CUDA
 ucp_contig_contig_cuda_tag_lat   -t tag_lat -D contig,contig -m cuda
 ucp_contig_contig_cuda_tag_bw    -t tag_bw  -D contig,contig -m cuda
+ucp_contig_cuda_stream_bw        -t stream_bw  -r recv_data -m cuda
+ucp_contig_cuda_stream_lat       -t stream_lat -r recv_data -m cuda
+ucp_contig_cuda_stream_bw        -t stream_bw  -r recv -m cuda
+ucp_contig_cuda_stream_lat       -t stream_lat -r recv -m cuda
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/contrib/valgrind.supp b/src/mpid/ch4/netmod/ucx/ucx/contrib/valgrind.supp
index c6f073767..1f9bdc5ef 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/contrib/valgrind.supp
+++ b/src/mpid/ch4/netmod/ucx/ucx/contrib/valgrind.supp
@@ -74,9 +74,73 @@
    ...
    fun:gdr_copy_to_bar
 }
+{
+   gdr_copy_cond_1
+   Memcheck:Cond
+   ...
+   fun:gdr_copy_from_bar
+}
 {
    gdr_copy_value8
    Memcheck:Value8
    ...
    fun:gdr_copy_to_bar
 }
+{
+   gdr_copy_value8_1
+   Memcheck:Value8
+   ...
+   fun:gdr_copy_from_bar
+}
+{
+    ibv_exp_reg_mr
+    Memcheck:Param
+    write(buf)
+    ...
+    fun:ibv_exp_reg_mr
+}
+{
+    ibv_exp_free_dm
+    Memcheck:Param
+    write(buf)
+    ...
+    fun:ibv_exp_free_dm
+}
+{
+    ibv_exp_cmd_free_dm
+    Memcheck:Param
+    write(buf)
+    ...
+    fun:ibv_exp_cmd_free_dm
+}
+{
+   res_domain_leak
+   Memcheck:Leak
+   ...
+   fun:ibv_exp_create_res_domain
+}
+{
+   ibverbs_get_device_list
+   Memcheck:Leak
+   ...
+   fun:ibverbs_get_device_list
+}
+{
+   fix_dereg_mr
+   Memcheck:Cond
+   ...
+   fun:ibv_dereg_mr@@IBVERBS_1.1
+}
+{
+   exp_query_device_cond
+   Memcheck:Cond
+   fun:ibv_exp_cmd_query_device
+}
+{
+   ucma_init_device
+   Memcheck:Leak
+   match-leak-kinds: possible
+   ...
+   fun:ucma_init_device
+}
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.dox b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.dox
deleted file mode 100644
index 9e0ff9f59..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.dox
+++ /dev/null
@@ -1,27 +0,0 @@
-/*
- * Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-/**
- * @page Conventions Conventions and Notations
- * This section describes the conventions and notations in the UCX specification.
- * 
- * \section Blocking Blocking Behavior
- * The blocking UCX routines return only when an UCX operation is complete.
- * After the return, the resources used in the UCX routine are available
- * for reuse. 
- * 
- * \section Non-blocking Non-blocking Behavior
- * The non-blocking UCX routines return immediately, independent of operation 
- * completion. After the return, the resources used for the routines are not
- * necessarily available for reuse.
- * 
- * \section Fairness Fairness
- * UCX routines do not guarantee fairness. However, the routines 
- * enable UCX consumers to write efficient and fair programs.
- *
- * \section Interaction with Signal Handler Functions
- * If UCX routines are invoked from signal a handler function,
- * the behavior of the program is undefined.
- */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.md b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.md
new file mode 100644
index 000000000..0736181a9
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/conventions.md
@@ -0,0 +1,22 @@
+Conventions and Notations
+=========================
+
+This section describes the conventions and notations in the UCX specification.
+
+\section Blocking Blocking Behavior
+The blocking UCX routines return only when an UCX operation is complete.
+After the return, the resources used in the UCX routine are available
+for reuse.
+
+\section Non-blocking Non-blocking Behavior
+The non-blocking UCX routines return immediately, independent of operation
+completion. After the return, the resources used for the routines are not
+necessarily available for reuse.
+
+\section Fairness Fairness
+UCX routines do not guarantee fairness. However, the routines
+enable UCX consumers to write efficient and fair programs.
+
+\section Interaction with Signal Handler Functions
+If UCX routines are invoked from signal a handler function,
+the behavior of the program is undefined.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.dox b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.dox
deleted file mode 100644
index 6cd844ce8..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.dox
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-/**
- * \page Design
- *
- * The UCX framework consists of the three main components: UC-Services (UCS),
- * UC-Transports (UCT), and UC-Protocols (UCP). Each one of these components
- * exports a public API, and can be used as a stand-alone library.
- *
- * \image latex Architecture.pdf "UCX Framework Architecture"
- * \image html  Architecture.png "UCX Framework Architecture"
- *
- * \section UCS
- * UCS is a service layer that provides the necessary functionality for
- * implementing portable and efficient utilities. This layer includes the
- * following services: 
- * + an abstraction for accessing platform specific functionality (atomic operations, thread safety, etc.), 
- * + tools for efficient memory management (memory pools, memory allocators, and memory allocators hooks), 
- * + commonly used data structures (hashes, trees, lists).  
- *
- * \section UCT
- * UCT is a transport layer that abstracts
- * the differences across various hardware architectures and provides a
- * low-level API that enables the implementation of communication protocols.
- * The primary goal of the layer is to provide direct and efficient access to
- * hardware network functionality. For this purpose,
- * UCT relies on vendor provided low-level drivers such as InfiniBand
- * Verbs, Cray’s uGNI, libfabrics, etc. In addition, the layer provides
- * constructs for communication context management (thread-based and application level), and
- * allocation and management of device-specific memories including those found
- * in accelerators. In terms of communication APIs, UCT defines interfaces for
- * immediate (short), buffered copy-and-send (bcopy), and zero-copy (zcopy)
- * communication operations.
- *
- * \b Short: This type of operation is optimized for small messages that can be posted and completed
- * in place @anchor uct_short_protocol_desc.
- *
- * \b Bcopy: This type of operation is optimized for medium size messages that are typically sent through a
- * so-called bouncing-buffer. This auxiliary buffer is typically allocated given network constraints and ready for
- * immediate utilization by the hardware. Since a custom data packing routine could  be provided, this method
- * can be used for non-contiguos i/o @anchor uct_bcopy_protocol_desc.
- *
- * \b Zcopy: This type of operation exposes zero-copy memory-to-memory communication semantics, which means that
- * message is sent directly from user buffer, or received directly to user buffer, without being copied between
- * the network layers @anchor uct_zcopy_protocol_desc.
- *
- * \section UCP
- * UCP implements higher-level protocols that are typically used by message passing (MPI)
- * and PGAS programming models by using lower-level capabilities exposed
- * through the UCT layer. UCP is provides the following functionality: ability to select different transports for
- * communication, message fragmentation, multi-rail communication, and initializing and finalizing 
- * the library.
- * Currently, the API has the following classes of interfaces:
- * Initialization, Remote Memory Access (RMA) communication, Atomic Memory
- * Operations (AMO), Active Message, Tag-Matching, and Collectives.
- *
- * \b Initialization: This subset of interfaces defines the communication
- * context setup, queries the network capabilities, and initializes the local
- * communication endpoints. The context represented by the UCX context is an
- * abstraction of the network transport resources. The communication endpoint
- * setup interfaces initialize the UCP endpoint, which is an abstraction of all
- * the necessary resources associated with a particular connection. The
- * communication endpoints are used as input to all communication operations to
- * describe the source and destination of the communication.  
- *
- * \b RMA: This subset of interfaces defines one-sided communication operations such as PUT and
- * GET, required for implementing low overhead, direct memory access communications 
- * constructs needed by both distributed and shared memory
- * programming models. UCP includes a separate set of interfaces for
- * communicating non-contiguous data. This functionality was included to
- * support various programming models’ communication requirements and leverage
- * the scatter/gather capabilities of modern network hardware.  
- *
- * \b AMO: This subset of interfaces provides support for atomically performing operations
- * on the remote memory, an important class of operations for PGAS
- * programming models, particularly OpenSHMEM.  
- *
- * \b Tag \b Matching: This interface supports tag-matching for send-receive semantics which is a key
- * communication semantic defined by the MPI specification.  
- *
- * \b Stream : The API provides order and reliable communication semantics.
- * Data is treated as an ordered sequence of bytes pushed through the connection.
- * In contrast of tag-matching interface, the size of each individual send does
- * not necessarily have to match the size of each individual receive, as long as
- * the total number of bytes is the same. This API is designed to match widely
- * used BSD-socket based programming models.
- *
- * \b Active \b Message: A subset of functionality where the incoming packet invokes a
- * sender-specified callback in order to be processed by the receiving process.
- * As an example, the two-sided MPI interface can easily be implemented on top
- * of such a concept (TBD: cite openmpi ). However, these interfaces are more general and
- * suited for other programming paradigms where the receiver process does not
- * prepost receives, but expects to react to incoming packets directly. Like
- * RMA and tag-matching interfaces, the active message interface provides
- * separate APIs for different message types and non-contiguous data.
- *
- * \b Collectives: This subset of interfaces defines group com- munication and
- * synchronization operations. The collective operations include Barrier,
- * All-to-one, All-to-all, and reduction operations. When possible, we will
- * take advantage of hardware acceleration for collectives 
- * (e.g., InfiniBand Switch collective acceleration).
- */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.md b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.md
new file mode 100644
index 000000000..1c7911f97
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/design.md
@@ -0,0 +1,100 @@
+Design
+======
+
+The UCX framework consists of the three main components: UC-Services (UCS),
+UC-Transports (UCT), and UC-Protocols (UCP). Each one of these components
+exports a public API, and can be used as a stand-alone library.
+
+\image latex Architecture.pdf "UCX Framework Architecture"
+\image html  Architecture.png "UCX Framework Architecture"
+
+\section UCS
+UCS is a service layer that provides the necessary functionality for
+implementing portable and efficient utilities. This layer includes the
+following services:
++ an abstraction for accessing platform specific functionality (atomic operations, thread safety, etc.),
++ tools for efficient memory management (memory pools, memory allocators, and memory allocators hooks),
++ commonly used data structures (hashes, trees, lists).
+
+\section UCT
+UCT is a transport layer that abstracts
+the differences across various hardware architectures and provides a
+low-level API that enables the implementation of communication protocols.
+The primary goal of the layer is to provide direct and efficient access to
+hardware network functionality. For this purpose,
+UCT relies on vendor provided low-level drivers such as InfiniBand
+Verbs, Cray's uGNI, libfabrics, etc. In addition, the layer provides
+constructs for communication context management (thread-based and application level), and
+allocation and management of device-specific memories including those found
+in accelerators. In terms of communication APIs, UCT defines interfaces for
+immediate (short), buffered copy-and-send (bcopy), and zero-copy (zcopy)
+communication operations.
+
+\b Short: This type of operation is optimized for small messages that can be posted and completed
+in place @anchor uct_short_protocol_desc.
+
+\b Bcopy: This type of operation is optimized for medium size messages that are typically sent through a
+so-called bouncing-buffer. This auxiliary buffer is typically allocated given network constraints and ready for
+immediate utilization by the hardware. Since a custom data packing routine could  be provided, this method
+can be used for non-contiguos i/o @anchor uct_bcopy_protocol_desc.
+
+\b Zcopy: This type of operation exposes zero-copy memory-to-memory communication semantics, which means that
+message is sent directly from user buffer, or received directly to user buffer, without being copied between
+the network layers @anchor uct_zcopy_protocol_desc.
+
+\section UCP
+UCP implements higher-level protocols that are typically used by message passing (MPI)
+and PGAS programming models by using lower-level capabilities exposed
+through the UCT layer. UCP is provides the following functionality: ability to select different transports for
+communication, message fragmentation, multi-rail communication, and initializing and finalizing
+the library.
+Currently, the API has the following classes of interfaces:
+Initialization, Remote Memory Access (RMA) communication, Atomic Memory
+Operations (AMO), Active Message, Tag-Matching, and Collectives.
+
+\b Initialization: This subset of interfaces defines the communication
+context setup, queries the network capabilities, and initializes the local
+communication endpoints. The context represented by the UCX context is an
+abstraction of the network transport resources. The communication endpoint
+setup interfaces initialize the UCP endpoint, which is an abstraction of all
+the necessary resources associated with a particular connection. The
+communication endpoints are used as input to all communication operations to
+describe the source and destination of the communication.
+
+\b RMA: This subset of interfaces defines one-sided communication operations such as PUT and
+GET, required for implementing low overhead, direct memory access communications
+constructs needed by both distributed and shared memory
+programming models. UCP includes a separate set of interfaces for
+communicating non-contiguous data. This functionality was included to
+support various programming models' communication requirements and leverage
+the scatter/gather capabilities of modern network hardware.
+
+\b AMO: This subset of interfaces provides support for atomically performing operations
+on the remote memory, an important class of operations for PGAS
+programming models, particularly OpenSHMEM.
+
+\b Tag \b Matching: This interface supports tag-matching for send-receive semantics which is a key
+communication semantic defined by the MPI specification.
+
+\b Stream : The API provides order and reliable communication semantics.
+Data is treated as an ordered sequence of bytes pushed through the connection.
+In contrast of tag-matching interface, the size of each individual send does
+not necessarily have to match the size of each individual receive, as long as
+the total number of bytes is the same. This API is designed to match widely
+used BSD-socket based programming models.
+
+\b Active \b Message: A subset of functionality where the incoming packet invokes a
+sender-specified callback in order to be processed by the receiving process.
+As an example, the two-sided MPI interface can easily be implemented on top
+of such a concept (TBD: cite openmpi ). However, these interfaces are more general and
+suited for other programming paradigms where the receiver process does not
+prepost receives, but expects to react to incoming packets directly. Like
+RMA and tag-matching interfaces, the active message interface provides
+separate APIs for different message types and non-contiguous data.
+
+\b Collectives: This subset of interfaces defines group com- munication and
+synchronization operations. The collective operations include Barrier,
+All-to-one, All-to-all, and reduction operations. When possible, we will
+take advantage of hardware acceleration for collectives
+(e.g., InfiniBand Switch collective acceleration).
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/header.tex b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/header.tex.in
similarity index 94%
rename from doc/doxygen/header.tex
rename to doc/doxygen/header.tex.in
index af58b3966..460408753 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/header.tex
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/header.tex.in
@@ -72,8 +72,8 @@
 \fancyhead[RO]{\fancyplain{}{\bfseries\thepage}}
 \fancyfoot[LE]{\fancyplain{}{}}
 \fancyfoot[CE]{\fancyplain{}{}}
-\fancyfoot[RE]{\fancyplain{}{\bfseries\scriptsize \textcircled{c} 2015 Unified Communication X (UCX). All rights reserved. }}
-\fancyfoot[LO]{\fancyplain{}{\bfseries\scriptsize \textcircled{c} 2015 Unified Communication X (UCX). All rights reserved. }}
+\fancyfoot[RE]{\fancyplain{}{\bfseries\scriptsize \textcircled{c} \the\year\space Unified Communication X (UCX). All rights reserved. }}
+\fancyfoot[LO]{\fancyplain{}{\bfseries\scriptsize \textcircled{c} \the\year\space Unified Communication X (UCX). All rights reserved. }}
 \fancyfoot[CO]{\fancyplain{}{}}
 \fancyfoot[RO]{\fancyplain{}{}}
 \renewcommand{\footrulewidth}{0.4pt}
@@ -128,7 +128,7 @@
     {\Large Unified Communication X (UCX)}\\
 \vspace*{0.5cm}
 {\large API Standard}\\
-{\small Version 1.3}\\
+{\small Version @MAJOR_VERSION@.@MINOR_VERSION@}\\
 \vspace*{0.5cm}
 \includegraphics[width=6cm]{UCX_Logo_930x933.png}
 \vspace*{0.5cm}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.dox b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.dox
deleted file mode 100644
index 75dc39d6c..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.dox
+++ /dev/null
@@ -1,91 +0,0 @@
-/*
- * Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-/**
- * @page Introduction
-
-
- * \section Motivation
- * A communication middleware abstracts the vendor-specific software and hardware
- * interfaces. 
- * They bridge the semantic and functionality gap between the programming models
- * and the software and hardware network interfaces by providing
- * data transfer interfaces and implementation, optimized protocols for data
- * transfer between various memories, and managing network resources. There are many
- * communication middleware APIs and libraries to support parallel programming
- * models such as MPI, OpenSHMEM, and task-based models. 
- *
- * Current communication middleware designs typically take two approaches. First,
- * communication middleware such as Intel’s PSM (previously Qlogic), Mellanox’s
- * MXM, and IBM’s PAMI provide high-performance implementations for specific
- * network hardware. Second, communication middleware such as VMI, Cactus, ARMCI,
- * GASNet, and Open MPI are tightly coupled to a specific programming model. 
- * Communication middleware designed with either of this design approach 
- * requires significant porting effort to move a new network
- * interface or programming model. 
- *
- * To achieve functional and performance portability across
- * architectures and programming models, we introduce Unified Communication X 
- * (UCX).  
- * 
- * \section UCX
- * Unified Communication X (UCX) is a set of network APIs and their
- * implementations for high throughput computing. UCX is a combined
- * effort of national laboratories, industry, and academia to design and
- * implement a high-performing and highly-scalable network stack for next
- * generation applications and systems. UCX design provides the ability to
- * tailor its APIs and network functionality to suit a wide variety of
- * application domains. 
- * We envision that these APIs will satisfy the networking needs of many
- * programming models such as the Message Passing Interface (MPI), OpenSHMEM,
- * Partitioned Global Address Space (PGAS) languages, task-based paradigms, and
- * I/O bound applications.
- *
- * The initial focus is on supporting semantics such as point-to-point
- * communications (one-sided and two-sided), collective communication, 
- * and remote atomic operations required for popular parallel programming models. 
- * Also, the initial UCX reference implementation 
- * is targeted to support current network technologies such as: 
- * + Open Fabrics - InfiniBand (Mellanox, Qlogic, IBM), libfabrics, iWARP, RoCE
- * + Cray GEMINI \& ARIES
- * + Shared memory (MMAP, Posix, CMA, KNEM, XPMEM, etc.)
- * + Ethernet (TCP/UDP)
- *
-
- * UCX design goals are focused on performance and scalability, while efficiently supporting
- * popular and emerging programming models.
-
- * UCX’s API and design do not impose architectural constraints on the network hardware 
- * nor require any specific capabilities to the support the programming model functionality.
- * This is achieved by keeping the API flexible and ability to support the missing 
- * functionality efficiently in the software. 
- *
- *
- * Extreme scalability is an important design goal for UCX. 
- * To achieve this, UCX follows these design principles :
- * + Minimal memory consumption : Design avoids data-structures that scale with the number of 
- * processing   elements (i.e., order N data structures), and share resources among multiple 
- * programming models.
- * + Low-latency Interfaces: Design provides at least two sets of APIs with one set focused on the performance, 
- * and the other focused on functionality.
- * + High bandwidth - With minimal software overhead combined and support for multi-rail and multi-device 
- *  capabilities, the design provides all the hooks that are necessary for exploiting hardware bandwidth 
- * capabilities.
- * + Asynchronous Progress: API provides non-blocking communication interfaces and design supports asynchronous progress 
- * required for communication and computation overlap
- * + Resilience - the API exposes communication control hooks required for fault 
- * tolerant communication library implementation.
- *
- * UCX design provides native support for hybrid programming models. The 
- * design enables resource sharing, optimal memory usage, and progress engine
- * coordination to efficiently implement hybrid programming models. For example,
- * hybrid applications that use both OpenSHMEM and MPI programming models will be
- * able to select between a single-shared UCX network context or a stand
- * alone UCX network context for each one of them. Such flexibility,
- * optimized resource sharing, and reduced memory consumption, improve network
- * and application performance.
- * 
- * 
- */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.md b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.md
new file mode 100644
index 000000000..35404bbfd
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/intro.md
@@ -0,0 +1,82 @@
+Introduction
+============
+
+\section Motivation
+A communication middleware abstracts the vendor-specific software and hardware
+interfaces.
+They bridge the semantic and functionality gap between the programming models
+and the software and hardware network interfaces by providing
+data transfer interfaces and implementation, optimized protocols for data
+transfer between various memories, and managing network resources. There are many
+communication middleware APIs and libraries to support parallel programming
+models such as MPI, OpenSHMEM, and task-based models.
+
+Current communication middleware designs typically take two approaches. First,
+communication middleware such as Intel's PSM (previously Qlogic), Mellanox's
+MXM, and IBM's PAMI provide high-performance implementations for specific
+network hardware. Second, communication middleware such as VMI, Cactus, ARMCI,
+GASNet, and Open MPI are tightly coupled to a specific programming model.
+Communication middleware designed with either of this design approach
+requires significant porting effort to move a new network
+interface or programming model.
+
+To achieve functional and performance portability across
+architectures and programming models, we introduce Unified Communication X
+(UCX).
+
+\section UCX
+Unified Communication X (UCX) is a set of network APIs and their
+implementations for high throughput computing. UCX is a combined
+effort of national laboratories, industry, and academia to design and
+implement a high-performing and highly-scalable network stack for next
+generation applications and systems. UCX design provides the ability to
+tailor its APIs and network functionality to suit a wide variety of
+application domains.
+We envision that these APIs will satisfy the networking needs of many
+programming models such as the Message Passing Interface (MPI), OpenSHMEM,
+Partitioned Global Address Space (PGAS) languages, task-based paradigms, and
+I/O bound applications.
+
+The initial focus is on supporting semantics such as point-to-point
+communications (one-sided and two-sided), collective communication,
+and remote atomic operations required for popular parallel programming models.
+Also, the initial UCX reference implementation
+is targeted to support current network technologies such as:
++ Open Fabrics - InfiniBand (Mellanox, Qlogic, IBM), libfabrics, iWARP, RoCE
++ Cray GEMINI \& ARIES
++ Shared memory (MMAP, Posix, CMA, KNEM, XPMEM, etc.)
++ Ethernet (TCP/UDP)
+
+
+UCX design goals are focused on performance and scalability, while efficiently supporting
+popular and emerging programming models.
+
+UCX's API and design do not impose architectural constraints on the network hardware
+nor require any specific capabilities to the support the programming model functionality.
+This is achieved by keeping the API flexible and ability to support the missing
+functionality efficiently in the software.
+
+
+Extreme scalability is an important design goal for UCX.
+To achieve this, UCX follows these design principles:
++ Minimal memory consumption : Design avoids data-structures that scale with the number of
+processing   elements (i.e., order N data structures), and share resources among multiple
+programming models.
++ Low-latency Interfaces: Design provides at least two sets of APIs with one set focused on the performance,
+and the other focused on functionality.
++ High bandwidth - With minimal software overhead combined and support for multi-rail and multi-device
+ capabilities, the design provides all the hooks that are necessary for exploiting hardware bandwidth
+capabilities.
++ Asynchronous Progress: API provides non-blocking communication interfaces and design supports asynchronous progress
+required for communication and computation overlap
++ Resilience - the API exposes communication control hooks required for fault
+tolerant communication library implementation.
+
+UCX design provides native support for hybrid programming models. The
+design enables resource sharing, optimal memory usage, and progress engine
+coordination to efficiently implement hybrid programming models. For example,
+hybrid applications that use both OpenSHMEM and MPI programming models will be
+able to select between a single-shared UCX network context or a stand
+alone UCX network context for each one of them. Such flexibility,
+optimized resource sharing, and reduced memory consumption, improve network
+and application performance.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.dox b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.dox
deleted file mode 100644
index 417388d9d..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.dox
+++ /dev/null
@@ -1,32 +0,0 @@
-/*
- * Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-/**
- * \mainpage Preface
- * 
- * \section Scope Scope of the Document 
- * This document describes the UCX programming
- * interface.  The programming interface exposes a high performance
- * communication API, which provides basic building blocks for PGAS, Message
- * Passing Interface (MPI), Big-Data, Analytics, File I/O, and storage library developers.
- *
- * \section Audience Audience
- * This manual is intended for programmers who want to
- * develop parallel programming models like OpenSHMEM, MPI, UPC, Chapel, etc.
- * The manual assumes that the reader is familiar with the following:
- * + Basic concepts of two-sided, one-sided, atomic, and collective operations
- * + C programming language
- *
- * \section Status Document Status 
- * This section briefly describes a list of open
- * issues in the UCX specification.  
- * + UCP API - work in progress 
- * + UCT API - work in progress
- * 
- *
- * \section License 
- * UCX project follows open source development model and the software is 
- * licensed under BSD-3 license.
- */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.md b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.md
new file mode 100644
index 000000000..e3aef6f0e
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/preface.md
@@ -0,0 +1,25 @@
+Preface   {#mainpage}
+=======
+
+\section Scope Scope of the Document
+This document describes the UCX programming
+interface.  The programming interface exposes a high performance
+communication API, which provides basic building blocks for PGAS, Message
+Passing Interface (MPI), Big-Data, Analytics, File I/O, and storage library developers.
+
+\section Audience Audience
+This manual is intended for programmers who want to
+develop parallel programming models like OpenSHMEM, MPI, UPC, Chapel, etc.
+The manual assumes that the reader is familiar with the following:
++ Basic concepts of two-sided, one-sided, atomic, and collective operations
++ C programming language
+
+\section Status Document Status
+This section briefly describes a list of open
+issues in the UCX specification.
++ UCP API - work in progress
++ UCT API - work in progress
+
+\section License
+UCX project follows open source development model and the software is
+licensed under BSD-3 license.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/ucxdox b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/ucxdox
index 367fd9fa7..07a1822b3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/ucxdox
+++ b/src/mpid/ch4/netmod/ucx/ucx/doc/doxygen/ucxdox
@@ -1,4 +1,5 @@
 # Copyright (C) UT-Battelle, LLC. 2014-2015. ALL RIGHTS RESERVED.
+# Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
 # See file LICENSE for terms.
 #
 # Doxyfile 1.8.9.1
@@ -761,16 +762,18 @@ WARN_LOGFILE           =
 # spaces.
 # Note: If this tag is empty the current directory is searched.
 
-INPUT                  = $(SRCDIR)/src/ucp/api/   \
-                         $(SRCDIR)/src/uct/api/   \
-                         $(SRCDIR)/src/ucs/type/  \
-                         $(SRCDIR)/src/ucs/async/ \
-                         $(SRCDIR)/src/ucs/time/  \
-                         $(SRCDIR)/src/ucs/sys/   \
-                         $(SRCDIR)/doc/doxygen/preface.dox \
-                         $(SRCDIR)/doc/doxygen/intro.dox   \
-                         $(SRCDIR)/doc/doxygen/design.dox  \
-                         $(SRCDIR)/doc/doxygen/conventions.dox
+INPUT                  = $(SRCDIR)/src/ucp/api/             \
+                         $(SRCDIR)/src/uct/api/             \
+                         $(SRCDIR)/src/ucs/async/           \
+                         $(SRCDIR)/src/ucs/config/          \
+                         $(SRCDIR)/src/ucs/datastruct/      \
+                         $(SRCDIR)/src/ucs/time/            \
+                         $(SRCDIR)/src/ucs/type/            \
+                         $(SRCDIR)/src/ucs/sys/             \
+                         $(SRCDIR)/doc/doxygen/preface.md   \
+                         $(SRCDIR)/doc/doxygen/intro.md     \
+                         $(SRCDIR)/doc/doxygen/design.md    \
+                         $(SRCDIR)/doc/doxygen/conventions.md
 
 # This tag can be used to specify the character encoding of the source files
 # that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses
@@ -799,7 +802,9 @@ FILE_PATTERNS          = ucp.h          \
                          async_fwd.h    \
                          compiler_def.h \
                          time_def.h     \
-                         thread_mode.h
+                         thread_mode.h  \
+                         callbackq.h    \
+                         types.h
 
 # The RECURSIVE tag can be used to specify whether or not subdirectories should
 # be searched for input files as well.
@@ -1648,7 +1653,7 @@ EXTRA_PACKAGES         = times
 # to HTML_HEADER.
 # This tag requires that the tag GENERATE_LATEX is set to YES.
 
-LATEX_HEADER           = $(SRCDIR)/doc/doxygen/header.tex
+LATEX_HEADER           = doc/doxygen/header.tex
 
 # The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the
 # generated LaTeX document. The footer should contain everything after the last
@@ -1839,7 +1844,7 @@ MAN_SUBDIR             =
 # The default value is: NO.
 # This tag requires that the tag GENERATE_MAN is set to YES.
 
-MAN_LINKS              = NO
+MAN_LINKS              = YES
 
 #---------------------------------------------------------------------------
 # Configuration options related to the XML output
@@ -2383,3 +2388,9 @@ GENERATE_LEGEND        = YES
 # This tag requires that the tag HAVE_DOT is set to YES.
 
 DOT_CLEANUP            = YES
+
+# If the WARN_AS_ERROR tag is set to YES then doxygen will immediately stop when
+# a warning is encountered.
+# The default value is: NO.
+
+WARN_AS_ERROR          = YES
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/Makefile.am
index af9e7a1aa..99d6b2899 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/Makefile.am
@@ -32,7 +32,6 @@ endif
 
 ucx_info_SOURCES  = \
 	build_info.c \
-	cfg_info.c \
 	proto_info.c \
 	sys_info.c \
 	tl_info.c \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/cfg_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/cfg_info.c
deleted file mode 100644
index 49f32f501..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/cfg_info.c
+++ /dev/null
@@ -1,140 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#include "ucx_info.h"
-
-#include <ucs/sys/sys.h>
-#include <ucs/sys/string.h>
-#include <uct/base/uct_md.h>
-#include <string.h>
-
-
-static void print_tl_config(uct_md_h md, const char *tl_name,
-                            ucs_config_print_flags_t print_flags)
-{
-    char cfg_title[UCT_TL_NAME_MAX + 128];
-    uct_iface_config_t *config;
-    ucs_status_t status;
-
-    if (tl_name != NULL) {
-        snprintf(cfg_title, sizeof(cfg_title), "%s transport configuration", tl_name);
-    } else {
-        snprintf(cfg_title, sizeof(cfg_title), "%s client-server transport configuration",
-                 md->component->name);
-    }
-
-    status = uct_md_iface_config_read(md, tl_name, NULL, NULL, &config);
-    if (status != UCS_OK) {
-        printf("# < Failed to read configuration >\n");
-        return;
-    }
-
-    uct_config_print(config, stdout, cfg_title, print_flags);
-    uct_config_release(config);
-}
-
-void print_ucp_config(ucs_config_print_flags_t print_flags)
-{
-    ucp_config_t *config;
-    ucs_status_t status;
-
-    status = ucp_config_read(NULL, NULL, &config);
-    if (status != UCS_OK) {
-        printf("<Failed to read UCP configuraton>\n");
-        return;
-    }
-
-    ucp_config_print(config, stdout, "protocol layer configuration", print_flags);
-    ucp_config_release(config);
-}
-
-void print_uct_config(ucs_config_print_flags_t print_flags, const char *tl_name)
-{
-    uct_md_resource_desc_t *md_resources;
-    unsigned md_rsc_index, num_md_resources;
-    uct_tl_resource_desc_t *tl_resources;
-    unsigned tl_rsc_index, num_tl_resources;
-    char tl_names[UINT8_MAX][UCT_TL_NAME_MAX];
-    unsigned i, num_tls;
-    ucs_status_t status;
-    uct_md_h md;
-    uct_md_config_t *md_config;
-    uct_md_attr_t md_attr;
-
-    status = uct_query_md_resources(&md_resources, &num_md_resources);
-    if (status != UCS_OK) {
-        return;
-    }
-
-    uct_md_component_config_print(print_flags);
-
-    num_tls = 0;
-    for (md_rsc_index = 0; md_rsc_index < num_md_resources; ++md_rsc_index) {
-
-        status = uct_md_config_read(md_resources[md_rsc_index].md_name, NULL,
-                                    NULL, &md_config);
-        if (status != UCS_OK) {
-            continue;
-        }
-
-        status = uct_md_open(md_resources[md_rsc_index].md_name, md_config, &md);
-        uct_config_release(md_config);
-        if (status != UCS_OK) {
-            continue;
-        }
-
-        status = uct_md_query_tl_resources(md, &tl_resources, &num_tl_resources);
-        if (status != UCS_OK) {
-            uct_md_close(md);
-            continue;
-        }
-
-        /* handle the printing of a special case where cannot use tl_resources
-         * since there aren't any */
-        status = uct_md_query(md, &md_attr);
-        if (status != UCS_OK) {
-            uct_release_tl_resource_list(tl_resources);
-            uct_md_close(md);
-            continue;
-        }
-
-        if (md_attr.cap.flags & UCT_MD_FLAG_SOCKADDR) {
-            print_tl_config(md, NULL, print_flags);
-        }
-
-        for (tl_rsc_index = 0; tl_rsc_index < num_tl_resources; ++tl_rsc_index) {
-            i = 0;
-            while (i < num_tls) {
-                if (!strcmp(tl_names[i], tl_resources[tl_rsc_index].tl_name)) {
-                    break;
-                }
-                ++i;
-            }
-
-            /* Make sure this transport is not inserted to the array before, and
-             * if user selects a specific transport - also make sure this is it.
-             */
-            if ((i == num_tls) &&
-                ((tl_name == NULL) || !strcmp(tl_name, tl_resources[tl_rsc_index].tl_name)))
-            {
-                ucs_strncpy_zero(tl_names[num_tls], tl_resources[tl_rsc_index].tl_name,
-                                 UCT_TL_NAME_MAX);
-
-                print_tl_config(md, tl_names[num_tls], print_flags);
-
-                ++num_tls;
-            }
-        }
-
-        uct_release_tl_resource_list(tl_resources);
-        uct_md_close(md);
-    }
-
-    uct_release_md_resource_list(md_resources);
-}
-
-
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/proto_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/proto_info.c
index 29df72a34..658701624 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/proto_info.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/proto_info.c
@@ -89,8 +89,8 @@ static void print_resource_usage(const resource_usage_t *usage_before,
 }
 
 void print_ucp_info(int print_opts, ucs_config_print_flags_t print_flags,
-                    uint64_t features, size_t estimated_num_eps,
-                    unsigned dev_type_bitmap)
+                    uint64_t ctx_features, const ucp_ep_params_t *base_ep_params,
+                    size_t estimated_num_eps, unsigned dev_type_bitmap)
 {
     ucp_config_t *config;
     ucs_status_t status;
@@ -113,7 +113,7 @@ void print_ucp_info(int print_opts, ucs_config_print_flags_t print_flags,
     memset(&params, 0, sizeof(params));
     params.field_mask        = UCP_PARAM_FIELD_FEATURES |
                                UCP_PARAM_FIELD_ESTIMATED_NUM_EPS;
-    params.features          = features;
+    params.features          = ctx_features;
     params.estimated_num_eps = estimated_num_eps;
 
     get_resource_usage(&usage);
@@ -166,8 +166,10 @@ void print_ucp_info(int print_opts, ucs_config_print_flags_t print_flags,
             goto out_destroy_worker;
         }
 
-        ep_params.field_mask = UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
-        ep_params.address    = address;
+        ep_params             = *base_ep_params;
+
+        ep_params.field_mask |= UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
+        ep_params.address     = address;
 
         status = ucp_ep_create(worker, &ep_params, &ep);
         ucp_worker_release_address(worker, address);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/sys_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/sys_info.c
index fa2578ed2..426befb74 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/sys_info.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/sys_info.c
@@ -21,7 +21,11 @@ static const char* cpu_model_names[] = {
     [UCS_CPU_MODEL_INTEL_IVYBRIDGE]   = "IvyBridge",
     [UCS_CPU_MODEL_INTEL_SANDYBRIDGE] = "SandyBridge",
     [UCS_CPU_MODEL_INTEL_NEHALEM]     = "Nehalem",
-    [UCS_CPU_MODEL_INTEL_WESTMERE]    = "Westmere"
+    [UCS_CPU_MODEL_INTEL_WESTMERE]    = "Westmere",
+    [UCS_CPU_MODEL_INTEL_HASWELL]     = "Haswell",
+    [UCS_CPU_MODEL_INTEL_BROADWELL]   = "Broadwell",
+    [UCS_CPU_MODEL_INTEL_SKYLAKE]     = "Skylake",
+    [UCS_CPU_MODEL_ARM_AARCH64]       = "ARM 64-bit"
 };
 
 static double measure_memcpy_bandwidth(size_t size)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/tl_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/tl_info.c
index b0be10144..4e845093d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/tl_info.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/tl_info.c
@@ -17,18 +17,23 @@
 #define PRINT_CAP(_name, _cap_flags, _max) \
     if ((_cap_flags) & (UCT_IFACE_FLAG_##_name)) { \
         char *s = strduplower(#_name); \
-        printf("#         %12s: %s\n", s, size_limit_to_str(0, _max)); \
+        printf("#      %15s: %s\n", s, size_limit_to_str(0, _max)); \
         free(s); \
     }
 
-#define PRINT_ZCAP(_name, _cap_flags, _min, _max, _max_iov) \
-    if ((_cap_flags) & (UCT_IFACE_FLAG_##_name)) { \
+#define PRINT_ZCAP_NO_CHECK(_name, _min, _max, _max_iov) \
+    { \
         char *s = strduplower(#_name); \
-        printf("#         %12s: %s, up to %zu iov\n", s, \
+        printf("#      %15s: %s, up to %zu iov\n", s, \
                size_limit_to_str((_min), (_max)), (_max_iov)); \
         free(s); \
     }
 
+#define PRINT_ZCAP(_name, _cap_flags, _min, _max, _max_iov) \
+    if ((_cap_flags) & (UCT_IFACE_FLAG_##_name)) { \
+        PRINT_ZCAP_NO_CHECK(_name, _min, _max, _max_iov) \
+    }
+
 #define PRINT_ATOMIC_CAP(_name, _cap_flags) \
     if ((_cap_flags) & (UCT_IFACE_FLAG_##_name##32 | UCT_IFACE_FLAG_##_name##64)) { \
         char *s = strduplower(#_name); \
@@ -41,14 +46,22 @@
         if (ucs_test_all_flags(_cap_flags, \
                                UCT_IFACE_FLAG_##_name##32 | UCT_IFACE_FLAG_##_name##64)) \
         { \
-            printf("#         %12s: 32, 64 bit%s\n", s, domain); \
+            printf("#         %12s: 32, 64 bit%s (deprecated)\n", s, domain); \
         } else { \
-            printf("#         %12s: %d bit%s\n", s, \
+            printf("#         %12s: %d bit%s (deprecated)\n", s, \
                    ((_cap_flags) & UCT_IFACE_FLAG_##_name##32) ? 32 : 64, domain); \
         } \
         free(s); \
     }
 
+#define PRINT_ATOMIC_POST(_name, _cap)                   \
+    print_atomic_info(UCT_ATOMIC_OP_##_name, #_name, "", \
+                      _cap.atomic32.op_flags, _cap.atomic32.op_flags);
+
+#define PRINT_ATOMIC_FETCH(_name, _cap, _suffix) \
+    print_atomic_info(UCT_ATOMIC_OP_##_name, #_name, _suffix, \
+                      _cap.atomic32.fop_flags, _cap.atomic32.fop_flags);
+
 static char *strduplower(const char *str)
 {
     char *s, *p;
@@ -60,6 +73,27 @@ static char *strduplower(const char *str)
     return s;
 }
 
+static void print_atomic_info(uct_atomic_op_t opcode, const char *name,
+                              const char *suffix, uint64_t op32, uint64_t op64)
+{
+    char amo[256] = "atomic_";
+    char *s;
+
+    if ((op32 & UCS_BIT(opcode)) || (op64 & UCS_BIT(opcode))) {
+        s = strduplower(name);
+        strncat(amo, suffix, sizeof(amo) - strlen(amo) - 1);
+        strncat(amo, s, sizeof(amo) - strlen(amo) - 1);
+        free(s);
+
+        if ((op32 & UCS_BIT(opcode)) && (op64 & UCS_BIT(opcode))) {
+            printf("#         %12s: 32, 64 bit\n", amo);
+        } else {
+            printf("#         %12s: %d bit\n", amo,
+                   (op32 & UCS_BIT(opcode)) ? 32 : 64);
+        }
+    }
+}
+
 static const char *size_limit_to_str(size_t min_size, size_t max_size)
 {
     static char buf[128];
@@ -139,46 +173,90 @@ static void print_iface_info(uct_worker_h worker, uct_md_h md,
         PRINT_ZCAP(PUT_ZCOPY, iface_attr.cap.flags, iface_attr.cap.put.min_zcopy,
                    iface_attr.cap.put.max_zcopy, iface_attr.cap.put.max_iov);
 
-        if((iface_attr.cap.flags) & (UCT_IFACE_FLAG_PUT_ZCOPY)) {
+        if (iface_attr.cap.flags & UCT_IFACE_FLAG_PUT_ZCOPY) {
             printf("#  put_opt_zcopy_align: %s\n",
                    size_limit_to_str(0, iface_attr.cap.put.opt_zcopy_align));
             printf("#        put_align_mtu: %s\n",
                    size_limit_to_str(0, iface_attr.cap.put.align_mtu));
         }
-        if((iface_attr.cap.flags) & (UCT_IFACE_FLAG_GET_ZCOPY)) {
-           printf("#  get_opt_zcopy_align: %s\n",
-                  size_limit_to_str(0, iface_attr.cap.get.opt_zcopy_align));
-           printf("#        get_align_mtu: %s\n",
-                  size_limit_to_str(0, iface_attr.cap.get.align_mtu));
-        }
-        if((iface_attr.cap.flags) & (UCT_IFACE_FLAG_AM_ZCOPY)) {
-           printf("#   am_opt_zcopy_align: %s\n",
-                  size_limit_to_str(0, iface_attr.cap.am.opt_zcopy_align));
-           printf("#         am_align_mtu: %s\n",
-                  size_limit_to_str(0, iface_attr.cap.am.align_mtu));
-        }
 
+        PRINT_CAP(GET_SHORT, iface_attr.cap.flags, iface_attr.cap.get.max_short);
         PRINT_CAP(GET_BCOPY, iface_attr.cap.flags, iface_attr.cap.get.max_bcopy);
         PRINT_ZCAP(GET_ZCOPY, iface_attr.cap.flags, iface_attr.cap.get.min_zcopy,
                    iface_attr.cap.get.max_zcopy, iface_attr.cap.get.max_iov);
+        if (iface_attr.cap.flags & UCT_IFACE_FLAG_GET_ZCOPY) {
+            printf("#  get_opt_zcopy_align: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.get.opt_zcopy_align));
+            printf("#        get_align_mtu: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.get.align_mtu));
+        }
+
         PRINT_CAP(AM_SHORT,  iface_attr.cap.flags, iface_attr.cap.am.max_short);
         PRINT_CAP(AM_BCOPY,  iface_attr.cap.flags, iface_attr.cap.am.max_bcopy);
         PRINT_ZCAP(AM_ZCOPY,  iface_attr.cap.flags, iface_attr.cap.am.min_zcopy,
                    iface_attr.cap.am.max_zcopy, iface_attr.cap.am.max_iov);
         if (iface_attr.cap.flags & UCT_IFACE_FLAG_AM_ZCOPY) {
+            printf("#   am_opt_zcopy_align: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.am.opt_zcopy_align));
+            printf("#         am_align_mtu: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.am.align_mtu));
             printf("#            am header: %s\n",
                    size_limit_to_str(0, iface_attr.cap.am.max_hdr));
         }
 
-        PRINT_ATOMIC_CAP(ATOMIC_ADD,   iface_attr.cap.flags);
-        PRINT_ATOMIC_CAP(ATOMIC_FADD,  iface_attr.cap.flags);
-        PRINT_ATOMIC_CAP(ATOMIC_SWAP,  iface_attr.cap.flags);
-        PRINT_ATOMIC_CAP(ATOMIC_CSWAP, iface_attr.cap.flags);
+        PRINT_CAP(TAG_EAGER_SHORT, iface_attr.cap.flags,
+                  iface_attr.cap.tag.eager.max_short);
+        PRINT_CAP(TAG_EAGER_BCOPY, iface_attr.cap.flags,
+                  iface_attr.cap.tag.eager.max_bcopy);
+        PRINT_ZCAP(TAG_EAGER_ZCOPY, iface_attr.cap.flags, 0,
+                   iface_attr.cap.tag.eager.max_zcopy,
+                   iface_attr.cap.tag.eager.max_iov);
+
+        if (iface_attr.cap.flags & UCT_IFACE_FLAG_TAG_RNDV_ZCOPY) {
+            PRINT_ZCAP_NO_CHECK(TAG_RNDV_ZCOPY, 0,
+                                iface_attr.cap.tag.rndv.max_zcopy,
+                                iface_attr.cap.tag.rndv.max_iov);
+            printf("#  rndv private header: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.tag.rndv.max_hdr));
+        }
+
+        if (iface_attr.cap.flags & (UCT_IFACE_FLAG_TAG_EAGER_SHORT |
+                                    UCT_IFACE_FLAG_TAG_EAGER_BCOPY |
+                                    UCT_IFACE_FLAG_TAG_EAGER_ZCOPY |
+                                    UCT_IFACE_FLAG_TAG_RNDV_ZCOPY)) {
+            PRINT_ZCAP_NO_CHECK(TAG_RECV, iface_attr.cap.tag.recv.min_recv,
+                                iface_attr.cap.tag.recv.max_zcopy,
+                                iface_attr.cap.tag.recv.max_iov);
+            printf("#  tag_max_outstanding: %s\n",
+                   size_limit_to_str(0, iface_attr.cap.tag.recv.max_outstanding));
+        }
+
+        if (iface_attr.cap.atomic32.op_flags  ||
+            iface_attr.cap.atomic64.op_flags  ||
+            iface_attr.cap.atomic32.fop_flags ||
+            iface_attr.cap.atomic64.fop_flags) {
+            if (iface_attr.cap.flags & UCT_IFACE_FLAG_ATOMIC_DEVICE) {
+                printf("#               domain: device\n");
+            } else if (iface_attr.cap.flags & UCT_IFACE_FLAG_ATOMIC_CPU) {
+                printf("#               domain: cpu\n");
+            }
+
+            PRINT_ATOMIC_POST(ADD, iface_attr.cap);
+            PRINT_ATOMIC_POST(AND, iface_attr.cap);
+            PRINT_ATOMIC_POST(OR,  iface_attr.cap);
+            PRINT_ATOMIC_POST(XOR, iface_attr.cap);
+
+            PRINT_ATOMIC_FETCH(ADD,   iface_attr.cap, "f");
+            PRINT_ATOMIC_FETCH(AND,   iface_attr.cap, "f");
+            PRINT_ATOMIC_FETCH(OR,    iface_attr.cap, "f");
+            PRINT_ATOMIC_FETCH(XOR,   iface_attr.cap, "f");
+            PRINT_ATOMIC_FETCH(SWAP , iface_attr.cap, "");
+            PRINT_ATOMIC_FETCH(CSWAP, iface_attr.cap, "");
+        }
 
         buf[0] = '\0';
         if (iface_attr.cap.flags & (UCT_IFACE_FLAG_CONNECT_TO_EP |
-                                    UCT_IFACE_FLAG_CONNECT_TO_IFACE))
-        {
+                                    UCT_IFACE_FLAG_CONNECT_TO_IFACE)) {
             if (iface_attr.cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {
                 strncat(buf, " to ep,", sizeof(buf) - 1);
             }
@@ -207,12 +285,11 @@ static void print_iface_info(uct_worker_h worker, uct_md_h md,
                                     UCT_IFACE_FLAG_ERRHANDLE_ZCOPY_BUF   |
                                     UCT_IFACE_FLAG_ERRHANDLE_AM_ID       |
                                     UCT_IFACE_FLAG_ERRHANDLE_REMOTE_MEM  |
-                                    UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE))
-        {
+                                    UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE)) {
+
             if (iface_attr.cap.flags & (UCT_IFACE_FLAG_ERRHANDLE_SHORT_BUF |
                                         UCT_IFACE_FLAG_ERRHANDLE_BCOPY_BUF |
-                                        UCT_IFACE_FLAG_ERRHANDLE_ZCOPY_BUF))
-            {
+                                        UCT_IFACE_FLAG_ERRHANDLE_ZCOPY_BUF)) {
                 strncat(buf, " buffer (", sizeof(buf) - 1);
                 if (iface_attr.cap.flags & UCT_IFACE_FLAG_ERRHANDLE_SHORT_BUF) {
                     strncat(buf, "short,", sizeof(buf) - 1);
@@ -263,7 +340,7 @@ static ucs_status_t print_tl_info(uct_md_h md, const char *tl_name,
     }
 
     /* coverity[alloc_arg] */
-    status = uct_worker_create(&async, UCS_THREAD_MODE_MULTI, &worker);
+    status = uct_worker_create(&async, UCS_THREAD_MODE_SINGLE, &worker);
     if (status != UCS_OK) {
         goto out;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/type_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/type_info.c
index 37fee06bf..cd36d4f3b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/type_info.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/type_info.c
@@ -18,12 +18,13 @@
 #include <ucs/datastruct/pgtable.h>
 #include <ucs/datastruct/ptr_array.h>
 #include <ucs/sys/rcache.h>
+#include <ucs/sys/rcache_int.h>
 #include <ucs/time/timerq.h>
 #include <ucs/time/timer_wheel.h>
 #include <ucs/type/class.h>
 #include <uct/base/uct_md.h>
 #include <uct/base/uct_iface.h>
-#include <uct/sm/self/self_ep.h>
+#include <uct/sm/self/self.h>
 #include <uct/tcp/tcp.h>
 #include <ucp/core/ucp_context.h>
 #include <ucp/core/ucp_ep.h>
@@ -50,7 +51,7 @@
 #  include <uct/ib/dc/base/dc_iface.h>
 #  include <uct/ib/dc/base/dc_ep.h>
 #  include <uct/ib/dc/verbs/dc_verbs.h>
-#  if HAVE_MLX5_HW
+#  if HAVE_MLX5_HW_DC
 #    include <uct/ib/dc/accel/dc_mlx5.h>
 #  endif
 #endif
@@ -58,7 +59,7 @@
 #if HAVE_TL_UD
 #  include <uct/ib/ud/base/ud_def.h>
 #  include <uct/ib/ud/verbs/ud_verbs.h>
-#  if HAVE_MLX5_HW
+#  if HAVE_MLX5_HW_UD
 #    include <uct/ib/ud/accel/ud_mlx5.h>
 #  endif
 #endif
@@ -216,7 +217,7 @@ void print_type_info(const char * tl_name)
             PRINT_SIZE(uct_dc_verbs_iface_t);
         }
 
-#if HAVE_MLX5_HW
+#if HAVE_MLX5_HW_DC
         if (tl_name == NULL || !strcasecmp(tl_name, "dc_mlx5")) {
             PRINT_SIZE(uct_dc_mlx5_ep_t);
             PRINT_SIZE(uct_dc_mlx5_grh_ep_t);
@@ -247,7 +248,7 @@ void print_type_info(const char * tl_name)
             PRINT_SIZE(uct_ud_verbs_iface_t);
         }
 
-#if HAVE_MLX5_HW
+#if HAVE_MLX5_HW_UD
         if (tl_name == NULL || !strcasecmp(tl_name, "ud_mlx5")) {
             PRINT_SIZE(uct_ud_mlx5_ep_t);
             PRINT_SIZE(uct_ud_mlx5_iface_t);
@@ -275,9 +276,13 @@ void print_type_info(const char * tl_name)
     PRINT_SIZE(ucp_context_t);
     PRINT_SIZE(ucp_worker_t);
     PRINT_SIZE(ucp_ep_t);
+    PRINT_SIZE(ucp_ep_ext_gen_t);
+    PRINT_SIZE(ucp_ep_ext_proto_t);
+    PRINT_SIZE(ucp_ep_match_entry_t);
     PRINT_SIZE(ucp_ep_config_key_t);
     PRINT_SIZE(ucp_ep_config_t);
     PRINT_SIZE(ucp_request_t);
+    PRINT_SIZE(ucp_recv_desc_t);
     PRINT_SIZE(ucp_tag_recv_info_t);
     PRINT_SIZE(ucp_mem_t);
     PRINT_SIZE(ucp_rkey_t);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.c
index 2499be719..e467a6bcd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.c
@@ -16,36 +16,42 @@
 
 static void usage() {
     printf("Usage: ucx_info [options]\n");
-    printf("Options are:\n");
-    printf("  -v         Version\n");
-    printf("  -s         System\n");
-    printf("  -d         Devices\n");
-    printf("  -c         Configuration\n");
-    printf("  -p         UCP context\n");
-    printf("  -w         UCP worker\n");
-    printf("  -e         UCP endpoint\n");
-    printf("  -u         UCP features to use. String of one or more of:\n");
-    printf("                'a' : atomic operations\n");
-    printf("                'r' : remote memory access\n");
-    printf("                't' : tag matching \n");
-    printf("                'w' : wakeup\n");
-    printf("  -D <type>  Set which device types to use\n");
-    printf("                'all'  : all possible devices (default)\n");
-    printf("                'shm'  : shared memory devices only\n");
-    printf("                'net'  : network devices only\n");
-    printf("                'self' : self transport only\n");
-    printf("  -n         Estimated UCP endpoint count (for ucp_init)\n");
-    printf("  -a         Show also hidden configuration\n");
-    printf("  -b         Build configuration\n");
-    printf("  -y         Type information\n");
-    printf("  -f         Fully decorated output\n");
-    printf("  -t <name>  Print information for a specific transport\n");
+    printf("At least one of the following options has to be set:\n");
+    printf("  -v              Show version information\n");
+    printf("  -d              Show devices and transports\n");
+    printf("  -b              Show build configuration\n");
+    printf("  -y              Show type and structures information\n");
+    printf("  -s              Show system information\n");
+    printf("  -c              Show UCX configuration\n");
+    printf("  -a              Show also hidden configuration\n");
+    printf("  -f              Display fully decorated output\n");
+    printf("\nUCP information (-u is required):\n");
+    printf("  -p              Show UCP context information\n");
+    printf("  -w              Show UCP worker information\n");
+    printf("  -e              Show UCP endpoint configuration\n");
+    printf("  -u <features>   UCP context features to use. String of one or more of:\n");
+    printf("                    'a' : atomic operations\n");
+    printf("                    'r' : remote memory access\n");
+    printf("                    't' : tag matching \n");
+    printf("                    'w' : wakeup\n");
+    printf("                  Modifiers to use in combination with above features:\n");
+    printf("                    'e' : error handling\n");
+    printf("\nOther settings:\n");
+    printf("  -t <name>       Filter devices information using specified transport (requires -d)\n");
+    printf("  -n <count>      Estimated UCP endpoint count (for ucp_init)\n");
+    printf("  -D <type>       Set which device types to use when creating UCP context:\n");
+    printf("                    'all'  : all possible devices (default)\n");
+    printf("                    'shm'  : shared memory devices only\n");
+    printf("                    'net'  : network devices only\n");
+    printf("                    'self' : self transport only\n");
+    printf("  -h              Show this help message\n");
     printf("\n");
 }
 
 int main(int argc, char **argv)
 {
     ucs_config_print_flags_t print_flags;
+    ucp_ep_params_t ucp_ep_params;
     unsigned dev_type_bitmap;
     uint64_t ucp_features;
     size_t ucp_num_eps;
@@ -54,12 +60,13 @@ int main(int argc, char **argv)
     const char *f;
     int c;
 
-    print_opts       = 0;
-    print_flags      = 0;
-    tl_name          = NULL;
-    ucp_features     = 0;
-    ucp_num_eps      = 1;
-    dev_type_bitmap  = -1;
+    print_opts               = 0;
+    print_flags              = 0;
+    tl_name                  = NULL;
+    ucp_features             = 0;
+    ucp_num_eps              = 1;
+    dev_type_bitmap          = -1;
+    ucp_ep_params.field_mask = 0;
     while ((c = getopt(argc, argv, "fahvcydbswpet:n:u:D:")) != -1) {
         switch (c) {
         case 'f':
@@ -116,6 +123,10 @@ int main(int argc, char **argv)
                 case 'w':
                     ucp_features |= UCP_FEATURE_WAKEUP;
                     break;
+                case 'e':
+                    ucp_ep_params.field_mask |= UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
+                    ucp_ep_params.err_mode    = UCP_ERR_HANDLING_MODE_PEER;
+                    break;
                 default:
                     usage();
                     return -1;
@@ -137,6 +148,8 @@ int main(int argc, char **argv)
             }
             break;
         case 'h':
+            usage();
+            return 0;
         default:
             usage();
             return -1;
@@ -165,10 +178,7 @@ int main(int argc, char **argv)
     }
 
     if (print_flags & UCS_CONFIG_PRINT_CONFIG) {
-        ucs_global_opts_print(stdout, print_flags);
-        print_ucp_config(print_flags);
-        print_uct_config(print_flags, tl_name);
-        ucm_config_print(stdout, print_flags);
+        ucs_config_parser_print_all_opts(stdout, print_flags);
     }
 
     if (print_opts & PRINT_DEVICES) {
@@ -177,11 +187,12 @@ int main(int argc, char **argv)
 
     if (print_opts & (PRINT_UCP_CONTEXT|PRINT_UCP_WORKER|PRINT_UCP_EP)) {
         if (ucp_features == 0) {
-            printf("Please select UCP features using -u switch\n");
+            printf("Please select UCP features using -u switch: a|r|t|w\n");
+            usage();
             return -1;
         }
-        print_ucp_info(print_opts, print_flags, ucp_features, ucp_num_eps,
-                       dev_type_bitmap);
+        print_ucp_info(print_opts, print_flags, ucp_features, &ucp_ep_params,
+                       ucp_num_eps, dev_type_bitmap);
     }
 
     return 0;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.h b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.h
index 313a98220..fc51f9b37 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/info/ucx_info.h
@@ -24,10 +24,6 @@ enum {
 };
 
 
-void print_ucp_config(ucs_config_print_flags_t print_flags);
-
-void print_uct_config(ucs_config_print_flags_t print_flags, const char *tl_name);
-
 void print_version();
 
 void print_sys_info();
@@ -40,15 +36,7 @@ void print_uct_info(int print_opts, ucs_config_print_flags_t print_flags,
 void print_type_info(const char * tl_name);
 
 void print_ucp_info(int print_opts, ucs_config_print_flags_t print_flags,
-                    uint64_t features, size_t estimated_num_eps,
-                    unsigned dev_type_bitmap);
-
-/**
- * @ingroup RESOURCE
- * @brief Print MD component configuration to a stream.
- *
- * @param [in]  print_flags   Controls how the configuration is printed.
- */
-void uct_md_component_config_print(ucs_config_print_flags_t print_flags);
+                    uint64_t ctx_features, const ucp_ep_params_t *base_ep_params,
+                    size_t estimated_num_eps, unsigned dev_type_bitmap);
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/Makefile.am
index e0e6633c2..e8607c3aa 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/Makefile.am
@@ -14,6 +14,9 @@ else
 ucx_perftest_CC = $(CC)
 endif
 
+# override CXXFLAGS to link C++ code with C linker
+CXXFLAGS += -nostdlib -fno-exceptions -fno-rtti
+
 perftestdir = $(pkgdatadir)/perftest
 dist_perftest_DATA = \
 	$(top_srcdir)/contrib/ucx_perftest_config/msg_pow2 \
@@ -43,7 +46,6 @@ libucxperf_la_LDFLAGS = \
 	$(RTE_LDFLAGS) \
 	$(OPENMP_CFLAGS)
 libucxperf_la_CXXFLAGS = \
-	-nostdlib -fno-exceptions -fno-rtti \
 	$(BASE_CXXFLAGS) \
 	$(OPENMP_CFLAGS)
 libucxperf_la_CFLAGS = \
@@ -56,16 +58,19 @@ libucxperf_la_LIBADD = \
 #libucxperf_la_LINK = $(CXX)
 
 ucx_perftest_SOURCES  = perftest.c
-ucx_perftest_LDFLAGS    = \
-     $(RTE_LDFLAGS) \
-     $(OPENMP_CFLAGS)
-ucx_perftest_CFLAGS   = \
-     $(OPENMP_CFLAGS)
-ucx_perftest_LDADD    = \
-     $(abs_top_builddir)/src/uct/libuct.la \
-     $(abs_top_builddir)/src/ucp/libucp.la \
-     $(abs_top_builddir)/src/ucs/libucs.la \
-     libucxperf.la
+ucx_perftest_LDFLAGS = \
+	$(RTE_LDFLAGS) \
+	$(OPENMP_CFLAGS)
+ucx_perftest_CPPFLAGS = \
+	$(BASE_CPPFLAGS)
+ucx_perftest_CFLAGS = \
+	$(BASE_CFLAGS) \
+	$(OPENMP_CFLAGS)
+ucx_perftest_LDADD = \
+	$(abs_top_builddir)/src/uct/libuct.la \
+	$(abs_top_builddir)/src/ucp/libucp.la \
+	$(abs_top_builddir)/src/ucs/libucs.la \
+	libucxperf.la
 ucx_perftest_SOURCE_FILES = \
 	$(patsubst %, $(srcdir)/%, $(ucx_perftest_SOURCES))
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.c
index 2214d4fa6..a98eca39a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.c
@@ -10,10 +10,29 @@
 #include "libperf_int.h"
 
 #include <ucs/debug/log.h>
+#include <ucs/arch/bitops.h>
 #include <string.h>
 #include <malloc.h>
 #include <unistd.h>
 
+#define ATOMIC_OP_CONFIG(_size, _op32, _op64, _op, _msg, _params, _status)        \
+    _status = __get_atomic_flag((_size), (_op32), (_op64), (_op));                \
+    if (_status != UCS_OK) {                                                      \
+        ucs_error("%s/%s does not support atomic %s for message size %zu bytes",  \
+                  (_params)->uct.tl_name, (_params)->uct.dev_name,                \
+                  (_msg)[_op], (_size));                                          \
+        return _status;                                                           \
+    }
+
+#define ATOMIC_OP_CHECK(_size, _attr, _required, _params, _msg)                   \
+    if (!ucs_test_all_flags(_attr, _required)) {                                  \
+        if ((_params)->flags & UCX_PERF_TEST_FLAG_VERBOSE) {                      \
+            ucs_error("%s/%s does not support required "#_size"-bit atomic: %s",  \
+                      (_params)->uct.tl_name, (_params)->uct.dev_name,            \
+                      (_msg)[ucs_ffs64(~(_attr) & (_required))]);                 \
+        }                                                                         \
+        return UCS_ERR_UNSUPPORTED;                                               \
+    }
 
 typedef struct {
     union {
@@ -30,6 +49,47 @@ typedef struct {
     unsigned long      recv_buffer;
 } ucx_perf_ep_info_t;
 
+static const char *perf_iface_ops[] = {
+    [ucs_ilog2(UCT_IFACE_FLAG_AM_SHORT)]         = "am short",
+    [ucs_ilog2(UCT_IFACE_FLAG_AM_BCOPY)]         = "am bcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_AM_ZCOPY)]         = "am zcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_PUT_SHORT)]        = "put short",
+    [ucs_ilog2(UCT_IFACE_FLAG_PUT_BCOPY)]        = "put bcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_PUT_ZCOPY)]        = "put zcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_GET_SHORT)]        = "get short",
+    [ucs_ilog2(UCT_IFACE_FLAG_GET_BCOPY)]        = "get bcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_GET_ZCOPY)]        = "get zcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE)] = "peer failure handler",
+    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_IFACE)] = "connect to iface",
+    [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_EP)]    = "connect to ep",
+    [ucs_ilog2(UCT_IFACE_FLAG_AM_DUP)]           = "full reliability",
+    [ucs_ilog2(UCT_IFACE_FLAG_CB_SYNC)]          = "sync callback",
+    [ucs_ilog2(UCT_IFACE_FLAG_CB_ASYNC)]         = "async callback",
+    [ucs_ilog2(UCT_IFACE_FLAG_EVENT_SEND_COMP)]  = "send completion event",
+    [ucs_ilog2(UCT_IFACE_FLAG_EVENT_RECV)]       = "tag or active message event",
+    [ucs_ilog2(UCT_IFACE_FLAG_EVENT_RECV_SIG)]   = "signaled message event",
+    [ucs_ilog2(UCT_IFACE_FLAG_PENDING)]          = "pending",
+    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_SHORT)]  = "tag eager short",
+    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_BCOPY)]  = "tag eager bcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_TAG_EAGER_ZCOPY)]  = "tag eager zcopy",
+    [ucs_ilog2(UCT_IFACE_FLAG_TAG_RNDV_ZCOPY)]   = "tag rndv zcopy"
+};
+
+static const char *perf_atomic_op[] = {
+     [UCT_ATOMIC_OP_ADD]   = "add",
+     [UCT_ATOMIC_OP_AND]   = "and",
+     [UCT_ATOMIC_OP_OR]    = "or" ,
+     [UCT_ATOMIC_OP_XOR]   = "xor"
+};
+
+static const char *perf_atomic_fop[] = {
+     [UCT_ATOMIC_OP_ADD]   = "fetch-add",
+     [UCT_ATOMIC_OP_AND]   = "fetch-and",
+     [UCT_ATOMIC_OP_OR]    = "fetch-or",
+     [UCT_ATOMIC_OP_XOR]   = "fetch-xor",
+     [UCT_ATOMIC_OP_SWAP]  = "swap",
+     [UCT_ATOMIC_OP_CSWAP] = "cswap"
+};
 
 /*
  *  This Quickselect routine is based on the algorithm described in
@@ -314,11 +374,17 @@ static inline uint64_t __get_flag(uct_perf_data_layout_t layout, uint64_t short_
            0;
 }
 
-static inline uint64_t __get_atomic_flag(size_t size, uint64_t flag32, uint64_t flag64)
+static inline ucs_status_t __get_atomic_flag(size_t size, uint64_t *op32,
+                                             uint64_t *op64, uint64_t op)
 {
-    return (size == 4) ? flag32 :
-           (size == 8) ? flag64 :
-           0;
+    if (size == sizeof(uint32_t)) {
+        *op32 = UCS_BIT(op);
+        return UCS_OK;
+    } else if (size == sizeof(uint64_t)) {
+        *op64 = UCS_BIT(op);
+        return UCS_OK;
+    }
+    return UCS_ERR_UNSUPPORTED;
 }
 
 static inline size_t __get_max_size(uct_perf_data_layout_t layout, size_t short_m,
@@ -333,9 +399,13 @@ static inline size_t __get_max_size(uct_perf_data_layout_t layout, size_t short_
 static ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,
                                                      uct_iface_h iface)
 {
+    uint64_t required_flags = 0;
+    uint64_t atomic_op32    = 0;
+    uint64_t atomic_op64    = 0;
+    uint64_t atomic_fop32   = 0;
+    uint64_t atomic_fop64   = 0;
     uct_iface_attr_t attr;
     ucs_status_t status;
-    uint64_t required_flags;
     size_t min_size, max_size, max_iov, message_size;
 
     status = uct_iface_query(iface, &attr);
@@ -367,32 +437,32 @@ static ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,
         max_iov  = attr.cap.put.max_iov;
         break;
     case UCX_PERF_CMD_GET:
-        required_flags = __get_flag(params->uct.data_layout, 0,
+        required_flags = __get_flag(params->uct.data_layout, UCT_IFACE_FLAG_GET_SHORT,
                                     UCT_IFACE_FLAG_GET_BCOPY, UCT_IFACE_FLAG_GET_ZCOPY);
         min_size = __get_max_size(params->uct.data_layout, 0, 0,
                                   attr.cap.get.min_zcopy);
-        max_size = __get_max_size(params->uct.data_layout, 0,
+        max_size = __get_max_size(params->uct.data_layout, attr.cap.get.max_short,
                                   attr.cap.get.max_bcopy, attr.cap.get.max_zcopy);
         max_iov  = attr.cap.get.max_iov;
         break;
     case UCX_PERF_CMD_ADD:
-        required_flags = __get_atomic_flag(message_size, UCT_IFACE_FLAG_ATOMIC_ADD32,
-                                           UCT_IFACE_FLAG_ATOMIC_ADD64);
+        ATOMIC_OP_CONFIG(message_size, &atomic_op32, &atomic_op64, UCT_ATOMIC_OP_ADD,
+                         perf_atomic_op, params, status);
         max_size = 8;
         break;
     case UCX_PERF_CMD_FADD:
-        required_flags = __get_atomic_flag(message_size, UCT_IFACE_FLAG_ATOMIC_FADD32,
-                                           UCT_IFACE_FLAG_ATOMIC_FADD64);
+        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_ADD,
+                         perf_atomic_fop, params, status);
         max_size = 8;
         break;
     case UCX_PERF_CMD_SWAP:
-        required_flags = __get_atomic_flag(message_size, UCT_IFACE_FLAG_ATOMIC_SWAP32,
-                                           UCT_IFACE_FLAG_ATOMIC_SWAP64);
+        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_SWAP,
+                         perf_atomic_fop, params, status);
         max_size = 8;
         break;
     case UCX_PERF_CMD_CSWAP:
-        required_flags = __get_atomic_flag(message_size, UCT_IFACE_FLAG_ATOMIC_CSWAP32,
-                                           UCT_IFACE_FLAG_ATOMIC_CSWAP64);
+        ATOMIC_OP_CONFIG(message_size, &atomic_fop32, &atomic_fop64, UCT_ATOMIC_OP_CSWAP,
+                         perf_atomic_fop, params, status);
         max_size = 8;
         break;
     default:
@@ -407,23 +477,35 @@ static ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,
         return status;
     }
 
-    if (!ucs_test_all_flags(attr.cap.flags, required_flags) || !required_flags) {
+    /* check atomics first */
+    ATOMIC_OP_CHECK(32, attr.cap.atomic32.op_flags, atomic_op32, params, perf_atomic_op);
+    ATOMIC_OP_CHECK(64, attr.cap.atomic64.op_flags, atomic_op64, params, perf_atomic_op);
+    ATOMIC_OP_CHECK(32, attr.cap.atomic32.fop_flags, atomic_fop32, params, perf_atomic_fop);
+    ATOMIC_OP_CHECK(64, attr.cap.atomic64.fop_flags, atomic_fop64, params, perf_atomic_fop);
+
+    /* check iface flags */
+    if (!(atomic_op32 | atomic_op64 | atomic_fop32 | atomic_fop64) &&
+        (!ucs_test_all_flags(attr.cap.flags, required_flags) || !required_flags)) {
         if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-            ucs_error("Device does not support required operation");
+            ucs_error("%s/%s does not support operation %s",
+                      params->uct.tl_name, params->uct.dev_name,
+                      perf_iface_ops[ucs_ffs64(~attr.cap.flags & required_flags)]);
         }
         return UCS_ERR_UNSUPPORTED;
     }
 
     if (message_size < min_size) {
         if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-            ucs_error("Message size too small");
+            ucs_error("Message size (%zu) is smaller than min supported (%zu)",
+                      message_size, min_size);
         }
         return UCS_ERR_UNSUPPORTED;
     }
 
     if (message_size > max_size) {
         if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-            ucs_error("Message size too big");
+            ucs_error("Message size (%zu) is larger than max supported (%zu)",
+                      message_size, max_size);
         }
         return UCS_ERR_UNSUPPORTED;
     }
@@ -439,25 +521,27 @@ static ucs_status_t uct_perf_test_check_capabilities(ucx_perf_params_t *params,
         }
 
         if ((params->uct.data_layout == UCT_PERF_DATA_LAYOUT_ZCOPY) &&
-                        (params->am_hdr_size > attr.cap.am.max_hdr))
+            (params->am_hdr_size > attr.cap.am.max_hdr))
         {
             if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-                ucs_error("AM header size too big");
+                ucs_error("AM header size (%zu) is larger than max supported (%zu)",
+                          params->am_hdr_size, attr.cap.am.max_hdr);
             }
             return UCS_ERR_UNSUPPORTED;
         }
 
         if (params->am_hdr_size > message_size) {
             if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-                ucs_error("AM header size larger than message size");
+                ucs_error("AM header size (%zu) is larger than message size (%zu)",
+                          params->am_hdr_size, message_size);
             }
             return UCS_ERR_INVALID_PARAM;
         }
 
         if (params->uct.fc_window > UCT_PERF_TEST_MAX_FC_WINDOW) {
             if (params->flags & UCX_PERF_TEST_FLAG_VERBOSE) {
-                ucs_error("AM flow-control window too large (should be <= %d)",
-                          UCT_PERF_TEST_MAX_FC_WINDOW);
+                ucs_error("AM flow-control window (%d) too large (should be <= %d)",
+                          params->uct.fc_window, UCT_PERF_TEST_MAX_FC_WINDOW);
             }
             return UCS_ERR_INVALID_PARAM;
         }
@@ -560,6 +644,7 @@ static ucs_status_t uct_perf_test_setup_endpoints(ucx_perf_context_t *perf)
     }
 
     if (info.rkey_size > 0) {
+        memset(rkey_buffer, 0, info.rkey_size);
         status = uct_md_mkey_pack(perf->uct.md, perf->uct.recv_mem.memh, rkey_buffer);
         if (status != UCS_OK) {
             ucs_error("Failed to uct_rkey_pack: %s", ucs_status_string(status));
@@ -653,7 +738,7 @@ static ucs_status_t uct_perf_test_setup_endpoints(ucx_perf_context_t *perf)
     uct_perf_iface_flush_b(perf);
 
     free(buffer);
-    rte_call(perf, barrier);
+    uct_perf_barrier(perf);
     return UCS_OK;
 
 err_destroy_eps:
@@ -676,9 +761,9 @@ static void uct_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)
 {
     unsigned group_size, group_index, i;
 
-    rte_call(perf, barrier);
+    uct_perf_barrier(perf);
 
-    uct_iface_set_am_handler(perf->uct.iface, UCT_PERF_TEST_AM_ID, NULL, NULL, UCT_CB_FLAG_SYNC);
+    uct_iface_set_am_handler(perf->uct.iface, UCT_PERF_TEST_AM_ID, NULL, NULL, 0);
 
     group_size  = rte_call(perf, group_size);
     group_index = rte_call(perf, group_index);
@@ -820,6 +905,20 @@ ucp_perf_test_alloc_cuda(void **addr, size_t length)
     return UCS_OK;
 }
 
+static ucs_status_t
+ucp_perf_test_alloc_cuda_managed(void **addr, size_t length)
+{
+#if HAVE_CUDA
+    cudaError_t cerr;
+
+    cerr = cudaMallocManaged(addr, length, cudaMemAttachGlobal);
+    if (cerr != cudaSuccess) {
+        return UCS_ERR_NO_MEMORY;
+    }
+#endif
+    return UCS_OK;
+}
+
 static ucs_status_t
 ucp_perf_test_alloc_contig(ucx_perf_context_t *perf, ucx_perf_params_t *params,
                            void **addr, size_t length, ucp_mem_h *memh,
@@ -830,6 +929,8 @@ ucp_perf_test_alloc_contig(ucx_perf_context_t *perf, ucx_perf_params_t *params,
                                         check_non_blk_flag);
     } else if (perf->params.mem_type == UCT_MD_MEM_TYPE_CUDA) {
         return ucp_perf_test_alloc_cuda(addr, length);
+    } else if (perf->params.mem_type == UCT_MD_MEM_TYPE_CUDA_MANAGED) {
+        return ucp_perf_test_alloc_cuda_managed(addr, length);
     }
 
     return UCS_ERR_UNSUPPORTED;
@@ -837,9 +938,15 @@ ucp_perf_test_alloc_contig(ucx_perf_context_t *perf, ucx_perf_params_t *params,
 
 static void ucp_perf_test_free_contig(ucx_perf_context_t *perf, void *addr, ucp_mem_h memh)
 {
+    ucs_status_t status;
+
     if (perf->params.mem_type == UCT_MD_MEM_TYPE_HOST) {
-        ucp_mem_unmap(perf->ucp.context, memh);
-    } else if (perf->params.mem_type == UCT_MD_MEM_TYPE_CUDA) {
+        status = ucp_mem_unmap(perf->ucp.context, memh);
+        if (status != UCS_OK) {
+            ucs_warn("ucp_mem_unmap() failed: %s", ucs_status_string(status));
+        }
+    } else if ((perf->params.mem_type == UCT_MD_MEM_TYPE_CUDA) ||
+               (perf->params.mem_type == UCT_MD_MEM_TYPE_CUDA_MANAGED)) {
 #if HAVE_CUDA
         cudaFree(addr);
 #endif
@@ -950,7 +1057,7 @@ static ucs_status_t ucp_perf_test_exchange_status(ucx_perf_context_t *perf,
                                                   ucs_status_t status)
 {
     unsigned group_size  = rte_call(perf, group_size);
-    ucs_status_t collective_status = UCS_OK;
+    ucs_status_t collective_status = status;
     struct iovec vec;
     void *req = NULL;
     unsigned i;
@@ -1081,6 +1188,13 @@ static ucs_status_t ucp_perf_test_setup_endpoints(ucx_perf_context_t *perf,
     if (status != UCS_OK) {
         ucp_perf_test_destroy_eps(perf, group_size);
     }
+
+    /* force wireup completion */
+    status = ucp_worker_flush(perf->ucp.worker);
+    if (status != UCS_OK) {
+        ucs_warn("ucp_worker_flush() failed: %s", ucs_status_string(status));
+    }
+
     return status;
 
 err_free_buffer:
@@ -1096,7 +1210,7 @@ static void ucp_perf_test_cleanup_endpoints(ucx_perf_context_t *perf)
 {
     unsigned group_size;
 
-    rte_call(perf, barrier);
+    ucp_perf_barrier(perf);
 
     group_size  = rte_call(perf, group_size);
 
@@ -1167,6 +1281,18 @@ out:
     return status;
 }
 
+void uct_perf_barrier(ucx_perf_context_t *perf)
+{
+    rte_call(perf, barrier, (void(*)(void*))uct_worker_progress,
+             (void*)perf->uct.worker);
+}
+
+void ucp_perf_barrier(ucx_perf_context_t *perf)
+{
+    rte_call(perf, barrier, (void(*)(void*))ucp_worker_progress,
+             (void*)perf->ucp.worker);
+}
+
 static ucs_status_t uct_perf_setup(ucx_perf_context_t *perf, ucx_perf_params_t *params)
 {
     uct_iface_config_t *iface_config;
@@ -1211,6 +1337,8 @@ static ucs_status_t uct_perf_setup(ucx_perf_context_t *perf, ucx_perf_params_t *
     }
 
     status = uct_perf_test_check_capabilities(params, perf->uct.iface);
+    /* sync status across all processes */
+    status = ucp_perf_test_exchange_status(perf, status);
     if (status != UCS_OK) {
         goto out_iface_close;
     }
@@ -1320,7 +1448,7 @@ err:
 static void ucp_perf_cleanup(ucx_perf_context_t *perf)
 {
     ucp_perf_test_cleanup_endpoints(perf);
-    rte_call(perf, barrier);
+    ucp_perf_barrier(perf);
     ucp_perf_test_free_mem(perf);
     ucp_worker_destroy(perf->ucp.worker);
     ucp_cleanup(perf->ucp.context);
@@ -1330,14 +1458,43 @@ static struct {
     ucs_status_t (*setup)(ucx_perf_context_t *perf, ucx_perf_params_t *params);
     void         (*cleanup)(ucx_perf_context_t *perf);
     ucs_status_t (*run)(ucx_perf_context_t *perf);
+    void         (*barrier)(ucx_perf_context_t *perf);
 } ucx_perf_funcs[] = {
-    [UCX_PERF_API_UCT] = {uct_perf_setup, uct_perf_cleanup, uct_perf_test_dispatch},
-    [UCX_PERF_API_UCP] = {ucp_perf_setup, ucp_perf_cleanup, ucp_perf_test_dispatch}
+    [UCX_PERF_API_UCT] = {uct_perf_setup, uct_perf_cleanup,
+                          uct_perf_test_dispatch, uct_perf_barrier},
+    [UCX_PERF_API_UCP] = {ucp_perf_setup, ucp_perf_cleanup,
+                          ucp_perf_test_dispatch, ucp_perf_barrier}
 };
 
 static int ucx_perf_thread_spawn(ucx_perf_context_t *perf,
                                  ucx_perf_result_t* result);
 
+#if HAVE_CUDA
+static ucs_status_t ucx_perf_init_cuda_device(ucx_perf_context_t *perf)
+{
+    cudaError_t cerr;
+    unsigned group_index;
+    int num_gpus;
+    int gpu_index;
+
+    group_index = rte_call(perf, group_index);
+
+    cerr = cudaGetDeviceCount(&num_gpus);
+    if (cerr != cudaSuccess) {
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    gpu_index = group_index % num_gpus;
+
+    cerr = cudaSetDevice(gpu_index);
+    if (cerr != cudaSuccess) {
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    return UCS_OK;
+}
+#endif
+
 ucs_status_t ucx_perf_run(ucx_perf_params_t *params, ucx_perf_result_t *result)
 {
     ucx_perf_context_t *perf;
@@ -1363,6 +1520,16 @@ ucs_status_t ucx_perf_run(ucx_perf_params_t *params, ucx_perf_result_t *result)
 
     ucx_perf_test_reset(perf, params);
 
+#if HAVE_CUDA
+    if ((params->mem_type == UCT_MD_MEM_TYPE_CUDA) ||
+        (params->mem_type == UCT_MD_MEM_TYPE_CUDA_MANAGED)) {
+        status = ucx_perf_init_cuda_device(perf);
+        if (status != UCS_OK) {
+            goto out_free;
+        }
+    }
+#endif
+
     status = ucx_perf_funcs[params->api].setup(perf, params);
     if (status != UCS_OK) {
         goto out_free;
@@ -1376,13 +1543,13 @@ ucs_status_t ucx_perf_run(ucx_perf_params_t *params, ucx_perf_result_t *result)
                 goto out_cleanup;
             }
 
-            rte_call(perf, barrier);
+            ucx_perf_funcs[params->api].barrier(perf);
             ucx_perf_test_reset(perf, params);
         }
 
         /* Run test */
         status = ucx_perf_funcs[params->api].run(perf);
-        rte_call(perf, barrier);
+        ucx_perf_funcs[params->api].barrier(perf);
         if (status == UCS_OK) {
             ucx_perf_calc_result(perf, result);
             rte_call(perf, report, result, perf->params.report_arg, 1);
@@ -1426,7 +1593,7 @@ static void* ucx_perf_thread_run_test(void* arg)
     if (params->warmup_iter > 0) {
         ucx_perf_set_warmup(perf, params);
         statuses[tid] = ucx_perf_funcs[params->api].run(perf);
-        rte_call(perf, barrier);
+        ucx_perf_funcs[params->api].barrier(perf);
         for (i = 0; i < tctx->ntid; i++) {
             if (UCS_OK != statuses[i]) {
                 goto out;
@@ -1439,7 +1606,7 @@ static void* ucx_perf_thread_run_test(void* arg)
     /* Run test */
 #pragma omp barrier
     statuses[tid] = ucx_perf_funcs[params->api].run(perf);
-    rte_call(perf, barrier);
+    ucx_perf_funcs[params->api].barrier(perf);
     for (i = 0; i < tctx->ntid; i++) {
         if (UCS_OK != statuses[i]) {
             goto out;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.h b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.h
index 9c6853b54..be584bd0c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf.h
@@ -78,8 +78,8 @@ typedef enum {
 
 enum ucx_perf_test_flags {
     UCX_PERF_TEST_FLAG_VALIDATE         = UCS_BIT(1), /* Validate data. Affects performance. */
-    UCX_PERF_TEST_FLAG_ONE_SIDED        = UCS_BIT(2), /* For test which involve only one side,
-                                                         the responder would not call progress(). */
+    UCX_PERF_TEST_FLAG_ONE_SIDED        = UCS_BIT(2), /* For tests which involves only one side,
+                                                         the responder should not call progress(). */
     UCX_PERF_TEST_FLAG_MAP_NONBLOCK     = UCS_BIT(3), /* Map memory in non-blocking mode */
     UCX_PERF_TEST_FLAG_TAG_WILDCARD     = UCS_BIT(4), /* For tag tests, use wildcard mask */
     UCX_PERF_TEST_FLAG_TAG_UNEXP_PROBE  = UCS_BIT(5), /* For tag tests, use probe to get unexpected receive */
@@ -128,7 +128,8 @@ typedef struct ucx_perf_rte {
     unsigned   (*group_index)(void *rte_group);
 
     /* Barrier */
-    void        (*barrier)(void *rte_group);
+    void        (*barrier)(void *rte_group, void (*progress)(void *arg),
+                           void *arg);
 
     /* Direct modex */
     void        (*post_vec)(void *rte_group, const struct iovec *iovec,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf_int.h b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf_int.h
index 77a6bf057..c2ffb6542 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf_int.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/libperf_int.h
@@ -114,6 +114,12 @@ ucs_status_t ucp_perf_test_dispatch(ucx_perf_context_t *perf);
 void ucx_perf_calc_result(ucx_perf_context_t *perf, ucx_perf_result_t *result);
 
 
+void uct_perf_barrier(ucx_perf_context_t *perf);
+
+
+void ucp_perf_barrier(ucx_perf_context_t *perf);
+
+
 static UCS_F_ALWAYS_INLINE int ucx_perf_context_done(ucx_perf_context_t *perf)
 {
     return ucs_unlikely((perf->current.iters >= perf->max_iter) ||
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/perftest.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/perftest.c
index ce83507b9..b770f43a3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/perftest.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/perftest.c
@@ -27,6 +27,7 @@
 #include <getopt.h>
 #include <string.h>
 #include <sys/types.h>
+#include <sys/poll.h>
 #include <locale.h>
 #if HAVE_MPI
 #  include <mpi.h>
@@ -76,7 +77,7 @@ struct perftest_context {
     sock_rte_group_t             sock_rte_group;
 };
 
-#define TEST_PARAMS_ARGS   "t:n:s:W:O:w:D:i:H:oSCqMr:T:d:x:A:BUm:"
+#define TEST_PARAMS_ARGS   "t:n:s:W:O:w:D:i:H:oSCqM:r:T:d:x:A:BUm:"
 
 
 test_type_t tests[] = {
@@ -152,36 +153,53 @@ test_type_t tests[] = {
      {NULL}
 };
 
-static int safe_send(int sock, void *data, size_t size)
+static int sock_io(int sock, ssize_t (*sock_call)(int, void *, size_t, int),
+                   int poll_events, void *data, size_t size,
+                   void (*progress)(void *arg), void *arg, const char *name)
 {
     size_t total = 0;
+    struct pollfd pfd;
     int ret;
 
     while (total < size) {
-        ret = send(sock, (char*)data + total, size - total, 0);
-        if (ret < 0) {
-            ucs_error("send() failed: %m");
+        pfd.fd      = sock;
+        pfd.events  = poll_events;
+        pfd.revents = 0;
+
+        ret = poll(&pfd, 1, 1); /* poll for 1ms */
+        if (ret > 0) {
+            ucs_assert(ret == 1);
+            ucs_assert(pfd.revents & poll_events);
+
+            ret = sock_call(sock, (char*)data + total, size - total, 0);
+            if (ret < 0) {
+                ucs_error("%s() failed: %m", name);
+                return -1;
+            }
+            total += ret;
+        } else if ((ret < 0) && (errno != EINTR)) {
+            ucs_error("poll(fd=%d) failed: %m", sock);
             return -1;
         }
-        total += ret;
+
+        /* progress user context */
+        if (progress != NULL) {
+            progress(arg);
+        }
     }
     return 0;
 }
 
-static int safe_recv(int sock, void *data, size_t size)
+static int safe_send(int sock, void *data, size_t size,
+                     void (*progress)(void *arg), void *arg)
 {
-    size_t total = 0;
-    int ret;
+    return sock_io(sock, (void*)send, POLLOUT, data, size, progress, arg, "send");
+}
 
-    while (total < size) {
-        ret = recv(sock, (char*)data + total, size - total, 0);
-        if (ret < 0) {
-            ucs_error("recv() failed: %m");
-            return -1;
-        }
-        total += ret;
-    }
-    return 0;
+static int safe_recv(int sock, void *data, size_t size,
+                     void (*progress)(void *arg), void *arg)
+{
+    return sock_io(sock, recv, POLLIN, data, size, progress, arg, "recv");
 }
 
 static void print_progress(char **test_names, unsigned num_names,
@@ -359,6 +377,7 @@ static void usage(const struct perftest_context *ctx, const char *program)
     printf("                        host - system memory(default)\n");
 #if HAVE_CUDA
     printf("                        cuda - NVIDIA GPU memory\n");
+    printf("                        cuda-managed - NVIDIA cuda managed/unified memory\n");
 #endif
     printf("     -h             show this help message\n");
     printf("\n");
@@ -441,8 +460,8 @@ static ucs_status_t parse_message_sizes_params(const char *optarg,
     }
     ++token_num;
 
-    free(params->msg_size_list); /* free previously allocated buffer */
-    params->msg_size_list = malloc(sizeof(*params->msg_size_list) * token_num);
+    params->msg_size_list = realloc(params->msg_size_list,
+                                    sizeof(*params->msg_size_list) * token_num);
     if (NULL == params->msg_size_list) {
         return UCS_ERR_NO_MEMORY;
     }
@@ -468,6 +487,7 @@ static ucs_status_t parse_message_sizes_params(const char *optarg,
 
 static void init_test_params(ucx_perf_params_t *params)
 {
+    memset(params, 0, sizeof(*params));
     params->api             = UCX_PERF_API_LAST;
     params->command         = UCX_PERF_CMD_LAST;
     params->test_type       = UCX_PERF_TEST_TYPE_LAST;
@@ -624,12 +644,13 @@ static ucs_status_t parse_test_params(ucx_perf_params_t *params, char opt, const
         if (!strcmp(optarg, "host")) {
             params->mem_type = UCT_MD_MEM_TYPE_HOST;
             return UCS_OK;
-        } else if(!strcmp(optarg, "cuda")) {
+        } else if(!strncmp(optarg, "cuda", 4)) {
 #if HAVE_CUDA
-            params->mem_type = UCT_MD_MEM_TYPE_CUDA;
+            params->mem_type = (!strcmp(optarg, "cuda-managed")) ?
+                UCT_MD_MEM_TYPE_CUDA_MANAGED : UCT_MD_MEM_TYPE_CUDA;
             return UCS_OK;
 #else
-            ucs_error("not build with cuda support");
+            ucs_error("not built with cuda support");
             return UCS_ERR_INVALID_PARAM;
 #endif
         }
@@ -639,7 +660,8 @@ static ucs_status_t parse_test_params(ucx_perf_params_t *params, char opt, const
     }
 }
 
-static ucs_status_t read_batch_file(FILE *batch_file, ucx_perf_params_t *params,
+static ucs_status_t read_batch_file(FILE *batch_file, const char *file_name,
+                                    int *line_num, ucx_perf_params_t *params,
                                     char** test_name_p)
 {
 #define MAX_SIZE 256
@@ -655,6 +677,7 @@ static ucs_status_t read_batch_file(FILE *batch_file, ucx_perf_params_t *params,
         if (fgets(buf, sizeof(buf) - 1, batch_file) == NULL) {
             return UCS_ERR_NO_ELEM;
         }
+        ++(*line_num);
 
         argc = 0;
         p = strtok(buf, " \t\n\r");
@@ -665,13 +688,12 @@ static ucs_status_t read_batch_file(FILE *batch_file, ucx_perf_params_t *params,
         argv[argc] = NULL;
     } while ((argc == 0) || (argv[0][0] == '#'));
 
-
     optind = 1;
     while ((c = getopt (argc, argv, TEST_PARAMS_ARGS)) != -1) {
         status = parse_test_params(params, c, optarg);
         if (status != UCS_OK) {
-            ucs_error("Invalid argument in batch file: -%c, status(%d):\"%s\"",
-                      c, status, ucs_status_string(status));
+            ucs_error("in batch file '%s' line %d: -%c %s: %s",
+                      file_name, *line_num, c, optarg, ucs_status_string(status));
             return status;
         }
     }
@@ -703,7 +725,7 @@ static ucs_status_t parse_opts(struct perftest_context *ctx, int mpi_initialized
             break;
         case 'b':
             if (ctx->num_batch_files < MAX_BATCH_FILES) {
-                ctx->batch_files[ctx->num_batch_files++] = strdup(optarg);
+                ctx->batch_files[ctx->num_batch_files++] = optarg;
             }
             break;
         case 'N':
@@ -755,7 +777,8 @@ static unsigned sock_rte_group_index(void *rte_group)
     return group->is_server ? 0 : 1;
 }
 
-static void sock_rte_barrier(void *rte_group)
+static void sock_rte_barrier(void *rte_group, void (*progress)(void *arg),
+                             void *arg)
 {
 #pragma omp master
   {
@@ -764,10 +787,10 @@ static void sock_rte_barrier(void *rte_group)
     unsigned sync;
 
     sync = magic;
-    safe_send(group->connfd, &sync, sizeof(unsigned));
+    safe_send(group->connfd, &sync, sizeof(unsigned), progress, arg);
 
     sync = 0;
-    safe_recv(group->connfd, &sync, sizeof(unsigned));
+    safe_recv(group->connfd, &sync, sizeof(unsigned), progress, arg);
 
     ucs_assert(sync == magic);
   }
@@ -786,9 +809,10 @@ static void sock_rte_post_vec(void *rte_group, const struct iovec *iovec,
         size += iovec[i].iov_len;
     }
 
-    safe_send(group->connfd, &size, sizeof(size));
+    safe_send(group->connfd, &size, sizeof(size), NULL, NULL);
     for (i = 0; i < iovcnt; ++i) {
-        safe_send(group->connfd, iovec[i].iov_base, iovec[i].iov_len);
+        safe_send(group->connfd, iovec[i].iov_base, iovec[i].iov_len, NULL,
+                  NULL);
     }
 }
 
@@ -805,9 +829,9 @@ static void sock_rte_recv(void *rte_group, unsigned src, void *buffer,
     }
 
     ucs_assert_always(src == (1 - group_index));
-    safe_recv(group->connfd, &size, sizeof(size));
+    safe_recv(group->connfd, &size, sizeof(size), NULL, NULL);
     ucs_assert_always(size <= max);
-    safe_recv(group->connfd, buffer, size);
+    safe_recv(group->connfd, buffer, size, NULL, NULL);
 }
 
 static void sock_rte_report(void *rte_group, const ucx_perf_result_t *result,
@@ -882,7 +906,7 @@ static ucs_status_t setup_sock_rte(struct perftest_context *ctx)
         }
 
         close(sockfd);
-        safe_recv(connfd, &ctx->params, sizeof(ctx->params));
+        safe_recv(connfd, &ctx->params, sizeof(ctx->params), NULL, NULL);
         if (ctx->params.msg_size_cnt) {
             ctx->params.msg_size_list = malloc(sizeof(*ctx->params.msg_size_list) *
                                                ctx->params.msg_size_cnt);
@@ -891,7 +915,8 @@ static ucs_status_t setup_sock_rte(struct perftest_context *ctx)
                 goto err_close_connfd;
             }
             safe_recv(connfd, ctx->params.msg_size_list,
-                      sizeof(*ctx->params.msg_size_list) * ctx->params.msg_size_cnt);
+                      sizeof(*ctx->params.msg_size_list) * ctx->params.msg_size_cnt,
+                      NULL, NULL);
         }
 
         ctx->sock_rte_group.connfd    = connfd;
@@ -918,10 +943,11 @@ static ucs_status_t setup_sock_rte(struct perftest_context *ctx)
             goto err_close_sockfd;
         }
 
-        safe_send(sockfd, &ctx->params, sizeof(ctx->params));
+        safe_send(sockfd, &ctx->params, sizeof(ctx->params), NULL, NULL);
         if (ctx->params.msg_size_cnt) {
             safe_send(sockfd, ctx->params.msg_size_list,
-                      sizeof(*ctx->params.msg_size_list) * ctx->params.msg_size_cnt);
+                      sizeof(*ctx->params.msg_size_list) * ctx->params.msg_size_cnt,
+                      NULL, NULL);
         }
 
         ctx->sock_rte_group.connfd    = sockfd;
@@ -969,10 +995,67 @@ static unsigned mpi_rte_group_index(void *rte_group)
     return rank;
 }
 
-static void mpi_rte_barrier(void *rte_group)
+static void mpi_rte_barrier(void *rte_group, void (*progress)(void *arg),
+                            void *arg)
 {
+    int group_size, my_rank, i;
+    MPI_Request *reqs;
+    int nreqs = 0;
+    int dummy;
+    int flag;
+
 #pragma omp master
-    MPI_Barrier(MPI_COMM_WORLD);
+
+    /*
+     * Naive non-blocking barrier implementation over send/recv, to call user
+     * progress while waiting for completion.
+     * Not using MPI_Ibarrier to be compatible with MPI-1.
+     */
+
+    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
+    MPI_Comm_size(MPI_COMM_WORLD, &group_size);
+
+    /* allocate maximal possible number of requests */
+    reqs = (MPI_Request*)alloca(sizeof(*reqs) * group_size);
+
+    if (my_rank == 0) {
+        /* root gathers "ping" from all other ranks */
+        for (i = 1; i < group_size; ++i) {
+            MPI_Irecv(&dummy, 0, MPI_INT,
+                      i /* source */,
+                      1 /* tag */,
+                      MPI_COMM_WORLD,
+                      &reqs[nreqs++]);
+        }
+    } else {
+        /* every non-root rank sends "ping" and waits for "pong" */
+        MPI_Send(&dummy, 0, MPI_INT,
+                 0 /* dest */,
+                 1 /* tag */,
+                 MPI_COMM_WORLD);
+        MPI_Irecv(&dummy, 0, MPI_INT,
+                  0 /* source */,
+                  2 /* tag */,
+                  MPI_COMM_WORLD,
+                  &reqs[nreqs++]);
+    }
+
+    /* Waiting for receive requests */
+    do {
+        MPI_Testall(nreqs, reqs, &flag, MPI_STATUSES_IGNORE);
+        progress(arg);
+    } while (!flag);
+
+    if (my_rank == 0) {
+        /* root sends "pong" to all ranks */
+        for (i = 1; i < group_size; ++i) {
+            MPI_Send(&dummy, 0, MPI_INT,
+                     i /* dest */,
+                     2 /* tag */,
+                     MPI_COMM_WORLD);
+       }
+    }
+
 #pragma omp barrier
 }
 
@@ -1054,7 +1137,8 @@ static unsigned ext_rte_group_index(void *rte_group)
     return rte_group_rank(group);
 }
 
-static void ext_rte_barrier(void *rte_group)
+static void ext_rte_barrier(void *rte_group, void (*progress)(void *arg),
+                            void *arg)
 {
 #pragma omp master
   {
@@ -1255,6 +1339,16 @@ static ucs_status_t check_system(struct perftest_context *ctx)
     return UCS_OK;
 }
 
+static void clone_params(ucx_perf_params_t *dest, const ucx_perf_params_t *src)
+{
+    size_t msg_size_list_size;
+
+    *dest               = *src;
+    msg_size_list_size  = dest->msg_size_cnt * sizeof(*dest->msg_size_list);
+    dest->msg_size_list = malloc(msg_size_list_size);
+    memcpy(dest->msg_size_list, src->msg_size_list, msg_size_list_size);
+}
+
 static ucs_status_t run_test_recurs(struct perftest_context *ctx,
                                     ucx_perf_params_t *parent_params,
                                     unsigned depth)
@@ -1263,6 +1357,7 @@ static ucs_status_t run_test_recurs(struct perftest_context *ctx,
     ucx_perf_result_t result;
     ucs_status_t status;
     FILE *batch_file;
+    int line_num;
 
     ucs_trace_func("depth=%u, num_files=%u", depth, ctx->num_batch_files);
 
@@ -1277,17 +1372,19 @@ static ucs_status_t run_test_recurs(struct perftest_context *ctx,
         return UCS_ERR_IO_ERROR;
     }
 
-    params = *parent_params;
-    while ((status = read_batch_file(batch_file, &params, &ctx->test_names[depth])) == UCS_OK) {
+    clone_params(&params, parent_params);
+    line_num = 0;
+    while ((status = read_batch_file(batch_file, ctx->batch_files[depth],
+                                     &line_num, &params,
+                                     &ctx->test_names[depth])) == UCS_OK) {
         status = run_test_recurs(ctx, &params, depth + 1);
+        free(params.msg_size_list);
         free(ctx->test_names[depth]);
-        if ((NULL == parent_params->msg_size_list) &&
-            (NULL != params.msg_size_list)) {
-            free(params.msg_size_list);
-            params.msg_size_list = NULL;
-        }
-        params = *parent_params;
+        ctx->test_names[depth] = NULL;
+
+        clone_params(&params, parent_params);
     }
+    free(params.msg_size_list);
 
     fclose(batch_file);
     return UCS_OK;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/ucp_tests.cc b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/ucp_tests.cc
index 7c06af6f2..c048dedcd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/ucp_tests.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/ucp_tests.cc
@@ -80,7 +80,9 @@ public:
     }
 
     void UCS_F_ALWAYS_INLINE progress_responder() {
-        if (!(FLAGS & UCX_PERF_TEST_FLAG_ONE_SIDED)) {
+        if (!(FLAGS & UCX_PERF_TEST_FLAG_ONE_SIDED) &&
+            !(m_perf.params.flags & UCX_PERF_TEST_FLAG_ONE_SIDED))
+        {
             ucp_worker_progress(m_perf.ucp.worker);
         }
     }
@@ -293,13 +295,14 @@ public:
 
 	if (m_perf.params.mem_type == UCT_MD_MEM_TYPE_HOST) {
             *((volatile uint8_t*)m_perf.recv_buffer + length - 1) = -1;
-	} else if (m_perf.params.mem_type == UCT_MD_MEM_TYPE_CUDA) {
+	} else if ((m_perf.params.mem_type == UCT_MD_MEM_TYPE_CUDA) ||
+                   (m_perf.params.mem_type == UCT_MD_MEM_TYPE_CUDA_MANAGED)) {
 #if HAVE_CUDA
             cudaMemset(((uint8_t*)m_perf.recv_buffer + length - 1), -1, 1);
 #endif
         }
 
-        rte_call(&m_perf, barrier);
+        ucp_perf_barrier(&m_perf);
 
         my_index      = rte_call(&m_perf, group_index);
 
@@ -339,7 +342,7 @@ public:
 
         wait_window(m_max_outstanding);
         ucp_worker_flush(m_perf.ucp.worker);
-        rte_call(&m_perf, barrier);
+        ucp_perf_barrier(&m_perf);
         return UCS_OK;
     }
 
@@ -360,7 +363,7 @@ public:
 
         ucp_perf_test_prepare_iov_buffers();
 
-        rte_call(&m_perf, barrier);
+        ucp_perf_barrier(&m_perf);
 
         my_index      = rte_call(&m_perf, group_index);
 
@@ -404,7 +407,7 @@ public:
             ucx_perf_update(&m_perf, 0, 0);
         }
 
-        rte_call(&m_perf, barrier);
+        ucp_perf_barrier(&m_perf);
         return UCS_OK;
     }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/uct_tests.cc b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/uct_tests.cc
index 59a452452..7bdd235e5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/uct_tests.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/perf/uct_tests.cc
@@ -37,15 +37,19 @@ public:
         uct_iface_attr_t attr;
         status = uct_iface_query(m_perf.uct.iface, &attr);
         ucs_assert_always(status == UCS_OK);
-        if (attr.cap.flags & (UCT_IFACE_FLAG_AM_SHORT|UCT_IFACE_FLAG_AM_BCOPY|UCT_IFACE_FLAG_AM_ZCOPY)) {
-            status = uct_iface_set_am_handler(m_perf.uct.iface, UCT_PERF_TEST_AM_ID,
-                                              am_hander, m_perf.recv_buffer, UCT_CB_FLAG_SYNC);
+        if (attr.cap.flags & (UCT_IFACE_FLAG_AM_SHORT |
+                              UCT_IFACE_FLAG_AM_BCOPY |
+                              UCT_IFACE_FLAG_AM_ZCOPY)) {
+            status = uct_iface_set_am_handler(m_perf.uct.iface,
+                                              UCT_PERF_TEST_AM_ID, am_hander,
+                                              m_perf.recv_buffer, 0);
             ucs_assert_always(status == UCS_OK);
         }
     }
 
     ~uct_perf_test_runner() {
-        uct_iface_set_am_handler(m_perf.uct.iface, UCT_PERF_TEST_AM_ID, NULL, NULL, UCT_CB_FLAG_SYNC);
+        uct_iface_set_am_handler(m_perf.uct.iface, UCT_PERF_TEST_AM_ID, NULL,
+                                 NULL, 0);
     }
 
     /**
@@ -213,29 +217,29 @@ public:
             }
         case UCX_PERF_CMD_ADD:
             if (length == sizeof(uint32_t)) {
-                return uct_ep_atomic_add32(ep, sn - prev_sn, remote_addr, rkey);
+                return uct_ep_atomic32_post(ep, UCT_ATOMIC_OP_ADD, sn - prev_sn, remote_addr, rkey);
             } else if (length == sizeof(uint64_t)) {
-                return uct_ep_atomic_add64(ep, sn - prev_sn, remote_addr, rkey);
+                return uct_ep_atomic64_post(ep, UCT_ATOMIC_OP_ADD, sn - prev_sn, remote_addr, rkey);
             } else {
                 return UCS_ERR_INVALID_PARAM;
             }
         case UCX_PERF_CMD_FADD:
             if (length == sizeof(uint32_t)) {
-                return uct_ep_atomic_fadd32(ep, sn - prev_sn, remote_addr, rkey,
-                                            (uint32_t*)buffer, comp);
+                return uct_ep_atomic32_fetch(ep, UCT_ATOMIC_OP_ADD, sn - prev_sn,
+                                             (uint32_t*)buffer, remote_addr, rkey, comp);
             } else if (length == sizeof(uint64_t)) {
-                return uct_ep_atomic_fadd64(ep, sn - prev_sn, remote_addr, rkey,
-                                            (uint64_t*)buffer, comp);
+                return uct_ep_atomic64_fetch(ep, UCT_ATOMIC_OP_ADD, sn - prev_sn,
+                                             (uint64_t*)buffer, remote_addr, rkey, comp);
             } else {
                 return UCS_ERR_INVALID_PARAM;
             }
         case UCX_PERF_CMD_SWAP:
             if (length == sizeof(uint32_t)) {
-                return uct_ep_atomic_swap32(ep, sn, remote_addr, rkey,
-                                            (uint32_t*)buffer, comp);
+                return uct_ep_atomic32_fetch(ep, UCT_ATOMIC_OP_SWAP, sn,
+                                             (uint32_t*)buffer, remote_addr, rkey, comp);
             } else if (length == sizeof(uint64_t)) {
-                return uct_ep_atomic_swap64(ep, sn, remote_addr, rkey,
-                                            (uint64_t*)buffer, comp);
+                return uct_ep_atomic64_fetch(ep, UCT_ATOMIC_OP_SWAP, sn,
+                                             (uint64_t*)buffer, remote_addr, rkey, comp);
             } else {
                 return UCS_ERR_INVALID_PARAM;
             }
@@ -312,7 +316,7 @@ public:
         uct_perf_test_prepare_iov_buffer();
 
         *recv_sn  = -1;
-        rte_call(&m_perf, barrier);
+        uct_perf_barrier(&m_perf);
 
         my_index = rte_call(&m_perf, group_index);
 
@@ -376,7 +380,7 @@ public:
                                             (psn_t*)m_perf.send_buffer;
         my_index = rte_call(&m_perf, group_index);
 
-        rte_call(&m_perf, barrier);
+        uct_perf_barrier(&m_perf);
 
         ucx_perf_test_start_clock(&m_perf);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/tools/profile/read_profile.c b/src/mpid/ch4/netmod/ucx/ucx/src/tools/profile/read_profile.c
index 5f428dec1..53e142c70 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/tools/profile/read_profile.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/tools/profile/read_profile.c
@@ -4,7 +4,7 @@
  * See file LICENSE for terms.
  */
 
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <ucs/datastruct/khash.h>
 
 #include <sys/signal.h>
@@ -455,6 +455,19 @@ static int show_profile_data(profile_data_t *data, options_t *opts)
     return 0;
 }
 
+static void usage()
+{
+    printf("Usage: ucx_read_profile [options] [profile-file]\n");
+    printf("Options are:\n");
+    printf("  -r              Show raw output\n");
+    printf("  -t <units>      Select time units to use:\n");
+    printf("                     sec  - seconds\n");
+    printf("                     msec - milliseconds\n");
+    printf("                     usec - microseconds (default)\n");
+    printf("                     nsec - nanoseconds\n");
+    printf("  -h              Show this help message\n");
+}
+
 int parse_args(int argc, char **argv, options_t *opts)
 {
     int c;
@@ -481,12 +494,17 @@ int parse_args(int argc, char **argv, options_t *opts)
             }
             break;
         case 'h':
+            usage();
+            return -127;
         default:
+            usage();
             return -1;
         }
     }
 
     if (optind >= argc) {
+        printf("Error: missing profile file argument\n");
+        usage();
         return -1;
     }
 
@@ -500,13 +518,9 @@ int main(int argc, char **argv)
     options_t opts;
     int ret;
 
-    if (parse_args(argc, argv, &opts) < 0) {
-        printf("Usage: %s [options] <file>\n", basename(argv[0]));
-        printf("Options:\n");
-        printf("      -r             raw output\n");
-        printf("      -t UNITS       select time units (sec/msec/usec/nsec)\n");
-        printf("\n");
-        return -1;
+    ret = parse_args(argc, argv, &opts);
+    if (ret < 0) {
+        return (ret == -127) ? 0 : ret;
     }
 
     if (read_profile_data(opts.filename, &data) < 0) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/Makefile.am
index 0a0b33705..d0ba1b74b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/Makefile.am
@@ -34,21 +34,33 @@ noinst_HEADERS = \
 	malloc/malloc_hook.h \
 	malloc/allocator.h \
 	mmap/mmap.h \
-	util/ucm_config.h \
 	util/replace.h \
 	util/log.h \
 	util/reloc.h \
-	util/sys.h
+	util/sys.h \
+	bistro/bistro_int.h \
+	bistro/bistro.h \
+	bistro/bistro_x86_64.h \
+	bistro/bistro_aarch64.h \
+	bistro/bistro_ppc64.h
+
+if HAVE_CUDA
+noinst_HEADERS += \
+	cuda/cudamem.h
+endif
 
 libucm_la_SOURCES = \
 	event/event.c \
 	malloc/malloc_hook.c \
 	mmap/install.c \
 	util/replace.c \
-	util/ucm_config.c \
 	util/log.c \
 	util/reloc.c \
-	util/sys.c
+	util/sys.c \
+	bistro/bistro.c \
+	bistro/bistro_x86_64.c \
+	bistro/bistro_aarch64.c \
+	bistro/bistro_ppc64.c
 
 if HAVE_CUDA
 libucm_la_SOURCES += \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/api/ucm.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/api/ucm.h
index 0b024e3d2..9d41118e3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/api/ucm.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/api/ucm.h
@@ -32,16 +32,40 @@ typedef enum ucm_event_type {
     UCM_EVENT_SHMAT           = UCS_BIT(3),
     UCM_EVENT_SHMDT           = UCS_BIT(4),
     UCM_EVENT_SBRK            = UCS_BIT(5),
+    UCM_EVENT_MADVISE         = UCS_BIT(6),
 
     /* Aggregate events */
     UCM_EVENT_VM_MAPPED       = UCS_BIT(16),
     UCM_EVENT_VM_UNMAPPED     = UCS_BIT(17),
 
+    /* Non-accessible memory alloc/free events */
+    UCM_EVENT_MEM_TYPE_ALLOC  = UCS_BIT(20),
+    UCM_EVENT_MEM_TYPE_FREE   = UCS_BIT(21),
+
     /* Auxiliary flags */
     UCM_EVENT_FLAG_NO_INSTALL = UCS_BIT(24)
 
 } ucm_event_type_t;
 
+/**
+ * @brief Memory types for alloc and free events
+ */
+typedef enum ucm_mem_type {
+    /*cuda memory */
+    UCM_MEM_TYPE_CUDA         = UCS_BIT(0),
+    UCM_MEM_TYPE_CUDA_MANAGED = UCS_BIT(1)
+} ucm_mem_type_t;
+
+
+/**
+ * @brief MMAP hook modes
+ */
+typedef enum ucm_mmap_hook_mode {
+    UCM_MMAP_HOOK_NONE,
+    UCM_MMAP_HOOK_RELOC,
+    UCM_MMAP_HOOK_BISTRO,
+    UCM_MMAP_HOOK_LAST
+} ucm_mmap_hook_mode_t;
 
 /**
  * @brief Memory event parameters and result.
@@ -113,6 +137,17 @@ typedef union ucm_event {
         intptr_t       increment;
     } sbrk;
 
+    /*
+     * UCM_EVENT_MADVISE
+     * madvise() is called.
+     */
+    struct {
+        int            result;
+        void           *addr;
+        size_t         length;
+        int            advice;
+    } madvise;
+
     /*
      * UCM_EVENT_VM_MAPPED, UCM_EVENT_VM_UNMAPPED
      *
@@ -128,9 +163,39 @@ typedef union ucm_event {
         size_t         size;
     } vm_mapped, vm_unmapped;
 
+    /*
+     * memory type allocation and deallocation event
+     */
+    struct {
+        void           *address;
+        size_t         size;
+        ucm_mem_type_t mem_type;
+    } mem_type;
+
 } ucm_event_t;
 
 
+/**
+ * @brief Global UCM configuration.
+ *
+ * Can be safely modified before using UCM functions.
+ */
+typedef struct ucm_global_config {
+    ucs_log_level_t      log_level;                   /* Logging level */
+    int                  enable_events;               /* Enable memory events */
+    ucm_mmap_hook_mode_t mmap_hook_mode;              /* MMAP hook mode */
+    int                  enable_malloc_hooks;         /* Enable installing malloc hooks */
+    int                  enable_malloc_reloc;         /* Enable installing malloc relocations */
+    int                  enable_cuda_reloc;           /* Enable installing CUDA relocations */
+    int                  enable_dynamic_mmap_thresh;  /* Enable adaptive mmap threshold */
+    size_t               alloc_alignment;             /* Alignment for memory allocations */
+} ucm_global_config_t;
+
+
+/* Global UCM configuration */
+extern ucm_global_config_t ucm_global_opts;
+
+
 /**
  * @brief Memory event callback.
  *
@@ -169,27 +234,6 @@ typedef void (*ucm_event_callback_t)(ucm_event_type_t event_type,
                                      ucm_event_t *event, void *arg);
 
 
-
-/**
- * @brief Print UCM global configuration to a stream.
- *
- * @param [in]  stream        Output stream to print to.
- * @param [in]  print_flags   Controls how the configuration is printed.
- */
-void ucm_config_print(FILE *stream, ucs_config_print_flags_t print_flags);
-
-
-/**
- * @brief Modify UCM global configuration.
- *
- * @param [in]  name          Configuration variable name.
- * @param [in]  value         Value to set.
- *
- * @return Error code.
- */
-ucs_status_t ucm_config_modify(const char *name, const char *value);
-
-
 /**
  * @brief Install a handler for memory events.
  *
@@ -297,6 +341,18 @@ int ucm_orig_shmdt(const void *shmaddr);
 void *ucm_orig_sbrk(intptr_t increment);
 
 
+/**
+ * @brief Call the original implementation of @ref brk without triggering events.
+ */
+int ucm_orig_brk(void *addr);
+
+
+/**
+ * @brief Call the original implementation of @ref madvise without triggering events.
+ */
+int ucm_orig_madvise(void *addr, size_t length, int advice);
+
+
 /**
  * @brief Call the original implementation of @ref mmap and all handlers
  * associated with it.
@@ -352,6 +408,20 @@ int ucm_shmdt(const void *shmaddr);
 void *ucm_sbrk(intptr_t increment);
 
 
+/**
+ * @brief Call the original implementation of @ref brk and all handlers
+ * associated with it.
+ */
+int ucm_brk(void *addr);
+
+
+/**
+ * @brief Call the original implementation of @ref ucm_madvise and all handlers
+ * associated with it.
+ */
+int ucm_madvise(void *addr, size_t length, int advice);
+
+
 END_C_DECLS
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.c
new file mode 100644
index 000000000..79bdbecb7
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.c
@@ -0,0 +1,106 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+#include <dlfcn.h>
+#include <stdlib.h>
+
+#include <ucm/bistro/bistro.h>
+#include <ucm/bistro/bistro_int.h>
+
+ucs_status_t ucm_bistro_remove_restore_point(ucm_bistro_restore_point_t *rp)
+{
+    ucs_assert(rp != NULL);
+    free(rp);
+    return UCS_OK;
+}
+
+static void *ucm_bistro_page_align_ptr(void *ptr)
+{
+    return (void*)ucs_align_down((uintptr_t)ptr, ucm_get_page_size());
+}
+
+static ucs_status_t ucm_bistro_protect(void *addr, size_t len, int prot)
+{
+    void *aligned = ucm_bistro_page_align_ptr(addr);
+    size_t size   = addr - aligned + len;
+    int res;
+
+    res = mprotect(aligned, size, prot) ? UCS_ERR_INVALID_PARAM : UCS_OK;
+    if (res) {
+        ucm_error("Failed to change page protection: %m");
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    return UCS_OK;
+}
+
+ucs_status_t ucm_bistro_apply_patch(void *dst, void *patch, size_t len)
+{
+    ucs_status_t status;
+
+    status = ucm_bistro_protect(dst, len, UCM_PROT_READ_WRITE_EXEC);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    memcpy(dst, patch, len);
+
+    status = ucm_bistro_protect(dst, len, UCM_PROT_READ_EXEC);
+    if (!UCS_STATUS_IS_ERR(status)) {
+        ucs_clear_cache(dst, dst + len);
+    }
+    return status;
+}
+
+#if defined(__x86_64__) || defined (__aarch64__)
+struct ucm_bistro_restore_point {
+    void               *addr; /* address of function to restore */
+    ucm_bistro_patch_t patch; /* original function body */
+};
+
+ucs_status_t ucm_bistro_create_restore_point(void *addr, ucm_bistro_restore_point_t **rp)
+{
+    ucm_bistro_restore_point_t *point;
+
+    if (rp == NULL) {
+        /* restore point is not required */
+        return UCS_OK;
+    }
+
+    point = malloc(sizeof(*point));
+    if (!point) {
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    point->addr  = addr;
+    point->patch = *(ucm_bistro_patch_t*)addr;
+    *rp          = point;
+    return UCS_OK;
+}
+
+ucs_status_t ucm_bistro_restore(ucm_bistro_restore_point_t *rp)
+{
+    ucs_status_t status;
+
+    status = ucm_bistro_apply_patch(rp->addr, &rp->patch, sizeof(rp->patch));
+    if (!UCS_STATUS_IS_ERR(status)) {
+        ucm_bistro_remove_restore_point(rp);
+    }
+
+    return status;
+}
+
+void *ucm_bistro_restore_addr(ucm_bistro_restore_point_t *rp)
+{
+    ucs_assert(rp != NULL);
+    return rp->addr;
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.h
new file mode 100644
index 000000000..16e988700
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro.h
@@ -0,0 +1,58 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+
+#ifndef UCM_BISTRO_BISTRO_H_
+#define UCM_BISTRO_BISTRO_H_
+
+#include <stdint.h>
+
+#include <ucs/type/status.h>
+
+typedef struct ucm_bistro_restore_point ucm_bistro_restore_point_t;
+
+#if defined(__powerpc64__)
+#  include "bistro_ppc64.h"
+#elif defined(__aarch64__)
+#  include "bistro_aarch64.h"
+#elif defined(__x86_64__)
+#  include "bistro_x86_64.h"
+#else
+#  error "Unsupported architecture"
+#endif
+
+
+/**
+ * Restore original function body using restore point created
+ * by @ref ucm_bistro_patch
+ *
+ * @param rp     restore point, is removed after success operation
+ *               completed
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucm_bistro_restore(ucm_bistro_restore_point_t *rp);
+
+/**
+ * Remove resore point created by @ref ucm_bistro_patch witout
+ * restore original function body
+ *
+ * @param rp     restore point
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucm_bistro_remove_restore_point(ucm_bistro_restore_point_t *rp);
+
+/**
+ * Get patch address for restore point
+ *
+ * @param rp     restore point
+ *
+ * @return Address of patched function body
+ */
+void *ucm_bistro_restore_addr(ucm_bistro_restore_point_t *rp);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.c
new file mode 100644
index 000000000..2cf09272f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.c
@@ -0,0 +1,86 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+/* *******************************************************
+ * ARM processors family                                 *
+ * ***************************************************** */
+#if defined(__aarch64__)
+
+#include <sys/mman.h>
+#include <dlfcn.h>
+#include <string.h>
+#include <stdlib.h>
+
+#include <ucm/bistro/bistro.h>
+#include <ucm/bistro/bistro_int.h>
+#include <ucm/util/sys.h>
+#include <ucs/sys/math.h>
+#include <ucs/arch/cpu.h>
+#include <ucs/debug/assert.h>
+
+
+/* Register number used to store indirect jump address.
+ * r15 is the highest numbered temporary register, assuming this one is safe
+ * to use. */
+#define R15 15
+
+#define _MOV(_reg, _shift, _val, _opcode) \
+    (((_opcode) << 23) + ((uint32_t)(_shift) << 21) + ((uint32_t)((_val) & 0xffff) << 5) + (_reg))
+
+/**
+ * @brief Generate a mov immediate instruction
+ *
+ * @param[in] _reg   register number (0-31)
+ * @param[in] _shift shift amount (0-3) * 16-bits
+ * @param[in] _value immediate value
+ */
+#define MOVZ(_reg, _shift, _val) _MOV(_reg, _shift, _val, 0x1a5)
+
+/**
+ * @brief Generate a mov immediate with keep instruction
+ *
+ * @param[in] _reg   register number (0-31)
+ * @param[in] _shift shift amount (0-3) * 16-bits
+ * @param[in] _value immediate value
+ */
+#define MOVK(_reg, _shift, _val) _MOV(_reg, _shift, _val, 0x1e5)
+
+/**
+ * @brief Branch to address stored in register
+ *
+ * @param[in] _reg   register number (0-31)
+ */
+#define BR(_reg) ((0xd61f << 16) + ((_reg) << 5))
+
+ucs_status_t ucm_bistro_patch(const char *symbol, void *hook,
+                              ucm_bistro_restore_point_t **rp)
+{
+    void *func;
+    ucs_status_t status;
+
+    ucm_bistro_patch_t patch = {
+        .reg3 = MOVZ(R15, 3, (uintptr_t)hook >> 48),
+        .reg2 = MOVK(R15, 2, (uintptr_t)hook >> 32),
+        .reg1 = MOVK(R15, 1, (uintptr_t)hook >> 16),
+        .reg0 = MOVK(R15, 0, (uintptr_t)hook),
+        .br   = BR(R15)
+    };
+
+    UCM_LOOKUP_SYMBOL(func, symbol);
+
+    status = ucm_bistro_create_restore_point(func, rp);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    return ucm_bistro_apply_patch(func, &patch, sizeof(patch));
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.h
new file mode 100644
index 000000000..487aa923d
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_aarch64.h
@@ -0,0 +1,41 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+
+#ifndef UCM_BISTRO_BISTRO_AARCH64_H_
+#define UCM_BISTRO_BISTRO_AARCH64_H_
+
+#include <stdint.h>
+
+#include <ucs/type/status.h>
+#include <ucs/sys/compiler_def.h>
+
+#define UCM_BISTRO_PROLOGUE
+#define UCM_BISTRO_EPILOGUE
+
+typedef struct ucm_bistro_patch {
+    uint32_t reg3;  /* movz    x15, addr, lsl #48 */
+    uint32_t reg2;  /* movk    x15, addr, lsl #32 */
+    uint32_t reg1;  /* movk    x15, addr, lsl #16 */
+    uint32_t reg0;  /* movk    x15, addr          */
+    uint32_t br;    /* br      x15                */
+} UCS_S_PACKED ucm_bistro_patch_t;
+
+/**
+ * Set library function call hook using Binary Instrumentation
+ * method (BISTRO): replace function body by user defined call
+ *
+ * @param symbol function name to replace
+ * @param hook   user-defined function-replacer
+ * @param rp     restore point used to restore original function,
+ *               optional, may be NULL
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucm_bistro_patch(const char *symbol, void *hook,
+                              ucm_bistro_restore_point_t **rp);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_int.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_int.h
new file mode 100644
index 000000000..40c80d56b
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_int.h
@@ -0,0 +1,48 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCM_BISTRO_BISTRO_INT_H_
+#define UCM_BISTRO_BISTRO_INT_H_
+
+#include <sys/mman.h>
+#include <dlfcn.h>
+#include <string.h>
+#include <stdlib.h>
+
+#include <ucm/bistro/bistro.h>
+#include <ucm/util/sys.h>
+#include <ucm/util/log.h>
+#include <ucs/sys/math.h>
+#include <ucs/arch/cpu.h>
+#include <ucs/debug/assert.h>
+
+#define UCM_PROT_READ_WRITE_EXEC (PROT_READ | PROT_WRITE | PROT_EXEC)
+#define UCM_PROT_READ_EXEC       (PROT_READ | PROT_EXEC)
+
+#define UCM_LOOKUP_SYMBOL(_func, _symbol) \
+    _func = ucm_bistro_lookup(_symbol);   \
+    if (!_func) {                         \
+        return UCS_ERR_NO_ELEM;           \
+    }
+
+ucs_status_t ucm_bistro_apply_patch(void *dst, void *patch, size_t len);
+
+ucs_status_t ucm_bistro_create_restore_point(void *addr, ucm_bistro_restore_point_t **rp);
+
+static inline void *ucm_bistro_lookup(const char *symbol)
+{
+    void *addr;
+
+    ucs_assert(symbol != NULL);
+
+    addr = dlsym(RTLD_NEXT, symbol);
+    if (!addr) {
+        addr = dlsym(RTLD_DEFAULT, symbol);
+    }
+    return addr;
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.c
new file mode 100644
index 000000000..4b14250cd
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.c
@@ -0,0 +1,209 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+/* *******************************************************
+ * POWER-PC processors family                            *
+ * ***************************************************** */
+#if defined (__powerpc64__)
+
+#include <sys/mman.h>
+#include <dlfcn.h>
+#include <string.h>
+#include <stdlib.h>
+
+#include <ucm/bistro/bistro.h>
+#include <ucm/bistro/bistro_int.h>
+#include <ucm/util/sys.h>
+#include <ucs/sys/math.h>
+#include <ucs/arch/cpu.h>
+#include <ucs/debug/assert.h>
+
+/* PowerPC instructions used in patching                  */
+/* Reference: "PowerPC User Instruction Set Architecture" */
+
+/* Use r11 register for jump address */
+#define R11 11
+
+#define OPCODE(_rt, _rs, _op) \
+     (((_op) << 26) + ((_rt) << 21) + ((_rs) << 16))
+
+#define OP0(_rt, _rs, _ui, _op) \
+     (OPCODE(_rt, _rs, _op) + ((_ui) & 0xffff))
+
+#define MTSPR(_spr, _rs) \
+    (OPCODE(_rs, (_spr) & 0x1f, 31) + (((_spr) & ~UCS_MASK(5)) << 6) + (467 << 1))
+
+#define BCCTR(_bo, _bi, _bh) \
+    (OPCODE(_bo, _bi, 19) + ((_bh) << 11) + (528<<1))
+
+#define RLDICR(_rt, _rs, _sh, _mb) \
+    (OPCODE(_rs, _rt, 30) + (((_sh) & UCS_MASK(5)) << 11) + ((_sh & ~UCS_MASK(5)) >> 4) + \
+    (((_mb) & UCS_MASK(5)) << 6) + ((_mb) && ~UCS_MASK(5)) + UCS_BIT(2))
+
+#define ADDIS(_rt, _rs, _ui) OP0(_rt, _rs, _ui, 15)
+#define ORI(_rt, _rs, _ui) OP0(_rs, _rt, _ui, 24)
+#define ORIS(_rt, _rs, _ui) OP0(_rs, _rt, _ui, 25)
+
+typedef struct ucm_bistro_base_patch {
+    uint32_t addis;    /* lis     r11,(addr >> 48)     */
+    uint32_t ori1;     /* ori     r11,r11,(addr >> 32) */
+    uint32_t rldicr;   /* rldicr  r11,r11,32,31        */
+    uint32_t oris;     /* oris    r11,r11,(addr >> 16) */
+    uint32_t ori2;     /* ori     r11,r11,addr         */
+} UCS_S_PACKED ucm_bistro_base_patch_t;
+
+typedef struct ucm_bistro_patch {
+    ucm_bistro_base_patch_t super;
+    uint32_t                mtspr;    /* mtspr r11 */
+    uint32_t                bcctr;    /* bcctr     */
+} UCS_S_PACKED ucm_bistro_patch_t;
+
+struct ucm_bistro_restore_point {
+    void                    *entry;
+    void                    *hook;
+    ucm_bistro_base_patch_t hook_patch;
+    void                    *func;
+    ucm_bistro_patch_t      func_patch;
+};
+
+static void ucm_bistro_fill_base_patch(ucm_bistro_base_patch_t *patch,
+                                       uint32_t reg, uintptr_t value)
+{
+    ucs_assert(patch != NULL);
+
+    patch->addis  = ADDIS ( reg, 0,   (value >> 48));
+    patch->ori1   = ORI   ( reg, reg, (value >> 32));
+    patch->rldicr = RLDICR( reg, reg, 32, 31);
+    patch->oris   = ORIS  ( reg, reg, (value >> 16));
+    patch->ori2   = ORI   ( reg, reg, (value >>  0));
+}
+
+static void ucm_bistro_fill_patch(ucm_bistro_patch_t *patch,
+                                  uint32_t reg, uintptr_t value)
+{
+    ucs_assert(patch != NULL);
+
+    ucm_bistro_fill_base_patch(&patch->super, reg, value);
+
+    patch->mtspr = MTSPR(9, reg);   /* 9 = CTR     */
+    patch->bcctr = BCCTR(20, 0, 0); /* 20 = always */
+}
+
+static ucs_status_t ucm_bistro_patch_hook(void *hook, ucm_bistro_restore_point_t *rp,
+                                          uint64_t toc)
+{
+    const uint32_t nop = 0x60000000;
+    uint32_t *toc_ptr;
+    ucm_bistro_base_patch_t *toc_patch;
+    ucm_bistro_base_patch_t patch;
+
+    /* locate reserved code space in hook function */
+    for (toc_ptr = hook;; toc_ptr++) {
+        toc_patch = (ucm_bistro_base_patch_t*)toc_ptr;
+        if ((toc_patch->addis  == nop) &&
+            (toc_patch->ori1   == nop) &&
+            (toc_patch->rldicr == nop) &&
+            (toc_patch->oris   == nop) &&
+            (toc_patch->ori2   == nop)) {
+            break;
+        }
+    }
+
+    if (rp) {
+        rp->hook       = toc_ptr;
+        rp->hook_patch = *toc_patch;
+    }
+
+    ucm_bistro_fill_base_patch(&patch, 2, toc);
+    return ucm_bistro_apply_patch(toc_ptr, &patch, sizeof(patch));
+}
+
+static void *ucm_bistro_get_text_addr(void *addr)
+{
+#if !defined (_CALL_ELF) || (_CALL_ELF != 2)
+    return addr ? *(void**)addr : 0;
+#else
+    return addr;
+#endif
+}
+
+ucs_status_t ucm_bistro_patch_toc(const char *symbol, void *hook,
+                                  ucm_bistro_restore_point_t **rp,
+                                  uint64_t toc)
+{
+    ucs_status_t status;
+    void *func;
+    ucm_bistro_restore_point_t restore;
+    ucm_bistro_patch_t patch;
+
+    UCM_LOOKUP_SYMBOL(func, symbol);
+
+    restore.entry = func;
+
+    func = ucm_bistro_get_text_addr(func);
+    hook = ucm_bistro_get_text_addr(hook);
+
+    status = ucm_bistro_patch_hook(hook, &restore, toc);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+#if defined(_CALL_ELF) && (_CALL_ELF == 2)
+    func += 8;
+    hook += 8;
+#endif
+
+    ucm_bistro_fill_patch(&patch, R11, (uintptr_t)hook);
+
+    restore.func       = func;
+    restore.func_patch = *(ucm_bistro_patch_t*)func;
+
+    status = ucm_bistro_apply_patch(func, &patch, sizeof(patch));
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    if (rp) {
+        *rp = malloc(sizeof(restore));
+        if (!(*rp)) {
+            return UCS_ERR_NO_MEMORY;
+        }
+        **rp = restore;
+    }
+
+    return UCS_OK;
+}
+
+ucs_status_t ucm_bistro_restore(ucm_bistro_restore_point_t *rp)
+{
+    ucs_status_t status;
+
+    ucs_assert(rp != NULL);
+
+    status = ucm_bistro_apply_patch(rp->func, &rp->func_patch, sizeof(rp->func_patch));
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    status = ucm_bistro_apply_patch(rp->hook, &rp->hook_patch, sizeof(rp->hook_patch));
+    if (!UCS_STATUS_IS_ERR(status)) {
+        ucm_bistro_remove_restore_point(rp);
+    }
+
+    return status;
+}
+
+void *ucm_bistro_restore_addr(ucm_bistro_restore_point_t *rp)
+{
+    ucs_assert(rp != NULL);
+    return rp->entry;
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.h
new file mode 100644
index 000000000..7b5c3b46e
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_ppc64.h
@@ -0,0 +1,51 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+
+#ifndef UCM_BISTRO_BISTRO_PPC64_H_
+#define UCM_BISTRO_BISTRO_PPC64_H_
+
+#include <stdint.h>
+
+#include <ucs/type/status.h>
+
+/* special processing for ppc64 to save and restore TOC (r2)
+ * Reference: "64-bit PowerPC ELF Application Binary Interface Supplement 1.9" */
+#define UCM_BISTRO_PROLOGUE                       \
+    uint64_t toc_save;                            \
+    asm volatile ("std 2, %0" : "=m" (toc_save)); \
+    asm volatile ("nop; nop; nop; nop; nop");
+#define UCM_BISTRO_EPILOGUE \
+    asm volatile ("ld  2, %0" : : "m" (toc_save));
+
+
+/**
+ * Set library function call hook using Binary Instrumentation
+ * method (BISTRO): replace function body by user defined call
+ *
+ * @param symbol function name to replace
+ * @param hook   user-defined function-replacer
+ * @param rp     restore point used to restore original function,
+ *               optional, may be NULL
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+/* we have to use inline proxy call to save TOC register
+ * value - PPC is very sensible to this register value */
+ucs_status_t ucm_bistro_patch_toc(const char *symbol, void *hook,
+                                  ucm_bistro_restore_point_t **rp,
+                                  uint64_t toc);
+
+static inline
+ucs_status_t ucm_bistro_patch(const char *symbol, void *hook,
+                              ucm_bistro_restore_point_t **rp)
+{
+    uint64_t toc;
+    asm volatile ("std 2, %0" : "=m" (toc));
+    return ucm_bistro_patch_toc(symbol, hook, rp, toc);
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.c
new file mode 100644
index 000000000..b2e57b07a
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.c
@@ -0,0 +1,51 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+/* *******************************************************
+ * x86 processors family                                 *
+ * ***************************************************** */
+#if defined(__x86_64__)
+
+#include <sys/mman.h>
+#include <dlfcn.h>
+#include <string.h>
+#include <stdlib.h>
+
+#include <ucm/bistro/bistro.h>
+#include <ucm/bistro/bistro_int.h>
+#include <ucm/util/sys.h>
+#include <ucs/sys/math.h>
+#include <ucs/arch/cpu.h>
+#include <ucs/debug/assert.h>
+
+static const ucm_bistro_patch_t patch_tmpl = {
+    .mov_r11 = {0x49, 0xbb},
+    .jmp_r11 = {0x41, 0xff, 0xe3}
+};
+
+ucs_status_t ucm_bistro_patch(const char *symbol, void *hook,
+                              ucm_bistro_restore_point_t **rp)
+{
+    ucm_bistro_patch_t patch = patch_tmpl;
+    ucs_status_t status;
+    void *func;
+
+    UCM_LOOKUP_SYMBOL(func, symbol);
+
+    patch.ptr = hook;
+
+    status = ucm_bistro_create_restore_point(func, rp);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    return ucm_bistro_apply_patch(func, &patch, sizeof(patch));
+}
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.h
new file mode 100644
index 000000000..bf8d5e9d8
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/bistro/bistro_x86_64.h
@@ -0,0 +1,39 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.       ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+
+#ifndef UCM_BISTRO_BISTRO_X86_64_H_
+#define UCM_BISTRO_BISTRO_X86_64_H_
+
+#include <stdint.h>
+
+#include <ucs/type/status.h>
+#include <ucs/sys/compiler_def.h>
+
+#define UCM_BISTRO_PROLOGUE
+#define UCM_BISTRO_EPILOGUE
+
+typedef struct ucm_bistro_patch {
+    uint8_t mov_r11[2];  /* mov %r11, addr */
+    void    *ptr;
+    uint8_t jmp_r11[3];  /* jmp r11        */
+} UCS_S_PACKED ucm_bistro_patch_t;
+
+/**
+ * Set library function call hook using Binary Instrumentation
+ * method (BISTRO): replace function body by user defined call
+ *
+ * @param symbol function name to replace
+ * @param hook   user-defined function-replacer
+ * @param rp     restore point used to restore original function,
+ *               optional, may be NULL
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucm_bistro_patch(const char *symbol, void *hook,
+                              ucm_bistro_restore_point_t **rp);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/cudamem.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/cudamem.h
index 76ae80eab..ec9f64828 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/cudamem.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/cudamem.h
@@ -13,8 +13,89 @@
 
 ucs_status_t ucm_cudamem_install();
 
-cudaError_t ucm_override_cudaFree(void *addr);
-cudaError_t ucm_orig_cudaFree(void *address);
-cudaError_t ucm_cudaFree(void *address);
+/*cuMemFree */
+CUresult ucm_override_cuMemFree(CUdeviceptr dptr);
+CUresult ucm_orig_cuMemFree(CUdeviceptr dptr);
+CUresult ucm_cuMemFree(CUdeviceptr dptr);
 
+/*cuMemFreeHost */
+CUresult ucm_override_cuMemFreeHost(void *p);
+CUresult ucm_orig_cuMemFreeHost(void *p);
+CUresult ucm_cuMemFreeHost(void *p);
+
+/*cuMemAlloc*/
+CUresult ucm_override_cuMemAlloc(CUdeviceptr *dptr, size_t size);
+CUresult ucm_orig_cuMemAlloc(CUdeviceptr *dptr, size_t size);
+CUresult ucm_cuMemAlloc(CUdeviceptr *dptr, size_t size);
+
+/*cuMemAllocManaged*/
+CUresult ucm_override_cuMemAllocManaged(CUdeviceptr *dptr, size_t size,
+                                        unsigned int flags);
+CUresult ucm_orig_cuMemAllocManaged(CUdeviceptr *dptr, size_t size, unsigned int flags);
+CUresult ucm_cuMemAllocManaged(CUdeviceptr *dptr, size_t size, unsigned int flags);
+
+/*cuMemAllocPitch*/
+CUresult ucm_override_cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
+                                      size_t WidthInBytes, size_t Height,
+                                      unsigned int ElementSizeBytes);
+CUresult ucm_orig_cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
+                                  size_t WidthInBytes, size_t Height,
+                                  unsigned int ElementSizeBytes);
+CUresult ucm_cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
+                             size_t WidthInBytes, size_t Height,
+                             unsigned int ElementSizeBytes);
+
+/*cuMemHostGetDevicePointer*/
+CUresult ucm_override_cuMemHostGetDevicePointer(CUdeviceptr *pdptr, void *p,
+                                                unsigned int Flags);
+CUresult ucm_orig_cuMemHostGetDevicePointer(CUdeviceptr *pdptr, void *p,
+                                            unsigned int Flags);
+CUresult ucm_cuMemHostGetDevicePointer(CUdeviceptr *pdptr, void *p, unsigned int Flags);
+
+/*cuMemHostUnregister */
+CUresult ucm_override_cuMemHostUnregister(void *p);
+CUresult ucm_orig_cuMemHostUnregister(void *p);
+CUresult ucm_cuMemHostUnregister(void *p);
+
+/*cudaFree*/
+cudaError_t ucm_override_cudaFree(void *devPtr);
+cudaError_t ucm_orig_cudaFree(void *devPtr);
+cudaError_t ucm_cudaFree(void *devPtr);
+
+/*cudaFreeHost*/
+cudaError_t ucm_override_cudaFreeHost(void *ptr);
+cudaError_t ucm_orig_cudaFreeHost(void *ptr);
+cudaError_t ucm_cudaFreeHost(void *ptr);
+
+/*cudaMalloc*/
+cudaError_t ucm_override_cudaMalloc(void **devPtr, size_t size);
+cudaError_t ucm_orig_cudaMalloc(void **devPtr, size_t size);
+cudaError_t ucm_cudaMalloc(void **devPtr, size_t size);
+
+/*cudaMallocManaged*/
+cudaError_t ucm_override_cudaMallocManaged(void **devPtr, size_t size,
+                                           unsigned int flags);
+cudaError_t ucm_orig_cudaMallocManaged(void **devPtr, size_t size, unsigned int flags);
+cudaError_t ucm_cudaMallocManaged(void **devPtr, size_t size, unsigned int flags);
+
+/*cudaMallocPitch*/
+cudaError_t ucm_override_cudaMallocPitch(void **devPtr, size_t *pitch,
+                                         size_t width, size_t height);
+cudaError_t ucm_orig_cudaMallocPitch(void **devPtr, size_t *pitch,
+                                     size_t width, size_t height);
+cudaError_t ucm_cudaMallocPitch(void **devPtr, size_t *pitch,
+                                size_t width, size_t height);
+
+/*cudaHostGetDevicePointer*/
+cudaError_t ucm_override_cudaHostGetDevicePointer(void **pDevice, void *pHost,
+                                                  unsigned int flags);
+cudaError_t ucm_orig_cudaHostGetDevicePointer(void **pDevice, void *pHost,
+                                              unsigned int flags);
+cudaError_t ucm_cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags);
+
+
+/*cudaHostUnregister*/
+cudaError_t ucm_override_cudaHostUnregister(void *ptr);
+cudaError_t ucm_orig_cudaHostUnregister(void *ptr);
+cudaError_t ucm_cudaHostUnregister(void *ptr);
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/install.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/install.c
index 092bde49f..2377aeddf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/install.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/cuda/install.c
@@ -14,22 +14,40 @@
 #include <ucm/event/event.h>
 #include <ucm/util/log.h>
 #include <ucm/util/reloc.h>
-#include <ucm/util/ucm_config.h>
 #include <ucs/sys/math.h>
+#include <ucs/sys/preprocessor.h>
 
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <unistd.h>
 #include <pthread.h>
 
-static ucm_reloc_patch_t patch = {"cudaFree",   ucm_override_cudaFree};
+static ucm_reloc_patch_t patches[] = {
+    {UCS_PP_MAKE_STRING(cuMemFree),                 ucm_override_cuMemFree},
+    {UCS_PP_MAKE_STRING(cuMemFreeHost),             ucm_override_cuMemFreeHost},
+    {UCS_PP_MAKE_STRING(cuMemAlloc),                ucm_override_cuMemAlloc},
+    {UCS_PP_MAKE_STRING(cuMemAllocManaged),         ucm_override_cuMemAllocManaged},
+    {UCS_PP_MAKE_STRING(cuMemAllocPitch),           ucm_override_cuMemAllocPitch},
+    {UCS_PP_MAKE_STRING(cuMemHostGetDevicePointer), ucm_override_cuMemHostGetDevicePointer},
+    {UCS_PP_MAKE_STRING(cuMemHostUnregister),       ucm_override_cuMemHostUnregister},
+    {UCS_PP_MAKE_STRING(cudaFree),                  ucm_override_cudaFree},
+    {UCS_PP_MAKE_STRING(cudaFreeHost),              ucm_override_cudaFreeHost},
+    {UCS_PP_MAKE_STRING(cudaMalloc),                ucm_override_cudaMalloc},
+    {UCS_PP_MAKE_STRING(cudaMallocManaged),         ucm_override_cudaMallocManaged},
+    {UCS_PP_MAKE_STRING(cudaMallocPitch),           ucm_override_cudaMallocPitch},
+    {UCS_PP_MAKE_STRING(cudaHostGetDevicePointer),  ucm_override_cudaHostGetDevicePointer},
+    {UCS_PP_MAKE_STRING(cudaHostUnregister),        ucm_override_cudaHostUnregister},
+    {NULL,                                          NULL}
+};
+
 ucs_status_t ucm_cudamem_install()
 {
     static int ucm_cudamem_installed = 0;
     static pthread_mutex_t install_mutex = PTHREAD_MUTEX_INITIALIZER;
+    ucm_reloc_patch_t *patch;
     ucs_status_t status;
 
-    if (!ucm_global_config.enable_cuda_hooks) {
+    if (!ucm_global_opts.enable_cuda_reloc) {
         ucm_debug("installing cudamem relocations is disabled by configuration");
         return UCS_ERR_UNSUPPORTED;
     }
@@ -39,10 +57,12 @@ ucs_status_t ucm_cudamem_install()
 
     pthread_mutex_lock(&install_mutex);
 
-    status = ucm_reloc_modify(&patch);
-    if (status != UCS_OK) {
-        ucm_warn("failed to install relocation table entry for '%s'", patch.symbol);
-        goto out_unlock;
+    for (patch = patches; patch->symbol != NULL; ++patch) {
+        status = ucm_reloc_modify(patch);
+        if (status != UCS_OK) {
+            ucm_warn("failed to install relocation table entry for '%s'", patch->symbol);
+            goto out_unlock;
+        }
     }
 
     ucm_cudamem_installed = 1;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/event/event.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/event/event.c
index e9383e4f7..fdcd160c3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/event/event.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/event/event.c
@@ -16,10 +16,11 @@
 #if HAVE_CUDA
 #include <ucm/cuda/cudamem.h>
 #endif
-#include <ucm/util/ucm_config.h>
 #include <ucm/util/log.h>
 #include <ucm/util/sys.h>
 #include <ucs/arch/cpu.h>
+#include <ucs/datastruct/khash.h>
+#include <ucs/type/component.h>
 
 #include <sys/mman.h>
 #include <pthread.h>
@@ -30,9 +31,14 @@
 #include <errno.h>
 
 
+static pthread_spinlock_t ucm_kh_lock;
+#define ucm_ptr_hash(_ptr)  kh_int64_hash_func((uintptr_t)(_ptr))
+KHASH_INIT(ucm_ptr_size, const void*, size_t, 1, ucm_ptr_hash, kh_int64_hash_equal)
+
 static pthread_rwlock_t ucm_event_lock = PTHREAD_RWLOCK_INITIALIZER;
 static ucs_list_link_t ucm_event_handlers;
 static int ucm_external_events = 0;
+static khash_t(ucm_ptr_size) ucm_shmat_ptrs;
 
 static size_t ucm_shm_size(int shmid)
 {
@@ -92,6 +98,13 @@ static void ucm_event_call_orig(ucm_event_type_t event_type, ucm_event_t *event,
             event->sbrk.result = ucm_orig_sbrk(event->sbrk.increment);
         }
         break;
+    case UCM_EVENT_MADVISE:
+        if (event->madvise.result == -1) {
+            event->madvise.result = ucm_orig_madvise(event->madvise.addr,
+                                                     event->madvise.length,
+                                                     event->madvise.advice);
+        }
+        break;
     default:
         ucm_warn("Got unknown event %d", event_type);
         break;
@@ -105,7 +118,8 @@ static void ucm_event_call_orig(ucm_event_type_t event_type, ucm_event_t *event,
 static ucm_event_handler_t ucm_event_orig_handler = {
     .list     = UCS_LIST_INITIALIZER(&ucm_event_handlers, &ucm_event_handlers),
     .events   = UCM_EVENT_MMAP | UCM_EVENT_MUNMAP | UCM_EVENT_MREMAP |
-                UCM_EVENT_SHMAT | UCM_EVENT_SHMDT | UCM_EVENT_SBRK, /* All events */
+                UCM_EVENT_SHMAT | UCM_EVENT_SHMDT | UCM_EVENT_SBRK |
+                UCM_EVENT_MADVISE,      /* All events */
     .priority = 0,                      /* Between negative and positive handlers */
     .cb       = ucm_event_call_orig
 };
@@ -156,6 +170,8 @@ ucm_dispatch_vm_mmap(void *addr, size_t length)
 {
     ucm_event_t event;
 
+    ucm_trace("vm_map addr=%p length=%zu", addr, length);
+
     event.vm_mapped.address = addr;
     event.vm_mapped.size    = length;
     ucm_event_dispatch(UCM_EVENT_VM_MAPPED, &event);
@@ -166,6 +182,8 @@ ucm_dispatch_vm_munmap(void *addr, size_t length)
 {
     ucm_event_t event;
 
+    ucm_trace("vm_unmap addr=%p length=%zu", addr, length);
+
     event.vm_unmapped.address = addr;
     event.vm_unmapped.size    = length;
     ucm_event_dispatch(UCM_EVENT_VM_UNMAPPED, &event);
@@ -180,6 +198,10 @@ void *ucm_mmap(void *addr, size_t length, int prot, int flags, int fd, off_t off
 
     ucm_event_enter();
 
+    if ((flags & MAP_FIXED) && (addr != NULL)) {
+        ucm_dispatch_vm_munmap(addr, length);
+    }
+
     event.mmap.result  = MAP_FAILED;
     event.mmap.address = addr;
     event.mmap.size    = length;
@@ -269,8 +291,11 @@ void *ucm_mremap(void *old_address, size_t old_size, size_t new_size, int flags)
 
 void *ucm_shmat(int shmid, const void *shmaddr, int shmflg)
 {
+    uintptr_t attach_addr;
     ucm_event_t event;
+    khiter_t iter;
     size_t size;
+    int result;
 
     ucm_event_enter();
 
@@ -278,14 +303,31 @@ void *ucm_shmat(int shmid, const void *shmaddr, int shmflg)
               shmid, shmaddr, shmflg);
 
     size = ucm_shm_size(shmid);
+
+    if ((shmflg & SHM_REMAP) && (shmaddr != NULL)) {
+        attach_addr = (uintptr_t)shmaddr;
+        if (shmflg & SHM_RND) {
+            attach_addr -= attach_addr % SHMLBA;
+        }
+        ucm_dispatch_vm_munmap((void*)shmaddr, size);
+    }
+
     event.shmat.result  = MAP_FAILED;
     event.shmat.shmid   = shmid;
     event.shmat.shmaddr = shmaddr;
     event.shmat.shmflg  = shmflg;
     ucm_event_dispatch(UCM_EVENT_SHMAT, &event);
 
+    pthread_spin_lock(&ucm_kh_lock);
     if (event.shmat.result != MAP_FAILED) {
+        iter = kh_put(ucm_ptr_size, &ucm_shmat_ptrs, event.mmap.result, &result);
+        if (result != -1) {
+            kh_value(&ucm_shmat_ptrs, iter) = size;
+        }
+        pthread_spin_unlock(&ucm_kh_lock);
         ucm_dispatch_vm_mmap(event.shmat.result, size);
+    } else {
+        pthread_spin_unlock(&ucm_kh_lock);
     }
 
     ucm_event_leave();
@@ -296,12 +338,24 @@ void *ucm_shmat(int shmid, const void *shmaddr, int shmflg)
 int ucm_shmdt(const void *shmaddr)
 {
     ucm_event_t event;
+    khiter_t iter;
+    size_t size;
 
     ucm_event_enter();
 
     ucm_debug("ucm_shmdt(shmaddr=%p)", shmaddr);
 
-    ucm_dispatch_vm_munmap((void*)shmaddr, ucm_get_shm_seg_size(shmaddr));
+    pthread_spin_lock(&ucm_kh_lock);
+    iter = kh_get(ucm_ptr_size, &ucm_shmat_ptrs, shmaddr);
+    if (iter != kh_end(&ucm_shmat_ptrs)) {
+        size = kh_value(&ucm_shmat_ptrs, iter);
+        kh_del(ucm_ptr_size, &ucm_shmat_ptrs, iter);
+    } else {
+        size = ucm_get_shm_seg_size(shmaddr);
+    }
+    pthread_spin_unlock(&ucm_kh_lock);
+
+    ucm_dispatch_vm_munmap((void*)shmaddr, size);
 
     event.shmdt.result  = -1;
     event.shmdt.shmaddr = shmaddr;
@@ -337,22 +391,347 @@ void *ucm_sbrk(intptr_t increment)
     return event.sbrk.result;
 }
 
+int ucm_brk(void *addr)
+{
+#if UCM_BISTRO_HOOKS
+    void *old_addr;
+    intptr_t increment;
+    ucm_event_t event;
+
+    old_addr  = ucm_brk_syscall(0);
+    /* in case if addr == NULL - it just returns current pointer */
+    increment = addr ? ((intptr_t)addr - (intptr_t)old_addr) : 0;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_brk(addr=%p)", addr);
+
+    if (increment < 0) {
+        ucm_dispatch_vm_munmap(old_addr + increment, -increment);
+    }
+
+    event.sbrk.result    = (void*)-1;
+    event.sbrk.increment = increment;
+    ucm_event_dispatch(UCM_EVENT_SBRK, &event);
+
+    if ((increment > 0) && (event.sbrk.result != MAP_FAILED)) {
+        ucm_dispatch_vm_mmap(old_addr, increment);
+    }
+
+    ucm_event_leave();
+
+    return event.sbrk.result == MAP_FAILED ? -1 : 0;
+#else
+    return -1;
+#endif
+}
+
+int ucm_madvise(void *addr, size_t length, int advice)
+{
+    ucm_event_t event;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_madvise(addr=%p length=%zu advice=%d)", addr, length, advice);
+
+    /* madvise(MADV_DONTNEED) and madvise(MADV_FREE) are releasing pages */
+    if ((advice == MADV_DONTNEED)
+#if HAVE_DECL_MADV_REMOVE
+        || (advice == MADV_REMOVE)
+#endif
+#if HAVE_DECL_POSIX_MADV_DONTNEED
+        || (advice == POSIX_MADV_DONTNEED)
+#endif
+#if HAVE_DECL_MADV_FREE
+        || (advice == MADV_FREE)
+#endif
+       ) {
+        ucm_dispatch_vm_munmap(addr, length);
+    }
+
+    event.madvise.result = -1;
+    event.madvise.addr   = addr;
+    event.madvise.length = length;
+    event.madvise.advice = advice;
+    ucm_event_dispatch(UCM_EVENT_MADVISE, &event);
+
+    ucm_event_leave();
+
+    return event.madvise.result;
+}
+
+
 #if HAVE_CUDA
-cudaError_t ucm_cudaFree(void *addr)
+static UCS_F_ALWAYS_INLINE void
+ucm_dispatch_mem_type_alloc(void *addr, size_t length, ucm_mem_type_t mem_type)
+{
+    ucm_event_t event;
+
+    event.mem_type.address  = addr;
+    event.mem_type.size     = length;
+    event.mem_type.mem_type = mem_type;
+    ucm_event_dispatch(UCM_EVENT_MEM_TYPE_ALLOC, &event);
+}
+
+static UCS_F_ALWAYS_INLINE void
+ucm_dispatch_mem_type_free(void *addr, size_t length, ucm_mem_type_t mem_type)
+{
+    ucm_event_t event;
+
+    event.mem_type.address  = addr;
+    event.mem_type.size     = length;
+    event.mem_type.mem_type = mem_type;
+    ucm_event_dispatch(UCM_EVENT_MEM_TYPE_FREE, &event);
+}
+
+static void ucm_cudafree_dispatch_events(void *dptr)
+{
+    CUresult ret;
+    CUdeviceptr pbase;
+    size_t psize;
+
+    if (dptr == NULL) {
+        return;
+    }
+
+    ret = cuMemGetAddressRange(&pbase, &psize, (CUdeviceptr) dptr);
+    if (ret != CUDA_SUCCESS) {
+        ucm_warn("cuMemGetAddressRange(devPtr=%p) failed", (void *)dptr);
+        psize = 1; /* set minimum length */
+    }
+    ucs_assert(dptr == (void *)pbase);
+
+    ucm_dispatch_mem_type_free((void *)dptr, psize, UCM_MEM_TYPE_CUDA);
+}
+
+CUresult ucm_cuMemFree(CUdeviceptr dptr)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_cuMemFree(dptr=%p)",(void *)dptr);
+
+    ucm_cudafree_dispatch_events((void *)dptr);
+
+    ret = ucm_orig_cuMemFree(dptr);
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemFreeHost(void *p)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_cuMemFreeHost(ptr=%p)", p);
+
+    ucm_dispatch_vm_munmap(p, 0);
+
+    ret = ucm_orig_cuMemFreeHost(p);
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemAlloc(CUdeviceptr *dptr, size_t size)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cuMemAlloc(dptr, size);
+    if (ret == CUDA_SUCCESS) {
+        ucm_trace("ucm_cuMemAlloc(dptr=%p size:%lu)",(void *)*dptr, size);
+        ucm_dispatch_mem_type_alloc((void *)*dptr, size, UCM_MEM_TYPE_CUDA);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemAllocManaged(CUdeviceptr *dptr, size_t size, unsigned int flags)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cuMemAllocManaged(dptr, size, flags);
+    if (ret == CUDA_SUCCESS) {
+        ucm_trace("ucm_cuMemAllocManaged(dptr=%p size:%lu, flags:%d)",
+                  (void *)*dptr, size, flags);
+        ucm_dispatch_mem_type_alloc((void *)*dptr, size, UCM_MEM_TYPE_CUDA_MANAGED);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
+                             size_t WidthInBytes, size_t Height,
+                             unsigned int ElementSizeBytes)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cuMemAllocPitch(dptr, pPitch, WidthInBytes, Height, ElementSizeBytes);
+    if (ret == CUDA_SUCCESS) {
+        ucm_trace("ucm_cuMemAllocPitch(dptr=%p size:%lu)",(void *)*dptr,
+                  (WidthInBytes * Height));
+        ucm_dispatch_mem_type_alloc((void *)*dptr, WidthInBytes * Height,
+                                    UCM_MEM_TYPE_CUDA);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemHostGetDevicePointer(CUdeviceptr *pdptr, void *p, unsigned int Flags)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cuMemHostGetDevicePointer(pdptr, p, Flags);
+    if (ret == CUDA_SUCCESS) {
+        ucm_trace("ucm_cuMemHostGetDevicePointer(pdptr=%p p=%p)",(void *)*pdptr, p);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+CUresult ucm_cuMemHostUnregister(void *p)
+{
+    CUresult ret;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_cuMemHostUnregister(ptr=%p)", p);
+
+    ret = ucm_orig_cuMemHostUnregister(p);
+
+    ucm_event_leave();
+    return ret;
+}
+
+cudaError_t ucm_cudaFree(void *devPtr)
 {
     cudaError_t ret;
 
     ucm_event_enter();
 
-    ucm_trace("ucm_cudaFree(addr=%p )", addr);
+    ucm_trace("ucm_cudaFree(devPtr=%p)", devPtr);
+
+    ucm_cudafree_dispatch_events((void *)devPtr);
 
-    ucm_dispatch_vm_munmap(addr, 0);
-    ret = ucm_orig_cudaFree(addr);
+    ret = ucm_orig_cudaFree(devPtr);
 
     ucm_event_leave();
 
     return ret;
 }
+
+cudaError_t ucm_cudaFreeHost(void *ptr)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_cudaFreeHost(ptr=%p)", ptr);
+
+    ucm_dispatch_vm_munmap(ptr, 0);
+
+    ret = ucm_orig_cudaFreeHost(ptr);
+
+    ucm_event_leave();
+    return ret;
+}
+
+cudaError_t ucm_cudaMalloc(void **devPtr, size_t size)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cudaMalloc(devPtr, size);
+    if (ret == cudaSuccess) {
+        ucm_trace("ucm_cudaMalloc(devPtr=%p size:%lu)", *devPtr, size);
+        ucm_dispatch_mem_type_alloc(*devPtr, size, UCM_MEM_TYPE_CUDA);
+    }
+
+    ucm_event_leave();
+
+    return ret;
+}
+
+cudaError_t ucm_cudaMallocManaged(void **devPtr, size_t size, unsigned int flags)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cudaMallocManaged(devPtr, size, flags);
+    if (ret == cudaSuccess) {
+        ucm_trace("ucm_cudaMallocManaged(devPtr=%p size:%lu flags:%d)",
+                  *devPtr, size, flags);
+        ucm_dispatch_mem_type_alloc(*devPtr, size, UCM_MEM_TYPE_CUDA_MANAGED);
+    }
+
+    ucm_event_leave();
+
+    return ret;
+}
+
+cudaError_t ucm_cudaMallocPitch(void **devPtr, size_t *pitch,
+                                size_t width, size_t height)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cudaMallocPitch(devPtr, pitch, width, height);
+    if (ret == cudaSuccess) {
+        ucm_trace("ucm_cudaMallocPitch(devPtr=%p size:%lu)",*devPtr, (width * height));
+        ucm_dispatch_mem_type_alloc(*devPtr, (width * height), UCM_MEM_TYPE_CUDA);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+cudaError_t ucm_cudaHostGetDevicePointer(void **pDevice, void *pHost, unsigned int flags)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ret = ucm_orig_cudaHostGetDevicePointer(pDevice, pHost, flags);
+    if (ret == cudaSuccess) {
+        ucm_trace("ucm_cuMemHostGetDevicePointer(pDevice=%p pHost=%p)", pDevice, pHost);
+    }
+
+    ucm_event_leave();
+    return ret;
+}
+
+cudaError_t ucm_cudaHostUnregister(void *ptr)
+{
+    cudaError_t ret;
+
+    ucm_event_enter();
+
+    ucm_trace("ucm_cudaHostUnregister(ptr=%p)", ptr);
+
+    ret = ucm_orig_cudaHostUnregister(ptr);
+
+    ucm_event_leave();
+    return ret;
+}
+
 #endif
 
 void ucm_event_handler_add(ucm_event_handler_t *handler)
@@ -381,18 +760,20 @@ void ucm_event_handler_remove(ucm_event_handler_t *handler)
 
 static ucs_status_t ucm_event_install(int events)
 {
+    int native_events, malloc_events;
     ucs_status_t status;
-    int native_events;
 
     /* Replace aggregate events with the native events which make them */
-    native_events = events & ~(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);
+    native_events = events & ~(UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED |
+                               UCM_EVENT_MEM_TYPE_ALLOC | UCM_EVENT_MEM_TYPE_FREE);
     if (events & UCM_EVENT_VM_MAPPED) {
         native_events |= UCM_EVENT_MMAP | UCM_EVENT_MREMAP |
                          UCM_EVENT_SHMAT | UCM_EVENT_SBRK;
     }
     if (events & UCM_EVENT_VM_UNMAPPED) {
-        native_events |= UCM_EVENT_MUNMAP | UCM_EVENT_MREMAP |
-                         UCM_EVENT_SHMDT | UCM_EVENT_SBRK;
+        native_events |= UCM_EVENT_MMAP | UCM_EVENT_MUNMAP | UCM_EVENT_MREMAP |
+                         UCM_EVENT_SHMDT | UCM_EVENT_SHMAT |
+                         UCM_EVENT_SBRK | UCM_EVENT_MADVISE;
     }
 
     /* TODO lock */
@@ -404,7 +785,9 @@ static ucs_status_t ucm_event_install(int events)
 
     ucm_debug("mmap hooks are ready");
 
-    status = ucm_malloc_install(native_events);
+    malloc_events = events & ~(UCM_EVENT_MEM_TYPE_ALLOC |
+                               UCM_EVENT_MEM_TYPE_FREE);
+    status = ucm_malloc_install(malloc_events);
     if (status != UCS_OK) {
         ucm_debug("failed to install malloc events");
         goto out_unlock;
@@ -413,12 +796,14 @@ static ucs_status_t ucm_event_install(int events)
     ucm_debug("malloc hooks are ready");
 
 #if HAVE_CUDA
-    status = ucm_cudamem_install();
-    if (status != UCS_OK) {
-        ucm_debug("failed to install cudamem events");
-        goto out_unlock;
+    if (events & (UCM_EVENT_MEM_TYPE_ALLOC | UCM_EVENT_MEM_TYPE_FREE)) {
+        status = ucm_cudamem_install();
+        if (status != UCS_OK) {
+            ucm_debug("failed to install cudamem events");
+            goto out_unlock;
+        }
+        ucm_debug("cudaFree hooks are ready");
     }
-    ucm_debug("cudaFree hooks are ready");
 #endif
 
     status = UCS_OK;
@@ -434,12 +819,12 @@ ucs_status_t ucm_set_event_handler(int events, int priority,
     ucm_event_handler_t *handler;
     ucs_status_t status;
 
-    if (!ucm_global_config.enable_events) {
+    if (!ucm_global_opts.enable_events) {
         return UCS_ERR_UNSUPPORTED;
     }
 
-    if (!(events & (UCM_EVENT_FLAG_NO_INSTALL | ucm_external_events))) {
-        status = ucm_event_install(events);
+    if (!(events & UCM_EVENT_FLAG_NO_INSTALL) && (events & ~ucm_external_events)) {
+        status = ucm_event_install(events & ~ucm_external_events);
         if (status != UCS_OK) {
             return status;
         }
@@ -500,3 +885,12 @@ void ucm_unset_event_handler(int events, ucm_event_callback_t cb, void *arg)
     }
 }
 
+UCS_STATIC_INIT {
+    pthread_spin_init(&ucm_kh_lock, PTHREAD_PROCESS_PRIVATE);
+    kh_init_inplace(ucm_ptr_size, &ucm_shmat_ptrs);
+}
+
+UCS_STATIC_CLEANUP {
+    kh_destroy_inplace(ucm_ptr_size, &ucm_shmat_ptrs);
+    pthread_spin_destroy(&ucm_kh_lock);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/malloc/malloc_hook.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/malloc/malloc_hook.c
index d558b6ae6..eee3f9416 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/malloc/malloc_hook.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/malloc/malloc_hook.c
@@ -21,10 +21,8 @@
 #include <ucm/util/log.h>
 #include <ucm/util/reloc.h>
 #include <ucm/util/sys.h>
-#include <ucm/util/ucm_config.h>
 #include <ucs/datastruct/queue.h>
 #include <ucs/type/component.h>
-#include <ucs/type/spinlock.h>
 #include <ucs/sys/compiler.h>
 #include <ucs/sys/math.h>
 #include <ucs/sys/checker.h>
@@ -90,7 +88,7 @@ typedef struct ucm_malloc_hook_state {
     /*
      * Track record of which pointers are ours
      */
-    ucs_spinlock_t        lock;       /* Protect heap counters.
+    pthread_spinlock_t    lock;       /* Protect heap counters.
                                          Note: Cannot modify events when this lock
                                          is held - may deadlock */
     /* Our heap address range. Used to identify whether a released pointer is ours,
@@ -137,14 +135,14 @@ static void ucm_malloc_mmaped_ptr_add(void *ptr)
     int hash_extra_status;
     khiter_t hash_it;
 
-    ucs_spin_lock(&ucm_malloc_hook_state.lock);
+    pthread_spin_lock(&ucm_malloc_hook_state.lock);
 
     hash_it = kh_put(mmap_ptrs, &ucm_malloc_hook_state.ptrs, ptr,
                      &hash_extra_status);
     ucs_assert_always(hash_extra_status >= 0);
     ucs_assert_always(hash_it != kh_end(&ucm_malloc_hook_state.ptrs));
 
-    ucs_spin_unlock(&ucm_malloc_hook_state.lock);
+    pthread_spin_unlock(&ucm_malloc_hook_state.lock);
 }
 
 static int ucm_malloc_mmaped_ptr_remove_if_exists(void *ptr)
@@ -152,7 +150,7 @@ static int ucm_malloc_mmaped_ptr_remove_if_exists(void *ptr)
     khiter_t hash_it;
     int found;
 
-    ucs_spin_lock(&ucm_malloc_hook_state.lock);
+    pthread_spin_lock(&ucm_malloc_hook_state.lock);
 
     hash_it = kh_get(mmap_ptrs, &ucm_malloc_hook_state.ptrs, ptr);
     if (hash_it == kh_end(&ucm_malloc_hook_state.ptrs)) {
@@ -162,7 +160,7 @@ static int ucm_malloc_mmaped_ptr_remove_if_exists(void *ptr)
         kh_del(mmap_ptrs, &ucm_malloc_hook_state.ptrs, hash_it);
     }
 
-    ucs_spin_unlock(&ucm_malloc_hook_state.lock);
+    pthread_spin_unlock(&ucm_malloc_hook_state.lock);
     return found;
 }
 
@@ -170,10 +168,10 @@ static int ucm_malloc_is_address_in_heap(void *ptr)
 {
     int in_heap;
 
-    ucs_spin_lock(&ucm_malloc_hook_state.lock);
+    pthread_spin_lock(&ucm_malloc_hook_state.lock);
     in_heap = (ptr >= ucm_malloc_hook_state.heap_start) &&
               (ptr < ucm_malloc_hook_state.heap_end);
-    ucs_spin_unlock(&ucm_malloc_hook_state.lock);
+    pthread_spin_unlock(&ucm_malloc_hook_state.lock);
     return in_heap;
 }
 
@@ -226,8 +224,8 @@ static void *ucm_malloc_impl(size_t size, const char *debug_name)
     void *ptr;
 
     ucm_malloc_hook_state.hook_called = 1;
-    if (ucm_global_config.alloc_alignment > 1) {
-        ptr = ucm_dlmemalign(ucm_global_config.alloc_alignment, size);
+    if (ucm_global_opts.alloc_alignment > 1) {
+        ptr = ucm_dlmemalign(ucm_global_opts.alloc_alignment, size);
     } else {
         ptr = ucm_dlmalloc(size);
     }
@@ -240,7 +238,11 @@ static void ucm_malloc_adjust_thresholds(size_t size)
     int mmap_thresh;
 
     if (size > ucm_malloc_hook_state.max_freed_size) {
-        if (ucm_global_config.enable_dynamic_mmap_thresh &&
+        /* Valgrind limits the size of brk() segments to 8mb, so must use mmap
+         * for large allocations.
+         */
+        if (!RUNNING_ON_VALGRIND &&
+            ucm_global_opts.enable_dynamic_mmap_thresh &&
             !ucm_malloc_hook_state.trim_thresh_set &&
             !ucm_malloc_hook_state.mmap_thresh_set) {
             /* new mmap threshold is increased to the size of released block,
@@ -287,7 +289,7 @@ static void *ucm_memalign_impl(size_t alignment, size_t size, const char *debug_
     void *ptr;
 
     ucm_malloc_hook_state.hook_called = 1;
-    ptr = ucm_dlmemalign(ucs_max(alignment, ucm_global_config.alloc_alignment), size);
+    ptr = ucm_dlmemalign(ucs_max(alignment, ucm_global_opts.alloc_alignment), size);
     ucm_malloc_allocated(ptr, size, debug_name);
     return ptr;
 }
@@ -493,7 +495,7 @@ out:
 static void ucm_malloc_sbrk(ucm_event_type_t event_type,
                             ucm_event_t *event, void *arg)
 {
-    ucs_spin_lock(&ucm_malloc_hook_state.lock);
+    pthread_spin_lock(&ucm_malloc_hook_state.lock);
 
     /* Copy return value from call. We assume the event handler uses a lock. */
     if (ucm_malloc_hook_state.heap_start == (void*)-1) {
@@ -505,7 +507,7 @@ static void ucm_malloc_sbrk(ucm_event_type_t event_type,
               event->sbrk.increment, event->sbrk.result,
               ucm_malloc_hook_state.heap_start, ucm_malloc_hook_state.heap_end);
 
-    ucs_spin_unlock(&ucm_malloc_hook_state.lock);
+    pthread_spin_unlock(&ucm_malloc_hook_state.lock);
 }
 
 static int ucs_malloc_is_ready(int events)
@@ -521,11 +523,13 @@ static int ucs_malloc_is_ready(int events)
            ucs_test_all_flags(ucm_malloc_hook_state.installed_events, events);
 }
 
-static ucm_event_handler_t ucm_malloc_sbrk_handler = {
-    .events   = UCM_EVENT_SBRK,
-    .priority = 1000,
-    .cb       = ucm_malloc_sbrk
-};
+static void ucm_malloc_event_test_callback(ucm_event_type_t event_type,
+                                           ucm_event_t *event, void *arg)
+{
+    int *out_events = arg;
+
+    *out_events |= event_type;
+}
 
 /* Has to be called with install_mutex held */
 static void ucm_malloc_test(int events)
@@ -545,33 +549,40 @@ static void ucm_malloc_test(int events)
      */
     handler.events   = events;
     handler.priority = -1;
-    handler.cb       = ucm_mmap_event_test_callback;
+    handler.cb       = ucm_malloc_event_test_callback;
     handler.arg      = &out_events;
     out_events       = 0;
 
     ucm_event_handler_add(&handler);
 
-    /* Trigger both small and large allocations
-     * TODO check address / stop all threads */
-    for (i = 0; i < small_alloc_count; ++i) {
-        p[i] = malloc(small_alloc_size);
-    }
-    for (i = 0; i < small_alloc_count; ++i) {
-        free(p[i]);
-    }
-    p[0] = malloc(large_alloc_size);
-    p[0] = realloc(p[0], large_alloc_size * 2);
-    free(p[0]);
+    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {
+        /* Trigger both small and large allocations
+         * TODO check address / stop all threads */
+        for (i = 0; i < small_alloc_count; ++i) {
+            p[i] = malloc(small_alloc_size);
+        }
+        for (i = 0; i < small_alloc_count; ++i) {
+            free(p[i]);
+        }
 
-    if (ucm_malloc_hook_state.hook_called) {
-        ucm_dlmalloc_trim(0);
+        p[0] = malloc(large_alloc_size);
+        p[0] = realloc(p[0], large_alloc_size * 2);
+        free(p[0]);
+
+        if (ucm_malloc_hook_state.hook_called) {
+            ucm_dlmalloc_trim(0);
+        }
+    } else {
+        /* in bistro mode we can't guarantee event fire on malloc calls,
+         * let's just try to call sbrk directly & catch it */
+        ucm_fire_mmap_events(events);
     }
 
     ucm_event_handler_remove(&handler);
 
     ucm_malloc_hook_state.installed_events |= out_events;
 
-    ucm_debug("malloc test: have 0x%x out of 0x%x, hooks were%s called",
+    ucm_debug("malloc test: have 0x%x out of 0x%x, malloc/free hooks were%s called",
               ucm_malloc_hook_state.installed_events, events,
               ucm_malloc_hook_state.hook_called ? "" : " not");
 }
@@ -615,20 +626,25 @@ static int ucm_malloc_mallopt(int param_number, int value)
     return success;
 }
 
+static char *ucm_malloc_blacklist[] = {
+    "libnvidia-fatbinaryloader.so",
+    NULL
+};
+
 static ucm_reloc_patch_t ucm_malloc_symbol_patches[] = {
-    { "free", ucm_free },
-    { "realloc", ucm_realloc },
-    { "malloc", ucm_malloc },
-    { "memalign", ucm_memalign },
-    { "calloc", ucm_calloc },
-    { "valloc", ucm_valloc },
-    { "posix_memalign", ucm_posix_memalign },
-    { "setenv", ucm_setenv },
-    { UCM_OPERATOR_NEW_SYMBOL, ucm_operator_new },
-    { UCM_OPERATOR_DELETE_SYMBOL, ucm_operator_delete },
-    { UCM_OPERATOR_VEC_NEW_SYMBOL, ucm_operator_vec_new },
-    { UCM_OPERATOR_VEC_DELETE_SYMBOL, ucm_operator_vec_delete },
-    { NULL, NULL }
+    { .symbol = "free", .value = ucm_free, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "realloc", .value = ucm_realloc, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "malloc", .value = ucm_malloc, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "memalign", .value = ucm_memalign, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "calloc", .value = ucm_calloc, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "valloc", .value = ucm_valloc, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "posix_memalign", .value = ucm_posix_memalign, .blacklist = ucm_malloc_blacklist },
+    { .symbol = "setenv", .value = ucm_setenv, .blacklist = ucm_malloc_blacklist },
+    { .symbol = UCM_OPERATOR_NEW_SYMBOL, .value = ucm_operator_new, .blacklist = ucm_malloc_blacklist },
+    { .symbol = UCM_OPERATOR_DELETE_SYMBOL, .value = ucm_operator_delete, .blacklist = ucm_malloc_blacklist },
+    { .symbol = UCM_OPERATOR_VEC_NEW_SYMBOL, .value = ucm_operator_vec_new, .blacklist = ucm_malloc_blacklist },
+    { .symbol = UCM_OPERATOR_VEC_DELETE_SYMBOL, .value = ucm_operator_vec_delete, .blacklist = ucm_malloc_blacklist },
+    { .symbol = NULL, .value = NULL }
 };
 
 static ucm_reloc_patch_t ucm_malloc_optional_symbol_patches[] = {
@@ -670,12 +686,15 @@ static void ucm_malloc_set_env_mallopt()
 
 ucs_status_t ucm_malloc_install(int events)
 {
+    static ucm_event_handler_t sbrk_handler = {
+        .events   = UCM_EVENT_SBRK,
+        .priority = 1000,
+        .cb       = ucm_malloc_sbrk
+    };
     ucs_status_t status;
 
     pthread_mutex_lock(&ucm_malloc_hook_state.install_mutex);
 
-    events &= UCM_EVENT_MMAP | UCM_EVENT_MUNMAP | UCM_EVENT_MREMAP | UCM_EVENT_SBRK;
-
     if (ucs_malloc_is_ready(events)) {
         goto out_succ;
     }
@@ -692,7 +711,7 @@ ucs_status_t ucm_malloc_install(int events)
 
     if (!(ucm_malloc_hook_state.install_state & UCM_MALLOC_INSTALLED_SBRK_EVH)) {
         ucm_debug("installing malloc-sbrk event handler");
-        ucm_event_handler_add(&ucm_malloc_sbrk_handler);
+        ucm_event_handler_add(&sbrk_handler);
         ucm_malloc_hook_state.install_state |= UCM_MALLOC_INSTALLED_SBRK_EVH;
     }
 
@@ -702,7 +721,7 @@ ucs_status_t ucm_malloc_install(int events)
      * valgrind anyway.
      */
 #if HAVE_MALLOC_HOOK
-    if (ucm_global_config.enable_malloc_hooks) {
+    if (ucm_global_opts.enable_malloc_hooks) {
         /* Install using malloc hooks.
          * TODO detect glibc support in configure-time.
          */
@@ -727,7 +746,7 @@ ucs_status_t ucm_malloc_install(int events)
     }
 
     /* Install using malloc symbols */
-    if (ucm_global_config.enable_malloc_reloc) {
+    if (ucm_global_opts.enable_malloc_reloc) {
         if (!(ucm_malloc_hook_state.install_state & UCM_MALLOC_INSTALLED_MALL_SYMS)) {
             ucm_debug("installing malloc relocations");
             ucm_malloc_populate_glibc_cache();
@@ -768,7 +787,7 @@ void ucm_malloc_state_reset(int default_mmap_thresh, int default_trim_thresh)
 }
 
 UCS_STATIC_INIT {
-    ucs_spinlock_init(&ucm_malloc_hook_state.lock);
+    pthread_spin_init(&ucm_malloc_hook_state.lock, 0);
     kh_init_inplace(mmap_ptrs, &ucm_malloc_hook_state.ptrs);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/install.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/install.c
index 9e903bb4a..ebf22aaa0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/install.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/install.c
@@ -15,44 +15,99 @@
 #include <ucm/event/event.h>
 #include <ucm/util/log.h>
 #include <ucm/util/reloc.h>
-#include <ucm/util/ucm_config.h>
+#include <ucm/util/sys.h>
+#include <ucm/bistro/bistro.h>
 #include <ucs/sys/math.h>
+#include <ucs/sys/checker.h>
+#include <ucs/debug/assert.h>
 
 #include <sys/mman.h>
 #include <sys/shm.h>
 #include <unistd.h>
 #include <pthread.h>
 
+#define UCM_IS_HOOK_ENABLED(_entry) \
+    ((_entry)->hook_type & UCS_BIT(ucm_mmap_hook_mode()))
+
+#define UCM_HOOK_STR \
+    ((ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) ?  "reloc" : "bistro")
+
+extern const char *ucm_mmap_hook_modes[];
+
+typedef enum ucm_mmap_hook_type {
+    UCM_HOOK_RELOC  = UCS_BIT(UCM_MMAP_HOOK_RELOC),
+    UCM_HOOK_BISTRO = UCS_BIT(UCM_MMAP_HOOK_BISTRO),
+    UCM_HOOK_BOTH   = UCM_HOOK_RELOC | UCM_HOOK_BISTRO
+} ucm_mmap_hook_type_t;
 
 typedef struct ucm_mmap_func {
-    ucm_reloc_patch_t patch;
-    ucm_event_type_t   event_type;
+    ucm_reloc_patch_t    patch;
+    ucm_event_type_t     event_type;
+    ucm_event_type_t     deps;
+    ucm_mmap_hook_type_t hook_type;
 } ucm_mmap_func_t;
 
 static ucm_mmap_func_t ucm_mmap_funcs[] = {
-    { {"mmap",   ucm_override_mmap},   UCM_EVENT_MMAP},
-    { {"munmap", ucm_override_munmap}, UCM_EVENT_MUNMAP},
-    { {"mremap", ucm_override_mremap}, UCM_EVENT_MREMAP},
-    { {"shmat",  ucm_override_shmat},  UCM_EVENT_SHMAT},
-    { {"shmdt",  ucm_override_shmdt},  UCM_EVENT_SHMDT},
-    { {"sbrk",   ucm_override_sbrk},   UCM_EVENT_SBRK},
+    { {"mmap",    ucm_override_mmap},    UCM_EVENT_MMAP,    0, UCM_HOOK_BOTH},
+    { {"munmap",  ucm_override_munmap},  UCM_EVENT_MUNMAP,  0, UCM_HOOK_BOTH},
+    { {"mremap",  ucm_override_mremap},  UCM_EVENT_MREMAP,  0, UCM_HOOK_BOTH},
+    { {"shmat",   ucm_override_shmat},   UCM_EVENT_SHMAT,   0, UCM_HOOK_BOTH},
+    { {"shmdt",   ucm_override_shmdt},   UCM_EVENT_SHMDT,   UCM_EVENT_SHMAT, UCM_HOOK_BOTH},
+    { {"sbrk",    ucm_override_sbrk},    UCM_EVENT_SBRK,    0, UCM_HOOK_RELOC},
+#if UCM_BISTRO_HOOKS
+    { {"brk",     ucm_override_brk},     UCM_EVENT_SBRK,    0, UCM_HOOK_BISTRO},
+#endif
+    { {"madvise", ucm_override_madvise}, UCM_EVENT_MADVISE, 0, UCM_HOOK_BOTH},
     { {NULL, NULL}, 0}
 };
 
-void ucm_mmap_event_test_callback(ucm_event_type_t event_type,
-                                  ucm_event_t *event, void *arg)
+static void ucm_mmap_event_test_callback(ucm_event_type_t event_type,
+                                         ucm_event_t *event, void *arg)
 {
     int *out_events = arg;
+
     *out_events |= event_type;
 }
 
+void ucm_fire_mmap_events(int events)
+{
+    void *p;
+
+    if (events & (UCM_EVENT_MMAP|UCM_EVENT_MUNMAP|UCM_EVENT_MREMAP|
+                  UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED)) {
+        p = mmap(NULL, 0, 0, 0, -1 ,0);
+        p = mremap(p, 0, 0, 0);
+        munmap(p, 0);
+    }
+
+    if (events & (UCM_EVENT_SHMAT|UCM_EVENT_SHMDT|UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED)) {
+        p = shmat(0, NULL, 0);
+        shmdt(p);
+    }
+
+    if (events & (UCM_EVENT_SBRK|UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED)) {
+        (void)sbrk(ucm_get_page_size());
+        (void)sbrk(-ucm_get_page_size());
+    }
+
+    if (events & UCM_EVENT_MADVISE) {
+        p = mmap(NULL, ucm_get_page_size(), PROT_READ|PROT_WRITE,
+                 MAP_PRIVATE|MAP_ANON, -1, 0);
+        if (p != MAP_FAILED) {
+            madvise(p, ucm_get_page_size(), MADV_NORMAL);
+            munmap(p, ucm_get_page_size());
+        } else {
+            ucm_debug("mmap failed: %m");
+        }
+    }
+}
+
 /* Called with lock held */
 static ucs_status_t ucm_mmap_test(int events)
 {
     static int installed_events = 0;
     ucm_event_handler_t handler;
     int out_events;
-    void *p;
 
     if (ucs_test_all_flags(installed_events, events)) {
         /* All requested events are already installed */
@@ -70,20 +125,7 @@ static ucs_status_t ucm_mmap_test(int events)
 
     ucm_event_handler_add(&handler);
 
-    if (events & (UCM_EVENT_MMAP|UCM_EVENT_MUNMAP|UCM_EVENT_MREMAP)) {
-        p = mmap(NULL, 0, 0, 0, -1 ,0);
-        p = mremap(p, 0, 0, 0);
-        munmap(p, 0);
-    }
-
-    if (events & (UCM_EVENT_SHMAT|UCM_EVENT_SHMDT)) {
-        p = shmat(0, NULL, 0);
-        shmdt(p);
-    }
-
-    if (events & UCM_EVENT_SBRK) {
-        (void)sbrk(0);
-    }
+    ucm_fire_mmap_events(events);
 
     ucm_event_handler_remove(&handler);
 
@@ -107,13 +149,13 @@ static ucs_status_t ucs_mmap_install_reloc(int events)
     ucm_mmap_func_t *entry;
     ucs_status_t status;
 
-    if (!ucm_global_config.enable_mmap_reloc) {
-        ucm_debug("installing mmap relocations is disabled by configuration");
+    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_NONE) {
+        ucm_debug("installing mmap hooks is disabled by configuration");
         return UCS_ERR_UNSUPPORTED;
     }
 
     for (entry = ucm_mmap_funcs; entry->patch.symbol != NULL; ++entry) {
-        if (!(entry->event_type & events)) {
+        if (!((entry->event_type|entry->deps) & events)) {
             /* Not required */
             continue;
         }
@@ -123,17 +165,24 @@ static ucs_status_t ucs_mmap_install_reloc(int events)
             continue;
         }
 
-        ucm_debug("mmap: installing relocation table entry for %s = %p for event 0x%x",
-                  entry->patch.symbol, entry->patch.value, entry->event_type);
-
-        status = ucm_reloc_modify(&entry->patch);
-        if (status != UCS_OK) {
-            ucm_warn("failed to install relocation table entry for '%s'",
-                     entry->patch.symbol);
-            return status;
+        if (UCM_IS_HOOK_ENABLED(entry)) {
+            ucm_debug("mmap: installing %s hook for %s = %p for event 0x%x", UCM_HOOK_STR,
+                      entry->patch.symbol, entry->patch.value, entry->event_type);
+
+            if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {
+                status = ucm_reloc_modify(&entry->patch);
+            } else {
+                ucs_assert(ucm_mmap_hook_mode() == UCM_MMAP_HOOK_BISTRO);
+                status = ucm_bistro_patch(entry->patch.symbol, entry->patch.value, NULL);
+            }
+            if (status != UCS_OK) {
+                ucm_warn("failed to install %s hook for '%s'",
+                         UCM_HOOK_STR, entry->patch.symbol);
+                return status;
+            }
+
+            installed_events |= entry->event_type;
         }
-
-        installed_events |= entry->event_type;
     }
 
     return UCS_OK;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/mmap.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/mmap.h
index d57b85a88..35ac47f0e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/mmap.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/mmap/mmap.h
@@ -8,12 +8,20 @@
 #define UCM_MMAP_H_
 
 #include <ucm/api/ucm.h>
+#include <ucs/sys/checker.h>
 
-ucs_status_t ucm_mmap_install(int events);
+#define UCM_MMAP_HOOK_RELOC_STR  "reloc"
+#define UCM_MMAP_HOOK_BISTRO_STR "bistro"
 
-void ucm_mmap_event_test_callback(ucm_event_type_t event_type,
-                                  ucm_event_t *event, void *arg);
+#if UCM_BISTRO_HOOKS
+#  define UCM_DEFAULT_HOOK_MODE UCM_MMAP_HOOK_BISTRO
+#  define UCM_DEFAULT_HOOK_MODE_STR UCM_MMAP_HOOK_BISTRO_STR
+#else
+#  define UCM_DEFAULT_HOOK_MODE UCM_MMAP_HOOK_RELOC
+#  define UCM_DEFAULT_HOOK_MODE_STR UCM_MMAP_HOOK_RELOC_STR
+#endif
 
+ucs_status_t ucm_mmap_install(int events);
 
 void *ucm_override_mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
 int ucm_override_munmap(void *addr, size_t length);
@@ -21,5 +29,19 @@ void *ucm_override_mremap(void *old_address, size_t old_size, size_t new_size, i
 void *ucm_override_shmat(int shmid, const void *shmaddr, int shmflg);
 int ucm_override_shmdt(const void *shmaddr);
 void *ucm_override_sbrk(intptr_t increment);
+void *ucm_sbrk_select(intptr_t increment);
+int ucm_override_brk(void *addr);
+void *ucm_brk_syscall(void *addr);
+int ucm_override_madvise(void *addr, size_t length, int advice);
+void ucm_fire_mmap_events(int events);
+
+static UCS_F_ALWAYS_INLINE ucm_mmap_hook_mode_t ucm_mmap_hook_mode(void)
+{
+    if (RUNNING_ON_VALGRIND && (ucm_global_opts.mmap_hook_mode == UCM_MMAP_HOOK_BISTRO)) {
+        return UCM_MMAP_HOOK_RELOC;
+    }
+
+    return ucm_global_opts.mmap_hook_mode;
+}
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.c
index 9a9a487a4..0ce92c9bf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.c
@@ -5,12 +5,14 @@
  */
 
 #include "log.h"
+#include "sys.h"
 
 #include <ucs/type/component.h>
 #include <sys/time.h>
 #include <string.h>
 #include <stdlib.h>
 #include <stdarg.h>
+#include <stdint.h>
 #include <unistd.h>
 #include <stdio.h>
 #include <ctype.h>
@@ -110,7 +112,7 @@ out:
 static void ucm_log_vsnprintf(char *buf, size_t max, const char *fmt, va_list ap)
 {
     const char *pf;
-    char *pb, *endb, *ps;
+    char *pb, *endb;
     union {
         char          *s;
         long          d;
@@ -147,16 +149,16 @@ static void ucm_log_vsnprintf(char *buf, size_t max, const char *fmt, va_list ap
 
             /* Error message */
             case 'm':
-                ps = strerror_r(eno, pb, endb - pb);
-                if (ps != pb) {
-                    strncpy(pb, ps, endb - pb);
-                }
+                ucm_strerror(eno, pb, endb - pb);
                 pb += strlen(pb);
                 goto done;
 
             /* String */
             case 's':
                 value.s = va_arg(ap, char *);
+                if (!value.s) {
+                    value.s = "(null)";
+                }
                 pad -= strlen(value.s);
                 if (!(flags & UCM_LOG_LTOA_PAD_LEFT)) {
                     pb = ucm_log_add_padding(pb, endb, pad, ' ');
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.h
index 3c186dc7d..c69782341 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/log.h
@@ -11,13 +11,12 @@
 #  include "config.h"
 #endif
 
+#include <ucm/api/ucm.h>
 #include <ucs/config/types.h>
 
-#include "ucm_config.h"
-
 
 #define ucm_log(_level, _message, ...) \
-    if (((_level) <= UCS_MAX_LOG_LEVEL) && ((_level) <= ucm_global_config.log_level)) { \
+    if (((_level) <= UCS_MAX_LOG_LEVEL) && ((_level) <= ucm_global_opts.log_level)) { \
         __ucm_log(__FILE__, __LINE__, __FUNCTION__, (_level), _message, \
                   ## __VA_ARGS__); \
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.c
index cce6493fa..e27b9d57c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.c
@@ -19,7 +19,7 @@
 #include <ucs/sys/compiler.h>
 #include <ucs/type/component.h>
 #include <ucm/util/log.h>
-#include <ucm/util/ucm_config.h>
+#include <ucm/util/sys.h>
 
 #include <sys/fcntl.h>
 #include <sys/mman.h>
@@ -39,6 +39,7 @@ typedef struct ucm_auxv {
 
 
 typedef struct ucm_reloc_dl_iter_context {
+    Dl_info            def_dlinfo;
     ucm_reloc_patch_t  *patch;
     ucs_status_t       status;
 } ucm_reloc_dl_iter_context_t;
@@ -143,7 +144,8 @@ static int ucm_reloc_get_aux_phsize()
 
 static ucs_status_t
 ucm_reloc_modify_got(ElfW(Addr) base, const ElfW(Phdr) *phdr, const char *phname,
-                     int phnum, int phsize, ucm_reloc_patch_t *patch)
+                     int phnum, int phsize,
+                     const ucm_reloc_dl_iter_context_t *ctx)
 {
     ElfW(Phdr) *dphdr;
     ElfW(Rela) *reloc;
@@ -156,12 +158,10 @@ ucm_reloc_modify_got(ElfW(Addr) base, const ElfW(Phdr) *phdr, const char *phname
     void *page;
     int ret;
     int i;
+    Dl_info entry_dlinfo;
+    int success;
 
-    page_size = sysconf(_SC_PAGESIZE);
-    if (page_size < 0) {
-        ucm_error("failed to get page size: %m");
-        return UCS_ERR_IO_ERROR;
-    }
+    page_size = ucm_get_page_size();
 
     /* find PT_DYNAMIC */
     dphdr = NULL;
@@ -184,10 +184,10 @@ ucm_reloc_modify_got(ElfW(Addr) base, const ElfW(Phdr) *phdr, const char *phname
     /* Find matching symbol and replace it */
     for (reloc = jmprel; (void*)reloc < jmprel + pltrelsz; ++reloc) {
         elf_sym = (char*)strtab + symtab[ELF64_R_SYM(reloc->r_info)].st_name;
-        if (!strcmp(patch->symbol, elf_sym)) {
+        if (!strcmp(ctx->patch->symbol, elf_sym)) {
             entry = (void *)(base + reloc->r_offset);
 
-            ucm_trace("'%s' entry in '%s' is at %p", patch->symbol,
+            ucm_trace("'%s' entry in '%s' is at %p", ctx->patch->symbol,
                       basename(phname), entry);
 
             page  = (void *)((intptr_t)entry & ~(page_size - 1));
@@ -196,8 +196,21 @@ ucm_reloc_modify_got(ElfW(Addr) base, const ElfW(Phdr) *phdr, const char *phname
                 ucm_error("failed to modify GOT page %p to rw: %m", page);
                 return UCS_ERR_UNSUPPORTED;
             }
-            patch->prev_value = *entry;
-            *entry = patch->value;
+
+            success = dladdr(*entry, &entry_dlinfo);
+            ucs_assertv_always(success, "can't find shared object with entry %p",
+                               *entry);
+
+            /* store default entry to prev_value to guarantee valid pointers
+             * throughout life time of the process */
+            if (ctx->def_dlinfo.dli_fbase == entry_dlinfo.dli_fbase) {
+                ctx->patch->prev_value = *entry;
+                ucm_trace("'%s' by address %p in '%s' is stored as original for %p",
+                          ctx->patch->symbol, *entry,
+                          basename(entry_dlinfo.dli_fname), ctx->patch->value);
+            }
+
+            *entry = ctx->patch->value;
             break;
         }
     }
@@ -209,6 +222,18 @@ static int ucm_reloc_phdr_iterator(struct dl_phdr_info *info, size_t size, void
 {
     ucm_reloc_dl_iter_context_t *ctx = data;
     int phsize;
+    int i;
+
+    /* check if module is black-listed for this patch */
+    if (ctx->patch->blacklist) {
+        for (i = 0; ctx->patch->blacklist[i]; i++) {
+            if (strstr(info->dlpi_name, ctx->patch->blacklist[i])) {
+                /* module is black-listed */
+                ctx->status = UCS_OK;
+                return 0;
+            }
+        }
+    }
 
     phsize = ucm_reloc_get_aux_phsize();
     if (phsize <= 0) {
@@ -219,7 +244,7 @@ static int ucm_reloc_phdr_iterator(struct dl_phdr_info *info, size_t size, void
 
     ctx->status = ucm_reloc_modify_got(info->dlpi_addr, info->dlpi_phdr,
                                        info->dlpi_name, info->dlpi_phnum,
-                                       phsize, ctx->patch);
+                                       phsize, ctx);
     if (ctx->status == UCS_OK) {
         return 0; /* continue iteration and patch all objects */
     } else {
@@ -230,10 +255,17 @@ static int ucm_reloc_phdr_iterator(struct dl_phdr_info *info, size_t size, void
 /* called with lock held */
 static ucs_status_t ucm_reloc_apply_patch(ucm_reloc_patch_t *patch)
 {
-    ucm_reloc_dl_iter_context_t ctx = {
-        .patch  = patch,
-        .status = UCS_OK
-    };
+    ucm_reloc_dl_iter_context_t ctx;
+    int                         success;
+
+    /* Find default shared object, usually libc */
+    success = dladdr(getpid, &ctx.def_dlinfo);
+    if (!success) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    ctx.patch  = patch;
+    ctx.status = UCS_OK;
 
     /* Avoid locks here because we don't modify ELF data structures.
      * Worst case the same symbol will be written more than once.
@@ -266,8 +298,8 @@ static void *ucm_dlopen(const char *filename, int flag)
          */
         pthread_mutex_lock(&ucm_reloc_patch_list_lock);
         ucs_list_for_each(patch, &ucm_reloc_patch_list, list) {
-            ucm_debug("in dlopen(), re-applying '%s' to %p", patch->symbol,
-                      patch->value);
+            ucm_debug("in dlopen(%s), re-applying '%s' to %p", filename,
+                      patch->symbol, patch->value);
             ucm_reloc_apply_patch(patch);
         }
         pthread_mutex_unlock(&ucm_reloc_patch_list_lock);
@@ -318,6 +350,8 @@ static ucs_status_t ucm_reloc_install_dlopen()
         return status;
     }
 
+    ucs_list_add_tail(&ucm_reloc_patch_list, &ucm_reloc_dlopen_patch.list);
+
     installed = 1;
     return UCS_OK;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.h
index 36b1f0c13..430781bdf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/reloc.h
@@ -22,6 +22,7 @@ typedef struct ucm_reloc_patch {
     void             *value;
     void             *prev_value;
     ucs_list_link_t  list;
+    char             **blacklist;
 } ucm_reloc_patch_t;
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.c
index cde61b60e..9cce7fe3e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.c
@@ -8,13 +8,19 @@
 #  include "config.h"
 #endif
 
+#include <errno.h>
+#include <unistd.h>
+#include <sys/syscall.h>
+
 #include <ucm/event/event.h>
 #include <ucm/util/log.h>
 #include <ucm/util/reloc.h>
 #include <ucm/util/replace.h>
+#include <ucm/mmap/mmap.h>
 #include <ucs/sys/compiler.h>
 #include <ucs/sys/preprocessor.h>
 #include <ucs/type/component.h>
+#include <ucm/bistro/bistro.h>
 
 #if HAVE_CUDA
 #include "ucm/cuda/cudamem.h"
@@ -26,28 +32,162 @@
 pthread_mutex_t ucm_reloc_get_orig_lock = PTHREAD_RECURSIVE_MUTEX_INITIALIZER_NP;
 pthread_t volatile ucm_reloc_get_orig_thread = -1;
 
-UCM_DEFINE_REPLACE_FUNC(mmap,   void*, MAP_FAILED, void*, size_t, int, int, int, off_t)
-UCM_DEFINE_REPLACE_FUNC(munmap, int,   -1,         void*, size_t)
-UCM_DEFINE_REPLACE_FUNC(mremap, void*, MAP_FAILED, void*, size_t, size_t, int)
-UCM_DEFINE_REPLACE_FUNC(shmat,  void*, MAP_FAILED, int, const void*, int)
-UCM_DEFINE_REPLACE_FUNC(shmdt,  int,   -1,         const void*)
-UCM_DEFINE_REPLACE_FUNC(sbrk,   void*, MAP_FAILED, intptr_t)
+UCM_DEFINE_REPLACE_FUNC(mmap,    void*, MAP_FAILED, void*, size_t, int, int, int, off_t)
+UCM_DEFINE_REPLACE_FUNC(munmap,  int,   -1,         void*, size_t)
+UCM_DEFINE_REPLACE_FUNC(mremap,  void*, MAP_FAILED, void*, size_t, size_t, int)
+UCM_DEFINE_REPLACE_FUNC(shmat,   void*, MAP_FAILED, int, const void*, int)
+UCM_DEFINE_REPLACE_FUNC(shmdt,   int,   -1,         const void*)
+UCM_DEFINE_REPLACE_FUNC(sbrk,    void*, MAP_FAILED, intptr_t)
+UCM_DEFINE_REPLACE_FUNC(brk,     int,   -1,         void*)
+UCM_DEFINE_REPLACE_FUNC(madvise, int,   -1,         void*, size_t, int)
 
-#if ENABLE_SYMBOL_OVERRIDE
-UCM_OVERRIDE_FUNC(mmap, void)
-UCM_OVERRIDE_FUNC(munmap, void)
-UCM_OVERRIDE_FUNC(mremap, void)
-UCM_OVERRIDE_FUNC(shmat, void)
-UCM_OVERRIDE_FUNC(shmdt, void)
-UCM_OVERRIDE_FUNC(sbrk, void)
-#endif
+UCM_DEFINE_SELECT_FUNC(mmap, void*, MAP_FAILED, SYS_mmap, void*, size_t, int, int, int, off_t)
+UCM_DEFINE_SELECT_FUNC(munmap, int, -1, SYS_munmap, void*, size_t)
+UCM_DEFINE_SELECT_FUNC(mremap, void*, MAP_FAILED, SYS_mremap, void*, size_t, size_t, int)
+UCM_DEFINE_SELECT_FUNC(madvise, int, -1, SYS_madvise, void*, size_t, int)
 
 #if HAVE_CUDA
 
-UCM_DEFINE_REPLACE_FUNC(cudaFree,   cudaError_t,  -1, void*)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemFree, CUresult,-1, CUdeviceptr)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemFreeHost, CUresult, -1, void *)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemAlloc, CUresult, -1, CUdeviceptr *, size_t)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemAllocManaged, CUresult, -1, CUdeviceptr *,
+                             size_t, unsigned int)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemAllocPitch, CUresult, -1, CUdeviceptr *, size_t *,
+                             size_t, size_t, unsigned int)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemHostGetDevicePointer, CUresult, -1, CUdeviceptr *,
+                             void *, unsigned int)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cuMemHostUnregister, CUresult, -1, void *)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaFree, cudaError_t, -1, void*)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaFreeHost, cudaError_t, -1, void*)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaMalloc, cudaError_t, -1, void**, size_t)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaMallocManaged, cudaError_t, -1, void**, size_t, unsigned int)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaMallocPitch, cudaError_t, -1, void**, size_t *,
+                             size_t, size_t)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaHostGetDevicePointer, cudaError_t, -1, void**,
+                             void *, unsigned int)
+UCM_DEFINE_REPLACE_DLSYM_FUNC(cudaHostUnregister, cudaError_t, -1, void*)
 
 #if ENABLE_SYMBOL_OVERRIDE
-UCM_OVERRIDE_FUNC(cudaFree, cudaError_t)
+UCM_OVERRIDE_FUNC(cuMemFree,                 CUresult)
+UCM_OVERRIDE_FUNC(cuMemFreeHost,             CUresult)
+UCM_OVERRIDE_FUNC(cuMemAlloc,                CUresult)
+UCM_OVERRIDE_FUNC(cuMemAllocManaged,         CUresult)
+UCM_OVERRIDE_FUNC(cuMemAllocPitch,           CUresult)
+UCM_OVERRIDE_FUNC(cuMemHostGetDevicePointer, CUresult)
+UCM_OVERRIDE_FUNC(cuMemHostUnregister,       CUresult)
+UCM_OVERRIDE_FUNC(cudaFree,                  cudaError_t)
+UCM_OVERRIDE_FUNC(cudaFreeHost,              cudaError_t)
+UCM_OVERRIDE_FUNC(cudaMalloc,                cudaError_t)
+UCM_OVERRIDE_FUNC(cudaMallocManaged,         cudaError_t)
+UCM_OVERRIDE_FUNC(cudaMallocPitch,           cudaError_t)
+UCM_OVERRIDE_FUNC(cudaHostGetDevicePointer,  cudaError_t)
+UCM_OVERRIDE_FUNC(cudaHostUnregister,        cudaError_t)
 #endif
 
 #endif
+
+#if UCM_BISTRO_HOOKS
+#if HAVE_DECL_SYS_SHMAT
+
+UCM_DEFINE_SELECT_FUNC(shmat, void*, MAP_FAILED, SYS_shmat, int, const void*, int)
+
+#elif HAVE_DECL_SYS_IPC
+#  ifndef IPCOP_shmat
+#    define IPCOP_shmat 21
+#  endif
+
+_UCM_DEFINE_DLSYM_FUNC(shmat, ucm_orig_dlsym_shmat, ucm_override_shmat,
+                       void*, MAP_FAILED, int, const void*, int)
+
+void *ucm_orig_shmat(int shmid, const void *shmaddr, int shmflg)
+{
+    unsigned long res;
+    void *addr;
+
+    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {
+        return ucm_orig_dlsym_shmat(shmid, shmaddr, shmflg);
+    } else {
+        /* Using IPC syscall of shmat implementation */
+        res = syscall(SYS_ipc, IPCOP_shmat, shmid, shmflg, &addr, shmaddr);
+
+        return res ? MAP_FAILED : addr;
+    }
+}
+
+#endif
+
+#if HAVE_DECL_SYS_SHMDT
+
+UCM_DEFINE_SELECT_FUNC(shmdt, int, -1, SYS_shmdt, const void*)
+
+#elif HAVE_DECL_SYS_IPC
+#  ifndef IPCOP_shmdt
+#    define IPCOP_shmdt 22
+#  endif
+
+_UCM_DEFINE_DLSYM_FUNC(shmdt, ucm_orig_dlsym_shmdt, ucm_override_shmdt,
+                       int, -1, const void*)
+
+int ucm_orig_shmdt(const void *shmaddr)
+{
+    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {
+        return ucm_orig_dlsym_shmdt(shmaddr);
+    } else {
+        /* Using IPC syscall of shmdt implementation */
+        return syscall(SYS_ipc, IPCOP_shmdt, 0, 0, 0, shmaddr);
+    }
+}
+
+#endif
+
+#if HAVE___CURBRK
+extern void *__curbrk;
+#endif
+
+_UCM_DEFINE_DLSYM_FUNC(brk, ucm_orig_dlsym_brk, ucm_override_brk, int, -1, void*)
+
+void *ucm_brk_syscall(void *addr)
+{
+    return (void*)syscall(SYS_brk, addr);
+}
+
+int ucm_orig_brk(void *addr)
+{
+    void *new_addr;
+
+#if HAVE___CURBRK
+    __curbrk =
+#endif
+    new_addr = ucm_brk_syscall(addr);
+
+    if (new_addr < addr) {
+        errno = ENOMEM;
+        return -1;
+    } else {
+        return 0;
+    }
+}
+
+_UCM_DEFINE_DLSYM_FUNC(sbrk, ucm_orig_dlsym_sbrk, ucm_override_sbrk,
+                       void*, MAP_FAILED, intptr_t)
+
+void *ucm_orig_sbrk(intptr_t increment)
+{
+    void *prev;
+
+    if (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_RELOC) {
+        return ucm_orig_dlsym_sbrk(increment);
+    } else {
+        prev = ucm_brk_syscall(0);
+        return ucm_orig_brk(prev + increment) ? (void*)-1 : prev;
+    }
+}
+
+#else /* UCM_BISTRO_HOOKS */
+
+UCM_DEFINE_DLSYM_FUNC(sbrk, void*, MAP_FAILED, intptr_t)
+UCM_DEFINE_DLSYM_FUNC(shmat, void*, MAP_FAILED, int, const void*, int)
+UCM_DEFINE_DLSYM_FUNC(shmdt, int, -1, const void*)
+
+#endif /* UCM_BISTRO_HOOKS */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.h
index 128e4a1a4..8e2c34bff 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/replace.h
@@ -20,12 +20,41 @@ extern pthread_t volatile ucm_reloc_get_orig_thread;
  * the event handler, and if event handler returns error code - calls the original
  * function.
  */
+
+/* Due to CUDA API redifinition we have to create proxy macro to eliminate
+ * redifinition of internal finction names */
 #define UCM_DEFINE_REPLACE_FUNC(_name, _rettype, _fail_val, ...) \
+    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, _rettype, _fail_val, __VA_ARGS__)
+
+#define _UCM_DEFINE_REPLACE_FUNC(_over_name, _ucm_name, _rettype, _fail_val, ...) \
     \
-    _rettype ucm_override_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)); \
+    /* Define a symbol which goes to the replacement - in case we are loaded first */ \
+    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
+    { \
+        _rettype res; \
+        UCM_BISTRO_PROLOGUE; \
+        ucm_trace("%s()", __FUNCTION__); \
+        \
+        if (ucs_unlikely(ucm_reloc_get_orig_thread == pthread_self())) { \
+            return _fail_val; \
+        } \
+        res = _ucm_name(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \
+        UCM_BISTRO_EPILOGUE; \
+        return res; \
+    }
+
+#define UCM_OVERRIDE_FUNC(_name, _rettype) \
+    _rettype _name() __attribute__ ((alias (UCS_PP_QUOTE(ucm_override_##_name)))); \
+
+#define UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \
+    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \
+                          _rettype, _fail_val, __VA_ARGS__)
+
+#define _UCM_DEFINE_DLSYM_FUNC(_name, _orig_name, _over_name, _rettype, _fail_val, ...) \
+    _rettype _over_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)); \
     \
     /* Call the original function using dlsym(RTLD_NEXT) */ \
-    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
+    _rettype _orig_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
     { \
         typedef _rettype (*func_ptr_t) (__VA_ARGS__); \
         static func_ptr_t orig_func_ptr = NULL; \
@@ -36,27 +65,41 @@ extern pthread_t volatile ucm_reloc_get_orig_thread;
             pthread_mutex_lock(&ucm_reloc_get_orig_lock); \
             ucm_reloc_get_orig_thread = pthread_self(); \
             orig_func_ptr = ucm_reloc_get_orig(UCS_PP_QUOTE(_name), \
-                                               ucm_override_##_name); \
+                                               _over_name); \
             ucm_reloc_get_orig_thread = -1; \
             pthread_mutex_unlock(&ucm_reloc_get_orig_lock); \
         } \
         return orig_func_ptr(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \
-    } \
-    \
-    /* Define a symbol which goes to the replacement - in case we are loaded first */ \
-    _rettype ucm_override_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
-    { \
-        ucm_trace("%s()", __FUNCTION__); \
-        \
-        if (ucs_unlikely(ucm_reloc_get_orig_thread == pthread_self())) { \
-            return _fail_val; \
-        } \
-        return ucm_##_name(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \
     }
 
-#define UCM_OVERRIDE_FUNC(_name, _rettype) \
-    _rettype _name() __attribute__ ((alias ("ucm_override_" UCS_PP_QUOTE(_name)))); \
+#define UCM_DEFINE_REPLACE_DLSYM_FUNC(_name, _rettype, _fail_val, ...) \
+    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name, ucm_override_##_name, \
+                          _rettype, _fail_val, __VA_ARGS__) \
+    _UCM_DEFINE_REPLACE_FUNC(ucm_override_##_name, ucm_##_name, \
+                             _rettype, _fail_val, __VA_ARGS__)
 
+#define UCM_DEFINE_SYSCALL_FUNC(_name, _rettype, _syscall_id, ...) \
+    /* Call syscall */ \
+    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
+    { \
+        return (_rettype)syscall(_syscall_id, UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \
+    }
+
+#if UCM_BISTRO_HOOKS
+#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \
+    _UCM_DEFINE_DLSYM_FUNC(_name, ucm_orig_##_name##_dlsym, ucm_override_##_name, \
+                          _rettype, _fail_val, __VA_ARGS__) \
+    UCM_DEFINE_SYSCALL_FUNC(_name##_syscall, _rettype, _syscall_id, __VA_ARGS__) \
+    _rettype ucm_orig_##_name(UCM_FUNC_DEFINE_ARGS(__VA_ARGS__)) \
+    { \
+        return (ucm_mmap_hook_mode() == UCM_MMAP_HOOK_BISTRO) ? \
+               ucm_orig_##_name##_syscall(UCM_FUNC_PASS_ARGS(__VA_ARGS__)) : \
+               ucm_orig_##_name##_dlsym(UCM_FUNC_PASS_ARGS(__VA_ARGS__)); \
+    }
+#else
+#  define UCM_DEFINE_SELECT_FUNC(_name, _rettype, _fail_val, _syscall_id, ...) \
+    UCM_DEFINE_DLSYM_FUNC(_name, _rettype, _fail_val, __VA_ARGS__)
+#endif
 
 /*
  * Define argument list with given types.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.c
index bee9b48f2..ea98df966 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.c
@@ -6,8 +6,13 @@
 
 #include "sys.h"
 
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
 #include <ucm/api/ucm.h>
 #include <ucm/util/log.h>
+#include <ucm/mmap/mmap.h>
 #include <ucs/sys/math.h>
 #include <linux/mman.h>
 #include <sys/mman.h>
@@ -20,8 +25,18 @@
 
 #define UCM_PROC_SELF_MAPS "/proc/self/maps"
 
-
-static size_t ucm_get_page_size()
+ucm_global_config_t ucm_global_opts = {
+    .log_level                  = UCS_LOG_LEVEL_WARN,
+    .enable_events              = 1,
+    .mmap_hook_mode             = UCM_DEFAULT_HOOK_MODE,
+    .enable_malloc_hooks        = 1,
+    .enable_malloc_reloc        = 0,
+    .enable_cuda_reloc          = 1,
+    .enable_dynamic_mmap_thresh = 1,
+    .alloc_alignment            = 16
+};
+
+size_t ucm_get_page_size()
 {
     static long page_size = -1;
     long value;
@@ -229,3 +244,15 @@ size_t ucm_get_shm_seg_size(const void *shmaddr)
     ucm_parse_proc_self_maps(ucm_get_shm_seg_size_cb, &ctx);
     return ctx.seg_size;
 }
+
+void ucm_strerror(int eno, char *buf, size_t max)
+{
+#if STRERROR_R_CHAR_P
+    char *ret = strerror_r(eno, buf, max);
+    if (ret != buf) {
+        strncpy(buf, ret, max);
+    }
+#else
+    (void)strerror_r(eno, buf, max);
+#endif
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.h
index 3ac09ebcd..9cb7deafb 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/sys.h
@@ -35,6 +35,12 @@ void *ucm_sys_realloc(void *oldptr, size_t newsize);
 typedef int (*ucm_proc_maps_cb_t)(void *arg, void *addr, size_t length, int prot);
 
 
+/**
+ * @return Page size on the system.
+ */
+size_t ucm_get_page_size();
+
+
 /**
  * Read and process entries from /proc/self/maps.
  *
@@ -54,4 +60,14 @@ void ucm_parse_proc_self_maps(ucm_proc_maps_cb_t cb, void *arg);
 size_t ucm_get_shm_seg_size(const void *shmaddr);
 
 
+/**
+ * @brief Convert a errno number to error string
+ *
+ *  @param [in]  en    errno value
+ *  @param [out] buf   Buffer to put the error string in
+ *  @param [in]  max   Size of the buffer
+ */
+void ucm_strerror(int eno, char *buf, size_t max);
+
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.c
deleted file mode 100644
index 29657e9d2..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.c
+++ /dev/null
@@ -1,222 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
- *
- * See file LICENSE for terms.
- */
-
-#include "ucm_config.h"
-
-#include <ucm/util/log.h>
-#include <ucs/config/parser.h>
-#include <ucs/type/component.h>
-#include <ucs/sys/checker.h>
-#include <string.h>
-#include <stdlib.h>
-
-#define UCM_ENV_PREFIX           UCS_CONFIG_PREFIX "MEM_"
-
-#define UCM_LOG_LEVEL_VAR        "LOG_LEVEL"
-#define UCM_ALLOC_ALIGN_VAR      "ALLOC_ALIGN"
-#define UCM_EN_EVENTS_VAR        "EVENTS"
-#define UCM_EN_MMAP_RELOC_VAR    "MMAP_RELOC"
-#define UCM_EN_MALLOC_HOOKS_VAR  "MALLOC_HOOKS"
-#define UCM_EN_MALLOC_RELOC_VAR  "MALLOC_RELOC"
-#define UCM_EN_DYNAMIC_MMAP_VAR  "DYNAMIC_MMAP_THRESH"
-#define UCM_EN_CUDA_HOOKS_VAR    "CUDA_HOOKS"
-
-
-ucm_config_t ucm_global_config = {
-    .log_level                  = UCS_LOG_LEVEL_WARN,
-    .alloc_alignment            = 16,
-    .enable_events              = 1,
-    .enable_mmap_reloc          = 1,
-    .enable_malloc_hooks        = 1,
-    .enable_malloc_reloc        = 0,
-    .enable_dynamic_mmap_thresh = 1,
-#if HAVE_CUDA
-    .enable_cuda_hooks          = 1
-#endif
-};
-
-static const char *ucm_config_bool_to_string(int value)
-{
-    return value ? "yes" : "no";
-}
-
-static void ucm_config_print_doc(FILE *stream, const char *doc, const char *syntax,
-                                      ucs_config_print_flags_t print_flags)
-{
-    if (!(print_flags & UCS_CONFIG_PRINT_DOC)) {
-        return;
-    }
-
-    fprintf(stream, "\n");
-    fprintf(stream, "#\n");
-    fprintf(stream, "# %s\n", doc);
-    fprintf(stream, "#\n");
-    fprintf(stream, "# Syntax: %s\n", syntax);
-    fprintf(stream, "#\n");
-}
-
-static void ucm_config_print_bool_doc(FILE *stream, const char *doc,
-                                      ucs_config_print_flags_t print_flags)
-{
-    ucm_config_print_doc(stream, doc, "<yes|no>", print_flags);
-}
-
-void ucm_config_print(FILE *stream, ucs_config_print_flags_t print_flags)
-{
-    if (print_flags & UCS_CONFIG_PRINT_HEADER) {
-        fprintf(stream, "#\n");
-        fprintf(stream, "# UCM configuration\n");
-        fprintf(stream, "#\n");
-    }
-
-    if (!(print_flags & UCS_CONFIG_PRINT_CONFIG)) {
-        return;
-    }
-
-    ucm_config_print_doc(stream,
-                         "Logging level", "<fatal|error|warn|info|debug|trace>",
-                         print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_LOG_LEVEL_VAR,
-            ucm_log_level_names[ucm_global_config.log_level]);
-
-    ucm_config_print_doc(stream,
-                         "Minimal alignment of allocated blocks",
-                         "long integer", print_flags);
-    fprintf(stream, "%s%s=%zu\n", UCM_ENV_PREFIX, UCM_ALLOC_ALIGN_VAR,
-            ucm_global_config.alloc_alignment);
-
-    ucm_config_print_bool_doc(stream,
-                              "Enable memory events",
-                              print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_EVENTS_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_events));
-
-    ucm_config_print_bool_doc(stream,
-                              "Enable installing mmap symbols in the relocation table",
-                              print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_MMAP_RELOC_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_mmap_reloc));
-
-    ucm_config_print_bool_doc(stream,
-                              "Enable using glibc malloc hooks",
-                              print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_MALLOC_HOOKS_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_malloc_hooks));
-
-    ucm_config_print_bool_doc(stream,
-                              "Enable installing malloc symbols in the relocation table.\n"
-                              "This is unsafe and off by default, because sometimes glibc\n"
-                              "calls malloc/free without going through the relocation table,\n"
-                              "which would use the original implementation and not ours.",
-                              print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_MALLOC_RELOC_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_malloc_reloc));
-
-
-    ucm_config_print_bool_doc(stream,
-                              "Enable dynamic mmap threshold: for every released block, the\n"
-                              "mmap threshold is adjusted upward to the size of the size of\n"
-                              "the block, and trim threshold is adjust to twice the size of\n"
-                              "the dynamic mmap threshold.",
-                              print_flags);
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_DYNAMIC_MMAP_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_dynamic_mmap_thresh));
-
-
-#if HAVE_CUDA
-    fprintf(stream, "%s%s=%s\n", UCM_ENV_PREFIX, UCM_EN_CUDA_HOOKS_VAR,
-            ucm_config_bool_to_string(ucm_global_config.enable_cuda_hooks));
-#endif
-}
-
-static void ucm_config_set_value_table(const char *str_value, const char **table,
-                                       int *value)
-{
-    int i;
-
-    for (i = 0; table[i] != NULL; ++i) {
-        if (!strcasecmp(table[i], str_value)) {
-            ucm_global_config.log_level = i;
-            return;
-        }
-    }
-}
-
-static void ucm_config_set_value_bool(const char *str_value, int *value)
-{
-    if (!strcasecmp(str_value, "1") || !strcasecmp(str_value, "y") || !strcasecmp(str_value, "yes")) {
-        *value = 1;
-    } else if (!strcasecmp(str_value, "0") || !strcasecmp(str_value, "n") || !strcasecmp(str_value, "no")) {
-        *value = 0;
-    }
-}
-
-static void ucm_config_set_value_size(const char *str_value, size_t *value)
-{
-    char *endptr;
-    size_t n;
-
-    n = strtoul(str_value, &endptr, 10);
-    if (*endptr == '\0') {
-        *value = n;
-    }
-}
-
-ucs_status_t ucm_config_modify(const char *name, const char *value)
-{
-    if (!strcmp(name, UCM_LOG_LEVEL_VAR)) {
-        ucm_config_set_value_table(value, ucm_log_level_names,
-                                   (int*)&ucm_global_config.log_level);
-    } else if (!strcmp(name, UCM_ALLOC_ALIGN_VAR)) {
-        ucm_config_set_value_size(value, &ucm_global_config.alloc_alignment);
-    } else if (!strcmp(name, UCM_EN_EVENTS_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_events);
-    } else if (!strcmp(name, UCM_EN_MMAP_RELOC_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_mmap_reloc);
-    } else if (!strcmp(name, UCM_EN_MALLOC_HOOKS_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_malloc_hooks);
-    } else if (!strcmp(name, UCM_EN_MALLOC_RELOC_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_malloc_reloc);
-    } else if (!strcmp(name, UCM_EN_DYNAMIC_MMAP_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_dynamic_mmap_thresh);
-#if HAVE_CUDA
-    } else if (!strcmp(name, UCM_EN_CUDA_HOOKS_VAR)) {
-        ucm_config_set_value_bool(value, &ucm_global_config.enable_cuda_hooks);
-#endif
-    } else {
-        return UCS_ERR_INVALID_PARAM;
-    }
-
-    return UCS_OK;
-}
-
-static void ucm_config_set(const char *name)
-{
-    char var_name[64];
-    char *str_value;
-
-    snprintf(var_name, sizeof(var_name), "%s%s", UCM_ENV_PREFIX, name);
-    str_value = getenv(var_name);
-    if (str_value != NULL) {
-        ucm_config_modify(name, str_value);
-    }
-}
-
-UCS_STATIC_INIT {
-    if (RUNNING_ON_VALGRIND) {
-        /* Valgrind limits the size of brk() segments to 8mb, so must use mmap
-         * for large allocations.
-         */
-        ucm_global_config.enable_dynamic_mmap_thresh = 0;
-    }
-    ucm_config_set(UCM_LOG_LEVEL_VAR);
-    ucm_config_set(UCM_ALLOC_ALIGN_VAR);
-    ucm_config_set(UCM_EN_EVENTS_VAR);
-    ucm_config_set(UCM_EN_MMAP_RELOC_VAR);
-    ucm_config_set(UCM_EN_MALLOC_HOOKS_VAR);
-    ucm_config_set(UCM_EN_MALLOC_RELOC_VAR);
-    ucm_config_set(UCM_EN_DYNAMIC_MMAP_VAR);
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.h
deleted file mode 100644
index c99fcd503..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucm/util/ucm_config.h
+++ /dev/null
@@ -1,31 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
- *
- * See file LICENSE for terms.
- */
-
-#ifndef UCM_UTIL_CONFIG_H_
-#define UCM_UTIL_CONFIG_H_
-
-#include <ucs/config/types.h>
-#include <ucs/debug/log.h>
-#include <stdio.h>
-
-
-typedef struct ucm_config {
-    ucs_log_level_t log_level;
-    int             enable_events;
-    int             enable_mmap_reloc;
-    int             enable_malloc_hooks;
-    int             enable_malloc_reloc;
-    int             enable_dynamic_mmap_thresh;
-#if HAVE_CUDA
-    int             enable_cuda_hooks;
-#endif
-    size_t          alloc_alignment;
-} ucm_config_t;
-
-
-extern ucm_config_t ucm_global_config;
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/Makefile.am
index 4c6a63e5b..f690c3c92 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/Makefile.am
@@ -22,7 +22,6 @@ nobase_dist_libucp_la_HEADERS = \
 endif
 
 noinst_HEADERS = \
-	amo/amo.inl \
 	core/ucp_context.h \
 	core/ucp_ep.h \
 	core/ucp_ep.inl \
@@ -41,19 +40,20 @@ noinst_HEADERS = \
 	dt/dt_generic.h \
 	proto/proto.h \
 	proto/proto_am.inl \
+	rma/rma.h \
+	rma/rma.inl \
 	tag/eager.h \
 	tag/rndv.h \
 	tag/tag_match.h \
 	tag/tag_match.inl \
 	tag/offload.h \
 	wireup/address.h \
+	wireup/ep_match.h \
 	wireup/wireup_ep.h \
 	wireup/wireup.h \
 	stream/stream.h
 
 libucp_la_SOURCES = \
-	amo/basic_amo.c \
-	amo/nb_amo.c \
 	core/ucp_context.c \
 	core/ucp_ep.c \
 	core/ucp_listener.c \
@@ -68,7 +68,12 @@ libucp_la_SOURCES = \
 	dt/dt_generic.c \
 	dt/dt.c \
 	proto/proto_am.c \
-	rma/basic_rma.c \
+	rma/amo_basic.c \
+	rma/amo_send.c \
+	rma/amo_sw.c \
+	rma/rma_basic.c \
+	rma/rma_send.c \
+	rma/rma_sw.c \
 	rma/flush.c \
 	tag/eager_rcv.c \
 	tag/eager_snd.c \
@@ -79,6 +84,7 @@ libucp_la_SOURCES = \
 	tag/tag_send.c \
 	tag/offload.c \
 	wireup/address.c \
+	wireup/ep_match.c \
 	wireup/select.c \
 	wireup/signaling_ep.c \
 	wireup/wireup_ep.c \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/amo.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/amo.inl
deleted file mode 100644
index 75f1ad425..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/amo.inl
+++ /dev/null
@@ -1,270 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
- * Copyright (C) UT-Battelle, LLC. 2016.  ALL RIGHTS RESERVED.
- *
- * See file LICENSE for terms.
- */
-
-#ifndef UCP_AMO_INL_
-#define UCP_AMO_INL_
-
-#include <inttypes.h>
-#include <ucp/core/ucp_request.inl>
-#include <ucp/core/ucp_mm.h>
-#include <ucp/core/ucp_ep.inl>
-#include <ucs/sys/preprocessor.h>
-#include <ucs/debug/log.h>
-#include <ucs/debug/profile.h>
-
-static inline
-ucs_status_t ucp_amo_check_send_status(ucp_request_t *req, ucs_status_t status);
-
-#define UCP_AMO_ONE_PARAM (value)
-#define UCP_AMO_TWO_PARAM (value, *result)
-
-#define _UCP_PROGRESS_AMO_NAME(_function)           \
-    UCS_PP_TOKENPASTE(ucp_amo_progress_, _function)
-
-#define UCP_PROGRESS_AMO_DECL(_type, _function, _params) \
-    static ucs_status_t _UCP_PROGRESS_AMO_NAME(_function)(uct_pending_req_t *_self) \
-    { \
-        ucp_request_t *req = ucs_container_of(_self, ucp_request_t, send.uct); \
-        ucp_rkey_h rkey    = req->send.amo.rkey; \
-        ucp_ep_t *ep       = req->send.ep; \
-        _type value = (_type)req->send.amo.value;         \
-        _type *result = (_type *)req->send.amo.result;    \
-        uint64_t remote_addr = req->send.amo.remote_addr; \
-        ucs_status_t status; \
-        \
-        status = UCP_RKEY_RESOLVE(rkey, ep, amo); \
-        if (status != UCS_OK) { \
-            return UCS_ERR_UNREACHABLE; \
-        } \
-        \
-        req->send.lane = rkey->cache.amo_lane; \
-        status = _function(ep->uct_eps[req->send.lane], \
-                           UCS_PP_TUPLE_BREAK _params, \
-                           remote_addr, rkey->cache.amo_rkey, result, \
-                           &req->send.state.uct_comp); \
-        return ucp_amo_check_send_status(req, status); \
-    }
-
-UCP_PROGRESS_AMO_DECL(uint32_t, uct_ep_atomic_swap32, UCP_AMO_ONE_PARAM)
-
-UCP_PROGRESS_AMO_DECL(uint32_t, uct_ep_atomic_fadd32, UCP_AMO_ONE_PARAM)
-
-UCP_PROGRESS_AMO_DECL(uint32_t, uct_ep_atomic_cswap32, UCP_AMO_TWO_PARAM)
-
-UCP_PROGRESS_AMO_DECL(uint64_t, uct_ep_atomic_swap64, UCP_AMO_ONE_PARAM)
-
-UCP_PROGRESS_AMO_DECL(uint64_t, uct_ep_atomic_fadd64, UCP_AMO_ONE_PARAM)
-
-UCP_PROGRESS_AMO_DECL(uint64_t, uct_ep_atomic_cswap64, UCP_AMO_TWO_PARAM)
-
-#define UCP_POST_AMO_DECL(_type, _function) \
-    static ucs_status_t _UCP_PROGRESS_AMO_NAME(_function)(uct_pending_req_t *_self) \
-    { \
-        ucp_request_t *req = ucs_container_of(_self, ucp_request_t, send.uct); \
-        ucp_rkey_h rkey    = req->send.amo.rkey; \
-        ucp_ep_t *ep       = req->send.ep; \
-        _type value = (_type)req->send.amo.value;         \
-        uint64_t remote_addr = req->send.amo.remote_addr; \
-        ucs_status_t status; \
-        \
-        status = UCP_RKEY_RESOLVE(rkey, ep, amo); \
-        if (status != UCS_OK) { \
-            return UCS_ERR_UNREACHABLE; \
-        } \
-        \
-        req->send.lane = rkey->cache.amo_lane; \
-        status = UCS_PROFILE_CALL(_function, ep->uct_eps[req->send.lane], value, \
-                                  remote_addr, rkey->cache.amo_rkey); \
-        return ucp_amo_check_send_status(req, status); \
-    }
-
-UCP_POST_AMO_DECL(uint64_t, uct_ep_atomic_add64)
-
-UCP_POST_AMO_DECL(uint32_t, uct_ep_atomic_add32)
-
-#define UCP_AMO_WITHOUT_RESULT(_ep, _param, _remote_addr, _rkey, _uct_func, _size) \
-    { \
-        ucs_status_t status; \
-        \
-        status = ucp_rma_check_atomic(_remote_addr, _size); \
-        if (status != UCS_OK) { \
-            goto out; \
-        } \
-        \
-        UCP_THREAD_CS_ENTER_CONDITIONAL(&(_ep)->worker->mt_lock); \
-        for (;;) { \
-            status = UCP_RKEY_RESOLVE(_rkey, _ep, amo); \
-            if (status != UCS_OK) { \
-                goto out_unlock; \
-            } \
-            \
-            status = UCS_PROFILE_CALL(_uct_func, (_ep)->uct_eps[(_rkey)->cache.amo_lane], \
-                                      _param, _remote_addr, (_rkey)->cache.amo_rkey); \
-            if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) { \
-                UCP_THREAD_CS_EXIT_CONDITIONAL(&(_ep)->worker->mt_lock); \
-                return status; \
-            } \
-            ucp_worker_progress((_ep)->worker); \
-        } \
-        \
-        status = UCS_OK; \
-    out_unlock: \
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&(_ep)->worker->mt_lock); \
-    out: \
-        return status; \
-    }
-
-#define UCP_RMA_CHECK_ATOMIC_PTR(_addr, _op_size) \
-    do { \
-        ucs_status_t status = ucp_rma_check_atomic(_addr, _op_size); \
-        \
-        if (status != UCS_OK) { \
-            return UCS_STATUS_PTR(status); \
-        } \
-    } while(0)
-
-static inline 
-ucs_status_t ucp_amo_check_send_status(ucp_request_t *req, ucs_status_t status)
-{
-    if (status == UCS_INPROGRESS) {
-        return UCS_OK;
-    }
-    /* Complete for UCS_OK and unexpected errors */
-    if (status != UCS_ERR_NO_RESOURCE) {
-        ucp_request_complete_send(req, status);
-    }
-    return status;
-}
-
-static void ucp_amo_completed_single(uct_completion_t *self,
-                                     ucs_status_t status)
-{
-    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
-                                          send.state.uct_comp);
-    ucs_trace("Invoking completion on AMO request %p", req);
-    ucp_request_complete_send(req, status);
-}
-
-static inline ucs_status_t ucp_rma_check_atomic(uint64_t remote_addr, size_t size)
-{
-    if (ENABLE_PARAMS_CHECK && ((remote_addr % size) != 0)) {
-        ucs_debug("Error: Atomic variable must be naturally aligned "
-                  "(got address 0x%"PRIx64", atomic size %zu)", (remote_addr),
-                  (size));
-        return UCS_ERR_INVALID_PARAM;
-    }
-    return UCS_OK;
-}
-
-static inline ucs_status_ptr_t 
-ucp_amo_send_request(ucp_request_t *req, ucp_send_callback_t cb)
-{
-    ucs_status_t status = ucp_request_send(req);
-
-    if (req->flags & UCP_REQUEST_FLAG_COMPLETED) {
-        ucs_trace_req("releasing send request %p, returning status %s", req,
-                      ucs_status_string(status));
-        ucs_mpool_put(req);
-        return UCS_STATUS_PTR(status);
-    }
-    ucs_trace_req("returning amo request %p, status %s", req,
-                  ucs_status_string(status));
-    ucp_request_set_callback(req, send.cb, cb);
-    return req + 1;
-}
-
-static inline
-uct_pending_callback_t ucp_amo_select_uct_func(ucp_atomic_fetch_op_t opcode, size_t op_size)
-{
-    uct_pending_callback_t progress_func;
-
-    if (op_size == sizeof(uint64_t)) {
-        switch (opcode) {
-        case UCP_ATOMIC_FETCH_OP_CSWAP:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_cswap64);
-            break;
-        case UCP_ATOMIC_FETCH_OP_SWAP:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_swap64);
-            break;
-        case UCP_ATOMIC_FETCH_OP_FADD:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_fadd64);
-            break;
-        default:
-            progress_func = NULL;
-        }
-    } else {
-        switch (opcode) {
-        case UCP_ATOMIC_FETCH_OP_CSWAP:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_cswap32);
-            break;
-        case UCP_ATOMIC_FETCH_OP_SWAP:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_swap32);
-            break;
-        case UCP_ATOMIC_FETCH_OP_FADD:
-            progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_fadd32);
-            break;
-        default:
-            progress_func = NULL;
-        }
-    }
-    return progress_func;
-}
-
-static inline
-uct_pending_callback_t ucp_amo_post_select_uct_func(ucp_atomic_post_op_t opcode, size_t op_size)
-{
-    uct_pending_callback_t progress_func;
-
-    if (opcode != UCP_ATOMIC_POST_OP_ADD) {
-        return NULL;
-    }
-    switch (op_size) {
-    case sizeof(uint32_t):
-        progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_add32);
-        break;
-    case sizeof(uint64_t):
-        progress_func = _UCP_PROGRESS_AMO_NAME(uct_ep_atomic_add64);
-        break;
-    default:
-        progress_func = NULL;
-    }
-
-    return progress_func;
-}
-
-static inline void init_amo_common(ucp_request_t *req, ucp_ep_h ep, uint64_t remote_addr,
-                                   ucp_rkey_h rkey, uint64_t value)
-{
-    req->flags                = 0;
-    req->send.ep              = ep;
-    req->send.amo.remote_addr = remote_addr;
-    req->send.amo.rkey        = rkey;
-    req->send.amo.value       = value;
-#if ENABLE_ASSERT
-    req->send.lane            = UCP_NULL_LANE;
-#endif
-}
-
-static inline void init_amo_req(ucp_request_t *req, ucp_ep_h ep, void *buffer,
-                                ucp_atomic_fetch_op_t op, size_t op_size, uint64_t remote_addr,
-                                ucp_rkey_h rkey, uint64_t value)
-{
-    init_amo_common(req, ep, remote_addr, rkey, value);
-    req->send.state.uct_comp.count  = 1;
-    req->send.state.uct_comp.func   = ucp_amo_completed_single;
-    req->send.amo.result            = buffer;
-    req->send.uct.func              = ucp_amo_select_uct_func(op, op_size);
-}
-
-static inline void init_amo_post(ucp_request_t *req, ucp_ep_h ep, ucp_atomic_post_op_t op,
-                                 size_t op_size, uint64_t remote_addr, ucp_rkey_h rkey,
-                                 uint64_t value)
-{
-    init_amo_common(req, ep, remote_addr, rkey, value);
-    req->send.uct.func = ucp_amo_post_select_uct_func(op, op_size);
-}
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/basic_amo.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/basic_amo.c
deleted file mode 100644
index cce66acee..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/basic_amo.c
+++ /dev/null
@@ -1,120 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
-* Copyright (C) UT-Battelle, LLC. 2016.  ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#include <ucp/amo/amo.inl>
-#include <ucp/core/ucp_mm.h>
-#include <ucp/core/ucp_ep.inl>
-#include <ucs/sys/preprocessor.h>
-#include <ucs/debug/log.h>
-#include <ucs/debug/profile.h>
-#include <inttypes.h>
-
-#define UCP_AMO_WITH_RESULT(_ep, _params, _remote_addr, _rkey, _result, _uct_func, _size) \
-    { \
-        uct_completion_t comp; \
-        ucs_status_t status; \
-        \
-        status = ucp_rma_check_atomic(_remote_addr, _size); \
-        if (status != UCS_OK) { \
-            goto out; \
-        } \
-        \
-        UCP_THREAD_CS_ENTER_CONDITIONAL(&(_ep)->worker->mt_lock); \
-        comp.count = 2; \
-        \
-        for (;;) { \
-            status = UCP_RKEY_RESOLVE(_rkey, _ep, amo); \
-            if (status != UCS_OK) { \
-                goto out_unlock; \
-            } \
-            \
-            status = UCS_PROFILE_CALL(_uct_func, (_ep)->uct_eps[(_rkey)->cache.amo_lane], \
-                                      UCS_PP_TUPLE_BREAK _params, _remote_addr, \
-                                      (_rkey)->cache.amo_rkey, _result, &comp); \
-            if (ucs_likely(status == UCS_OK)) { \
-                goto out_unlock; \
-            } else if (status == UCS_INPROGRESS) { \
-                goto out_wait; \
-            } else if (status != UCS_ERR_NO_RESOURCE) { \
-                goto out_unlock; \
-            } \
-            ucp_worker_progress((_ep)->worker); \
-        } \
-    out_wait: \
-        do { \
-            ucp_worker_progress((_ep)->worker); \
-        } while (comp.count != 1); \
-        status = UCS_OK; \
-    out_unlock: \
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&(_ep)->worker->mt_lock); \
-    out: \
-        return status; \
-    }
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_add32, (ep, add, remote_addr, rkey),
-                 ucp_ep_h ep, uint32_t add, uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    UCP_AMO_WITHOUT_RESULT(ep, add, remote_addr, rkey,
-                           uct_ep_atomic_add32, sizeof(uint32_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_add64, (ep, add, remote_addr, rkey),
-                 ucp_ep_h ep, uint64_t add, uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    UCP_AMO_WITHOUT_RESULT(ep, add, remote_addr, rkey,
-                           uct_ep_atomic_add64, sizeof(uint64_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_fadd32, (ep, add, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint32_t add, uint64_t remote_addr, ucp_rkey_h rkey,
-                 uint32_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (add), remote_addr, rkey, result,
-                        uct_ep_atomic_fadd32, sizeof(uint32_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_fadd64, (ep, add, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint64_t add, uint64_t remote_addr, ucp_rkey_h rkey,
-                 uint64_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (add), remote_addr, rkey, result,
-                        uct_ep_atomic_fadd64, sizeof(uint64_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_swap32, (ep, swap, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint32_t swap, uint64_t remote_addr, ucp_rkey_h rkey,
-                 uint32_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (swap), remote_addr, rkey, result,
-                               uct_ep_atomic_swap32, sizeof(uint32_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_swap64, (ep, swap, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint64_t swap, uint64_t remote_addr, ucp_rkey_h rkey,
-                 uint64_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (swap), remote_addr, rkey, result,
-                        uct_ep_atomic_swap64, sizeof(uint64_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_cswap32,
-                 (ep, compare, swap, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint32_t compare, uint32_t swap,
-                 uint64_t remote_addr, ucp_rkey_h rkey, uint32_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (compare, swap), remote_addr, rkey, result,
-                        uct_ep_atomic_cswap32, sizeof(uint32_t));
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_cswap64,
-                 (ep, compare, swap, remote_addr, rkey, result),
-                 ucp_ep_h ep, uint64_t compare, uint64_t swap,
-                 uint64_t remote_addr, ucp_rkey_h rkey, uint64_t *result)
-{
-    UCP_AMO_WITH_RESULT(ep, (compare, swap), remote_addr, rkey, result,
-                        uct_ep_atomic_cswap64, sizeof(uint64_t));
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/nb_amo.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/nb_amo.c
deleted file mode 100644
index c629d0e07..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/amo/nb_amo.c
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
- * Copyright (C) UT-Battelle, LLC. 2016.  ALL RIGHTS RESERVED.
- *
- * See file LICENSE for terms.
- */
-
-#include "amo.inl"
-#include <ucp/core/ucp_mm.h>
-#include <ucp/core/ucp_ep.inl>
-#include <ucs/sys/preprocessor.h>
-#include <ucs/debug/profile.h>
-#include <ucs/debug/log.h>
-#include <inttypes.h>
-
-ucs_status_ptr_t ucp_atomic_fetch_nb(ucp_ep_h ep, ucp_atomic_fetch_op_t opcode,
-                                     uint64_t value, void *result, size_t op_size,
-                                     uint64_t remote_addr, ucp_rkey_h rkey,
-                                     ucp_send_callback_t cb)
-{
-    ucp_request_t *req;
-    ucs_status_ptr_t status;
-    UCP_RMA_CHECK_ATOMIC_PTR(remote_addr, op_size);
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-    req = ucp_request_get(ep->worker);
-    if (ucs_unlikely(NULL == req)) {
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-        return UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
-    }
-    init_amo_req(req, ep, result, opcode, op_size, remote_addr, rkey, value);
-    status = ucp_amo_send_request(req, cb);
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
-
-ucs_status_t ucp_atomic_post(ucp_ep_h ep, ucp_atomic_post_op_t opcode, uint64_t value,
-                             size_t op_size, uint64_t remote_addr, ucp_rkey_h rkey)    
-{
-    ucs_status_ptr_t status_p;
-    ucs_status_t status;
-    ucp_request_t *req;
-
-    if (ucs_unlikely(opcode != UCP_ATOMIC_POST_OP_ADD)) {
-        return UCS_ERR_INVALID_PARAM;
-    }
-    status = ucp_rma_check_atomic(remote_addr, op_size);
-    if (status != UCS_OK) {
-        return status;
-    }
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    status = UCP_RKEY_RESOLVE(rkey, ep, amo);
-    if (status != UCS_OK) {
-        goto out;
-    }
-    if (op_size == sizeof(uint32_t)) {
-        status = UCS_PROFILE_CALL(uct_ep_atomic_add32, ep->uct_eps[rkey->cache.amo_lane],
-                                  (uint32_t)value, remote_addr, rkey->cache.amo_rkey);
-    } else if (op_size == sizeof(uint64_t)) {
-        status = UCS_PROFILE_CALL(uct_ep_atomic_add64, ep->uct_eps[rkey->cache.amo_lane],
-                                  (uint64_t)value, remote_addr, rkey->cache.amo_rkey);
-    }
-    if (ucs_unlikely(status == UCS_ERR_NO_RESOURCE)) {
-        req = ucp_request_get(ep->worker);
-        if (ucs_unlikely(NULL == req)) {
-            UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-            return UCS_ERR_NO_MEMORY;
-        }
-        init_amo_post(req, ep, opcode, op_size, remote_addr, rkey, value);
-        status_p = ucp_amo_send_request(req, (ucp_send_callback_t)ucs_empty_function);
-        if (UCS_PTR_IS_PTR(status_p)) {
-            ucp_request_release(status_p);
-            status = UCS_INPROGRESS;
-        } else {
-            status = UCS_PTR_STATUS(status_p);
-        }
-    }
-out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp.h
index 4fe28ce03..e029d5bbd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp.h
@@ -2,6 +2,7 @@
 * Copyright (C) Mellanox Technologies Ltd. 2001-2014.  ALL RIGHTS RESERVED.
 * Copyright (C) UT-Battelle, LLC. 2014-2017. ALL RIGHTS RESERVED.
 * Copyright (C) ARM Ltd. 2016-2017.  ALL RIGHTS RESERVED.
+* Copyright (C) Los Alamos National Security, LLC. 2018 ALL RIGHTS RESERVED.
 * See file LICENSE for terms.
 */
 
@@ -172,13 +173,19 @@ enum ucp_worker_params_field {
  * are present. It is used for the enablement of backward compatibility support.
  */
 enum ucp_listener_params_field {
-    UCP_LISTENER_PARAM_FIELD_SOCK_ADDR       = UCS_BIT(0), /**< Sock address and
-                                                                length */
-    UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER  = UCS_BIT(1)  /**< User's callback
-                                                                and argument
-                                                                for handling the
-                                                                creation of an
-                                                                endpoint */
+    /**
+     * Sock address and length.
+     */
+    UCP_LISTENER_PARAM_FIELD_SOCK_ADDR           = UCS_BIT(0),
+
+    /**
+     * User's callback and argument for handling the creation of an endpoint.
+     * */
+    UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER      = UCS_BIT(1),
+
+    /**< User's callback and argument for handling the incoming connection
+     *   request. */
+    UCP_LISTENER_PARAM_FIELD_CONN_HANDLER        = UCS_BIT(2)
 };
 
 
@@ -198,7 +205,8 @@ enum ucp_ep_params_field {
                                                             transport level errors */
     UCP_EP_PARAM_FIELD_USER_DATA         = UCS_BIT(3), /**< User data pointer */
     UCP_EP_PARAM_FIELD_SOCK_ADDR         = UCS_BIT(4), /**< Socket address field */
-    UCP_EP_PARAM_FIELD_FLAGS             = UCS_BIT(5)  /**< Endpoint flags */
+    UCP_EP_PARAM_FIELD_FLAGS             = UCS_BIT(5), /**< Endpoint flags */
+    UCP_EP_PARAM_FIELD_CONN_REQUEST      = UCS_BIT(6)  /**< Connection request field */
 };
 
 
@@ -210,7 +218,7 @@ enum ucp_ep_params_field {
  * @ref ucp_ep_create() function.
  */
 enum ucp_ep_params_flags_field {
-    UCP_EP_PARAMS_FLAGS_CLIENT_SERVER  = UCS_BIT(0)   /**< Using a client-server
+    UCP_EP_PARAMS_FLAGS_CLIENT_SERVER  = UCS_BIT(0),  /**< Using a client-server
                                                            connection establishment
                                                            mechanism.
                                                            @ref ucs_sock_addr_t
@@ -218,6 +226,15 @@ enum ucp_ep_params_flags_field {
                                                            must be provided and
                                                            contain the address
                                                            of the remote peer */
+    UCP_EP_PARAMS_FLAGS_NO_LOOPBACK    = UCS_BIT(1)   /**< Avoid connecting the
+                                                           endpoint to itself when
+                                                           connecting the endpoint
+                                                           to the same worker it
+                                                           was created on.
+                                                           Affects protocols which
+                                                           send to a particular
+                                                           remote endpoint, for
+                                                           example stream */
 };
 
 
@@ -257,14 +274,12 @@ enum ucp_ep_close_mode {
 enum ucp_mem_map_params_field {
     UCP_MEM_MAP_PARAM_FIELD_ADDRESS = UCS_BIT(0), /**< Address of the memory that
                                                        would be used in the
-                                                       @ref ucp_mem_map routine,
-                                                       see @ref ucp_mem_map_matrix
-                                                       for details */
+                                                       @ref ucp_mem_map routine. */
     UCP_MEM_MAP_PARAM_FIELD_LENGTH  = UCS_BIT(1), /**< The size of memory that
                                                        would be allocated or
                                                        registered in the
                                                        @ref ucp_mem_map routine.*/
-    UCP_MEM_MAP_PARAM_FIELD_FLAGS   = UCS_BIT(2)  /**< Allocation flags */
+    UCP_MEM_MAP_PARAM_FIELD_FLAGS   = UCS_BIT(2)  /**< Allocation flags. */
 };
 
 /**
@@ -276,7 +291,7 @@ enum ucp_mem_map_params_field {
  */
 enum ucp_mem_advise_params_field {
     UCP_MEM_ADVISE_PARAM_FIELD_ADDRESS = UCS_BIT(0), /**< Address of the memory */
-    UCP_MEM_ADVISE_PARAM_FIELD_LENGTH  = UCS_BIT(1), /**< The size of memory */ 
+    UCP_MEM_ADVISE_PARAM_FIELD_LENGTH  = UCS_BIT(1), /**< The size of memory */
     UCP_MEM_ADVISE_PARAM_FIELD_ADVICE  = UCS_BIT(2)  /**< Advice on memory usage */
 };
 
@@ -357,6 +372,9 @@ enum {
  */
 typedef enum {
     UCP_ATOMIC_POST_OP_ADD, /**< Atomic add */
+    UCP_ATOMIC_POST_OP_AND, /**< Atomic and */
+    UCP_ATOMIC_POST_OP_OR,  /**< Atomic or  */
+    UCP_ATOMIC_POST_OP_XOR, /**< Atomic xor */
     UCP_ATOMIC_POST_OP_LAST
 } ucp_atomic_post_op_t;
 
@@ -370,13 +388,32 @@ typedef enum {
  * will fetch data from the remote node.
  */
 typedef enum {
-    UCP_ATOMIC_FETCH_OP_FADD, /**< Atomic Fetch and add */
-    UCP_ATOMIC_FETCH_OP_SWAP, /**< Atomic swap */
+    UCP_ATOMIC_FETCH_OP_FADD,  /**< Atomic Fetch and add    */
+    UCP_ATOMIC_FETCH_OP_SWAP,  /**< Atomic swap             */
     UCP_ATOMIC_FETCH_OP_CSWAP, /**< Atomic conditional swap */
+    UCP_ATOMIC_FETCH_OP_FAND,  /**< Atomic Fetch and and    */
+    UCP_ATOMIC_FETCH_OP_FOR,   /**< Atomic Fetch and or     */
+    UCP_ATOMIC_FETCH_OP_FXOR,  /**< Atomic Fetch and xor    */
     UCP_ATOMIC_FETCH_OP_LAST
 } ucp_atomic_fetch_op_t;
 
 
+/**
+ * @ingroup UCP_COMM
+ * @brief Flags to define behavior of @ref ucp_stream_recv_nb function
+ *
+ * This enumeration defines behavior of @ref ucp_stream_recv_nb function.
+ */
+typedef enum {
+    UCP_STREAM_RECV_FLAG_WAITALL = UCS_BIT(0)  /**< This flag requests that
+                                                    operation will not be
+                                                    completed untill all amout
+                                                    of requested data is
+                                                    received and placed in the
+                                                    user buffer. */
+} ucp_stream_recv_flags_t;
+
+
 /**
  * @ingroup UCP_DATATYPE
  * @brief Generate an identifier for contiguous data type.
@@ -513,15 +550,15 @@ typedef struct ucp_generic_dt_ops {
      * The pointer refers to application defined unpack routine.
      *
      * @param [in]  state          State as returned by
-     *                             @ref ucp_generic_dt_ops::start_pack
-     *                             "start_pack()" routine.
+     *                             @ref ucp_generic_dt_ops::start_unpack
+     *                             "start_unpack()" routine.
      * @param [in]  offset         Virtual offset in the input stream.
      * @param [in]  src            Source to unpack the data from.
      * @param [in]  length         Length to unpack.
      *
      * @return UCS_OK or an error if unpacking failed.
      */
-    ucs_status_t (*unpack)(void *state, size_t offset, const void *src, size_t count);
+    ucs_status_t (*unpack)(void *state, size_t offset, const void *src, size_t length);
 
     /**
      * @ingroup UCP_DATATYPE
@@ -596,7 +633,7 @@ typedef struct ucp_params {
      * Pointer to a routine that is responsible for final cleanup of the memory
      * associated with the request. This routine may not be called every time a
      * request is released. For some implementations, the cleanup call may be
-     * delayed and only invoked at @ref ucp_worker_cleanup.
+     * delayed and only invoked at @ref ucp_worker_destroy.
      *
      * @e NULL can be used if no such function is required, which is also the
      * default if this field is not specified by @ref field_mask.
@@ -620,6 +657,9 @@ typedef struct ucp_params {
      * thread safety; if the context is used by worker 1 and worker 2,
      * and worker 1 is used by thread 1 and worker 2 is used by thread 2,
      * then this context needs thread safety.
+     * Note that actual thread mode may be different from mode passed
+     * to @ref ucp_init. To get actual thread mode use
+     * @ref ucp_context_query.
      */
     int                                mt_workers_shared;
 
@@ -706,11 +746,16 @@ typedef struct ucp_worker_params {
     uint64_t                field_mask;
 
     /**
-     * Thread safety "mode" for the worker object and resources associated with it.
-     * This value is optional.
-     * If it's not set (along with its corresponding bit in the field_mask -
-     * UCP_WORKER_PARAM_FIELD_THREAD_MODE), the UCS_THREAD_MODE_SINGLE mode
-     * will be used.
+     * The parameter thread_mode suggests the thread safety mode which worker
+     * and the associated resources should be created with. This is an
+     * optional parameter. The default value is UCS_THREAD_MODE_SINGLE and
+     * it is used when the value of the parameter is not set. When this
+     * parameter along with its corresponding bit in the
+     * field_mask - UCP_WORKER_PARAM_FIELD_THREAD_MODE is set, the
+     * @ref ucp_worker_create attempts to create worker with this thread mode.
+     * The thread mode with which worker is created can differ from the
+     * suggested mode. The actual thread mode of the worker should be obtained
+     * using the query interface @ref ucp_worker_query.
      */
     ucs_thread_mode_t       thread_mode;
 
@@ -745,12 +790,12 @@ typedef struct ucp_worker_params {
      * This value is optional.
      * If @ref UCP_WORKER_PARAM_FIELD_EVENT_FD is set in the field_mask, events
      * on the worker will be reported on the provided event file descriptor. In
-     * this case, calling @ref ucp_worker_get_efd() will result in an error.
+     * this case, calling @ref ucp_worker_get_efd will result in an error.
      * The provided file descriptor must be capable of aggregating notifications
      * for arbitrary events, for example @c epoll(7) on Linux systems.
      * @ref user_data will be used as the event user-data on systems which
      * support it. For example, on Linux, it will be placed in
-     * @ref epoll_data_t::ptr, when returned from @ref epoll_wait().
+     * @c epoll_data_t::ptr, when returned from @c epoll_wait(2).
      *
      * Otherwise, events would be reported to the event file descriptor returned
      * from @ref ucp_worker_get_efd().
@@ -774,7 +819,7 @@ typedef struct ucp_listener_params {
      * Fields not specified in this mask would be ignored.
      * Provides ABI compatibility with respect to adding new fields.
      */
-    uint64_t                       field_mask;
+    uint64_t                            field_mask;
 
     /**
      * An address in the form of a sockaddr.
@@ -783,7 +828,7 @@ typedef struct ucp_listener_params {
      * The @ref ucp_listener_create routine will return with an error if sockaddr
      * is not specified.
      */
-    ucs_sock_addr_t                sockaddr;
+    ucs_sock_addr_t                     sockaddr;
 
     /**
      * Handler to endpoint creation in a client-server connection flow.
@@ -791,73 +836,16 @@ typedef struct ucp_listener_params {
      * UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER needs to be set in the
      * field_mask.
      */
-    ucp_listener_accept_handler_t  accept_handler;
-} ucp_listener_params_t;
-
+    ucp_listener_accept_handler_t       accept_handler;
 
-/**
- * @ingroup UCP_ENDPOINT
- * @brief Tuning parameters for the UCP endpoint.
- *
- * The structure defines the parameters that are used for the
- * UCP endpoint tuning during the UCP ep @ref ucp_ep_create "creation".
- */
-typedef struct ucp_ep_params {
     /**
-     * Mask of valid fields in this structure, using bits from
-     * @ref ucp_ep_params_field.
-     * Fields not specified in this mask would be ignored.
-     * Provides ABI compatibility with respect to adding new fields.
-     */
-    uint64_t                field_mask;
-
-    /**
-     * Destination address; if the @ref UCP_EP_PARAMS_FLAGS_CLIENT_SERVER flag
-     * is not set, this address is mandatory for filling
-     * (along with its corresponding bit in the field_mask - @ref
-     * UCP_EP_PARAM_FIELD_REMOTE_ADDRESS) and must be obtained using
-     * @ref ucp_worker_get_address. This field cannot be changed by
-     * @ref ucp_ep_modify_nb.
-     */
-    const ucp_address_t     *address;
-
-    /**
-     * Desired error handling mode, optional parameter. Default value is
-     * @ref UCP_ERR_HANDLING_MODE_NONE. This field cannot be changed by
-     * @ref ucp_ep_modify_nb.
-     */
-    ucp_err_handling_mode_t err_mode;
-
-    /**
-     * Handler to process transport level failure.
-     */
-    ucp_err_handler_t       err_handler;
-
-    /**
-     * User data associated with an endpoint. See @ref ucp_stream_poll_ep_t and
-     * @ref ucp_err_handler_t
-     */
-    void                    *user_data;
-
-    /**
-     * Endpoint flags from @ref ucp_ep_params_flags_field.
-     * This value is optional.
-     * If it's not set (along with its corresponding bit in the field_mask -
-     * @ref UCP_EP_PARAM_FIELD_FLAGS), the @ref ucp_ep_create() routine will
-     * consider the flags as set to zero.
-     */
-     unsigned               flags;
-
-    /**
-     * Destination address in the form of a sockaddr;
-     * if the @ref UCP_EP_PARAMS_FLAGS_CLIENT_SERVER flag is set, this address
-     * is mandatory for filling (along with its corresponding bit in the
-     * field_mask - @ref UCP_EP_PARAM_FIELD_SOCK_ADDR) and should be obtained
-     * from the user. This field cannot be changed by @ref ucp_ep_modify_nb.
+     * Handler of an incoming connection request in a client-server connection
+     * flow. In order for the callback inside this handler to be invoked, the
+     * @ref UCP_LISTENER_PARAM_FIELD_CONN_HANDLER needs to be set in the
+     * field_mask.
      */
-    ucs_sock_addr_t         sockaddr;
-
-} ucp_ep_params_t;
+    ucp_listener_conn_handler_t         conn_handler;
+} ucp_listener_params_t;
 
 
 /**
@@ -962,7 +950,7 @@ struct ucp_tag_recv_info {
  * the run-time environment. Then, the fetched descriptor is used for
  * UCP library @ref ucp_init "initialization". The Application can print out the
  * descriptor using @ref ucp_config_print "print" routine. In addition
- * the application is responsible to @ref ucp_config_free "free" the
+ * the application is responsible to @ref ucp_config_release "release" the
  * descriptor back to UCP library.
  *
  * @param [in]  env_prefix    If non-NULL, the routine searches for the
@@ -1308,8 +1296,8 @@ unsigned ucp_worker_progress(ucp_worker_h worker);
  * @brief Poll for endpoints that are ready to consume streaming data.
  *
  * This non-blocking routine returns endpoints on a worker which are ready
- * to consume streaming data. The ready endpoints are placed in @poll_eps array,
- * and the function return value indicates how many are there.
+ * to consume streaming data. The ready endpoints are placed in @a poll_eps
+ * array, and the function return value indicates how many are there.
  *
  * @param [in]   worker    Worker to poll.
  * @param [out]  poll_eps  Pointer to array of endpoints, should be
@@ -1318,7 +1306,7 @@ unsigned ucp_worker_progress(ucp_worker_h worker);
  *                         in @a poll_eps.
  * @param [in]   flags     Reserved for future use.
  *
- * @return Negative value indicates an error according to @ref ucp_status_t.
+ * @return Negative value indicates an error according to @ref ucs_status_t.
  *         On success, non-negative value (less or equal @a max_eps) indicates
  *         actual number of endpoints filled in @a poll_eps array.
  *
@@ -1412,6 +1400,7 @@ ucs_status_t ucp_worker_wait(ucp_worker_h worker);
  * an opportunity for energy savings for architectures that support this
  * functionality.
  *
+ * @param [in] worker           Worker to wait for updates on.
  * @param [in] address          Local memory address
  */
 void ucp_worker_wait_mem(ucp_worker_h worker, void *address);
@@ -1440,26 +1429,31 @@ void ucp_worker_wait_mem(ucp_worker_h worker, void *address);
  *
  * @code {.c}
  * void application_initialization() {
+ * // should be called once in application init flow and before
+ * // process_comminucation() is used
  *     ...
  *     status = ucp_worker_get_efd(worker, &fd);
  *     ...
  * }
  *
  * void process_comminucation() {
+ * // should be called every time need to wait for some condition such as
+ * // ucp request completion in sleep mode.
+ *
  *     for (;;) {
- *         // check for events as long as progress is made
+ *         // check for stop condition as long as progress is made
  *         if (check_for_events()) {
- *              break;                    // got something interesting, exit
+ *              break;
  *         } else if (ucp_worker_progress(worker)) {
- *              continue;                 // got something uninteresting, retry
+ *              continue;                 // some progress happened but condition not met
  *         }
  *
  *         // arm the worker and clean-up fd
  *         status = ucp_worker_arm(worker);
  *         if (UCS_OK == status) {
- *             poll(&fds, nfds, timeout);  // wait for events
+ *             poll(&fds, nfds, timeout);  // wait for events (sleep mode)
  *         } else if (UCS_ERR_BUSY == status) {
- *             continue;                   // poll for more events
+ *             continue;                   // could not arm, need to progress more
  *         } else {
  *             abort();
  *         }
@@ -1493,6 +1487,11 @@ ucs_status_t ucp_worker_arm(ucp_worker_h worker);
  * waiting on a file descriptor from @ref ucp_worker_get_efd to return, even
  * if no event from the underlying interfaces has taken place.
  *
+ * @note It’s safe to use this routine from any thread, even if UCX is compiled
+ *       without multi-threading support and/or initialized with any value of
+ *       @ref ucp_params_t::mt_workers_shared and
+ *       @ref ucp_worker_params_t::thread_mode parameters
+ *
  * @param [in]  worker    Worker to wait for events on.
  *
  * @return Error code as defined by @ref ucs_status_t
@@ -1555,42 +1554,23 @@ void ucp_listener_destroy(ucp_listener_h listener);
  * @param [out] ep_p        A handle to the created endpoint.
  *
  * @return Error code as defined by @ref ucs_status_t
+ *
+ * @note One of the following fields has to be specified:
+ *  - ucp_ep_params_t::address
+ *  - ucp_ep_params_t::sockaddr
+ *  - ucp_ep_params_t::conn_request
+
+ * @note By default, ucp_ep_create() will connect an endpoint to itself if
+ * the endpoint is destined to the same @a worker on which it was created,
+ * i.e. @a params.address belongs to @a worker. This behavior can be changed by
+ * passing the @ref UCP_EP_PARAMS_FLAGS_NO_LOOPBACK flag in @a params.flags.
+ * In that case, the endpoint will be connected to the *next* endpoint created
+ * in the same way on the same @a worker.
  */
 ucs_status_t ucp_ep_create(ucp_worker_h worker, const ucp_ep_params_t *params,
                            ucp_ep_h *ep_p);
 
 
-/**
- * @ingroup UCP_ENDPOINT
- * @brief Modify endpoint parameters.
- *
- * This routine modifies @ref ucp_ep_h "endpoint" created by @ref ucp_ep_create
- * or @ref ucp_listener_accept_callback_t. For example, this API can be used
- * to setup custom parameters like @ref ucp_ep_params_t::user_data or
- * @ref ucp_ep_params_t::err_handler_cb to endpoint created by 
- * @ref ucp_listener_accept_callback_t.
- *
- * @param [in]  ep          A handle to the endpoint.
- * @param [in]  params      User defined @ref ucp_ep_params_t configurations
- *                          for the @ref ucp_ep_h "UCP endpoint".
- *
- * @return NULL             - The endpoint is modified successfully.
- * @return UCS_PTR_IS_ERR(_ptr) - The reconfiguration failed and an error code
- *                                indicates the status. However, the @a endpoint
- *                                is not modified and can be used further.
- * @return otherwise        - The reconfiguration process is started, and can be
- *                            completed at any point in time. A request handle
- *                            is returned to the application in order to track
- *                            progress of the endpoint modification.
- *                            The application is responsible for releasing the
- *                            handle using the @ref ucp_request_free routine.
- *
- * @note See the documentation of @ref ucp_ep_params_t for details, only some of
- *       the parameters can be modified.
- */
-ucs_status_ptr_t ucp_ep_modify_nb(ucp_ep_h ep, const ucp_ep_params_t *params);
-
-
 /**
  * @ingroup UCP_ENDPOINT
  *
@@ -1614,12 +1594,32 @@ ucs_status_ptr_t ucp_ep_modify_nb(ucp_ep_h ep, const ucp_ep_params_t *params);
  *                            is responsible for releasing the handle using the
  *                            @ref ucp_request_free routine.
  *
- * @note @ref ucp_ep_close_nb replaces deprecated @ref ucp_disconnect_nb and 
+ * @note @ref ucp_ep_close_nb replaces deprecated @ref ucp_disconnect_nb and
  *       @ref ucp_ep_destroy
  */
 ucs_status_ptr_t ucp_ep_close_nb(ucp_ep_h ep, unsigned mode);
 
 
+/**
+ * @ingroup UCP_WORKER
+ *
+ * @brief Reject an incoming connection request.
+ *
+ * Reject the incoming connection request and release associated resources. If
+ * the remote initiator endpoint has set an @ref ucp_ep_params_t::err_handler,
+ * it will be invoked with status @ref UCS_ERR_REJECTED.
+ *
+ * @param [in]  listener        Handle to the listener on which the connection
+ *                              request was received.
+ * @param [in]  conn_request    Handle to the connection request to reject.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ *
+ */
+ucs_status_t ucp_listener_reject(ucp_listener_h listener,
+                                 ucp_conn_request_h conn_request);
+
+
 /**
  * @ingroup UCP_ENDPOINT
  * @brief Print endpoint information.
@@ -1681,7 +1681,7 @@ void ucp_ep_print_info(ucp_ep_h ep, FILE *stream);
  *             ucp_worker_progress(worker);
  *             status = ucp_request_check_status(request);
  *         } while (status == UCS_INPROGRESS);
- *         ucp_request_release(request);
+ *         ucp_request_free(request);
  *         return status;
  *     }
  * }
@@ -1723,14 +1723,14 @@ ucs_status_ptr_t ucp_ep_flush_nb(ucp_ep_h ep, unsigned flags,
  * management.
  *
  * <table>
- * <caption id="ucp_mem_map_matrix">Matrix of behavior</caption>
+ * <caption>Matrix of behavior</caption>
  * <tr><th>parameter/flag <td align="center">@ref UCP_MEM_MAP_NONBLOCK "NONBLOCK"</td>
  *                        <td align="center">@ref UCP_MEM_MAP_ALLOCATE "ALLOCATE"</td>
  *                        <td align="center">@ref UCP_MEM_MAP_FIXED "FIXED"</td>
  *                        <td align="center">@ref ucp_mem_map_params.address "address"</td>
  *                        <td align="center">@b result
  * <tr><td rowspan="8" align="center">@b value <td rowspan="8" align="center">0/1 - the value\n only affects the\n register/map\n phase</td>
- *                                               <td align="center">0 <td align="center">0 <td align="center">0 <td align="center">@ref anch_err "error"
+ *                                               <td align="center">0 <td align="center">0 <td align="center">0 <td align="center">@ref anch_err "error" if length > 0
  * <tr>                                          <td align="center">1 <td align="center">0 <td align="center">0 <td align="center">@ref anch_alloc_reg "alloc+register"
  * <tr>                                          <td align="center">0 <td align="center">1 <td align="center">0 <td align="center">@ref anch_err "error"</td>
  * <tr>                                          <td align="center">0 <td align="center">0 <td align="center">defined <td align="center">@ref anch_reg "register"
@@ -1825,7 +1825,7 @@ typedef enum ucp_mem_advice {
     UCP_MADV_NORMAL   = 0,  /**< No special treatment */
     UCP_MADV_WILLNEED       /**< can be used on the memory mapped with
                                  @ref UCP_MEM_MAP_NONBLOCK to speed up memory
-                                 mapping and to avoid page faults when 
+                                 mapping and to avoid page faults when
                                  the memory is accessed for the first time. */
 } ucp_mem_advice_t;
 
@@ -1835,7 +1835,7 @@ typedef enum ucp_mem_advice {
  * @brief Tuning parameters for the UCP memory advice.
  *
  * This structure defines the parameters that are used for the
- * UCP memory advice tuning during the @ref ucp_mem_advise "ucp_mem_advise" 
+ * UCP memory advice tuning during the @ref ucp_mem_advise "ucp_mem_advise"
  * routine.
  */
 typedef struct ucp_mem_advise_params {
@@ -1847,7 +1847,7 @@ typedef struct ucp_mem_advise_params {
     uint64_t                field_mask;
 
     /**
-     * Memory base address. 
+     * Memory base address.
      */
      void                   *address;
 
@@ -1869,20 +1869,20 @@ typedef struct ucp_mem_advise_params {
  *
  * This routine advises the UCP about how to handle memory range beginning at
  * address and size of length bytes. This call does not influence the semantics
- * of the application, but may influence its performance. The UCP may ignore 
+ * of the application, but may influence its performance. The UCP may ignore
  * the advice.
  *
  * @param [in]  context     Application @ref ucp_context_h "context" which was
  *                          used to allocate/map the memory.
  * @param [in]  memh        @ref ucp_mem_h "Handle" to memory region.
- * @param [in]  params      Memory base address and length. The advice field 
- *                          is used to pass memory use advice as defined in 
+ * @param [in]  params      Memory base address and length. The advice field
+ *                          is used to pass memory use advice as defined in
  *                          the @ref ucp_mem_advice list
  *                          The memory range must belong to the @a memh
  *
  * @return Error code as defined by @ref ucs_status_t
  */
-ucs_status_t ucp_mem_advise(ucp_context_h context, ucp_mem_h memh,  
+ucs_status_t ucp_mem_advise(ucp_context_h context, ucp_mem_h memh,
                             ucp_mem_advise_params_t *params);
 
 
@@ -1970,13 +1970,13 @@ ucs_status_t ucp_ep_rkey_unpack(ucp_ep_h ep, const void *rkey_buffer,
  * that are reacheble via shared memory.
  *
  * @param [in]  rkey          A remote key handle.
- * @param [in]  raddr         A remote address within the memory area
+ * @param [in]  raddr         A remote memory address within the memory area
  *                            described by the rkey.
  * @param [out] addr_p        A pointer that can be used for direct
  *                            access to the remote memory.
  *
  * @return Error code as defined by @ref ucs_status_t if the remote memory
- *         cannot be accessed directly or the remote address is not valid.
+ *         cannot be accessed directly or the remote memory address is not valid.
  */
 ucs_status_t ucp_rkey_ptr(ucp_rkey_h rkey, uint64_t raddr, void **addr_p);
 
@@ -2208,7 +2208,7 @@ ucs_status_ptr_t ucp_tag_send_sync_nb(ucp_ep_h ep, const void *buffer, size_t co
 
 /**
  * @ingroup UCP_COMM
- * @brief Non-blocking stream receive operation of structured data into a 
+ * @brief Non-blocking stream receive operation of structured data into a
  *        user-supplied buffer.
  *
  * This routine receives data that is described by the local address @a buffer,
@@ -2235,7 +2235,7 @@ ucs_status_ptr_t ucp_tag_send_sync_nb(ucp_ep_h ep, const void *buffer, size_t co
  *                          valid only if return code is UCS_OK.
  * @note                    The amount of data received, in bytes, is always an
  *                          integral multiple of the @a datatype size.
- * @param [in]     flags    Reserved for future use.
+ * @param [in]     flags    Flags defined in @ref ucp_stream_recv_flags_t.
  *
  * @return UCS_OK               - The receive operation was completed
  *                                immediately.
@@ -2449,30 +2449,6 @@ ucs_status_ptr_t ucp_tag_msg_recv_nb(ucp_worker_h worker, void *buffer,
                                      ucp_tag_recv_callback_t cb);
 
 
-/**
- * @ingroup UCP_COMM
- * @brief Blocking remote memory put operation.
- *
- * This routine stores contiguous block of data that is described by the
- * local address @a buffer in the remote contiguous memory region described by
- * @a remote_addr address and the @ref ucp_rkey_h "memory handle" @a rkey.  The
- * routine returns when it is safe to reuse the source address @e buffer.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  buffer       Pointer to the local source address.
- * @param [in]  length       Length of the data (in bytes) stored under the
- *                           source address.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           to write to.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_put(ucp_ep_h ep, const void *buffer, size_t length,
-                     uint64_t remote_addr, ucp_rkey_h rkey);
-
-
 /**
  * @ingroup UCP_COMM
  * @brief Non-blocking implicit remote memory put operation.
@@ -2492,41 +2468,62 @@ ucs_status_t ucp_put(ucp_ep_h ep, const void *buffer, size_t length,
  * @param [in]  buffer       Pointer to the local source address.
  * @param [in]  length       Length of the data (in bytes) stored under the
  *                           source address.
- * @param [in]  remote_addr  Pointer to the destination remote address
+ * @param [in]  remote_addr  Pointer to the destination remote memory address
  *                           to write to.
  * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
+ *                           remote memory address.
  *
  * @return Error code as defined by @ref ucs_status_t
  */
 ucs_status_t ucp_put_nbi(ucp_ep_h ep, const void *buffer, size_t length,
                          uint64_t remote_addr, ucp_rkey_h rkey);
 
-
 /**
  * @ingroup UCP_COMM
- * @brief Blocking remote memory get operation.
+ * @brief Non-blocking remote memory put operation.
  *
- * This routine loads contiguous block of data that is described by the remote
- * address @a remote_addr and the @ref ucp_rkey_h "memory handle" @a rkey in
- * the local contiguous memory region described by @a buffer address.  The
- * routine returns when remote data is loaded and stored under the local address
- * @e buffer.
+ * This routine initiates a storage of contiguous block of data that is
+ * described by the local address @a buffer in the remote contiguous memory
+ * region described by @a remote_addr address and the @ref ucp_rkey_h "memory
+ * handle" @a rkey.  The routine returns immediately and @b does @b not
+ * guarantee re-usability of the source address @e buffer. If the operation is
+ * completed immediately the routine return UCS_OK, otherwise UCS_INPROGRESS
+ * or an error is returned to user. If the put operation completes immediately,
+ * the routine returns UCS_OK and the call-back routine @a cb is @b not
+ * invoked. If the operation is @b not completed immediately and no error is
+ * reported, then the UCP library will schedule invocation of the call-back
+ * routine @a cb upon completion of the put operation. In other words, the
+ * completion of a put operation can be signaled by the return code or
+ * execution of the call-back.
  *
+ * @note A user can use @ref ucp_worker_flush_nb "ucp_worker_flush_nb()"
+ * in order to guarantee re-usability of the source address @e buffer.
  *
  * @param [in]  ep           Remote endpoint handle.
  * @param [in]  buffer       Pointer to the local source address.
  * @param [in]  length       Length of the data (in bytes) stored under the
  *                           source address.
- * @param [in]  remote_addr  Pointer to the destination remote address
+ * @param [in]  remote_addr  Pointer to the destination remote memory address
  *                           to write to.
  * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
+ *                           remote memory address.
+ * @param [in]  cb           Call-back function that is invoked whenever the
+ *                           put operation is completed and the local buffer
+ *                           can be modified. Does not guarantee remote
+ *                           completion.
  *
- * @return Error code as defined by @ref ucs_status_t
+ * @return UCS_OK               - The operation was completed immediately.
+ * @return UCS_PTR_IS_ERR(_ptr) - The operation failed.
+ * @return otherwise            - Operation was scheduled and can be
+ *                              completed at any point in time. The request handle
+ *                              is returned to the application in order to track
+ *                              progress of the operation. The application is
+ *                              responsible for releasing the handle using
+ *                              @ref ucp_request_free "ucp_request_free()" routine.
  */
-ucs_status_t ucp_get(ucp_ep_h ep, void *buffer, size_t length,
-                     uint64_t remote_addr, ucp_rkey_h rkey);
+ucs_status_ptr_t ucp_put_nb(ucp_ep_h ep, const void *buffer, size_t length,
+                            uint64_t remote_addr, ucp_rkey_h rkey,
+                            ucp_send_callback_t cb);
 
 
 /**
@@ -2534,7 +2531,7 @@ ucs_status_t ucp_get(ucp_ep_h ep, void *buffer, size_t length,
  * @brief Non-blocking implicit remote memory get operation.
  *
  * This routine initiate a load of contiguous block of data that is described
- * by the remote address @a remote_addr and the @ref ucp_rkey_h "memory handle"
+ * by the remote memory address @a remote_addr and the @ref ucp_rkey_h "memory handle"
  * @a rkey in the local contiguous memory region described by @a buffer
  * address.  The routine returns immediately and @b does @b not guarantee that
  * remote data is loaded and stored under the local address @e buffer.
@@ -2547,264 +2544,61 @@ ucs_status_t ucp_get(ucp_ep_h ep, void *buffer, size_t length,
  * @param [in]  buffer       Pointer to the local source address.
  * @param [in]  length       Length of the data (in bytes) stored under the
  *                           source address.
- * @param [in]  remote_addr  Pointer to the destination remote address
+ * @param [in]  remote_addr  Pointer to the destination remote memory address
  *                           to write to.
  * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
+ *                           remote memory address.
  *
  * @return Error code as defined by @ref ucs_status_t
  */
 ucs_status_t ucp_get_nbi(ucp_ep_h ep, void *buffer, size_t length,
                          uint64_t remote_addr, ucp_rkey_h rkey);
 
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic add operation for 32 bit integers
- *
- * This routine performs an add operation on a 32 bit integer value atomically.
- * The remote integer value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a add value is the value that is used for the add operation.
- * When the operation completes the sum of the original remote value and the
- * operand value (@a add) is stored in remote memory.
- * The call to the routine returns immediately, independent of operation
- * completion.
- *
- * @note The remote address must be aligned to 32 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  add          Value to add.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_add32(ucp_ep_h ep, uint32_t add,
-                              uint64_t remote_addr, ucp_rkey_h rkey);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic add operation for 64 bit integers
- *
- * This routine performs an add operation on a 64 bit integer value atomically.
- * The remote integer value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a add value is the value that is used for the add operation.
- * When the operation completes the sum of the original remote value and the
- * operand value (@a add) is stored in remote memory.
- * The call to the routine returns immediately, independent of operation
- * completion.
- *
- * @note The remote address must be aligned to 64 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  add          Value to add.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_add64(ucp_ep_h ep, uint64_t add,
-                              uint64_t remote_addr, ucp_rkey_h rkey);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic fetch and add operation for 32 bit integers
- *
- * This routine performs an add operation on a 32 bit integer value atomically.
- * The remote integer value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a add value is the value that is used for the add operation.
- * When the operation completes, the original remote value is stored in the
- * local memory @a result, and the sum of the original remote value and the
- * operand value is stored in remote memory.
- * The call to the routine returns when the operation is completed and the
- * @a result value is updated.
- *
- * @note The remote address must be aligned to 32 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  add          Value to add.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_fadd32(ucp_ep_h ep, uint32_t add, uint64_t remote_addr,
-                               ucp_rkey_h rkey, uint32_t *result);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic fetch and add operation for 64 bit integers
- *
- * This routine performs an add operation on a 64 bit integer value atomically.
- * The remote integer value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a add value is the value that is used for the add operation.
- * When the operation completes, the original remote value is stored in the
- * local memory @a result, and the sum of the original remote value and the
- * operand value is stored in remote memory.
- * The call to the routine returns when the operation is completed and the
- * @a result value is updated.
- *
- * @note The remote address must be aligned to 64 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  add          Value to add.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_fadd64(ucp_ep_h ep, uint64_t add, uint64_t remote_addr,
-                               ucp_rkey_h rkey, uint64_t *result);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic swap operation for 32 bit values
- *
- * This routine swaps a 32 bit value between local and remote memory.
- * The remote value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a swap value is the value that is used for the swap operation.
- * When the operation completes, the remote value is stored in the
- * local memory @a result, and the operand value (@a swap) is stored in remote
- * memory.  The call to the routine returns when the operation is completed and
- * the @a result value is updated.
- *
- * @note The remote address must be aligned to 32 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  swap         Value to swap.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_swap32(ucp_ep_h ep, uint32_t swap, uint64_t remote_addr,
-                               ucp_rkey_h rkey, uint32_t *result);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic swap operation for 64 bit values
- *
- * This routine swaps a 64 bit value between local and remote memory.
- * The remote value is described by the combination of the remote
- * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
- * @a rkey. The @a swap value is the value that is used for the swap operation.
- * When the operation completes, the remote value is stored in the
- * local memory @a result, and the operand value (@a swap) is stored in remote
- * memory.  The call to the routine returns when the operation is completed and
- * the @a result value is updated.
- *
- * @note The remote address must be aligned to 64 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  swap         Value to swap.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_swap64(ucp_ep_h ep, uint64_t swap, uint64_t remote_addr,
-                               ucp_rkey_h rkey, uint64_t *result);
-
-
-/**
- * @ingroup UCP_COMM
- * @brief Blocking atomic conditional swap (cswap) operation for 32 bit values.
- *
- * This routine conditionally swaps a 32 bit value between local and remote
- * memory. The swap occurs only if the condition value (@a continue) is equal
- * to the remote value, otherwise the remote memory is not modified.  The
- * remote value is described by the combination of the remote memory address @p
- * remote_addr and the @ref ucp_rkey_h "remote memory handle" @a rkey. The @p
- * swap value is the value that is used to update the remote memory if the
- * condition is true.  The call to the routine returns when the operation is
- * completed and the @a result value is updated.
- *
- * @note The remote address must be aligned to 32 bit.
- *
- * @param [in]  ep           Remote endpoint handle.
- * @param [in]  compare      Value to compare to.
- * @param [in]  swap         Value to swap.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
- * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
- *
- * @return Error code as defined by @ref ucs_status_t
- */
-ucs_status_t ucp_atomic_cswap32(ucp_ep_h ep, uint32_t compare, uint32_t swap,
-                                uint64_t remote_addr, ucp_rkey_h rkey,
-                                uint32_t *result);
-
-
 /**
  * @ingroup UCP_COMM
- * @brief Blocking atomic conditional swap (cswap) operation for 64 bit values.
+ * @brief Non-blocking remote memory get operation.
+ *
+ * This routine initiates a load of a contiguous block of data that is
+ * described by the remote memory address @a remote_addr and the @ref ucp_rkey_h
+ * "memory handle" @a rkey in the local contiguous memory region described
+ * by @a buffer address. The routine returns immediately and @b does @b not
+ * guarantee that remote data is loaded and stored under the local address @e
+ * buffer. If the operation is completed immediately the routine return UCS_OK,
+ * otherwise UCS_INPROGRESS or an error is returned to user. If the get
+ * operation completes immediately, the routine returns UCS_OK and the
+ * call-back routine @a cb is @b not invoked. If the operation is @b not
+ * completed immediately and no error is reported, then the UCP library will
+ * schedule invocation of the call-back routine @a cb upon completion of the
+ * get operation. In other words, the completion of a get operation can be
+ * signaled by the return code or execution of the call-back.
  *
- * This routine conditionally swaps a 64 bit value between local and remote
- * memory. The swap occurs only if the condition value (@a continue) is equal
- * to the remote value, otherwise the remote memory is not modified.  The
- * remote value is described by the combination of the remote memory address @p
- * remote_addr and the @ref ucp_rkey_h "remote memory handle" @a rkey. The @p
- * swap value is the value that is used to update the remote memory if the
- * condition is true.  The call to the routine returns when the operation is
- * completed and the @a result value is updated.
- *
- * @note The remote address must be aligned to 64 bit.
+ * @note A user can use @ref ucp_worker_flush_nb "ucp_worker_flush_nb()"
+ * in order to guarantee re-usability of the source address @e buffer.
  *
  * @param [in]  ep           Remote endpoint handle.
- * @param [in]  compare      Value to compare to.
- * @param [in]  swap         Value to swap.
- * @param [in]  remote_addr  Pointer to the destination remote address
- *                           of the atomic variable.
+ * @param [in]  buffer       Pointer to the local source address.
+ * @param [in]  length       Length of the data (in bytes) stored under the
+ *                           source address.
+ * @param [in]  remote_addr  Pointer to the destination remote memory address
+ *                           to write to.
  * @param [in]  rkey         Remote memory key associated with the
- *                           remote address.
- * @param [out] result       Pointer to the address that is used to store
- *                           the previous value of the atomic variable described
- *                           by the @a remote_addr
+ *                           remote memory address.
+ * @param [in]  cb           Call-back function that is invoked whenever the
+ *                           get operation is completed and the data is
+ *                           visible to the local process.
  *
- * @return Error code as defined by @ref ucs_status_t
+ * @return UCS_OK               - The operation was completed immediately.
+ * @return UCS_PTR_IS_ERR(_ptr) - The operation failed.
+ * @return otherwise            - Operation was scheduled and can be
+ *                              completed at any point in time. The request handle
+ *                              is returned to the application in order to track
+ *                              progress of the operation. The application is
+ *                              responsible for releasing the handle using
+ *                              @ref ucp_request_free "ucp_request_free()" routine.
  */
-ucs_status_t ucp_atomic_cswap64(ucp_ep_h ep, uint64_t compare, uint64_t swap,
-                                uint64_t remote_addr, ucp_rkey_h rkey,
-                                uint64_t *result);
-
+ucs_status_ptr_t ucp_get_nb(ucp_ep_h ep, void *buffer, size_t length,
+                            uint64_t remote_addr, ucp_rkey_h rkey,
+                            ucp_send_callback_t cb);
 
 /**
  * @ingroup UCP_COMM
@@ -2823,7 +2617,7 @@ ucs_status_t ucp_atomic_cswap64(ucp_ep_h ep, uint64_t compare, uint64_t swap,
  * @param [in] value       Source operand for the atomic operation.
  * @param [in] op_size     Size of value in bytes
  * @param [in] remote_addr Remote address to operate on.
- * @param [in] rkey        Remote key handle for the remote address.
+ * @param [in] rkey        Remote key handle for the remote memory address.
  *
  * @return Error code as defined by @ref ucs_status_t
  */
@@ -2862,7 +2656,7 @@ ucs_status_t ucp_atomic_post(ucp_ep_h ep, ucp_atomic_post_op_t opcode, uint64_t
  *                         is true.
  * @param [in] op_size     Size of value in bytes and pointer type for result
  * @param [in] remote_addr Remote address to operate on.
- * @param [in] rkey        Remote key handle for the remote address.
+ * @param [in] rkey        Remote key handle for the remote memory address.
  * @param [in] cb          Call-back function that is invoked whenever the
  *                         send operation is completed. It is important to note
  *                         that the call-back function is only invoked in a case when
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_compat.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_compat.h
index e8309e497..fdc4e6e0d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_compat.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_compat.h
@@ -14,6 +14,18 @@
 
 BEGIN_C_DECLS
 
+
+/**
+ * @ingroup UCP_WORKER
+ * @deprecated Replaced by @ref ucp_listener_conn_handler_t.
+ */
+typedef struct ucp_listener_accept_handler {
+   ucp_listener_accept_callback_t  cb;       /**< Endpoint creation callback */
+   void                            *arg;     /**< User defined argument for the
+                                                  callback */
+} ucp_listener_accept_handler_t;
+
+
 /**
  * @ingroup UCP_COMM
  * @deprecated Replaced by @ref ucp_request_test.
@@ -69,6 +81,28 @@ ucs_status_t ucp_ep_flush(ucp_ep_h ep);
  *
  * @brief Flush outstanding AMO and RMA operations on the @ref ucp_worker_h
  * "worker"
+ * @deprecated Replaced by @ref ucp_worker_flush_nb. The following example
+ * implements the same functionality using @ref ucp_worker_flush_nb :
+ * @code
+ * ucs_status_t worker_flush(ucp_worker_h worker)
+ * {
+ *     void *request = ucp_worker_flush_nb(worker);
+ *     if (request == NULL) {
+ *         return UCS_OK;
+ *     } else if (UCS_PTR_IS_ERR(request)) {
+ *         return UCS_PTR_STATUS(request);
+ *     } else {
+ *         ucs_status_t status;
+ *         do {
+ *             ucp_worker_progress(worker);
+ *             status = ucp_request_check_status(request);
+ *         } while (status == UCS_INPROGRESS);
+ *         ucp_request_release(request);
+ *         return status;
+ *     }
+ * }
+ * @endcode
+ *
  *
  * This routine flushes all outstanding AMO and RMA communications on the
  * @ref ucp_worker_h "worker". All the AMO and RMA operations issued on the
@@ -85,6 +119,385 @@ ucs_status_t ucp_ep_flush(ucp_ep_h ep);
  */
 ucs_status_t ucp_worker_flush(ucp_worker_h worker);
 
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking remote memory put operation.
+ * @deprecated Replaced by @ref ucp_put_nb. The following example implements
+ * the same functionality using @ref ucp_put_nb :
+ * @code
+ * void empty_callback(void *request, ucs_status_t status)
+ * {
+ * }
+ *
+ * ucs_status_t put(ucp_ep_h ep, const void *buffer, size_t length,
+ *                   uint64_t remote_addr, ucp_rkey_h rkey)
+ * {
+ *     void *request = ucp_put_nb(ep, buffer, length, remote_addr, rkey,
+ *                                empty_callback),
+ *     if (request == NULL) {
+ *         return UCS_OK;
+ *     } else if (UCS_PTR_IS_ERR(request)) {
+ *         return UCS_PTR_STATUS(request);
+ *     } else {
+ *         ucs_status_t status;
+ *         do {
+ *             ucp_worker_progress(worker);
+ *             status = ucp_request_check_status(request);
+ *         } while (status == UCS_INPROGRESS);
+ *         ucp_request_release(request);
+ *         return status;
+ *     }
+ * }
+ * @endcode
+ *
+ * This routine stores contiguous block of data that is described by the
+ * local address @a buffer in the remote contiguous memory region described by
+ * @a remote_addr address and the @ref ucp_rkey_h "memory handle" @a rkey.  The
+ * routine returns when it is safe to reuse the source address @e buffer.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  buffer       Pointer to the local source address.
+ * @param [in]  length       Length of the data (in bytes) stored under the
+ *                           source address.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           to write to.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_put(ucp_ep_h ep, const void *buffer, size_t length,
+                     uint64_t remote_addr, ucp_rkey_h rkey);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking remote memory get operation.
+ * @deprecated Replaced by @ref ucp_get_nb. @see ucp_put.
+ *
+ * This routine loads contiguous block of data that is described by the remote
+ * address @a remote_addr and the @ref ucp_rkey_h "memory handle" @a rkey in
+ * the local contiguous memory region described by @a buffer address.  The
+ * routine returns when remote data is loaded and stored under the local address
+ * @e buffer.
+ *
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  buffer       Pointer to the local source address.
+ * @param [in]  length       Length of the data (in bytes) stored under the
+ *                           source address.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           to write to.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_get(ucp_ep_h ep, void *buffer, size_t length,
+                     uint64_t remote_addr, ucp_rkey_h rkey);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic add operation for 32 bit integers
+ * @deprecated Replaced by @ref ucp_atomic_post with opcode UCP_ATOMIC_POST_OP_ADD.
+ * @see ucp_put.
+ *
+ * This routine performs an add operation on a 32 bit integer value atomically.
+ * The remote integer value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a add value is the value that is used for the add operation.
+ * When the operation completes the sum of the original remote value and the
+ * operand value (@a add) is stored in remote memory.
+ * The call to the routine returns immediately, independent of operation
+ * completion.
+ *
+ * @note The remote address must be aligned to 32 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  add          Value to add.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_add32(ucp_ep_h ep, uint32_t add,
+                              uint64_t remote_addr, ucp_rkey_h rkey);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic add operation for 64 bit integers
+ * @deprecated Replaced by @ref ucp_atomic_post with opcode UCP_ATOMIC_POST_OP_ADD.
+ * @see ucp_put.
+ *
+ * This routine performs an add operation on a 64 bit integer value atomically.
+ * The remote integer value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a add value is the value that is used for the add operation.
+ * When the operation completes the sum of the original remote value and the
+ * operand value (@a add) is stored in remote memory.
+ * The call to the routine returns immediately, independent of operation
+ * completion.
+ *
+ * @note The remote address must be aligned to 64 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  add          Value to add.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_add64(ucp_ep_h ep, uint64_t add,
+                              uint64_t remote_addr, ucp_rkey_h rkey);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic fetch and add operation for 32 bit integers
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_FADD.
+ * @see ucp_put.
+ *
+ * This routine performs an add operation on a 32 bit integer value atomically.
+ * The remote integer value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a add value is the value that is used for the add operation.
+ * When the operation completes, the original remote value is stored in the
+ * local memory @a result, and the sum of the original remote value and the
+ * operand value is stored in remote memory.
+ * The call to the routine returns when the operation is completed and the
+ * @a result value is updated.
+ *
+ * @note The remote address must be aligned to 32 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  add          Value to add.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_fadd32(ucp_ep_h ep, uint32_t add, uint64_t remote_addr,
+                               ucp_rkey_h rkey, uint32_t *result);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic fetch and add operation for 64 bit integers
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_FADD.
+ * @see ucp_put.
+ *
+ * This routine performs an add operation on a 64 bit integer value atomically.
+ * The remote integer value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a add value is the value that is used for the add operation.
+ * When the operation completes, the original remote value is stored in the
+ * local memory @a result, and the sum of the original remote value and the
+ * operand value is stored in remote memory.
+ * The call to the routine returns when the operation is completed and the
+ * @a result value is updated.
+ *
+ * @note The remote address must be aligned to 64 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  add          Value to add.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_fadd64(ucp_ep_h ep, uint64_t add, uint64_t remote_addr,
+                               ucp_rkey_h rkey, uint64_t *result);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic swap operation for 32 bit values
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_SWAP.
+ * @see ucp_put.
+ *
+ * This routine swaps a 32 bit value between local and remote memory.
+ * The remote value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a swap value is the value that is used for the swap operation.
+ * When the operation completes, the remote value is stored in the
+ * local memory @a result, and the operand value (@a swap) is stored in remote
+ * memory.  The call to the routine returns when the operation is completed and
+ * the @a result value is updated.
+ *
+ * @note The remote address must be aligned to 32 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  swap         Value to swap.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_swap32(ucp_ep_h ep, uint32_t swap, uint64_t remote_addr,
+                               ucp_rkey_h rkey, uint32_t *result);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic swap operation for 64 bit values
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_SWAP.
+ * @see ucp_put.
+ *
+ * This routine swaps a 64 bit value between local and remote memory.
+ * The remote value is described by the combination of the remote
+ * memory address @a remote_addr and the @ref ucp_rkey_h "remote memory handle"
+ * @a rkey. The @a swap value is the value that is used for the swap operation.
+ * When the operation completes, the remote value is stored in the
+ * local memory @a result, and the operand value (@a swap) is stored in remote
+ * memory.  The call to the routine returns when the operation is completed and
+ * the @a result value is updated.
+ *
+ * @note The remote address must be aligned to 64 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  swap         Value to swap.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_swap64(ucp_ep_h ep, uint64_t swap, uint64_t remote_addr,
+                               ucp_rkey_h rkey, uint64_t *result);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic conditional swap (cswap) operation for 32 bit values.
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_CSWAP.
+ * @see ucp_put.
+ *
+ * This routine conditionally swaps a 32 bit value between local and remote
+ * memory. The swap occurs only if the condition value (@a continue) is equal
+ * to the remote value, otherwise the remote memory is not modified.  The
+ * remote value is described by the combination of the remote memory address @p
+ * remote_addr and the @ref ucp_rkey_h "remote memory handle" @a rkey. The @p
+ * swap value is the value that is used to update the remote memory if the
+ * condition is true.  The call to the routine returns when the operation is
+ * completed and the @a result value is updated.
+ *
+ * @note The remote address must be aligned to 32 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  compare      Value to compare to.
+ * @param [in]  swap         Value to swap.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_cswap32(ucp_ep_h ep, uint32_t compare, uint32_t swap,
+                                uint64_t remote_addr, ucp_rkey_h rkey,
+                                uint32_t *result);
+
+
+/**
+ * @ingroup UCP_COMM
+ * @brief Blocking atomic conditional swap (cswap) operation for 64 bit values.
+ * @deprecated Replaced by @ref ucp_atomic_fetch_nb with opcode UCP_ATOMIC_FETCH_OP_CSWAP.
+ * @see ucp_put.
+ *
+ * This routine conditionally swaps a 64 bit value between local and remote
+ * memory. The swap occurs only if the condition value (@a continue) is equal
+ * to the remote value, otherwise the remote memory is not modified.  The
+ * remote value is described by the combination of the remote memory address @p
+ * remote_addr and the @ref ucp_rkey_h "remote memory handle" @a rkey. The @p
+ * swap value is the value that is used to update the remote memory if the
+ * condition is true.  The call to the routine returns when the operation is
+ * completed and the @a result value is updated.
+ *
+ * @note The remote address must be aligned to 64 bit.
+ *
+ * @param [in]  ep           Remote endpoint handle.
+ * @param [in]  compare      Value to compare to.
+ * @param [in]  swap         Value to swap.
+ * @param [in]  remote_addr  Pointer to the destination remote address
+ *                           of the atomic variable.
+ * @param [in]  rkey         Remote memory key associated with the
+ *                           remote address.
+ * @param [out] result       Pointer to the address that is used to store
+ *                           the previous value of the atomic variable described
+ *                           by the @a remote_addr
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t ucp_atomic_cswap64(ucp_ep_h ep, uint64_t compare, uint64_t swap,
+                                uint64_t remote_addr, ucp_rkey_h rkey,
+                                uint64_t *result);
+
+
+/**
+ * @ingroup UCP_ENDPOINT
+ * @brief Modify endpoint parameters.
+ *
+ * @deprecated Use @ref ucp_listener_conn_handler_t instead of @ref
+ *             ucp_listener_accept_handler_t, if you have other use case please
+ *             submit an issue on https://github.com/openucx/ucx or report to
+ *             ucx-group@elist.ornl.gov
+ *
+ * This routine modifies @ref ucp_ep_h "endpoint" created by @ref ucp_ep_create
+ * or @ref ucp_listener_accept_callback_t. For example, this API can be used
+ * to setup custom parameters like @ref ucp_ep_params_t::user_data or
+ * @ref ucp_ep_params_t::err_handler to endpoint created by
+ * @ref ucp_listener_accept_callback_t.
+ *
+ * @param [in]  ep          A handle to the endpoint.
+ * @param [in]  params      User defined @ref ucp_ep_params_t configurations
+ *                          for the @ref ucp_ep_h "UCP endpoint".
+ *
+ * @return NULL             - The endpoint is modified successfully.
+ * @return UCS_PTR_IS_ERR(_ptr) - The reconfiguration failed and an error code
+ *                                indicates the status. However, the @a endpoint
+ *                                is not modified and can be used further.
+ * @return otherwise        - The reconfiguration process is started, and can be
+ *                            completed at any point in time. A request handle
+ *                            is returned to the application in order to track
+ *                            progress of the endpoint modification.
+ *                            The application is responsible for releasing the
+ *                            handle using the @ref ucp_request_free routine.
+ *
+ * @note See the documentation of @ref ucp_ep_params_t for details, only some of
+ *       the parameters can be modified.
+ */
+ucs_status_ptr_t ucp_ep_modify_nb(ucp_ep_h ep, const ucp_ep_params_t *params);
+
+
 END_C_DECLS
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_def.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_def.h
index e2318e1a2..8b8cf4e0e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_def.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/api/ucp_def.h
@@ -10,6 +10,7 @@
 #define UCP_DEF_H_
 
 #include <ucs/type/status.h>
+#include <ucs/config/types.h>
 #include <stddef.h>
 #include <stdint.h>
 
@@ -81,6 +82,16 @@ typedef struct ucp_config                ucp_config_t;
 typedef struct ucp_ep                    *ucp_ep_h;
 
 
+/**
+ * @ingroup UCP_ENDPOINT
+ * @brief UCP connection request
+ *
+ * A server-side handle to incoming connection request. Can be used to create an
+ * endpoint which connects back to the client.
+ */
+typedef struct ucp_conn_request          *ucp_conn_request_h;
+
+
 /**
  * @ingroup UCP_WORKER
  * @brief UCP worker address
@@ -94,14 +105,18 @@ typedef struct ucp_address               ucp_address_t;
 /**
  * @ingroup UCP_ENDPOINT
  * @brief Error handling mode for the UCP endpoint.
- * 
+ *
  * Specifies error handling mode for the UCP endpoint.
  */
 typedef enum {
     UCP_ERR_HANDLING_MODE_NONE,             /**< No guarantees about error
                                              *   reporting, imposes minimal
                                              *   overhead from a performance
-                                             *   perspective */
+                                             *   perspective. @note In this
+                                             *   mode, any error reporting will
+                                             *   not generate calls to @ref
+                                             *   ucp_ep_params_t::err_handler.
+                                             */
     UCP_ERR_HANDLING_MODE_PEER              /**< Guarantees that send requests
                                              *   are always completed
                                              *   (successfully or error) even in
@@ -307,15 +322,16 @@ typedef void (*ucp_err_handler_cb_t)(void *arg, ucp_ep_h ep, ucs_status_t status
  /**
  * @ingroup UCP_COMM
  * @brief UCP endpoint error handling context.
- * 
+ *
  * This structure should be initialized in @ref ucp_ep_params_t to handle peer failure
  */
 typedef struct ucp_err_handler {
-    ucp_err_handler_cb_t cb;       /**< Error handler callback */
+    ucp_err_handler_cb_t cb;       /**< Error handler callback, if NULL, will
+                                        not be called. */
     void                 *arg;     /**< User defined argument associated with
                                         an endpoint, it will be overridden by
                                         @ref ucp_ep_params_t::user_data if both
-                                        are set */
+                                        are set. */
 } ucp_err_handler_t;
 
 
@@ -338,22 +354,42 @@ typedef void (*ucp_listener_accept_callback_t)(ucp_ep_h ep, void *arg);
 
 /**
  * @ingroup UCP_WORKER
- * @brief UCP callback to handle the creation of an endpoint in a client-server
+ * @brief A callback for handling of incoming connection request @a conn_request
+ * from a client.
+ *
+ * This callback routine is invoked on the server side to handle incoming
+ * connections from remote clients. The user can pass an argument to this
+ * callback. The @a conn_request handle has to be released, either by @ref
+ * ucp_ep_create or @ref ucp_listener_reject routine.
+ *
+ *  @param [in]  conn_request   Connection request handle.
+ *  @param [in]  arg            User's argument for the callback.
+ */
+typedef void
+(*ucp_listener_conn_callback_t)(ucp_conn_request_h conn_request, void *arg);
+
+
+/**
+ * @ingroup UCP_WORKER
+ * @brief UCP callback to handle the connection request in a client-server
  * connection establishment flow.
  *
- * This structure is used for handling the creation of an endpoint
- * to the remote peer after an incoming connection request on the listener.
- * Other than communication progress routines, it is allowed to call other
- * communication routines from the callback in the struct.
- * The callback should be thread safe with respect to the worker it is invoked
- * on. If the callback is called from different threads, this callback needs
- * thread safety support.
+ * This structure is used for handling an incoming connection request on
+ * the listener. Setting this type of handler allows creating an endpoint on
+ * any other worker and not limited to the worker on which the listener was
+ * created.
+ * @note
+ * - Other than communication progress routines, it is allowed to call all
+ *   other communication routines from the callback in the struct.
+ * - The callback is thread safe with respect to the worker it is invoked on.
+ * - It is the user's responsibility to avoid potential dead lock accessing
+ *   different worker.
  */
-typedef struct ucp_listener_accept_handler {
-   ucp_listener_accept_callback_t  cb;       /**< Endpoint creation callback */
-   void                            *arg;     /**< User defined argument for the
-                                                  callback */
-} ucp_listener_accept_handler_t;
+typedef struct ucp_listener_conn_handler {
+   ucp_listener_conn_callback_t cb;      /**< Connection request callback */
+   void                         *arg;    /**< User defined argument for the
+                                              callback */
+} ucp_listener_conn_handler_t;
 
 
 /**
@@ -433,4 +469,77 @@ typedef enum ucp_wakeup_event_types {
                                               ones. */
 } ucp_wakeup_event_t;
 
+
+/**
+ * @ingroup UCP_ENDPOINT
+ * @brief Tuning parameters for the UCP endpoint.
+ *
+ * The structure defines the parameters that are used for the
+ * UCP endpoint tuning during the UCP ep @ref ucp_ep_create "creation".
+ */
+typedef struct ucp_ep_params {
+    /**
+     * Mask of valid fields in this structure, using bits from
+     * @ref ucp_ep_params_field.
+     * Fields not specified in this mask would be ignored.
+     * Provides ABI compatibility with respect to adding new fields.
+     */
+    uint64_t                field_mask;
+
+    /**
+     * Destination address; this field should be set along with its
+     * corresponding bit in the field_mask - @ref
+     * UCP_EP_PARAM_FIELD_REMOTE_ADDRESS and must be obtained using @ref
+     * ucp_worker_get_address.
+     */
+    const ucp_address_t     *address;
+
+    /**
+     * Desired error handling mode, optional parameter. Default value is
+     * @ref UCP_ERR_HANDLING_MODE_NONE.
+     */
+    ucp_err_handling_mode_t err_mode;
+
+    /**
+     * Handler to process transport level failure.
+     */
+    ucp_err_handler_t       err_handler;
+
+    /**
+     * User data associated with an endpoint. See @ref ucp_stream_poll_ep_t and
+     * @ref ucp_err_handler_t
+     */
+    void                    *user_data;
+
+    /**
+     * Endpoint flags from @ref ucp_ep_params_flags_field.
+     * This value is optional.
+     * If it's not set (along with its corresponding bit in the field_mask -
+     * @ref UCP_EP_PARAM_FIELD_FLAGS), the @ref ucp_ep_create() routine will
+     * consider the flags as set to zero.
+     */
+     unsigned               flags;
+
+    /**
+     * Destination address in the form of a sockaddr; this field should be set
+     * along with its corresponding bit in the field_mask - @ref
+     * UCP_EP_PARAM_FIELD_SOCK_ADDR and must be obtained from the user, it means
+     * that this type of the endpoint creation is possible only on client side
+     * in client-server connection establishment flow.
+     */
+    ucs_sock_addr_t         sockaddr;
+
+    /**
+     * Connection request from client; this field should be set along with its
+     * corresponding bit in the field_mask - @ref
+     * UCP_EP_PARAM_FIELD_CONN_REQUEST and must be obtained from @ref
+     * ucp_listener_conn_callback_t, it means that this type of the endpoint
+     * creation is possible only on server side in client-server connection
+     * establishment flow.
+     */
+    ucp_conn_request_h      conn_request;
+
+} ucp_ep_params_t;
+
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.c
index f7bf5529f..bce7c074f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.c
@@ -14,6 +14,7 @@
 #include <ucs/datastruct/mpool.inl>
 #include <ucs/datastruct/queue.h>
 #include <ucs/debug/log.h>
+#include <ucs/debug/debug.h>
 #include <ucs/sys/compiler.h>
 #include <ucs/sys/string.h>
 #include <ucs/arch/bitops.h>
@@ -45,6 +46,11 @@ static const char * ucp_rndv_modes[] = {
     [UCP_RNDV_MODE_LAST]      = NULL,
 };
 
+uct_memory_type_t ucm_to_uct_mem_type_map[] = {
+    [UCM_MEM_TYPE_CUDA]         = UCT_MD_MEM_TYPE_CUDA,
+    [UCM_MEM_TYPE_CUDA_MANAGED] = UCT_MD_MEM_TYPE_CUDA_MANAGED
+};
+
 static ucs_config_field_t ucp_config_table[] = {
   {"NET_DEVICES", UCP_RSC_CONFIG_ALL,
    "Specifies which network device(s) to use. The order is not meaningful.\n"
@@ -68,15 +74,16 @@ static ucs_config_field_t ucp_config_table[] = {
 
   {"TLS", UCP_RSC_CONFIG_ALL,
    "Comma-separated list of transports to use. The order is not meaningful.\n"
-   "In addition it's possible to use a combination of the following aliases:\n"
    " - all    : use all the available transports.\n"
    " - sm/shm : all shared memory transports.\n"
    " - mm     : shared memory transports - only memory mappers.\n"
    " - ugni   : ugni_rdma and ugni_udt.\n"
    " - ib     : all infiniband transports.\n"
-   " - rc     : rc and ud.\n"
-   " - rc_x   : rc with accelerated verbs and ud.\n"
+   " - rc     : rc verbs (uses ud for bootstrap).\n"
+   " - rc_x   : rc with accelerated verbs (uses ud_x for bootstrap).\n"
+   " - ud     : ud verbs.\n"
    " - ud_x   : ud with accelerated verbs.\n"
+   " - dc     : dc verbs.\n"
    " - dc_x   : dc with accelerated verbs.\n"
    " Using a \\ prefix before a transport name treats it as an explicit transport name\n"
    " and disables aliasing.\n",
@@ -89,6 +96,15 @@ static ucs_config_field_t ucp_config_table[] = {
    "name, or a wildcard - '*' - which expands to all MD components.",
    ucs_offsetof(ucp_config_t, alloc_prio), UCS_CONFIG_TYPE_STRING_ARRAY},
 
+  {"SOCKADDR_AUX_TLS", "ud,ud_x",
+   "Transports to use for exchanging additional address information while\n"
+   "establishing client/server connection. ",
+   ucs_offsetof(ucp_config_t, sockaddr_aux_tls), UCS_CONFIG_TYPE_STRING_ARRAY},
+
+  {"WARN_INVALID_CONFIG", "y",
+   "Issue a warning in case of invalid device and/or transport configuration.",
+   ucs_offsetof(ucp_config_t, warn_invalid_config), UCS_CONFIG_TYPE_BOOL},
+
   {"BCOPY_THRESH", "0",
    "Threshold for switching from short to bcopy protocol",
    ucs_offsetof(ucp_config_t, ctx.bcopy_thresh), UCS_CONFIG_TYPE_MEMUNITS},
@@ -98,7 +114,8 @@ static ucs_config_field_t ucp_config_table[] = {
    ucs_offsetof(ucp_config_t, ctx.rndv_thresh), UCS_CONFIG_TYPE_MEMUNITS},
 
   {"RNDV_SEND_NBR_THRESH", "256k",
-   "Threshold for switching from eager to rendezvous protocol in ucp_tag_send_nbr()",
+   "Threshold for switching from eager to rendezvous protocol in ucp_tag_send_nbr().\n"
+   "Relevant only if UCX_RNDV_THRESH is set to \"auto\".",
    ucs_offsetof(ucp_config_t, ctx.rndv_send_nbr_thresh), UCS_CONFIG_TYPE_MEMUNITS},
 
   {"RNDV_THRESH_FALLBACK", "inf",
@@ -111,11 +128,17 @@ static ucs_config_field_t ucp_config_table[] = {
    "the eager_zcopy protocol",
    ucs_offsetof(ucp_config_t, ctx.rndv_perf_diff), UCS_CONFIG_TYPE_DOUBLE},
 
-  {"MAX_EAGER_LANES", "1",
+  {"MAX_EAGER_LANES", NULL, "",
+   ucs_offsetof(ucp_config_t, ctx.max_eager_lanes), UCS_CONFIG_TYPE_UINT},
+
+  {"MAX_EAGER_RAILS", "1",
    "Maximal number of devices on which an eager operation may be executed in parallel",
    ucs_offsetof(ucp_config_t, ctx.max_eager_lanes), UCS_CONFIG_TYPE_UINT},
 
-  {"MAX_RNDV_LANES", "1",
+  {"MAX_RNDV_LANES", NULL,"",
+   ucs_offsetof(ucp_config_t, ctx.max_rndv_lanes), UCS_CONFIG_TYPE_UINT},
+
+  {"MAX_RNDV_RAILS", "1",
    "Maximal number of devices on which a rendezvous operation may be executed in parallel",
    ucs_offsetof(ucp_config_t, ctx.max_rndv_lanes), UCS_CONFIG_TYPE_UINT},
 
@@ -146,7 +169,13 @@ static ucs_config_field_t ucp_config_table[] = {
    ucs_offsetof(ucp_config_t, ctx.atomic_mode), UCS_CONFIG_TYPE_ENUM(ucp_atomic_modes)},
 
   {"MAX_WORKER_NAME", UCS_PP_MAKE_STRING(UCP_WORKER_NAME_MAX),
-   "Maximal length of worker name. Affects the size of worker address in debug builds.",
+   "Maximal length of worker name. "
+#if ENABLE_DEBUG_DATA
+   "Sent to remote peer as part of worker address."
+#else
+   "Not sent to remote peer per build configuration."
+#endif
+   ,
    ucs_offsetof(ucp_config_t, ctx.max_worker_name), UCS_CONFIG_TYPE_UINT},
 
   {"USE_MT_MUTEX", "n", "Use mutex for multithreading support in UCP.\n"
@@ -155,7 +184,7 @@ static ucs_config_field_t ucp_config_table[] = {
    ucs_offsetof(ucp_config_t, ctx.use_mt_mutex), UCS_CONFIG_TYPE_BOOL},
 
   {"ADAPTIVE_PROGRESS", "y",
-   "Enable apaptive progress mechanism, which turns on polling only on active\n"
+   "Enable adaptive progress mechanism, which turns on polling only on active\n"
    "transport interfaces.",
    ucs_offsetof(ucp_config_t, ctx.adaptive_progress), UCS_CONFIG_TYPE_BOOL},
 
@@ -163,19 +192,19 @@ static ucs_config_field_t ucp_config_table[] = {
    "Size of a segment in the worker preregistered memory pool.",
    ucs_offsetof(ucp_config_t, ctx.seg_size), UCS_CONFIG_TYPE_MEMUNITS},
 
-  {"TM_THRESH", "1024", /* TODO: calculate automaticlly */
+  {"TM_THRESH", "1024", /* TODO: calculate automatically */
    "Threshold for using tag matching offload capabilities.\n"
    "Smaller buffers will not be posted to the transport.",
    ucs_offsetof(ucp_config_t, ctx.tm_thresh), UCS_CONFIG_TYPE_MEMUNITS},
 
-  {"TM_MAX_BCOPY", "1024", /* TODO: calculate automaticlly */
-   "Maximal size for posting \"bounce buffer\" (UCX interal preregistered memory) for\n"
+  {"TM_MAX_BB_SIZE", "1024", /* TODO: calculate automatically */
+   "Maximal size for posting \"bounce buffer\" (UCX internal preregistered memory) for\n"
    "tag offload receives. When message arrives, it is copied into the user buffer (similar\n"
    "to eager protocol). The size values has to be equal or less than segment size.\n"
    "Also the value has to be bigger than UCX_TM_THRESH to take an effect." ,
-   ucs_offsetof(ucp_config_t, ctx.tm_max_bcopy), UCS_CONFIG_TYPE_MEMUNITS},
+   ucs_offsetof(ucp_config_t, ctx.tm_max_bb_size), UCS_CONFIG_TYPE_MEMUNITS},
 
-  {"TM_FORCE_THRESH", "8192", /* TODO: calculate automaticlly */
+  {"TM_FORCE_THRESH", "8192", /* TODO: calculate automatically */
    "Threshold for forcing tag matching offload mode. Every tag receive operation\n"
    "with buffer bigger than this threshold would force offloading of all uncompleted\n"
    "non-offloaded receive operations to the transport (e. g. operations with\n"
@@ -195,12 +224,24 @@ static ucs_config_field_t ucp_config_table[] = {
    "RNDV fragment size \n",
    ucs_offsetof(ucp_config_t, ctx.rndv_frag_size), UCS_CONFIG_TYPE_MEMUNITS},
 
+  {"MEMTYPE_CACHE", "y",
+   "Enable memory type(cuda) cache \n",
+   ucs_offsetof(ucp_config_t, ctx.enable_memtype_cache), UCS_CONFIG_TYPE_BOOL},
+
+  {"FLUSH_WORKER_EPS", "y",
+   "Enable flushing the worker by flushing its endpoints. Allows completing\n"
+   "the flush operation in a bounded time even if there are new requests on\n"
+   "another thread, or incoming active messages, but consumes more resources.",
+   ucs_offsetof(ucp_config_t, ctx.flush_worker_eps), UCS_CONFIG_TYPE_BOOL},
+
   {NULL}
 };
+UCS_CONFIG_REGISTER_TABLE(ucp_config_table, "UCP context", NULL, ucp_config_t)
+
 
 static ucp_tl_alias_t ucp_tl_aliases[] = {
-  { "sm",    { "mm", "knem", "cma", NULL } },
-  { "shm",   { "mm", "knem", "cma", NULL } },
+  { "sm",    { "mm", "knem", "cma", "rdmacm", NULL } },
+  { "shm",   { "mm", "knem", "cma", "rdmacm", NULL } },
   { "ib",    { "rc", "ud", "dc", "rc_mlx5", "ud_mlx5", "dc_mlx5", "rdmacm", NULL } },
   { "ud",    { "ud", "rdmacm", NULL } },
   { "ud_x",  { "ud_mlx5", "rdmacm", NULL } },
@@ -317,12 +358,10 @@ static int ucp_tls_array_is_present(const char **tls, unsigned count,
     }
 }
 
-static int ucp_config_is_tl_enabled(const ucp_config_t *config, const char *tl_name,
-                                    int is_alias, uint8_t *rsc_flags,
-                                    uint64_t *tl_cfg_mask)
+static int ucp_config_is_tl_enabled(const char **names, unsigned count,
+                                    const char *tl_name, int is_alias,
+                                    uint8_t *rsc_flags, uint64_t *tl_cfg_mask)
 {
-    const char **names = (const char**)config->tls.names;
-    unsigned count     = config->tls.count;
     char strict_name[UCT_TL_NAME_MAX + 1];
 
     snprintf(strict_name, sizeof(strict_name), "\\%s", tl_name);
@@ -368,28 +407,20 @@ static int ucp_is_resource_in_device_list(const uct_tl_resource_desc_t *resource
     return !!mask;
 }
 
-static int ucp_is_resource_enabled(const uct_tl_resource_desc_t *resource,
-                                   const ucp_config_t *config, uint8_t *rsc_flags,
-                                   uint64_t dev_cfg_masks[], uint64_t *tl_cfg_mask)
+static int ucp_is_resource_in_transports_list(const char *tl_name,
+                                              const char **names, unsigned count,
+                                              uint8_t *rsc_flags, uint64_t *tl_cfg_mask)
 {
-    int device_enabled, tl_enabled;
+    uint64_t dummy_mask, tmp_tl_cfg_mask;
+    uint8_t tmp_rsc_flags;
     ucp_tl_alias_t *alias;
-    uint64_t dummy_mask;
-    uint8_t tmp_flags;
+    int tl_enabled;
     char info[32];
-    unsigned count;
+    unsigned alias_arr_count;
 
-    /* Find the enabled devices */
-    device_enabled = (*rsc_flags & UCP_TL_RSC_FLAG_SOCKADDR) ||
-                     ucp_is_resource_in_device_list(resource, config->devices,
-                                                    &dev_cfg_masks[resource->dev_type],
-                                                    resource->dev_type);
-
-
-    /* Find the enabled UCTs */
-    ucs_assert(config->tls.count > 0);
-    if (ucp_config_is_tl_enabled(config, resource->tl_name, 0, rsc_flags,
-                                 tl_cfg_mask)) {
+    ucs_assert(count > 0);
+    if (ucp_config_is_tl_enabled(names, count, tl_name, 0,
+                                 rsc_flags, tl_cfg_mask)) {
         tl_enabled = 1;
     } else {
         tl_enabled = 0;
@@ -399,20 +430,44 @@ static int ucp_is_resource_enabled(const uct_tl_resource_desc_t *resource,
             /* If an alias is enabled, and the transport is part of this alias,
              * enable the transport.
              */
-            count = ucp_tl_alias_count(alias);
+            alias_arr_count = ucp_tl_alias_count(alias);
             snprintf(info, sizeof(info), "for alias '%s'", alias->alias);
-            tmp_flags = 0;
-            if (ucp_config_is_tl_enabled(config, alias->alias, 1, &tmp_flags,
-                                         tl_cfg_mask) &&
-                ucp_tls_array_is_present(alias->tls, count, resource->tl_name,
-                                         info, &tmp_flags, &dummy_mask)) {
-                *rsc_flags |= tmp_flags;
+            tmp_rsc_flags = 0;
+            tmp_tl_cfg_mask = 0;
+            if (ucp_config_is_tl_enabled(names, count, alias->alias, 1,
+                                         &tmp_rsc_flags, &tmp_tl_cfg_mask) &&
+                ucp_tls_array_is_present(alias->tls, alias_arr_count, tl_name,
+                                         info, &tmp_rsc_flags, &dummy_mask)) {
+                *rsc_flags   |= tmp_rsc_flags;
+                *tl_cfg_mask |= tmp_tl_cfg_mask;
                 tl_enabled  = 1;
                 break;
             }
         }
     }
 
+    return tl_enabled;
+}
+
+static int ucp_is_resource_enabled(const uct_tl_resource_desc_t *resource,
+                                   const ucp_config_t *config, uint8_t *rsc_flags,
+                                   uint64_t dev_cfg_masks[], uint64_t *tl_cfg_mask)
+{
+    int device_enabled, tl_enabled;
+
+    /* Find the enabled devices */
+    device_enabled = (*rsc_flags & UCP_TL_RSC_FLAG_SOCKADDR) ||
+                     ucp_is_resource_in_device_list(resource, config->devices,
+                                                    &dev_cfg_masks[resource->dev_type],
+                                                    resource->dev_type);
+
+
+    /* Find the enabled UCTs */
+    tl_enabled = ucp_is_resource_in_transports_list(resource->tl_name,
+                                                    (const char**)config->tls.names,
+                                                    config->tls.count, rsc_flags,
+                                                    tl_cfg_mask);
+
     ucs_trace(UCT_TL_RESOURCE_DESC_FMT " is %sabled",
               UCT_TL_RESOURCE_DESC_ARG(resource),
               (device_enabled && tl_enabled) ? "en" : "dis");
@@ -509,7 +564,7 @@ static ucs_status_t ucp_add_tl_resources(ucp_context_h context, ucp_tl_md_t *md,
                                        dev_cfg_masks, tl_cfg_mask);
     }
 
-    /* add sockaddr dummy resource, if md suports it */
+    /* add sockaddr dummy resource, if md supports it */
     if (md->attr.cap.flags & UCT_MD_FLAG_SOCKADDR) {
         sa_rsc.dev_type = UCT_DEVICE_TYPE_NET;
         ucs_snprintf_zero(sa_rsc.tl_name, UCT_TL_NAME_MAX, "%s", md->rsc.md_name);
@@ -571,10 +626,77 @@ static ucs_status_t ucp_check_tl_names(ucp_context_t *context)
     return UCS_OK;
 }
 
+const char* ucp_tl_bitmap_str(ucp_context_h context, uint64_t tl_bitmap,
+                              char *str, size_t max_str_len)
+{
+    ucp_rsc_index_t i;
+    char *p, *endp;
+
+    p    = str;
+    endp = str + max_str_len;
+
+    ucs_for_each_bit(i, tl_bitmap) {
+        ucs_snprintf_zero(p, endp - p, "%s ",
+                          context->tl_rscs[i].tl_rsc.tl_name);
+        p += strlen(p);
+    }
+
+    return str;
+}
+
+static const char* ucp_feature_flag_str(unsigned feature_flag)
+{
+    switch (feature_flag) {
+    case UCP_FEATURE_TAG:
+        return "UCP_FEATURE_TAG";
+    case UCP_FEATURE_RMA:
+        return "UCP_FEATURE_RMA";
+    case UCP_FEATURE_AMO32:
+        return "UCP_FEATURE_AMO32";
+    case UCP_FEATURE_AMO64:
+        return "UCP_FEATURE_AMO64";
+    case UCP_FEATURE_WAKEUP:
+        return "UCP_FEATURE_WAKEUP";
+    case UCP_FEATURE_STREAM:
+        return "UCP_FEATURE_STREAM";
+    default:
+        ucs_fatal("Unknown feature flag value %u", feature_flag);
+    }
+}
+
+const char* ucp_feature_flags_str(unsigned feature_flags, char *str,
+                                  size_t max_str_len)
+{
+    unsigned i, count;
+    char *p, *endp;
+
+    p    = str;
+    endp = str + max_str_len;
+    count = 0;
+
+    ucs_for_each_bit(i, feature_flags) {
+        ucs_snprintf_zero(p, endp - p, "%s%s", (count == 0) ? "" : "|",
+                          ucp_feature_flag_str(UCS_BIT(i)));
+        count++;
+        p += strlen(p);
+    }
+
+    if (count == 0) {
+        ucs_assert(max_str_len > 0);
+        str[0] = '\0'; /* empty string */
+    }
+
+    return str;
+}
+
 static void ucp_free_resources(ucp_context_t *context)
 {
     ucp_rsc_index_t i;
 
+    if (context->memtype_cache != NULL) {
+        ucs_memtype_cache_destroy(context->memtype_cache);
+    }
+
     ucs_free(context->tl_rscs);
     for (i = 0; i < context->num_mds; ++i) {
         uct_md_close(context->tl_mds[i].md);
@@ -697,6 +819,28 @@ static void ucp_resource_config_str(const ucp_config_t *config, char *buf,
     }
 }
 
+static void ucp_fill_sockaddr_aux_tls_config(ucp_context_h context,
+                                             const ucp_config_t *config)
+{
+    const char **tl_names = (const char**)config->sockaddr_aux_tls.aux_tls;
+    unsigned count = config->sockaddr_aux_tls.count;
+    ucp_rsc_index_t tl_id;
+    uint8_t dummy_flags;
+    uint64_t dummy_mask;
+
+    context->config.sockaddr_aux_rscs_bitmap = 0;
+
+    /* Check if any of the context's resources are present in the sockaddr
+     * auxiliary transports for the client-server flow */
+    for (tl_id = 0; tl_id < context->num_tls; ++tl_id) {
+        if (ucp_is_resource_in_transports_list(context->tl_rscs[tl_id].tl_rsc.tl_name,
+                                               tl_names, count, &dummy_flags,
+                                               &dummy_mask)) {
+            context->config.sockaddr_aux_rscs_bitmap |= UCS_BIT(tl_id);
+        }
+    }
+}
+
 static ucs_status_t ucp_check_resources(ucp_context_h context,
                                         const ucp_config_t *config)
 {
@@ -751,6 +895,7 @@ static ucs_status_t ucp_fill_resources(ucp_context_h context,
     context->tl_rscs     = NULL;
     context->num_tls     = 0;
     context->num_mem_type_mds = 0;
+    context->memtype_cache = NULL;
 
     status = ucp_check_resource_config(config);
     if (status != UCS_OK) {
@@ -784,7 +929,7 @@ static ucs_status_t ucp_fill_resources(ucp_context_h context,
     for (i = 0; i < num_md_resources; ++i) {
         status = ucp_fill_tl_md(&md_rscs[i], &context->tl_mds[md_index]);
         if (status != UCS_OK) {
-            goto err_free_context_resources;
+            continue;
         }
 
         /* Add communication resources of each MD */
@@ -815,6 +960,14 @@ static ucs_status_t ucp_fill_resources(ucp_context_h context,
         }
     }
 
+    if (context->num_mem_type_mds && context->config.ext.enable_memtype_cache) {
+        status = ucs_memtype_cache_create(&context->memtype_cache);
+        if (status != UCS_OK) {
+            ucs_debug("could not create memtype cache for mem_type allocations");
+            goto err_free_context_resources;
+        }
+    }
+
     /* Validate context resources */
     status = ucp_check_resources(context, config);
     if (status != UCS_OK) {
@@ -823,13 +976,17 @@ static ucs_status_t ucp_fill_resources(ucp_context_h context,
 
     uct_release_md_resource_list(md_rscs);
 
-    /* Notify the user if there are devices or transports from the command line
-     * that are not available
-     */
-    for (i = 0; i < UCT_DEVICE_TYPE_LAST; ++i) {
-        ucp_report_unavailable(&config->devices[i], dev_cfg_masks[i], "device");
+    if (config->warn_invalid_config) {
+        /* Notify the user if there are devices or transports from the command line
+         * that are not available
+         */
+        for (i = 0; i < UCT_DEVICE_TYPE_LAST; ++i) {
+            ucp_report_unavailable(&config->devices[i], dev_cfg_masks[i], "device");
+        }
+        ucp_report_unavailable(&config->tls, tl_cfg_mask, "transport");
     }
-    ucp_report_unavailable(&config->tls, tl_cfg_mask, "transport");
+
+    ucp_fill_sockaddr_aux_tls_config(context, config);
 
     return UCS_OK;
 
@@ -970,20 +1127,20 @@ static ucs_status_t ucp_fill_config(ucp_context_h context,
     }
 
     /* Need to check MAX_BCOPY value if it is enabled only */
-    if (context->config.ext.tm_max_bcopy > context->config.ext.tm_thresh) {
-        if (context->config.ext.tm_max_bcopy < sizeof(ucp_request_hdr_t)) {
+    if (context->config.ext.tm_max_bb_size > context->config.ext.tm_thresh) {
+        if (context->config.ext.tm_max_bb_size < sizeof(ucp_request_hdr_t)) {
             /* In case of expected SW RNDV message, the header (ucp_request_hdr_t) is
              * scattered to UCP user buffer. Make sure that bounce buffer is used for
              * messages which can not fit SW RNDV hdr. */
-            context->config.ext.tm_max_bcopy = sizeof(ucp_request_hdr_t);
-            ucs_info("UCX_TM_MAX_BCOPY value: %zu, adjusted to: %zu",
-                     context->config.ext.tm_max_bcopy, sizeof(ucp_request_hdr_t));
+            context->config.ext.tm_max_bb_size = sizeof(ucp_request_hdr_t);
+            ucs_info("UCX_TM_MAX_BB_SIZE value: %zu, adjusted to: %zu",
+                     context->config.ext.tm_max_bb_size, sizeof(ucp_request_hdr_t));
         }
 
-        if (context->config.ext.tm_max_bcopy > context->config.ext.seg_size) {
-            context->config.ext.tm_max_bcopy = context->config.ext.seg_size;
-            ucs_info("Wrong UCX_TM_MAX_BCOPY value: %zu, adjusted to: %zu",
-                     context->config.ext.tm_max_bcopy,
+        if (context->config.ext.tm_max_bb_size > context->config.ext.seg_size) {
+            context->config.ext.tm_max_bb_size = context->config.ext.seg_size;
+            ucs_info("Wrong UCX_TM_MAX_BB_SIZE value: %zu, adjusted to: %zu",
+                     context->config.ext.tm_max_bb_size,
                      context->config.ext.seg_size);
         }
     }
@@ -1014,24 +1171,35 @@ ucs_status_t ucp_init_version(unsigned api_major_version, unsigned api_minor_ver
                               ucp_context_h *context_p)
 {
     unsigned major_version, minor_version, release_number;
+    ucp_config_t *dfl_config = NULL;
     ucp_context_t *context;
     ucs_status_t status;
+    ucs_debug_address_info_t addr_info;
 
     ucp_get_version(&major_version, &minor_version, &release_number);
 
-    if ((api_major_version != major_version) || (api_minor_version != minor_version)) {
-        ucs_error("UCP version is incompatible, required: %d.%d, actual: %d.%d (release %d)",
+    if ((api_major_version != major_version) ||
+        ((api_major_version == major_version) && (api_minor_version > minor_version))) {
+        status = ucs_debug_lookup_address(ucp_init_version, &addr_info);
+        ucs_warn("UCP version is incompatible, required: %d.%d, actual: %d.%d (release %d %s)",
                   api_major_version, api_minor_version,
-                  major_version, minor_version, release_number);
-        status = UCS_ERR_NOT_IMPLEMENTED;
-        goto err;
+                  major_version, minor_version, release_number,
+                  status == UCS_OK ? addr_info.file.path : "");
+    }
+
+    if (config == NULL) {
+        status = ucp_config_read(NULL, NULL, &dfl_config);
+        if (status != UCS_OK) {
+            goto err;
+        }
+        config = dfl_config;
     }
 
     /* allocate a ucp context */
     context = ucs_calloc(1, sizeof(*context), "ucp context");
     if (context == NULL) {
         status = UCS_ERR_NO_MEMORY;
-        goto err;
+        goto err_release_config;
     }
 
     status = ucp_fill_config(context, params, config);
@@ -1054,6 +1222,10 @@ ucs_status_t ucp_init_version(unsigned api_major_version, unsigned api_minor_ver
         goto err_free_resources;
     }
 
+    if (dfl_config != NULL) {
+        ucp_config_release(dfl_config);
+    }
+
     ucs_debug("created ucp context %p [%d mds %d tls] features 0x%lx", context,
               context->num_mds, context->num_tls, context->config.features);
 
@@ -1066,6 +1238,10 @@ err_free_config:
     ucp_free_config(context);
 err_free_ctx:
     ucs_free(context);
+err_release_config:
+    if (dfl_config != NULL) {
+        ucp_config_release(dfl_config);
+    }
 err:
     return status;
 }
@@ -1104,12 +1280,24 @@ void ucp_dump_payload(ucp_context_h context, char *buffer, size_t max,
     }
 }
 
-uint64_t ucp_context_uct_atomic_iface_flags(ucp_context_h context)
+void ucp_context_uct_atomic_iface_flags(ucp_context_h context,
+                                        ucp_tl_iface_atomic_flags_t *atomic)
 {
-    return ((context->config.features & UCP_FEATURE_AMO32) ?
-            UCP_UCT_IFACE_ATOMIC32_FLAGS : 0) |
-           ((context->config.features & UCP_FEATURE_AMO64) ?
-            UCP_UCT_IFACE_ATOMIC64_FLAGS : 0);
+    if (context->config.features & UCP_FEATURE_AMO32) {
+        atomic->atomic32.op_flags  = UCP_ATOMIC_OP_MASK;
+        atomic->atomic32.fop_flags = UCP_ATOMIC_FOP_MASK;
+    } else {
+        atomic->atomic32.op_flags  = 0;
+        atomic->atomic32.fop_flags = 0;
+    }
+
+    if (context->config.features & UCP_FEATURE_AMO64) {
+        atomic->atomic64.op_flags  = UCP_ATOMIC_OP_MASK;
+        atomic->atomic64.fop_flags = UCP_ATOMIC_FOP_MASK;
+    } else {
+        atomic->atomic64.op_flags  = 0;
+        atomic->atomic64.fop_flags = 0;
+    }
 }
 
 ucs_status_t ucp_context_query(ucp_context_h context, ucp_context_attr_t *attr)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.h
index beae69c2c..84cc8b3ae 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_context.h
@@ -16,6 +16,7 @@
 #include <uct/api/uct.h>
 #include <ucs/datastruct/mpool.h>
 #include <ucs/datastruct/queue_types.h>
+#include <ucs/sys/memtype_cache.h>
 #include <ucs/type/component.h>
 #include <ucs/type/spinlock.h>
 
@@ -61,7 +62,7 @@ typedef struct ucp_context_config {
     size_t                                 tm_force_thresh;
     /** Upper bound for posting tm offload receives with internal UCP
      *  preregistered bounce buffers. */
-    size_t                                 tm_max_bcopy;
+    size_t                                 tm_max_bb_size;
     /** Maximal size of worker name for debugging */
     unsigned                               max_worker_name;
     /** Atomic mode */
@@ -76,6 +77,10 @@ typedef struct ucp_context_config {
     unsigned                               max_rndv_lanes;
     /** Estimated number of endpoints */
     size_t                                 estimated_num_eps;
+    /** Memtype cache */
+    int                                    enable_memtype_cache;
+    /** Enable flushing endpoints while flushing a worker */
+    int                                    flush_worker_eps;
 } ucp_context_config_t;
 
 
@@ -88,6 +93,10 @@ struct ucp_config {
     ucs_config_names_array_t               tls;
     /** Array of memory allocation methods */
     UCS_CONFIG_STRING_ARRAY_FIELD(methods) alloc_prio;
+    /** Array of transports for partial worker address to pack */
+    UCS_CONFIG_STRING_ARRAY_FIELD(aux_tls) sockaddr_aux_tls;
+    /** Warn on invalid configuration */
+    int                                    warn_invalid_config;
     /** Configuration saved directly in the context */
     ucp_context_config_t                   ctx;
 };
@@ -135,6 +144,7 @@ typedef struct ucp_context {
     /* List of MDs which detect non host memory type */
     ucp_rsc_index_t               mem_type_tl_mds[UCT_MD_MEM_TYPE_LAST];
     ucp_rsc_index_t               num_mem_type_mds;  /* Number of mem type MDs */
+    ucs_memtype_cache_t           *memtype_cache;    /* mem type allocation cache*/
 
     ucp_tl_resource_desc_t        *tl_rscs;   /* Array of communication resources */
     ucp_rsc_index_t               num_tls;    /* Number of resources in the array*/
@@ -169,6 +179,9 @@ typedef struct ucp_context {
         } *alloc_methods;
         unsigned                  num_alloc_methods;
 
+        /* Bitmap of sockaddr auxiliary transports to pack for client/server flow */
+        uint64_t                  sockaddr_aux_rscs_bitmap;
+
         /* Configuration supplied by the user */
         ucp_context_config_t      ext;
 
@@ -188,6 +201,26 @@ typedef struct ucp_am_handler {
     uct_am_callback_t             proxy_cb;
 } ucp_am_handler_t;
 
+typedef struct ucp_tl_iface_atomic_flags {
+    struct {
+        uint64_t                  op_flags;  /**< Attributes for atomic-post operations */
+        uint64_t                  fop_flags; /**< Attributes for atomic-fetch operations */
+    } atomic32, atomic64;
+} ucp_tl_iface_atomic_flags_t;
+
+
+#define UCP_ATOMIC_OP_MASK  (UCS_BIT(UCT_ATOMIC_OP_ADD)  | \
+                             UCS_BIT(UCT_ATOMIC_OP_AND)  | \
+                             UCS_BIT(UCT_ATOMIC_OP_OR)   | \
+                             UCS_BIT(UCT_ATOMIC_OP_XOR))
+
+#define UCP_ATOMIC_FOP_MASK (UCS_BIT(UCT_ATOMIC_OP_ADD)  | \
+                             UCS_BIT(UCT_ATOMIC_OP_AND)  | \
+                             UCS_BIT(UCT_ATOMIC_OP_OR)   | \
+                             UCS_BIT(UCT_ATOMIC_OP_XOR)  | \
+                             UCS_BIT(UCT_ATOMIC_OP_SWAP) | \
+                             UCS_BIT(UCT_ATOMIC_OP_CSWAP))
+
 
 /*
  * Define UCP active message handler.
@@ -229,6 +262,24 @@ typedef struct ucp_am_handler {
     };
 
 
+/**
+ * Check if at least one feature flag from @a _flags is initialized.
+ */
+#define UCP_CONTEXT_CHECK_FEATURE_FLAGS(_context, _flags, _action) \
+    do { \
+        if (ENABLE_PARAMS_CHECK && \
+            ucs_unlikely(!((_context)->config.features & (_flags)))) { \
+            size_t feature_list_str_max = 512; \
+            char *feature_list_str = ucs_alloca(feature_list_str_max); \
+            ucs_error("feature flags %s were not set for ucp_init()", \
+                      ucp_feature_flags_str((_flags) & \
+                                            ~(_context)->config.features, \
+                      feature_list_str, feature_list_str_max)); \
+            _action; \
+        } \
+    } while (0)
+
+
 #define UCP_PARAM_VALUE(_obj, _params, _name, _flag, _default) \
     (((_params)->field_mask & (UCP_##_obj##_PARAM_FIELD_##_flag)) ? \
                     (_params)->_name : (_default))
@@ -242,22 +293,45 @@ void ucp_dump_payload(ucp_context_h context, char *buffer, size_t max,
 
 void ucp_context_tag_offload_enable(ucp_context_h context);
 
-uint64_t ucp_context_uct_atomic_iface_flags(ucp_context_h context);
+void ucp_context_uct_atomic_iface_flags(ucp_context_h context,
+                                        ucp_tl_iface_atomic_flags_t *atomic);
 
 const char * ucp_find_tl_name_by_csum(ucp_context_t *context, uint16_t tl_name_csum);
 
+const char* ucp_tl_bitmap_str(ucp_context_h context, uint64_t tl_bitmap,
+                              char *str, size_t max_str_len);
+
+const char* ucp_feature_flags_str(unsigned feature_flags, char *str,
+                                  size_t max_str_len);
+
 static UCS_F_ALWAYS_INLINE double
 ucp_tl_iface_latency(ucp_context_h context, const uct_iface_attr_t *iface_attr)
 {
     return iface_attr->latency.overhead +
            (iface_attr->latency.growth * context->config.est_num_eps);
 }
+extern uct_memory_type_t ucm_to_uct_mem_type_map[];
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_memory_type_detect_mds(ucp_context_h context, void *addr, size_t length,
                            uct_memory_type_t *mem_type_p)
 {
     unsigned i, md_index;
+    ucm_mem_type_t ucm_mem_type;
+
+    *mem_type_p = UCT_MD_MEM_TYPE_HOST;
+
+    if (ucs_likely(!context->num_mem_type_mds)) {
+        return UCS_OK;
+    }
+
+    if (context->memtype_cache != NULL) {
+        if (ucs_memtype_cache_lookup(context->memtype_cache, addr,
+                                     length, &ucm_mem_type) == UCS_OK) {
+            *mem_type_p = ucm_to_uct_mem_type_map[ucm_mem_type];
+        }
+        return UCS_OK;
+    }
 
     for (i = 0; i < context->num_mem_type_mds; ++i) {
         md_index = context->mem_type_tl_mds[i];
@@ -266,7 +340,7 @@ ucp_memory_type_detect_mds(ucp_context_h context, void *addr, size_t length,
             return UCS_OK;
         }
     }
-    *mem_type_p = UCT_MD_MEM_TYPE_HOST;
+
     return UCS_OK;
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.c
index 0e1f239cc..3945a80f9 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.c
@@ -14,6 +14,7 @@
 #include <ucp/tag/eager.h>
 #include <ucp/tag/offload.h>
 #include <ucp/stream/stream.h>
+#include <ucp/core/ucp_listener.h>
 #include <ucs/datastruct/queue.h>
 #include <ucs/debug/memtrack.h>
 #include <ucs/debug/log.h>
@@ -23,8 +24,6 @@
 
 extern const ucp_proto_t ucp_stream_am_proto;
 
-static void ucp_ep_cleanup_lanes(ucp_ep_h ep);
-
 #if ENABLE_STATS
 static ucs_stats_class_t ucp_ep_stats_class = {
     .name           = "ucp_ep",
@@ -55,41 +54,15 @@ void ucp_ep_config_key_reset(ucp_ep_config_key_t *key)
     memset(key->amo_lanes,    UCP_NULL_LANE, sizeof(key->amo_lanes));
 }
 
-void ucp_ep_add_to_hash(ucp_ep_h ep)
-{
-    ucp_worker_h worker = ep->worker;
-    int hash_extra_status = 0;
-    khiter_t hash_it;
-
-    hash_it = kh_put(ucp_worker_ep_hash, &worker->ep_hash, ep->dest_uuid,
-                     &hash_extra_status);
-    if (ucs_unlikely(hash_it == kh_end(&worker->ep_hash))) {
-        ucs_fatal("Hash failed with ep %p to %s 0x%"PRIx64"->0x%"PRIx64
-                  "with status %d", ep, ucp_ep_peer_name(ep), worker->uuid,
-                  ep->dest_uuid, hash_extra_status);
-    }
-    kh_value(&worker->ep_hash, hash_it) = ep;
-}
-
-void ucp_ep_delete_from_hash(ucp_ep_h ep)
-{
-    khiter_t hash_it;
-
-    hash_it = kh_get(ucp_worker_ep_hash, &ep->worker->ep_hash, ep->dest_uuid);
-    if (hash_it != kh_end(&ep->worker->ep_hash)) {
-        kh_del(ucp_worker_ep_hash, &ep->worker->ep_hash, hash_it);
-    }
-}
-
-ucs_status_t ucp_ep_new(ucp_worker_h worker, uint64_t dest_uuid,
-                        const char *peer_name, const char *message,
-                        ucp_ep_h *ep_p)
+ucs_status_t ucp_ep_new(ucp_worker_h worker, const char *peer_name,
+                        const char *message, ucp_ep_h *ep_p)
 {
     ucs_status_t status;
     ucp_ep_config_key_t key;
+    ucp_lane_index_t lane;
     ucp_ep_h ep;
 
-    ep = ucs_calloc(1, sizeof(*ep), "ucp ep");
+    ep = ucs_strided_alloc_get(&worker->ep_alloc, "ucp_ep");
     if (ep == NULL) {
         ucs_error("Failed to allocate ep");
         status = UCS_ERR_NO_MEMORY;
@@ -97,28 +70,25 @@ ucs_status_t ucp_ep_new(ucp_worker_h worker, uint64_t dest_uuid,
     }
 
     ucp_ep_config_key_reset(&key);
-    ep->worker           = worker;
-    ep->dest_uuid        = dest_uuid;
-    ep->user_data        = NULL;
-    ep->cfg_index        = ucp_worker_get_ep_config(worker, &key);
-    ep->am_lane          = UCP_NULL_LANE;
-    ep->flags            = 0;
-
-    if (worker->context->config.features & UCP_FEATURE_STREAM) {
-
-        ep->ext.stream = ucs_calloc(1, sizeof(*ep->ext.stream),
-                                    "ucp ep stream extension");
-        if (ep->ext.stream == NULL) {
-            ucs_error("Failed to allocate ucp ep stream extension");
-            status = UCS_ERR_NO_MEMORY;
-            goto err_free_ep;
-        }
-
-        ucs_queue_head_init(&ep->ext.stream->match_q);
-        ep->ext.stream->ucp_ep  = ep;
-        ep->ext.stream->flags   = UCP_EP_STREAM_FLAG_VALID;
-    } else {
-        ep->ext.stream = NULL;
+    ep->worker                      = worker;
+    ep->cfg_index                   = ucp_worker_get_ep_config(worker, &key);
+    ep->am_lane                     = UCP_NULL_LANE;
+    ep->flags                       = 0;
+    ep->conn_sn                     = -1;
+    ucp_ep_ext_gen(ep)->user_data   = NULL;
+    ucp_ep_ext_gen(ep)->dest_ep_ptr = 0;
+    ucp_ep_ext_gen(ep)->err_cb      = NULL;
+    UCS_STATIC_ASSERT(sizeof(ucp_ep_ext_gen(ep)->ep_match) >=
+                      sizeof(ucp_ep_ext_gen(ep)->listener));
+    UCS_STATIC_ASSERT(sizeof(ucp_ep_ext_gen(ep)->ep_match) >=
+                      sizeof(ucp_ep_ext_gen(ep)->flush_state));
+    memset(&ucp_ep_ext_gen(ep)->ep_match, 0,
+           sizeof(ucp_ep_ext_gen(ep)->ep_match));
+
+    ucp_stream_ep_init(ep);
+
+    for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
+        ep->uct_eps[lane] = NULL;
     }
 
 #if ENABLE_DEBUG_DATA
@@ -129,106 +99,76 @@ ucs_status_t ucp_ep_new(ucp_worker_h worker, uint64_t dest_uuid,
     status = UCS_STATS_NODE_ALLOC(&ep->stats, &ucp_ep_stats_class,
                                   worker->stats, "-%p", ep);
     if (status != UCS_OK) {
-        goto err_free_ext_ep;
+        goto err_free_ep;
     }
 
-    ucp_ep_add_to_hash(ep);
-
+    ucs_list_add_tail(&worker->all_eps, &ucp_ep_ext_gen(ep)->ep_list);
     *ep_p = ep;
-    ucs_debug("created ep %p to %s %s", ucp_ep_peer_name(ep), peer_name, message);
+    ucs_debug("created ep %p to %s %s", ep, ucp_ep_peer_name(ep), message);
     return UCS_OK;
 
-err_free_ext_ep:
-    ucs_free(ep->ext.stream);
 err_free_ep:
     ucs_free(ep);
 err:
     return status;
 }
 
-static void ucp_ep_delete(ucp_ep_h ep)
+void ucp_ep_delete(ucp_ep_h ep)
 {
-    ucp_ep_delete_from_hash(ep);
     UCS_STATS_NODE_FREE(ep->stats);
-    ucs_free(ep->ext.stream);
-    ucs_free(ep);
+    ucs_list_del(&ucp_ep_ext_gen(ep)->ep_list);
+    ucs_strided_alloc_put(&ep->worker->ep_alloc, ep);
 }
 
-void ucp_ep_config_key_set_params(ucp_ep_config_key_t *key,
-                                  const ucp_ep_params_t *params)
-{
-    key->err_mode = UCP_PARAM_VALUE(EP, params, err_mode, ERR_HANDLING_MODE,
-                                    UCP_ERR_HANDLING_MODE_NONE);
-}
-
-ucs_status_t ucp_ep_create_stub(ucp_worker_h worker, uint64_t dest_uuid,
-                                const ucp_ep_params_t *params,
-                                const char *peer_name, const char *message,
-                                ucp_ep_h *ep_p)
+ucs_status_t ucp_ep_create_sockaddr_aux(ucp_worker_h worker,
+                                        const ucp_ep_params_t *params,
+                                        const ucp_unpacked_address_t *remote_address,
+                                        ucp_ep_h *ep_p)
 {
+    ucp_wireup_ep_t *wireup_ep;
     ucs_status_t status;
-    ucp_ep_config_key_t key;
-    ucp_ep_h ep = NULL;
+    ucp_ep_h ep;
 
-    status = ucp_ep_new(worker, dest_uuid, peer_name, message, &ep);
+    /* allocate endpoint */
+    status = ucp_ep_new(worker, remote_address->name, "listener", &ep);
     if (status != UCS_OK) {
         goto err;
     }
 
-    ucp_ep_config_key_reset(&key);
-
-    if (params != NULL) {
-        ucp_ep_config_key_set_params(&key, params);
+    status = ucp_ep_init_create_wireup(ep, params, &wireup_ep);
+    if (status != UCS_OK) {
+        goto err_delete;
     }
 
-    /* all operations will use the first lane, which is a stub endpoint */
-    key.num_lanes             = 1;
-    key.lanes[0].rsc_index    = UCP_NULL_RESOURCE;
-    key.lanes[0].dst_md_index = UCP_NULL_RESOURCE;
-    key.am_lane               = 0;
-    key.wireup_lane           = 0;
-    key.tag_lane              = 0;
-    key.am_bw_lanes[0]        = 0;
-    key.rma_lanes[0]          = 0;
-    key.rma_bw_lanes[0]       = 0;
-    key.amo_lanes[0]          = 0;
-
-    ep->cfg_index        = ucp_worker_get_ep_config(worker, &key);
-    ep->am_lane          = 0;
-
-    status = ucp_wireup_ep_create(ep, &ep->uct_eps[0]);
+    status = ucp_wireup_ep_connect_aux(wireup_ep, params,
+                                       remote_address->address_count,
+                                       remote_address->address_list);
     if (status != UCS_OK) {
-        goto err_delete;
+        goto err_destroy_wireup_ep;
     }
 
     *ep_p = ep;
-    return UCS_OK;
+    return status;
 
+err_destroy_wireup_ep:
+    uct_ep_destroy(ep->uct_eps[0]);
 err_delete:
     ucp_ep_delete(ep);
 err:
     return status;
 }
 
-int ucp_ep_is_stub(ucp_ep_h ep)
+void ucp_ep_config_key_set_params(ucp_ep_config_key_t *key,
+                                  const ucp_ep_params_t *params)
 {
-    return ucp_ep_get_rsc_index(ep, 0) == UCP_NULL_RESOURCE;
+    key->err_mode = UCP_PARAM_VALUE(EP, params, err_mode, ERR_HANDLING_MODE,
+                                    UCP_ERR_HANDLING_MODE_NONE);
 }
 
-static void
-ucp_ep_setup_err_handler(ucp_ep_h ep, const ucp_err_handler_t *err_handler)
+int ucp_ep_is_sockaddr_stub(ucp_ep_h ep)
 {
-    khiter_t hash_it;
-    int hash_extra_status = 0;
-
-    hash_it = kh_put(ucp_ep_errh_hash, &ep->worker->ep_errh_hash, (uintptr_t)ep,
-                     &hash_extra_status);
-    if (ucs_unlikely(hash_it == kh_end(&ep->worker->ep_errh_hash))) {
-        ucs_fatal("Hash failed on setup error handler of endpoint %p with status %d ",
-                  ep, hash_extra_status);
-    }
-    kh_value(&ep->worker->ep_errh_hash, hash_it) = err_handler->cb;
-    ep->user_data = err_handler->arg;
+    /* Only a sockaddr client-side endpoint may be created as a "stub" */
+    return ucp_ep_get_rsc_index(ep, 0) == UCP_NULL_RESOURCE;
 }
 
 static ucs_status_t
@@ -245,11 +185,14 @@ ucp_ep_adjust_params(ucp_ep_h ep, const ucp_ep_params_t *params)
     }
 
     if (params->field_mask & UCP_EP_PARAM_FIELD_ERR_HANDLER) {
-        ucp_ep_setup_err_handler(ep, &params->err_handler);
+        ucp_ep_ext_gen(ep)->user_data = params->err_handler.arg;
+        ucp_ep_ext_gen(ep)->err_cb    = params->err_handler.cb;
     }
 
-    ep->user_data = UCP_PARAM_VALUE(EP, params, user_data, USER_DATA,
-                                    ep->user_data);
+    if (params->field_mask & UCP_EP_PARAM_FIELD_USER_DATA) {
+        /* user_data overrides err_handler.arg */
+        ucp_ep_ext_gen(ep)->user_data = params->user_data;
+    }
 
     return UCS_OK;
 }
@@ -257,9 +200,10 @@ ucp_ep_adjust_params(ucp_ep_h ep, const ucp_ep_params_t *params)
 ucs_status_t ucp_worker_create_mem_type_endpoints(ucp_worker_h worker)
 {
     ucp_context_h context = worker->context;
+    ucp_unpacked_address_t local_address;
     unsigned i, mem_type, md_index;
     ucs_status_t status;
-    void *address;
+    void *address_buffer;
     size_t address_length;
     ucp_ep_params_t params;
 
@@ -271,34 +215,40 @@ ucs_status_t ucp_worker_create_mem_type_endpoints(ucp_worker_h worker)
         return UCS_OK;
     }
 
-    params.field_mask = UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
+    params.field_mask = 0;
 
     for (i = 0; i < context->num_mem_type_mds; ++i) {
         md_index = context->mem_type_tl_mds[i];
         mem_type = context->tl_mds[md_index].attr.cap.mem_type;
 
         status = ucp_address_pack(worker, NULL, context->mem_type_tls[mem_type], NULL,
-                                  &address_length, &address);
+                                  &address_length, &address_buffer);
         if (status != UCS_OK) {
             goto err_cleanup_eps;
         }
 
-        /*reset uuid to mem_type id*/
-        *(uint64_t*)address = mem_type;
-
-        params.address    = (ucp_address_t*)address;
+        status = ucp_address_unpack(address_buffer, &local_address);
+        if (status != UCS_OK) {
+            goto err_free_address_buffer;
+        }
 
-        status = ucp_ep_create_to_worker_addr(worker, &params, UCP_EP_INIT_FLAG_MEM_TYPE,
-                                              "mem type", &worker->mem_type_ep[mem_type]);
+        status = ucp_ep_create_to_worker_addr(worker, &params, &local_address,
+                                              UCP_EP_INIT_FLAG_MEM_TYPE, "mem type",
+                                              &worker->mem_type_ep[mem_type]);
         if (status != UCS_OK) {
-            goto err_cleanup_eps;
+            goto err_free_address_list;
         }
 
-        ucs_free(address);
+        ucs_free(local_address.address_list);
+        ucs_free(address_buffer);
     }
 
     return UCS_OK;
 
+err_free_address_list:
+    ucs_free(local_address.address_list);
+err_free_address_buffer:
+    ucs_free(address_buffer);
 err_cleanup_eps:
     for (i = 0; i < UCT_MD_MEM_TYPE_LAST; i++) {
         if (worker->mem_type_ep[i]) {
@@ -308,70 +258,78 @@ err_cleanup_eps:
     return status;
 }
 
-ucs_status_t ucp_ep_create_to_worker_addr(ucp_worker_h worker,
-                                          const ucp_ep_params_t *params,
-                                          unsigned ep_init_flags,
-                                          const char *message, ucp_ep_h *ep_p)
+ucs_status_t ucp_ep_init_create_wireup(ucp_ep_h ep,
+                                       const ucp_ep_params_t *params,
+                                       ucp_wireup_ep_t **wireup_ep)
 {
-    ucp_address_entry_t *address_list;
-    uint8_t addr_indices[UCP_MAX_LANES];
-    char peer_name[UCP_WORKER_NAME_MAX];
-    unsigned address_count;
+    ucp_ep_config_key_t key;
     ucs_status_t status;
-    uint64_t dest_uuid;
-    ucp_ep_h ep;
 
-    if (!(params->field_mask & UCP_EP_PARAM_FIELD_REMOTE_ADDRESS)) {
-        status = UCS_ERR_INVALID_PARAM;
-        ucs_error("remote worker address is missing");
-        goto out;
-    }
+    ucp_ep_config_key_reset(&key);
+    ucp_ep_config_key_set_params(&key, params);
 
-    UCP_CHECK_PARAM_NON_NULL(params->address, status, goto out);
+    /* all operations will use the first lane, which is a stub endpoint */
+    key.num_lanes             = 1;
+    key.lanes[0].rsc_index    = UCP_NULL_RESOURCE;
+    key.lanes[0].dst_md_index = UCP_NULL_RESOURCE;
+    key.am_lane               = 0;
+    key.wireup_lane           = 0;
+    key.tag_lane              = 0;
+    key.am_bw_lanes[0]        = 0;
+    key.rma_lanes[0]          = 0;
+    key.rma_bw_lanes[0]       = 0;
+    key.amo_lanes[0]          = 0;
+
+    ep->cfg_index             = ucp_worker_get_ep_config(ep->worker, &key);
+    ep->am_lane               = 0;
+    ep->flags                |= UCP_EP_FLAG_CONNECT_REQ_QUEUED;
 
-    status = ucp_address_unpack(params->address, &dest_uuid, peer_name,
-                                sizeof(peer_name), &address_count, &address_list);
+    status = ucp_wireup_ep_create(ep, &ep->uct_eps[0]);
     if (status != UCS_OK) {
-        ucs_error("failed to unpack remote address: %s", ucs_status_string(status));
-        goto out;
+        return status;
     }
 
-    ep = ucp_worker_ep_find(worker, dest_uuid);
-    if (ep != NULL) {
-        status = ucp_ep_adjust_params(ep, params);
-        if ((status == UCS_OK) && (ep->ext.stream != NULL)) {
-            ep->ext.stream->flags |= UCP_EP_STREAM_FLAG_VALID;
-        }
+    *wireup_ep = ucs_derived_of(ep->uct_eps[0], ucp_wireup_ep_t);
+    return UCS_OK;
+}
 
-        goto out_free_address;
-    }
+ucs_status_t ucp_ep_create_to_worker_addr(ucp_worker_h worker,
+                                          const ucp_ep_params_t *params,
+                                          const ucp_unpacked_address_t *remote_address,
+                                          unsigned ep_init_flags,
+                                          const char *message, ucp_ep_h *ep_p)
+{
+    uint8_t addr_indices[UCP_MAX_LANES];
+    ucs_status_t status;
+    ucp_ep_h ep;
 
     /* allocate endpoint */
-    status = ucp_ep_new(worker, dest_uuid, peer_name, message, &ep);
+    status = ucp_ep_new(worker, remote_address->name, message, &ep);
     if (status != UCS_OK) {
-        goto out_free_address;
+        goto err;
     }
 
-    ep->flags |= UCP_EP_FLAG_DEST_UUID_PEER;
-
     /* initialize transport endpoints */
-    status = ucp_wireup_init_lanes(ep, params, ep_init_flags, address_count,
-                                   address_list, addr_indices);
+    status = ucp_wireup_init_lanes(ep, params, ep_init_flags,
+                                   remote_address->address_count,
+                                   remote_address->address_list, addr_indices);
     if (status != UCS_OK) {
         goto err_delete;
     }
 
-    status = UCS_OK;
-    goto out_free_address;
+    status = ucp_ep_adjust_params(ep, params);
+    if (status != UCS_OK) {
+        goto err_cleanup_lanes;
+    }
 
+    *ep_p = ep;
+    return UCS_OK;
+
+err_cleanup_lanes:
+    ucp_ep_cleanup_lanes(ep);
 err_delete:
     ucp_ep_delete(ep);
-out_free_address:
-    ucs_free(address_list);
-    if (status == UCS_OK) {
-        *ep_p = ep;
-    }
-out:
+err:
     return status;
 }
 
@@ -380,6 +338,7 @@ static ucs_status_t ucp_ep_create_to_sock_addr(ucp_worker_h worker,
                                                ucp_ep_h *ep_p)
 {
     char peer_name[UCS_SOCKADDR_STRING_LEN];
+    ucp_wireup_ep_t *wireup_ep;
     ucs_status_t status;
     ucp_ep_h ep;
 
@@ -393,12 +352,22 @@ static ucs_status_t ucp_ep_create_to_sock_addr(ucp_worker_h worker,
 
     /* allocate endpoint */
     ucs_sockaddr_str(params->sockaddr.addr, peer_name, sizeof(peer_name));
-    status = ucp_ep_create_stub(worker, ucs_generate_uuid(0), params, peer_name,
-                                "from api call", &ep);
+
+    status = ucp_ep_new(worker, peer_name, "from api call", &ep);
     if (status != UCS_OK) {
         goto err;
     }
 
+    status = ucp_ep_init_create_wireup(ep, params, &wireup_ep);
+    if (status != UCS_OK) {
+        goto err_delete;
+    }
+
+    status = ucp_ep_adjust_params(ep, params);
+    if (status != UCS_OK) {
+        goto err_cleanup_lanes;
+    }
+
     status = ucp_wireup_ep_connect_to_sockaddr(ep->uct_eps[0], params);
     if (status != UCS_OK) {
         goto err_cleanup_lanes;
@@ -408,11 +377,204 @@ static ucs_status_t ucp_ep_create_to_sock_addr(ucp_worker_h worker,
     return UCS_OK;
 
 err_cleanup_lanes:
-    ucp_ep_destroy_internal(ep);
+    ucp_ep_cleanup_lanes(ep);
+err_delete:
+    ucp_ep_delete(ep);
 err:
     return status;
 }
 
+/**
+ * Create an endpoint on the server side connected to the client endpoint.
+ */
+ucs_status_t ucp_ep_create_accept(ucp_worker_h worker,
+                                  const ucp_wireup_client_data_t *client_data,
+                                  ucp_ep_h *ep_p)
+{
+    ucp_ep_params_t        params;
+    ucp_unpacked_address_t remote_address;
+    ucs_status_t           status;
+
+    params.field_mask = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
+    params.err_mode   = client_data->err_mode;
+
+    status = ucp_address_unpack(client_data + 1, &remote_address);
+    if (status != UCS_OK) {
+        goto out;
+    }
+
+    if (client_data->is_full_addr) {
+        /* create endpoint to the worker address we got in the private data */
+        status = ucp_ep_create_to_worker_addr(worker, &params, &remote_address,
+                                              UCP_EP_CREATE_AM_LANE, "listener",
+                                              ep_p);
+        if (status == UCS_OK) {
+            ucp_ep_flush_state_reset(*ep_p);
+        } else {
+            goto out_free_address;
+        }
+    } else {
+        status = ucp_ep_create_sockaddr_aux(worker, &params, &remote_address,
+                                            ep_p);
+        if (status == UCS_OK) {
+            /* the server's ep should be aware of the sent address from the client */
+            (*ep_p)->flags |= UCP_EP_FLAG_LISTENER;
+            /* NOTE: protect union */
+            ucs_assert(!((*ep_p)->flags & (UCP_EP_FLAG_ON_MATCH_CTX |
+                                           UCP_EP_FLAG_FLUSH_STATE_VALID)));
+        } else {
+            goto out_free_address;
+        }
+    }
+
+    ucp_ep_update_dest_ep_ptr(*ep_p, client_data->ep_ptr);
+
+out_free_address:
+    ucs_free(remote_address.address_list);
+out:
+    return status;
+}
+
+static ucs_status_t
+ucp_ep_create_api_conn_request(ucp_worker_h worker,
+                               const ucp_ep_params_t *params, ucp_ep_h *ep_p)
+{
+    ucp_conn_request_h conn_request = params->conn_request;
+    ucp_ep_h           ep;
+    ucs_status_t       status;
+
+    /* coverity[overrun-buffer-val] */
+    status = ucp_ep_create_accept(worker, &conn_request->client_data, &ep);
+    if (status != UCS_OK) {
+        goto out;
+    }
+
+    status = ucp_ep_adjust_params(ep, params);
+    if (status != UCS_OK) {
+        goto out_ep_destroy;
+    }
+
+    if (ep->flags & UCP_EP_FLAG_LISTENER) {
+        status = ucp_wireup_send_pre_request(ep);
+    } else {
+        /* send wireup request message, to connect the client to the server's
+           new endpoint */
+        ucs_assert(!(ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED));
+        status = ucp_wireup_send_request(ep);
+    }
+
+    if (status == UCS_OK) {
+        *ep_p = ep;
+        goto out;
+    }
+
+out_ep_destroy:
+    ucp_ep_destroy_internal(ep);
+out:
+    if (status == UCS_OK) {
+        status = uct_iface_accept(conn_request->listener->wiface.iface,
+                                  conn_request->uct_req);
+    } else {
+        uct_iface_reject(conn_request->listener->wiface.iface,
+                         conn_request->uct_req);
+    }
+    ucs_free(params->conn_request);
+
+    return status;
+}
+
+static ucs_status_t
+ucp_ep_create_api_to_worker_addr(ucp_worker_h worker,
+                                 const ucp_ep_params_t *params, ucp_ep_h *ep_p)
+{
+    ucp_unpacked_address_t remote_address;
+    ucp_ep_conn_sn_t conn_sn;
+    ucs_status_t status;
+    unsigned flags;
+    ucp_ep_h ep;
+
+    if (!(params->field_mask & UCP_EP_PARAM_FIELD_REMOTE_ADDRESS)) {
+        status = UCS_ERR_INVALID_PARAM;
+        ucs_error("remote worker address is missing");
+        goto out;
+    }
+
+    UCP_CHECK_PARAM_NON_NULL(params->address, status, goto out);
+
+    status = ucp_address_unpack(params->address, &remote_address);
+    if (status != UCS_OK) {
+        goto out;
+    }
+
+    /* Check if there is already an unconnected internal endpoint to the same
+     * destination address.
+     * In case of loopback connection, search the hash table for an endpoint with
+     * even/odd matching, so that every 2 endpoints connected to the local worker
+     * with be paired to each other.
+     * Note that if a loopback endpoint had the UCP_EP_PARAMS_FLAGS_NO_LOOPBACK
+     * flag set, it will not be added to ep_match as an unexpected ep. Because
+     * dest_ep_ptr will be initialized, a WIREUP_REQUEST (if sent) will have
+     * dst_ep != 0. So, ucp_wireup_request() will not create an unexpected ep
+     * in ep_match.
+     */
+    conn_sn = ucp_ep_match_get_next_sn(&worker->ep_match_ctx, remote_address.uuid);
+    ep = ucp_ep_match_retrieve_unexp(&worker->ep_match_ctx, remote_address.uuid,
+                                     conn_sn ^ (remote_address.uuid == worker->uuid));
+    if (ep != NULL) {
+        status = ucp_ep_adjust_params(ep, params);
+        if (status != UCS_OK) {
+            ucp_ep_destroy_internal(ep);
+        }
+
+        ucp_ep_flush_state_reset(ep);
+        ucp_stream_ep_activate(ep);
+        goto out_free_address;
+    }
+
+    status = ucp_ep_create_to_worker_addr(worker, params, &remote_address, 0,
+                                          "from api call", &ep);
+    if (status != UCS_OK) {
+        goto out_free_address;
+    }
+
+    ep->conn_sn = conn_sn;
+
+    /*
+     * If we are connecting to our own worker, and loopback is allowed, connect
+     * the endpoint to itself by updating dest_ep_ptr.
+     * Otherwise, add the new ep to the matching context as an expected endpoint,
+     * waiting for connection request from the peer endpoint
+     */
+    flags = UCP_PARAM_VALUE(EP, params, flags, FLAGS, 0);
+    if ((remote_address.uuid == worker->uuid) &&
+        !(flags & UCP_EP_PARAMS_FLAGS_NO_LOOPBACK)) {
+        ucp_ep_update_dest_ep_ptr(ep, (uintptr_t)ep);
+        ucp_ep_flush_state_reset(ep);
+    } else {
+        ucp_ep_match_insert_exp(&worker->ep_match_ctx, remote_address.uuid, ep);
+    }
+
+    /* if needed, send initial wireup message */
+    if (!(ep->flags & UCP_EP_FLAG_LOCAL_CONNECTED)) {
+        ucs_assert(!(ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED));
+        status = ucp_wireup_send_request(ep);
+        if (status != UCS_OK) {
+            ucp_ep_destroy_internal(ep);
+            goto out_free_address;
+        }
+    }
+
+    status = UCS_OK;
+
+out_free_address:
+    ucs_free(remote_address.address_list);
+out:
+    if (status == UCS_OK) {
+        *ep_p = ep;
+    }
+    return status;
+}
+
 ucs_status_t ucp_ep_create(ucp_worker_h worker, const ucp_ep_params_t *params,
                            ucp_ep_h *ep_p)
 {
@@ -420,39 +582,27 @@ ucs_status_t ucp_ep_create(ucp_worker_h worker, const ucp_ep_params_t *params,
     unsigned flags;
     ucp_ep_h ep = NULL;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
     UCS_ASYNC_BLOCK(&worker->async);
 
     flags = UCP_PARAM_VALUE(EP, params, flags, FLAGS, 0);
     if (flags & UCP_EP_PARAMS_FLAGS_CLIENT_SERVER) {
         status = ucp_ep_create_to_sock_addr(worker, params, &ep);
-        if (status != UCS_OK) {
-            goto out;
-        }
+    } else if (params->field_mask & UCP_EP_PARAM_FIELD_CONN_REQUEST) {
+        status = ucp_ep_create_api_conn_request(worker, params, &ep);
+    } else if (params->field_mask & UCP_EP_PARAM_FIELD_REMOTE_ADDRESS) {
+        status = ucp_ep_create_api_to_worker_addr(worker, params, &ep);
     } else {
-        status = ucp_ep_create_to_worker_addr(worker, params, 0, "from api call",
-                                              &ep);
-        if (status != UCS_OK) {
-            goto out;
-        }
-
-        if (!(ep->flags & UCP_EP_FLAG_LOCAL_CONNECTED)) {
-            /* send initial wireup message */
-            status = ucp_wireup_send_request(ep, ep->dest_uuid);
-            if (status != UCS_OK) {
-                ucp_ep_destroy_internal(ep);
-                goto out;
-            }
-        }
+        status = UCS_ERR_INVALID_PARAM;
     }
 
-    status = ucp_ep_adjust_params(ep, params);
-
-    *ep_p = ep;
+    if (status == UCS_OK) {
+        ep->flags |= UCP_EP_FLAG_USED;
+        *ep_p      = ep;
+    }
 
-out:
     UCS_ASYNC_UNBLOCK(&worker->async);
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return status;
 }
 
@@ -467,13 +617,13 @@ ucs_status_ptr_t ucp_ep_modify_nb(ucp_ep_h ep, const ucp_ep_params_t *params)
         return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM);
     }
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
     UCS_ASYNC_BLOCK(&worker->async);
 
     status = ucp_ep_adjust_params(ep, params);
 
     UCS_ASYNC_UNBLOCK(&worker->async);
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return UCS_STATUS_PTR(status);
 }
@@ -498,7 +648,7 @@ void ucp_ep_destroy_internal(ucp_ep_h ep)
     ucp_ep_delete(ep);
 }
 
-static void ucp_ep_cleanup_lanes(ucp_ep_h ep)
+void ucp_ep_cleanup_lanes(ucp_ep_h ep)
 {
     ucp_lane_index_t lane, proxy_lane;
     uct_ep_h uct_ep;
@@ -530,31 +680,29 @@ static void ucp_ep_cleanup_lanes(ucp_ep_h ep)
         ucs_debug("ep %p: destroy uct_ep[%d]=%p", ep, lane, uct_ep);
         uct_ep_destroy(uct_ep);
     }
+
+    for (lane = 0; lane < ucp_ep_num_lanes(ep); ++lane) {
+        ep->uct_eps[lane] = NULL;
+    }
 }
 
-static void ucp_ep_ext_stream_invalidate(ucp_ep_h ep)
+void ucp_ep_disconnected(ucp_ep_h ep, int force)
 {
-    ucp_ep_ext_stream_t *ep_stream = ep->ext.stream;
-    void                *data;
-    size_t              length;
+    /* remove pending slow-path progress in case it wasn't removed yet */
+    ucs_callbackq_remove_if(&ep->worker->uct->progress_q,
+                            ucp_worker_err_handle_remove_filter, ep);
 
-    if (ep_stream == NULL) {
-        return;
-    }
+    /* remove pending slow-path function it wasn't removed yet */
+    ucs_callbackq_remove_if(&ep->worker->uct->progress_q,
+                            ucp_listener_accept_cb_remove_filter, ep);
 
-    while ((data = ucp_stream_recv_data_nb(ep, &length)) != NULL) {
-        ucs_assert_always(!UCS_PTR_IS_ERR(data));
-        ucp_stream_data_release(ep, data);
-    }
+    ucp_stream_ep_cleanup(ep);
 
-    ep->ext.stream->flags &= ~UCP_EP_STREAM_FLAG_VALID;
-}
-
-static void ucp_ep_disconnected(ucp_ep_h ep, int force)
-{
-    ucp_ep_ext_stream_invalidate(ep);
+    ep->flags &= ~UCP_EP_FLAG_USED;
+    ep->flags |= UCP_EP_FLAG_CLOSED;
 
-    if ((ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED) && !force) {
+    if ((ep->flags & (UCP_EP_FLAG_CONNECT_REQ_QUEUED|UCP_EP_FLAG_REMOTE_CONNECTED))
+        && !force) {
         /* Endpoints which have remote connection are destroyed only when the
          * worker is destroyed, to enable remote endpoints keep sending
          * TODO negotiate disconnect.
@@ -563,7 +711,7 @@ static void ucp_ep_disconnected(ucp_ep_h ep, int force)
         return;
     }
 
-    ucp_ep_delete_from_hash(ep);
+    ucp_ep_match_remove_ep(&ep->worker->ep_match_ctx, ep);
     ucp_ep_destroy_internal(ep);
 }
 
@@ -608,21 +756,22 @@ ucs_status_ptr_t ucp_ep_close_nb(ucp_ep_h ep, unsigned mode)
         return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM);
     }
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     UCS_ASYNC_BLOCK(&worker->async);
+
     request = ucp_ep_flush_internal(ep,
                                     (mode == UCP_EP_CLOSE_MODE_FLUSH) ?
                                     UCT_FLUSH_FLAG_LOCAL : UCT_FLUSH_FLAG_CANCEL,
-                                    NULL, 0,
-                                    ucp_ep_close_flushed_callback);
+                                    NULL, 0, NULL,
+                                    ucp_ep_close_flushed_callback, "close");
     if (!UCS_PTR_IS_PTR(request)) {
         ucp_ep_disconnected(ep, mode == UCP_EP_CLOSE_MODE_FORCE);
     }
 
     UCS_ASYNC_UNBLOCK(&worker->async);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return request;
 }
@@ -638,7 +787,7 @@ void ucp_ep_destroy(ucp_ep_h ep)
     ucs_status_ptr_t *request;
     ucs_status_t status;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
     request = ucp_disconnect_nb(ep);
     if (request == NULL) {
         goto out;
@@ -651,11 +800,12 @@ void ucp_ep_destroy(ucp_ep_h ep)
             ucp_worker_progress(worker);
             status = ucp_request_check_status(request);
         } while (status == UCS_INPROGRESS);
+
         ucp_request_release(request);
     }
 
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return;
 }
 
@@ -749,76 +899,63 @@ static size_t ucp_ep_config_calc_rndv_thresh(ucp_context_h context,
     }
 }
 
-static void
-ucp_ep_config_set_am_rndv_send_nbr_thresh(ucp_context_h context,
-                                          ucp_ep_config_t *config,
-                                          size_t adjust_min_val)
+static size_t ucp_ep_thresh(size_t thresh_value, size_t min_value,
+                            size_t max_value)
 {
-    size_t rndv_thresh;
- 
-    if (config->key.err_mode == UCP_ERR_HANDLING_MODE_PEER) {
-        rndv_thresh = SIZE_MAX;
-    } else {
-        rndv_thresh = ucs_min(context->config.ext.rndv_send_nbr_thresh,
-                              adjust_min_val);
-    }
+    size_t thresh;
+
+    ucs_assert(min_value <= max_value);
 
-    config->tag.rndv_send_nbr.am_thresh = rndv_thresh;
+    thresh = ucs_max(min_value, thresh_value);
+    thresh = ucs_min(max_value, thresh);
+
+    return thresh;
 }
 
 static void ucp_ep_config_set_am_rndv_thresh(ucp_context_h context, uct_iface_attr_t *iface_attr,
                                              uct_md_attr_t *md_attr, ucp_ep_config_t *config,
-                                             size_t adjust_min_val)
+                                             size_t max_rndv_thresh)
 {
-    size_t rndv_thresh;
+    size_t rndv_thresh, rndv_nbr_thresh;
 
     ucs_assert(config->key.am_lane != UCP_NULL_LANE);
     ucs_assert(config->key.lanes[config->key.am_lane].rsc_index != UCP_NULL_RESOURCE);
 
     if (config->key.err_mode == UCP_ERR_HANDLING_MODE_PEER) {
         /* Disable RNDV */
-        rndv_thresh = SIZE_MAX;
+        rndv_thresh = rndv_nbr_thresh = SIZE_MAX;
     } else if (context->config.ext.rndv_thresh == UCS_CONFIG_MEMUNITS_AUTO) {
         /* auto - Make UCX calculate the AM rndv threshold on its own.*/
-        rndv_thresh = ucp_ep_config_calc_rndv_thresh(context, iface_attr, md_attr,
-                                                     context->config.ext.bcopy_bw,
-                                                     0);
-        ucs_trace("Active Message rendezvous threshold is %zu", rndv_thresh);
+        rndv_thresh     = ucp_ep_config_calc_rndv_thresh(context, iface_attr, md_attr,
+                                                         context->config.ext.bcopy_bw,
+                                                         0);
+        rndv_nbr_thresh = context->config.ext.rndv_send_nbr_thresh;
     } else {
-        rndv_thresh = context->config.ext.rndv_thresh;
+        rndv_thresh     = context->config.ext.rndv_thresh;
+        rndv_nbr_thresh = context->config.ext.rndv_thresh;
     }
 
-    ucs_assert(iface_attr->cap.am.min_zcopy <= iface_attr->cap.am.max_zcopy);
-    /* use rendezvous only starting from minimal zero-copy am size */
-    rndv_thresh = ucs_max(rndv_thresh, iface_attr->cap.am.min_zcopy);
-    config->tag.rndv.am_thresh = ucs_min(rndv_thresh, adjust_min_val);
+    config->tag.rndv.am_thresh = ucp_ep_thresh(rndv_thresh,
+                                               iface_attr->cap.am.min_zcopy,
+                                               max_rndv_thresh);
 
-    ucp_ep_config_set_am_rndv_send_nbr_thresh(context, config, adjust_min_val);
-}
+    config->tag.rndv_send_nbr.am_thresh = ucp_ep_thresh(rndv_nbr_thresh,
+                                                        iface_attr->cap.am.min_zcopy,
+                                                        max_rndv_thresh);
 
-static void
-ucp_ep_config_set_rndv_send_nbr_thresh(ucp_context_h context,
-                                       ucp_ep_config_t *config,
-                                       size_t adjust_min_val,
-                                       size_t adjust_max_val)
-{
-    size_t rndv_thresh;
-
-    rndv_thresh = context->config.ext.rndv_send_nbr_thresh;
-    rndv_thresh = ucs_max(rndv_thresh, adjust_max_val);
-
-    config->tag.rndv_send_nbr.rma_thresh = ucs_min(rndv_thresh, adjust_min_val);
+    ucs_trace("Active Message rndv threshold is %zu (send_nbr: %zu)",
+              config->tag.rndv.am_thresh, config->tag.rndv_send_nbr.am_thresh);
 }
 
 static void ucp_ep_config_set_rndv_thresh(ucp_worker_t *worker,
                                           ucp_ep_config_t *config,
                                           ucp_lane_index_t lane,
                                           uint64_t rndv_cap_flag,
-                                          size_t adjust_min_val)
+                                          size_t max_rndv_thresh)
 {
     ucp_context_t *context = worker->context;
     ucp_rsc_index_t rsc_index;
-    size_t rndv_thresh;
+    size_t rndv_thresh, rndv_nbr_thresh;
     uct_iface_attr_t *iface_attr;
     uct_md_attr_t *md_attr;
 
@@ -838,22 +975,26 @@ static void ucp_ep_config_set_rndv_thresh(ucp_worker_t *worker,
 
     if (context->config.ext.rndv_thresh == UCS_CONFIG_MEMUNITS_AUTO) {
         /* auto - Make UCX calculate the RMA (get_zcopy) rndv threshold on its own.*/
-        rndv_thresh = ucp_ep_config_calc_rndv_thresh(context, iface_attr,
-                                                     md_attr, SIZE_MAX, 1);
+        rndv_thresh     = ucp_ep_config_calc_rndv_thresh(context, iface_attr,
+                                                         md_attr, SIZE_MAX, 1);
+        rndv_nbr_thresh = context->config.ext.rndv_send_nbr_thresh;
     } else {
-        rndv_thresh = context->config.ext.rndv_thresh;
+        rndv_thresh     = context->config.ext.rndv_thresh;
+        rndv_nbr_thresh = context->config.ext.rndv_thresh;
     }
 
-    /* use rendezvous only starting from minimal zero-copy get size */
-    ucs_assert(iface_attr->cap.get.min_zcopy <= iface_attr->cap.get.max_zcopy);
-    rndv_thresh = ucs_max(rndv_thresh, iface_attr->cap.get.min_zcopy);
-
     config->tag.rndv.max_get_zcopy = iface_attr->cap.get.max_zcopy;
     config->tag.rndv.max_put_zcopy = iface_attr->cap.put.max_zcopy;
-    config->tag.rndv.rma_thresh    = ucs_min(rndv_thresh, adjust_min_val);
+    config->tag.rndv.rma_thresh    = ucp_ep_thresh(rndv_thresh,
+                                                   iface_attr->cap.get.min_zcopy,
+                                                   max_rndv_thresh);
 
-    ucp_ep_config_set_rndv_send_nbr_thresh(context, config, adjust_min_val,
-                                           iface_attr->cap.get.max_zcopy);
+    config->tag.rndv_send_nbr.rma_thresh = ucp_ep_thresh(rndv_nbr_thresh,
+                                                         iface_attr->cap.get.min_zcopy,
+                                                         max_rndv_thresh);
+
+    ucs_trace("rndv threshold is %zu (send_nbr: %zu)",
+              config->tag.rndv.rma_thresh, config->tag.rndv_send_nbr.rma_thresh);
 }
 
 static void ucp_ep_config_init_attrs(ucp_worker_t *worker, ucp_rsc_index_t rsc_index,
@@ -868,6 +1009,7 @@ static void ucp_ep_config_init_attrs(ucp_worker_t *worker, ucp_rsc_index_t rsc_i
     uct_md_attr_t *md_attr       = &context->tl_mds[context->tl_rscs[rsc_index].md_index].attr;
     size_t it;
     size_t zcopy_thresh;
+    int mem_type;
 
     if (iface_attr->cap.flags & short_flag && !context->num_mem_type_mds) {
         config->max_short = max_short - hdr_len;
@@ -902,6 +1044,14 @@ static void ucp_ep_config_init_attrs(ucp_worker_t *worker, ucp_rsc_index_t rsc_i
         config->sync_zcopy_thresh[0] = config->zcopy_thresh[0] =
                 ucs_min(context->config.ext.zcopy_thresh, adjust_min_val);
     }
+
+    for (mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
+        if (UCP_MEM_IS_HOST(mem_type)) {
+            config->mem_type_zcopy_thresh[mem_type] = config->zcopy_thresh[0];
+        } else if (md_attr->cap.reg_mem_types & UCS_BIT(mem_type)) {
+            config->mem_type_zcopy_thresh[mem_type] = 1;
+        }
+    }
 }
 
 void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
@@ -910,6 +1060,7 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
     ucp_ep_rma_config_t *rma_config;
     uct_iface_attr_t *iface_attr;
     uct_md_attr_t *md_attr;
+    uct_memory_type_t mem_type;
     ucp_rsc_index_t rsc_index;
     ucp_lane_index_t lane;
     size_t it;
@@ -923,6 +1074,12 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
         config->tag.eager.zcopy_thresh[it]       = SIZE_MAX;
         config->tag.eager.sync_zcopy_thresh[it]  = SIZE_MAX;
     }
+
+    for (mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
+        config->am.mem_type_zcopy_thresh[mem_type]        = SIZE_MAX;
+        config->tag.eager.mem_type_zcopy_thresh[mem_type] = SIZE_MAX;
+    }
+
     config->tag.eager.zcopy_auto_thresh = 0;
     config->am.zcopy_auto_thresh        = 0;
     config->p2p_lanes                   = 0;
@@ -939,6 +1096,8 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
     config->tag.rndv.rkey_size          = ucp_rkey_packed_size(context,
                                                                config->key.rma_bw_md_map);
     config->stream.proto                = &ucp_stream_am_proto;
+    config->tag.offload.max_eager_short = -1;
+    config->tag.max_eager_short         = -1;
     max_rndv_thresh                     = SIZE_MAX;
     max_am_rndv_thresh                  = SIZE_MAX;
 
@@ -970,16 +1129,18 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
                                      UCT_IFACE_FLAG_TAG_EAGER_ZCOPY, 0,
                                      iface_attr->cap.tag.eager.max_bcopy);
 
-            config->tag.offload.max_rndv_iov   = iface_attr->cap.tag.rndv.max_iov;
-            config->tag.offload.max_rndv_zcopy = iface_attr->cap.tag.rndv.max_zcopy;
-            config->tag.sync_proto             = &ucp_tag_offload_sync_proto;
-            config->tag.proto                  = &ucp_tag_offload_proto;
-            config->tag.lane                   = lane;
-            max_rndv_thresh                    = iface_attr->cap.tag.eager.max_zcopy;
-            max_am_rndv_thresh                 = iface_attr->cap.tag.eager.max_bcopy;
+            config->tag.offload.max_rndv_iov    = iface_attr->cap.tag.rndv.max_iov;
+            config->tag.offload.max_rndv_zcopy  = iface_attr->cap.tag.rndv.max_zcopy;
+            config->tag.offload.max_eager_short = config->tag.eager.max_short;
+            config->tag.sync_proto              = &ucp_tag_offload_sync_proto;
+            config->tag.proto                   = &ucp_tag_offload_proto;
+            config->tag.lane                    = lane;
+            max_rndv_thresh                     = iface_attr->cap.tag.eager.max_zcopy;
+            max_am_rndv_thresh                  = iface_attr->cap.tag.eager.max_bcopy;
 
             ucs_assert_always(iface_attr->cap.tag.rndv.max_hdr >=
                               sizeof(ucp_tag_offload_unexp_rndv_hdr_t));
+            ucs_assert_always(config->tag.offload.max_eager_short >= 0);
 
             if (config->key.am_lane != UCP_NULL_LANE) {
                 /* Must have active messages for using rendezvous */
@@ -1024,8 +1185,9 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
                                               config->key.rma_bw_lanes[0],
                                               UCT_IFACE_FLAG_GET_ZCOPY,
                                               max_rndv_thresh);
-                config->tag.eager      = config->am;
-                config->tag.lane       = lane;
+                config->tag.eager           = config->am;
+                config->tag.lane            = lane;
+                config->tag.max_eager_short = config->tag.eager.max_short;
             }
         } else {
             /* Stub endpoint */
@@ -1033,18 +1195,22 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
         }
     }
 
+    memset(&config->rma, 0, sizeof(config->rma));
+
     /* Configuration for remote memory access */
     for (lane = 0; lane < config->key.num_lanes; ++lane) {
+        rma_config                   = &config->rma[lane];
+        rma_config->put_zcopy_thresh = SIZE_MAX;
+        rma_config->get_zcopy_thresh = SIZE_MAX;
+        rma_config->max_put_short    = -1;
+        rma_config->max_get_short    = -1;
+
         if (ucp_ep_config_get_multi_lane_prio(config->key.rma_lanes, lane) == -1) {
             continue;
         }
 
-        rma_config = &config->rma[lane];
         rsc_index  = config->key.lanes[lane].rsc_index;
 
-        rma_config->put_zcopy_thresh = SIZE_MAX;
-        rma_config->get_zcopy_thresh = SIZE_MAX;
-
         if (rsc_index != UCP_NULL_RESOURCE) {
             iface_attr = &worker->ifaces[rsc_index].attr;
             if (iface_attr->cap.flags & UCT_IFACE_FLAG_PUT_SHORT) {
@@ -1064,6 +1230,9 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
                 rma_config->put_zcopy_thresh = ucs_max(rma_config->put_zcopy_thresh,
                                                        iface_attr->cap.put.min_zcopy);
             }
+            if (iface_attr->cap.flags & UCT_IFACE_FLAG_GET_SHORT) {
+                rma_config->max_get_short = iface_attr->cap.get.max_short;
+            }
             if (iface_attr->cap.flags & UCT_IFACE_FLAG_GET_BCOPY) {
                 rma_config->max_get_bcopy = iface_attr->cap.get.max_bcopy;
             }
@@ -1071,7 +1240,7 @@ void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config)
                 /* TODO: formula */
                 rma_config->max_get_zcopy = iface_attr->cap.get.max_zcopy;
                 if (context->config.ext.zcopy_thresh == UCS_CONFIG_MEMUNITS_AUTO) {
-                    rma_config->get_zcopy_thresh = 16384; 
+                    rma_config->get_zcopy_thresh = 16384;
                 } else {
                     rma_config->get_zcopy_thresh = context->config.ext.zcopy_thresh; 
                 }
@@ -1315,19 +1484,12 @@ void ucp_ep_print_info(ucp_ep_h ep, FILE *stream)
     ucp_lane_index_t wireup_lane;
     uct_ep_h wireup_ep;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
     fprintf(stream, "#\n");
     fprintf(stream, "# UCP endpoint\n");
     fprintf(stream, "#\n");
-
-    fprintf(stream, "#               peer: %s%suuid 0x%"PRIx64"\n",
-#if ENABLE_DEBUG_DATA
-            ucp_ep_peer_name(ep), ", ",
-#else
-            "", "",
-#endif
-            ep->dest_uuid);
+    fprintf(stream, "#               peer: %s\n", ucp_ep_peer_name(ep));
 
     /* if there is a wireup lane, set aux_rsc_index to the stub ep resource */
     aux_rsc_index = UCP_NULL_RESOURCE;
@@ -1344,7 +1506,7 @@ void ucp_ep_print_info(ucp_ep_h ep, FILE *stream)
 
     fprintf(stream, "#\n");
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
 }
 
 size_t ucp_ep_config_get_zcopy_auto_thresh(size_t iovcnt,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.h
index ef3f2d9ea..395fa2459 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.h
@@ -9,11 +9,11 @@
 
 #include "ucp_types.h"
 
+#include <ucp/wireup/ep_match.h>
 #include <uct/api/uct.h>
 #include <ucs/datastruct/queue.h>
-#include <ucs/debug/log.h>
 #include <ucs/stats/stats.h>
-#include <limits.h>
+#include <ucs/datastruct/strided_alloc.h>
 
 #define UCP_MAX_IOV                16UL
 
@@ -22,21 +22,43 @@
 typedef uint16_t                   ucp_ep_cfg_index_t;
 
 
+/* Endpoint flags type */
+#if ENABLE_DEBUG_DATA || ENABLE_ASSERT
+typedef uint32_t                   ucp_ep_flags_t;
+#else
+typedef uint16_t                   ucp_ep_flags_t;
+#endif
+
+
 /**
  * Endpoint flags
  */
 enum {
-    UCP_EP_FLAG_LOCAL_CONNECTED     = UCS_BIT(0), /* All local endpoints are connected */
-    UCP_EP_FLAG_REMOTE_CONNECTED    = UCS_BIT(1), /* All remote endpoints are connected */
-    UCP_EP_FLAG_CONNECT_REQ_QUEUED  = UCS_BIT(2), /* Connection request was queued */
-    UCP_EP_FLAG_TAG_OFFLOAD_ENABLED = UCS_BIT(3), /* Endpoint uses tl offload for tag matching */
-    UCP_EP_FLAG_FAILED              = UCS_BIT(4), /* EP is in failed state */
+    UCP_EP_FLAG_LOCAL_CONNECTED        = UCS_BIT(0), /* All local endpoints are connected */
+    UCP_EP_FLAG_REMOTE_CONNECTED       = UCS_BIT(1), /* All remote endpoints are connected */
+    UCP_EP_FLAG_CONNECT_REQ_QUEUED     = UCS_BIT(2), /* Connection request was queued */
+    UCP_EP_FLAG_FAILED                 = UCS_BIT(3), /* EP is in failed state */
+    UCP_EP_FLAG_USED                   = UCS_BIT(4), /* EP is in use by the user */
+    UCP_EP_FLAG_STREAM_HAS_DATA        = UCS_BIT(5), /* EP has data in the ext.stream.match_q */
+    UCP_EP_FLAG_ON_MATCH_CTX           = UCS_BIT(6), /* EP is on match queue */
+    UCP_EP_FLAG_DEST_EP                = UCS_BIT(7), /* dest_ep_ptr is valid */
+    UCP_EP_FLAG_LISTENER               = UCS_BIT(8), /* EP holds pointer to a listener
+                                                        (on server side due to receiving partial
+                                                        worker address from the client) */
+    UCP_EP_FLAG_CONNECT_PRE_REQ_QUEUED = UCS_BIT(9), /* Pre-Connection request was queued */
+    UCP_EP_FLAG_CLOSED                 = UCS_BIT(10),/* EP was closed */
 
     /* DEBUG bits */
-    UCP_EP_FLAG_CONNECT_REQ_SENT    = UCS_BIT(8), /* DEBUG: Connection request was sent */
-    UCP_EP_FLAG_CONNECT_REP_SENT    = UCS_BIT(9), /* DEBUG: Connection reply was sent */
-    UCP_EP_FLAG_CONNECT_ACK_SENT    = UCS_BIT(10),/* DEBUG: Connection ACK was sent */
-    UCP_EP_FLAG_DEST_UUID_PEER      = UCS_BIT(11) /* DEBUG: dest_uuid is of the remote worker */
+    UCP_EP_FLAG_CONNECT_REQ_SENT       = UCS_BIT(16),/* DEBUG: Connection request was sent */
+    UCP_EP_FLAG_CONNECT_REP_SENT       = UCS_BIT(17),/* DEBUG: Connection reply was sent */
+    UCP_EP_FLAG_CONNECT_ACK_SENT       = UCS_BIT(18),/* DEBUG: Connection ACK was sent */
+    UCP_EP_FLAG_CONNECT_REQ_IGNORED    = UCS_BIT(19),/* DEBUG: Connection request was ignored */
+    UCP_EP_FLAG_CONNECT_PRE_REQ_SENT   = UCS_BIT(20),/* DEBUG: Connection pre-request was sent */
+    UCP_EP_FLAG_SOCKADDR_PARTIAL_ADDR  = UCS_BIT(21),/* DEBUG: Partial worker address was sent
+                                                               to the remote peer when starting
+                                                               connection establishment on this EP */
+    UCP_EP_FLAG_FLUSH_STATE_VALID      = UCS_BIT(22) /* DEBUG: flush_state is valid */
+
 };
 
 
@@ -55,7 +77,8 @@ enum {
  * Endpoint init flags
  */
 enum {
-    UCP_EP_INIT_FLAG_MEM_TYPE          = UCS_BIT(0)  /**< Endpoint for local mem type transfers */
+    UCP_EP_INIT_FLAG_MEM_TYPE          = UCS_BIT(0),  /**< Endpoint for local mem type transfers */
+    UCP_EP_CREATE_AM_LANE              = UCS_BIT(1)   /**< Endpoint requires an AM lane */
 };
 
 
@@ -119,6 +142,7 @@ typedef struct ucp_ep_rma_config {
     size_t                 max_put_short;    /* Maximal payload of put short */
     size_t                 max_put_bcopy;    /* Maximal total size of put_bcopy */
     size_t                 max_put_zcopy;
+    size_t                 max_get_short;    /* Maximal payload of get short */
     size_t                 max_get_bcopy;    /* Maximal total size of get_bcopy */
     size_t                 max_get_zcopy;
     size_t                 put_zcopy_thresh;
@@ -138,6 +162,9 @@ typedef struct ucp_ep_msg_config {
         /* zero-copy threshold for operations which do not have to wait for remote side */
         size_t             zcopy_thresh[UCP_MAX_IOV];
 
+        /* zero-copy threshold for mem type buffers */
+        size_t             mem_type_zcopy_thresh[UCT_MD_MEM_TYPE_LAST];
+
         /* zero-copy threshold for operations which anyways have to wait for remote side */
         size_t             sync_zcopy_thresh[UCP_MAX_IOV];
         uint8_t            zcopy_auto_thresh; /* if != 0 the zcopy enabled */
@@ -177,6 +204,8 @@ typedef struct ucp_ep_config {
         /* Lane used for tag matching operations. */
         ucp_lane_index_t    lane;
 
+        ssize_t             max_eager_short;
+
         /* Configuration of the lane used for eager protocols
          * (can be AM or tag offload). */
         ucp_ep_msg_config_t eager;
@@ -203,6 +232,8 @@ typedef struct ucp_ep_config {
         } rndv_send_nbr;
 
         struct {
+            /* Maximal size for eager short */
+            ssize_t         max_eager_short;
             /* Maximal iov count for RNDV offload */
             size_t          max_rndv_iov;
             /* Maximal total size for RNDV offload */
@@ -219,59 +250,90 @@ typedef struct ucp_ep_config {
 
 
 /**
- * UCP_FEATURE_STREAM specific extention of the remote protocol layer endpoint
- */
-typedef struct ucp_ep_ext_stream {
-    /* List entry in worker's EP list */
-    ucs_list_link_t         list;
-    /* Queue of receive data or requests depends on flags field */
-    ucs_queue_head_t        match_q;
-    /* EP which owns the extension */
-    ucp_ep_h                ucp_ep;
-    /* Describes the state */
-    uint8_t                 flags;
-} ucp_ep_ext_stream_t;
-
-
-/**
- * Remote protocol layer endpoint
+ * Protocol layer endpoint, represents a connection to a remote worker
  */
 typedef struct ucp_ep {
     ucp_worker_h                  worker;        /* Worker this endpoint belongs to */
 
     ucp_ep_cfg_index_t            cfg_index;     /* Configuration index */
+    ucp_ep_conn_sn_t              conn_sn;       /* Sequence number for remote connection */
     ucp_lane_index_t              am_lane;       /* Cached value */
-#if ENABLE_ASSERT || ENABLE_DEBUG_DATA
-    uint16_t                      flags;         /* Endpoint flags */
-#else
-    uint8_t                       flags;         /* Endpoint flags */
-#endif
-
-    uint64_t                      dest_uuid;     /* Destination worker uuid */
-    void                          *user_data;    /* user data associated with
-                                                    the endpoint */
+    ucp_ep_flags_t                flags;         /* Endpoint flags */
 
-    UCS_STATS_NODE_DECLARE(stats);
+    /* TODO allocate ep dynamically according to number of lanes */
+    uct_ep_h                      uct_eps[UCP_MAX_LANES]; /* Transports for every lane */
 
 #if ENABLE_DEBUG_DATA
     char                          peer_name[UCP_WORKER_NAME_MAX];
 #endif
 
-    /* TODO allocate ep dynamically according to number of lanes */
-    uct_ep_h                      uct_eps[UCP_MAX_LANES]; /* Transports for every lane */
+    UCS_STATS_NODE_DECLARE(stats);
 
-    /* Feature specific extensions allocated on demand */
-    struct {
-        ucp_ep_ext_stream_t       *stream;      /* UCP_FEATURE_STREAM */
-    } ext;
 } ucp_ep_t;
 
 
-void ucp_ep_config_key_reset(ucp_ep_config_key_t *key);
+/**
+ * Status of protocol-level remote completions
+ */
+typedef struct {
+    ucs_queue_head_t              reqs;         /* Queue of flush requests which
+                                                   are waiting for remote completion */
+    uint32_t                      send_sn;      /* Sequence number of sent operations */
+    uint32_t                      cmpl_sn;      /* Sequence number of completions */
+} ucp_ep_flush_state_t;
+
+
+/*
+ * Endpoint extension for generic non fast-path data
+ */
+typedef struct {
+    uintptr_t                     dest_ep_ptr;   /* Remote EP pointer */
+    void                          *user_data;    /* User data associated with ep */
+    ucs_list_link_t               ep_list;       /* List entry in worker's all eps list */
+    ucp_err_handler_cb_t          err_cb;        /* Error handler */
+
+    /* Endpoint match context and remote completion status are mutually exclusive,
+     * since remote completions are counted only after the endpoint is already
+     * matched to a remote peer.
+     */
+    union {
+        ucp_ep_match_t            ep_match;      /* Matching with remote endpoints */
+        ucp_ep_flush_state_t      flush_state;   /* Remove completion status */
+        ucp_listener_h            listener;      /* Listener that may be associated with ep */
+    };
+} ucp_ep_ext_gen_t;
+
+
+/*
+ * Endpoint extension for specific protocols
+ */
+typedef struct {
+    struct {
+        ucs_list_link_t           ready_list;    /* List entry in worker's EP list */
+        ucs_queue_head_t          match_q;       /* Queue of receive data or requests,
+                                                    depends on UCP_EP_FLAG_STREAM_HAS_DATA */
+    } stream;
+} ucp_ep_ext_proto_t;
+
+
+typedef struct ucp_wireup_client_data {
+    uintptr_t                 ep_ptr;        /**< Client-side endpoint pointer */
+    ucp_err_handling_mode_t   err_mode;      /**< Error handling mode */
+    uint8_t                   is_full_addr;  /**< Whether the attached address is
+                                                  full or partial */
+    /* packed worker address follows */
+} UCS_S_PACKED ucp_wireup_client_data_t;
 
-void ucp_ep_add_to_hash(ucp_ep_h ep);
 
-void ucp_ep_delete_from_hash(ucp_ep_h ep);
+typedef struct ucp_conn_request {
+    ucp_listener_h              listener;
+    uct_conn_request_h          uct_req;
+    ucp_wireup_client_data_t    client_data;
+    /* packed worker address follows */
+} ucp_conn_request_t;
+
+
+void ucp_ep_config_key_reset(ucp_ep_config_key_t *key);
 
 void ucp_ep_config_lane_info_str(ucp_context_h context,
                                  const ucp_ep_config_key_t *key,
@@ -280,33 +342,49 @@ void ucp_ep_config_lane_info_str(ucp_context_h context,
                                  ucp_rsc_index_t aux_rsc_index,
                                  char *buf, size_t max);
 
-ucs_status_t ucp_ep_new(ucp_worker_h worker, uint64_t dest_uuid,
-                        const char *peer_name, const char *message,
-                        ucp_ep_h *ep_p);
+ucs_status_t ucp_ep_new(ucp_worker_h worker, const char *peer_name,
+                        const char *message, ucp_ep_h *ep_p);
 
-ucs_status_t ucp_ep_create_stub(ucp_worker_h worker, uint64_t dest_uuid,
-                                const ucp_ep_params_t *params,
-                                const char *peer_name, const char *message,
-                                ucp_ep_h *ep_p);
+void ucp_ep_delete(ucp_ep_h ep);
+
+ucs_status_t ucp_ep_init_create_wireup(ucp_ep_h ep,
+                                       const ucp_ep_params_t *params,
+                                       ucp_wireup_ep_t **wireup_ep);
 
 ucs_status_t ucp_ep_create_to_worker_addr(ucp_worker_h worker,
                                           const ucp_ep_params_t *params,
+                                          const ucp_unpacked_address_t *remote_address,
                                           unsigned ep_init_flags,
                                           const char *message, ucp_ep_h *ep_p);
 
+ucs_status_t ucp_ep_create_accept(ucp_worker_h worker,
+                                  const ucp_wireup_client_data_t *client_data,
+                                  ucp_ep_h *ep_p);
+
 ucs_status_ptr_t ucp_ep_flush_internal(ucp_ep_h ep, unsigned uct_flags,
                                        ucp_send_callback_t req_cb,
                                        unsigned req_flags,
-                                       ucp_request_callback_t flushed_cb);
+                                       ucp_request_t *worker_req,
+                                       ucp_request_callback_t flushed_cb,
+                                       const char *debug_name);
+
+ucs_status_t ucp_ep_create_sockaddr_aux(ucp_worker_h worker,
+                                        const ucp_ep_params_t *params,
+                                        const ucp_unpacked_address_t *remote_address,
+                                        ucp_ep_h *ep_p);
 
 void ucp_ep_config_key_set_params(ucp_ep_config_key_t *key,
                                   const ucp_ep_params_t *params);
 
 void ucp_ep_err_pending_purge(uct_pending_req_t *self, void *arg);
 
+void ucp_ep_disconnected(ucp_ep_h ep, int force);
+
 void ucp_ep_destroy_internal(ucp_ep_h ep);
 
-int ucp_ep_is_stub(ucp_ep_h ep);
+void ucp_ep_cleanup_lanes(ucp_ep_h ep);
+
+int ucp_ep_is_sockaddr_stub(ucp_ep_h ep);
 
 void ucp_ep_config_init(ucp_worker_h worker, ucp_ep_config_t *config);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.inl
index 69ae9941e..fbce49869 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_ep.inl
@@ -12,6 +12,7 @@
 #include "ucp_worker.h"
 #include "ucp_context.h"
 
+#include <ucp/wireup/wireup.h>
 #include <ucs/arch/bitops.h>
 
 
@@ -111,6 +112,72 @@ static inline const uct_md_attr_t* ucp_ep_md_attr(ucp_ep_h ep, ucp_lane_index_t
     return &context->tl_mds[ucp_ep_md_index(ep, lane)].attr;
 }
 
+static UCS_F_ALWAYS_INLINE ucp_ep_ext_gen_t* ucp_ep_ext_gen(ucp_ep_h ep)
+{
+    return (ucp_ep_ext_gen_t*)ucs_strided_elem_get(ep, 0, 1);
+}
+
+static UCS_F_ALWAYS_INLINE ucp_ep_ext_proto_t* ucp_ep_ext_proto(ucp_ep_h ep)
+{
+    return (ucp_ep_ext_proto_t*)ucs_strided_elem_get(ep, 0, 2);
+}
+
+static UCS_F_ALWAYS_INLINE ucp_ep_h ucp_ep_from_ext_gen(ucp_ep_ext_gen_t *ep_ext)
+{
+    return (ucp_ep_h)ucs_strided_elem_get(ep_ext, 1, 0);
+}
+
+static UCS_F_ALWAYS_INLINE ucp_ep_h ucp_ep_from_ext_proto(ucp_ep_ext_proto_t *ep_ext)
+{
+    return (ucp_ep_h)ucs_strided_elem_get(ep_ext, 2, 0);
+}
+
+static UCS_F_ALWAYS_INLINE ucp_ep_flush_state_t* ucp_ep_flush_state(ucp_ep_h ep)
+{
+    ucs_assert(ep->flags & UCP_EP_FLAG_FLUSH_STATE_VALID);
+    ucs_assert(!(ep->flags & UCP_EP_FLAG_ON_MATCH_CTX));
+    ucs_assert(!(ep->flags & UCP_EP_FLAG_LISTENER));
+    return &ucp_ep_ext_gen(ep)->flush_state;
+}
+
+static UCS_F_ALWAYS_INLINE uintptr_t ucp_ep_dest_ep_ptr(ucp_ep_h ep)
+{
+#if ENABLE_ASSERT
+    if (!(ep->flags & UCP_EP_FLAG_DEST_EP)) {
+        return 0; /* Let remote side assert if it gets NULL pointer */
+    }
+#endif
+    return ucp_ep_ext_gen(ep)->dest_ep_ptr;
+}
+
+/*
+ * Make sure we have a valid dest_ep_ptr value, so protocols which require a
+ * reply from remote side could be used.
+ */
+static UCS_F_ALWAYS_INLINE ucs_status_t
+ucp_ep_resolve_dest_ep_ptr(ucp_ep_h ep, ucp_lane_index_t lane)
+{
+    if (ep->flags & UCP_EP_FLAG_DEST_EP) {
+        return UCS_OK;
+    }
+
+    return ucp_wireup_connect_remote(ep, lane);
+}
+
+static inline void ucp_ep_update_dest_ep_ptr(ucp_ep_h ep, uintptr_t ep_ptr)
+{
+    if (ep->flags & UCP_EP_FLAG_DEST_EP) {
+        ucs_assertv(ep_ptr == ucp_ep_ext_gen(ep)->dest_ep_ptr,
+                    "ep=%p ep_ptr=0x%lx ep->dest_ep_ptr=0x%lx",
+                    ep, ep_ptr, ucp_ep_ext_gen(ep)->dest_ep_ptr);
+    }
+
+    ucs_assert(ep_ptr != 0);
+    ucs_trace("ep %p: set dest_ep_ptr to 0x%lx", ep, ep_ptr);
+    ep->flags                      |= UCP_EP_FLAG_DEST_EP;
+    ucp_ep_ext_gen(ep)->dest_ep_ptr = ep_ptr;
+}
+
 static inline const char* ucp_ep_peer_name(ucp_ep_h ep)
 {
 #if ENABLE_DEBUG_DATA
@@ -120,4 +187,22 @@ static inline const char* ucp_ep_peer_name(ucp_ep_h ep)
 #endif
 }
 
+static inline void ucp_ep_flush_state_reset(ucp_ep_h ep)
+{
+    ucp_ep_flush_state_t *flush_state = &ucp_ep_ext_gen(ep)->flush_state;
+
+    ucs_assert(!(ep->flags & (UCP_EP_FLAG_ON_MATCH_CTX |
+                              UCP_EP_FLAG_LISTENER)));
+    if (!(ep->flags & UCP_EP_FLAG_FLUSH_STATE_VALID)) {
+        flush_state->send_sn = 0;
+        flush_state->cmpl_sn = 0;
+        ucs_queue_head_init(&flush_state->reqs);
+        ep->flags |= UCP_EP_FLAG_FLUSH_STATE_VALID;
+    } else {
+        ucs_assert(flush_state->send_sn == 0);
+        ucs_assert(flush_state->cmpl_sn == 0);
+        ucs_assert(ucs_queue_is_empty(&flush_state->reqs));
+    }
+}
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.c
index 608d610bf..a027eb6f1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.c
@@ -6,90 +6,167 @@
 
 #include "ucp_listener.h"
 
+#include <ucp/stream/stream.h>
 #include <ucp/wireup/wireup_ep.h>
+#include <ucp/core/ucp_ep.h>
+#include <ucp/core/ucp_ep.inl>
 #include <ucs/debug/log.h>
 #include <ucs/sys/string.h>
 
 
-static unsigned ucp_listener_conn_request_progress(void *arg)
+static unsigned ucp_listener_accept_cb_progress(void *arg)
 {
-    ucp_listener_accept_t *accept = arg;
+    ucp_ep_h       ep       = arg;
+    ucp_listener_h listener = ucp_ep_ext_gen(ep)->listener;
+
+    /* NOTE: protect union */
+    ucs_assert(!(ep->flags & (UCP_EP_FLAG_ON_MATCH_CTX |
+                              UCP_EP_FLAG_FLUSH_STATE_VALID)));
+    ucs_assert(ep->flags   & UCP_EP_FLAG_LISTENER);
+
+    ep->flags &= ~UCP_EP_FLAG_LISTENER;
+    ep->flags |= UCP_EP_FLAG_USED;
+    ucp_stream_ep_activate(ep);
+    ucp_ep_flush_state_reset(ep);
+
+    /*
+     * listener is NULL if the EP was created with UCP_EP_PARAM_FIELD_EP_ADDR
+     * and we are here because long address requires wireup protocol
+     */
+    if (listener && listener->accept_cb) {
+        listener->accept_cb(ep, listener->arg);
+    }
 
-    ucs_trace_func("listener=%p ep=%p", accept->listener, accept->ep);
+    return 1;
+}
 
-    accept->listener->cb(accept->ep, accept->listener->arg);
-    ucs_free(accept);
-    return 0;
+int ucp_listener_accept_cb_remove_filter(const ucs_callbackq_elem_t *elem,
+                                                void *arg)
+{
+    ucp_ep_h ep = elem->arg;
+
+    return (elem->cb == ucp_listener_accept_cb_progress) && (ep == arg);
 }
 
-static int ucp_listener_remove_filter(const ucs_callbackq_elem_t *elem,
-                                      void *arg)
+void ucp_listener_schedule_accept_cb(ucp_ep_h ep)
 {
-    ucp_listener_h *listener = elem->arg;
+    uct_worker_cb_id_t prog_id = UCS_CALLBACKQ_ID_NULL;
 
-    return (elem->cb == ucp_listener_conn_request_progress) && (listener == arg);
+    uct_worker_progress_register_safe(ep->worker->uct,
+                                      ucp_listener_accept_cb_progress,
+                                      ep, UCS_CALLBACKQ_FLAG_ONESHOT,
+                                      &prog_id);
 }
 
-static ucs_status_t ucp_listener_conn_request_callback(void *arg,
-                                                       const void *conn_priv_data,
-                                                       size_t length)
+static unsigned ucp_listener_conn_request_progress(void *arg)
 {
-    const ucp_wireup_sockaddr_priv_t *client_data = conn_priv_data;
-    ucp_listener_h listener                       = arg;
-    ucp_listener_accept_t *accept;
-    uct_worker_cb_id_t prog_id;
-    ucp_ep_params_t params;
-    ucs_status_t status;
-    ucp_ep_h ep;
+    ucp_conn_request_h               conn_request = arg;
+    ucp_listener_h                   listener     = conn_request->listener;
+    const ucp_wireup_client_data_t   *client_data = &conn_request->client_data;
+    ucp_worker_h                     worker;
+    ucp_ep_h                         ep;
+    ucs_status_t                     status;
+
+    ucs_trace_func("listener=%p", listener);
+
+    if (listener->conn_cb) {
+        listener->conn_cb(conn_request, listener->arg);
+        return 1;
+    }
 
-    ucs_trace("listener %p: got connection request", listener);
+    worker = listener->wiface.worker;
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
+    UCS_ASYNC_BLOCK(&worker->async);
+    /* coverity[overrun-buffer-val] */
+    status = ucp_ep_create_accept(worker, client_data, &ep);
 
-    params.field_mask = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE |
-                        UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
-    params.err_mode   = client_data->err_mode;
-    params.address    = (ucp_address_t*)(client_data + 1);
+    if (status != UCS_OK) {
+        goto out;
+    }
+
+    if (ep->flags & UCP_EP_FLAG_LISTENER) {
+        status = ucp_wireup_send_pre_request(ep);
+    } else {
+        /* send wireup request message, to connect the client to the server's
+           new endpoint */
+        ucs_assert(!(ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED));
+        status = ucp_wireup_send_request(ep);
+    }
 
-    /* create endpoint to the worker address we got in the private data */
-    status = ucp_ep_create_to_worker_addr(listener->wiface.worker, &params, 0,
-                                          "listener", &ep);
     if (status != UCS_OK) {
-        goto err;
+        goto out;
     }
 
-    /* send wireup request message, to connect the client to the new endpoint */
-    status = ucp_wireup_send_request(ep, client_data->ep_uuid);
+    status = uct_iface_accept(listener->wiface.iface, conn_request->uct_req);
     if (status != UCS_OK) {
-        goto err_destroy_ep;
+        ucp_ep_destroy_internal(ep);
+        goto out;
     }
 
-    /* if user provided a callback for accepting new connection, launch it on
-     * the main thread
-     */
-    if (listener->cb != NULL) {
-        accept = ucs_malloc(sizeof(*accept), "ucp_listener_accept");
-        if (accept == NULL) {
-            ucs_error("failed to allocate listener accept context");
-            status = UCS_ERR_NO_MEMORY;
-            goto err_destroy_ep;
+    if (listener->accept_cb != NULL) {
+        if (ep->flags & UCP_EP_FLAG_LISTENER) {
+            ucs_assert(!(ep->flags & UCP_EP_FLAG_USED));
+            ucp_ep_ext_gen(ep)->listener = listener;
+        } else {
+            ep->flags |= UCP_EP_FLAG_USED;
+            listener->accept_cb(ep, listener->arg);
         }
+    }
 
-        accept->listener = listener;
-        accept->ep       = ep;
+out:
+    if (status != UCS_OK) {
+        ucs_error("connection request failed on listener %p with status %s",
+                  listener, ucs_status_string(status));
+        uct_iface_reject(listener->wiface.iface, conn_request->uct_req);
+    }
+
+    UCS_ASYNC_UNBLOCK(&worker->async);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
+    ucs_free(conn_request);
+    return 1;
+}
 
-        /* defer user callback to be invoked from the main thread */
-        prog_id = UCS_CALLBACKQ_ID_NULL;
-        uct_worker_progress_register_safe(listener->wiface.worker->uct,
-                                          ucp_listener_conn_request_progress,
-                                          accept, UCS_CALLBACKQ_FLAG_ONESHOT,
-                                          &prog_id);
+static int ucp_listener_remove_filter(const ucs_callbackq_elem_t *elem,
+                                      void *arg)
+{
+    ucp_listener_h *listener = elem->arg;
+
+    return (elem->cb == ucp_listener_conn_request_progress) && (listener == arg);
+}
+
+static void ucp_listener_conn_request_callback(uct_iface_h tl_iface, void *arg,
+                                               uct_conn_request_h uct_req,
+                                               const void *conn_priv_data,
+                                               size_t length)
+{
+    ucp_listener_h     listener = arg;
+    uct_worker_cb_id_t prog_id  = UCS_CALLBACKQ_ID_NULL;
+    ucp_conn_request_h conn_request;
+
+    ucs_trace("listener %p: got connection request", listener);
+
+    /* Defer wireup init and user's callback to be invoked from the main thread */
+    conn_request = ucs_malloc(ucs_offsetof(ucp_conn_request_t, client_data) +
+                              length, "accept connection request");
+    if (conn_request == NULL) {
+        ucs_error("failed to allocate connect request, rejecting connection request %p on TL iface %p, reason %s",
+                  uct_req, tl_iface, ucs_status_string(UCS_ERR_NO_MEMORY));
+        uct_iface_reject(tl_iface, uct_req);
+        return;
     }
 
-    return UCS_OK;
+    conn_request->listener = listener;
+    conn_request->uct_req  = uct_req;
+    memcpy(&conn_request->client_data, conn_priv_data, length);
 
-err_destroy_ep:
-    ucp_ep_destroy_internal(ep);
-err:
-    return status;
+    uct_worker_progress_register_safe(listener->wiface.worker->uct,
+                                      ucp_listener_conn_request_progress,
+                                      conn_request, UCS_CALLBACKQ_FLAG_ONESHOT,
+                                      &prog_id);
+
+    /* If the worker supports the UCP_FEATURE_WAKEUP feature, signal the user so
+     * that he can wake-up on this event */
+    ucp_worker_signal_internal(listener->wiface.worker);
 }
 
 ucs_status_t ucp_listener_create(ucp_worker_h worker,
@@ -105,16 +182,22 @@ ucs_status_t ucp_listener_create(ucp_worker_h worker,
     ucp_tl_md_t *tl_md;
     char saddr_str[UCS_SOCKADDR_STRING_LEN];
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
-    UCS_ASYNC_BLOCK(&worker->async);
-
     if (!(params->field_mask & UCP_LISTENER_PARAM_FIELD_SOCK_ADDR)) {
         ucs_error("Missing sockaddr for listener");
-        status = UCS_ERR_INVALID_PARAM;
-        goto out;
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    UCP_CHECK_PARAM_NON_NULL(params->sockaddr.addr, status, return status);
+
+    if (ucs_test_all_flags(params->field_mask,
+                           UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER |
+                           UCP_LISTENER_PARAM_FIELD_CONN_HANDLER)) {
+        ucs_error("Only one accept handler should be provided");
+        return UCS_ERR_INVALID_PARAM;
     }
 
-    UCP_CHECK_PARAM_NON_NULL(params->sockaddr.addr, status, goto out);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
+    UCS_ASYNC_BLOCK(&worker->async);
 
     /* Go through all the available resources and for each one, check if the given
      * sockaddr is accessible from its md. Start listening on the first md that
@@ -130,18 +213,22 @@ ucs_status_t ucp_listener_create(ucp_worker_h worker,
             continue;
         }
 
-        listener = ucs_malloc(sizeof(*listener), "ucp_listener");
+        listener = ucs_calloc(1, sizeof(*listener), "ucp_listener");
         if (listener == NULL) {
             status = UCS_ERR_NO_MEMORY;
             goto out;
         }
 
         if (params->field_mask & UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER) {
-            UCP_CHECK_PARAM_NON_NULL(params->accept_handler.cb, status, goto err_free);
-            listener->cb  = params->accept_handler.cb;
-            listener->arg = params->accept_handler.arg;
-        } else {
-            listener->cb  = NULL;
+            UCP_CHECK_PARAM_NON_NULL(params->accept_handler.cb, status,
+                                     goto err_free);
+            listener->accept_cb = params->accept_handler.cb;
+            listener->arg       = params->accept_handler.arg;
+        } else if (params->field_mask & UCP_LISTENER_PARAM_FIELD_CONN_HANDLER) {
+            UCP_CHECK_PARAM_NON_NULL(params->conn_handler.cb, status,
+                                     goto err_free);
+            listener->conn_cb   = params->conn_handler.cb;
+            listener->arg       = params->conn_handler.arg;
         }
 
         memset(&iface_params, 0, sizeof(iface_params));
@@ -157,6 +244,13 @@ ucs_status_t ucp_listener_create(ucp_worker_h worker,
             goto err_free;
         }
 
+        if ((context->config.features & UCP_FEATURE_WAKEUP) &&
+            !(listener->wiface.attr.cap.flags & UCT_IFACE_FLAG_CB_ASYNC)) {
+            ucp_worker_iface_cleanup(&listener->wiface);
+            ucs_free(listener);
+            continue;
+        }
+
         ucs_trace("listener %p: accepting connections on %s", listener,
                   tl_md->rsc.md_name);
 
@@ -167,13 +261,14 @@ ucs_status_t ucp_listener_create(ucp_worker_h worker,
 
     ucs_error("none of the available transports can listen for connections on %s",
               ucs_sockaddr_str(params->sockaddr.addr, saddr_str, sizeof(saddr_str)));
-    status = UCS_ERR_INVALID_ADDR;
+    status = UCS_ERR_UNREACHABLE;
+    goto out;
 
 err_free:
     ucs_free(listener);
 out:
     UCS_ASYNC_UNBLOCK(&worker->async);
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return status;
 }
 
@@ -187,3 +282,21 @@ void ucp_listener_destroy(ucp_listener_h listener)
     ucp_worker_iface_cleanup(&listener->wiface);
     ucs_free(listener);
 }
+
+ucs_status_t ucp_listener_reject(ucp_listener_h listener,
+                                 ucp_conn_request_h conn_request)
+{
+    ucp_worker_h worker = listener->wiface.worker;
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
+    UCS_ASYNC_BLOCK(&worker->async);
+
+    uct_iface_reject(listener->wiface.iface, conn_request->uct_req);
+
+    UCS_ASYNC_UNBLOCK(&worker->async);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
+
+    ucs_free(conn_request);
+
+    return UCS_OK;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.h
index 6883f8b6b..8b8f33f38 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_listener.h
@@ -9,28 +9,30 @@
 #define UCP_LISTENER_H_
 
 #include "ucp_worker.h"
+#include "wireup/wireup_ep.h"
 
 
 /**
  * UCP listener
  */
 typedef struct ucp_listener {
-    ucp_worker_iface_t              wiface;  /* UCT iface to listen on */
-    ucp_listener_accept_callback_t  cb;      /* Listen accept callback */
-    void                            *arg;    /* User's arg for the accept callback */
-    uct_worker_cb_id_t              prog_id; /* Slow-path callback */
+    ucp_worker_iface_t                  wiface;    /* UCT iface to listen on */
+    ucp_listener_accept_callback_t      accept_cb; /* Listen accept callback
+                                                      which creates an endpoint
+                                                    */
+    ucp_listener_conn_callback_t        conn_cb;   /* Listen callback which
+                                                      creates a handle to
+                                                      connection request to the
+                                                      remote endpoint */
+    void                                *arg;      /* User's arg for the accept
+                                                      callback */
+    uct_worker_cb_id_t                  prog_id;   /* Slow-path callback */
 } ucp_listener_t;
 
 
-/**
- * Accepted connection on a listener
- */
-typedef struct ucp_listener_accept {
-    ucp_listener_h                  listener; /* Listener on which the connection
-                                                 was accepted */
-    ucp_ep_h                        ep;       /* New endpoint which was created
-                                                 for the connection */
-} ucp_listener_accept_t;
+void ucp_listener_schedule_accept_cb(ucp_ep_h ep);
 
+int ucp_listener_accept_cb_remove_filter(const ucs_callbackq_elem_t *elem,
+                                         void *arg);
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.c
index d2e35b993..3d8ab4172 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.c
@@ -37,12 +37,13 @@ ucs_status_t ucp_mem_rereg_mds(ucp_context_h context, ucp_md_map_t reg_md_map,
     unsigned prev_num_memh;
     unsigned md_index;
     ucs_status_t status;
+    int level;
 
     if (reg_md_map == *md_map_p) {
         return UCS_OK; /* shortcut - no changes required */
     }
 
-    prev_num_memh = ucs_count_one_bits(*md_map_p);
+    prev_num_memh = ucs_popcount(*md_map_p);
     prev_uct_memh = ucs_alloca(prev_num_memh * sizeof(*prev_uct_memh));
 
     /* Go over previous handles, save only the ones we will need */
@@ -77,7 +78,7 @@ ucs_status_t ucp_mem_rereg_mds(ucp_context_h context, ucp_md_map_t reg_md_map,
     }
 
     /* prev_uct_memh should contain the handles which should be reused */
-    ucs_assert(prev_memh_index == ucs_count_one_bits(*md_map_p & reg_md_map));
+    ucs_assert(prev_memh_index == ucs_popcount(*md_map_p & reg_md_map));
 
     /* Go over requested MD map, and use / register new handles */
     new_md_map      = 0;
@@ -99,10 +100,12 @@ ucs_status_t ucp_mem_rereg_mds(ucp_context_h context, ucp_md_map_t reg_md_map,
             status = uct_md_mem_reg(context->tl_mds[md_index].md, address,
                                     length, uct_flags, &uct_memh[memh_index]);
             if (status != UCS_OK) {
-                ucs_error("failed to register address %p length %zu on md[%d]=%s: %s",
-                          address, length, md_index,
-                          context->tl_mds[md_index].rsc.md_name,
-                          ucs_status_string(status));
+                level = (uct_flags & UCT_MD_MEM_FLAG_HIDE_ERRORS) ?
+                        UCS_LOG_LEVEL_DEBUG : UCS_LOG_LEVEL_ERROR;
+                ucs_log(level,
+                        "failed to register address %p length %zu on md[%d]=%s: %s",
+                        address, length, md_index, context->tl_mds[md_index].rsc.md_name,
+                        ucs_status_string(status));
                 ucp_mem_rereg_mds(context, 0, NULL, 0, 0, alloc_md, mem_type,
                                   alloc_md_memh_p, uct_memh, md_map_p);
                 return status;
@@ -221,16 +224,16 @@ ucp_mem_map_params2uct_flags(ucp_mem_map_params_t *params)
 }
 
 /* Matrix of behavior
- * |-----------------------------------------------------------------------------|
- * | parameter |                          value                                  |
- * |-----------|-----------------------------------------------------------------|
- * | ALLOCATE  |  0  |     1     |  0  |  0  |  1  |     1     |  0  |     1     |
- * | FIXED     |  0  |     0     |  1  |  0  |  1  |     0     |  1  |     1     |
- * | addr      |  0  |     0     |  0  |  1  |  0  |     1     |  1  |     1     |
- * |-----------|-----|-----------|-----|-----|-----|-----------|-----|-----------|
- * | result    | err | alloc/reg | err | reg | err | alloc/reg | err | alloc/reg |
- * |           |     |           |     |     |     |  (hint)   |     | (fixed)   |
- * |-----------------------------------------------------------------------------|
+ * |--------------------------------------------------------------------------------|
+ * | parameter |                             value                                  |
+ * |-----------|--------------------------------------------------------------------|
+ * | ALLOCATE  |  0     |     1     |  0  |  0  |  1  |     1     |  0  |     1     |
+ * | FIXED     |  0     |     0     |  1  |  0  |  1  |     0     |  1  |     1     |
+ * | addr      |  0     |     0     |  0  |  1  |  0  |     1     |  1  |     1     |
+ * |-----------|--------|-----------|-----|-----|-----|-----------|-----|-----------|
+ * | result    | err if | alloc/reg | err | reg | err | alloc/reg | err | alloc/reg |
+ * |           | len >0 |           |     |     |     |  (hint)   |     | (fixed)   |
+ * |--------------------------------------------------------------------------------|
  */
 static inline ucs_status_t ucp_mem_map_check_and_adjust_params(ucp_mem_map_params_t *params)
 {
@@ -260,8 +263,9 @@ static inline ucs_status_t ucp_mem_map_check_and_adjust_params(ucp_mem_map_param
 
     /* Now, lets check the rest of erroneous cases from the matrix */
     if (params->address == NULL) {
-        if (!(params->flags & UCP_MEM_MAP_ALLOCATE)) {
-            ucs_error("Undefined address requires UCP_MEM_MAP_ALLOCATE flag");
+        if (!(params->flags & UCP_MEM_MAP_ALLOCATE) && (params->length > 0)) {
+            ucs_error("Undefined address with nonzero length requires "
+                      "UCP_MEM_MAP_ALLOCATE flag");
             return UCS_ERR_INVALID_PARAM;
         }
     } else if (!(params->flags & UCP_MEM_MAP_ALLOCATE) &&
@@ -417,6 +421,72 @@ out:
     return status;
 }
 
+ucs_status_t ucp_mem_type_reg_buffers(ucp_worker_h worker, void *remote_addr,
+                                      size_t length, uct_memory_type_t mem_type,
+                                      unsigned md_index, uct_mem_h *memh,
+                                      ucp_md_map_t *md_map,
+                                      uct_rkey_bundle_t *rkey_bundle)
+{
+    ucp_context_h context = worker->context;
+    uct_md_h md;
+    ucs_status_t status;
+    char *rkey_buffer;
+
+    md = context->tl_mds[md_index].md;
+
+    *memh = UCT_MEM_HANDLE_NULL;
+    status = ucp_mem_rereg_mds(context, UCS_BIT(md_index), remote_addr, length,
+                               UCT_MD_MEM_ACCESS_ALL, NULL, mem_type,
+                               NULL, memh, md_map);
+    if (status != UCS_OK) {
+        goto err;
+    }
+
+    if (context->tl_mds[md_index].attr.cap.flags & UCT_MD_FLAG_NEED_RKEY) {
+        rkey_buffer = ucs_alloca(context->tl_mds[md_index].attr.rkey_packed_size);
+
+        status = uct_md_mkey_pack(md, memh[0], rkey_buffer);
+        if (status != UCS_OK) {
+            ucs_error("failed to pack key from md[%d]: %s",
+                      md_index, ucs_status_string(status));
+            goto err_dreg_mem;
+        }
+
+        status = uct_rkey_unpack(rkey_buffer, rkey_bundle);
+        if (status != UCS_OK) {
+            ucs_error("failed to unpack key from md[%d]: %s",
+                      md_index, ucs_status_string(status));
+            goto err_dreg_mem;
+        }
+    } else {
+        rkey_bundle->handle = NULL;
+        rkey_bundle->rkey   = UCT_INVALID_RKEY;
+        rkey_bundle->type   = NULL;
+    }
+
+    return UCS_OK;
+
+err_dreg_mem:
+    ucp_mem_rereg_mds(context, 0, NULL, 0, 0, NULL, mem_type, NULL,
+                      memh, md_map);
+err:
+    return status;
+}
+
+void ucp_mem_type_unreg_buffers(ucp_worker_h worker, uct_memory_type_t mem_type,
+                                uct_mem_h *memh, ucp_md_map_t *md_map,
+                                uct_rkey_bundle_t *rkey_bundle)
+{
+    ucp_context_h context = worker->context;
+
+    if (rkey_bundle->rkey != UCT_INVALID_RKEY) {
+        uct_rkey_release(rkey_bundle);
+    }
+
+    ucp_mem_rereg_mds(context, 0, NULL, 0, 0, NULL, mem_type, NULL,
+                      memh, md_map);
+}
+
 ucs_status_t ucp_mem_query(const ucp_mem_h memh, ucp_mem_attr_t *attr)
 {
     if (attr->field_mask & UCP_MEM_ATTR_FIELD_ADDRESS) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.h
index 128c8937b..be2e1d494 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_mm.h
@@ -38,8 +38,11 @@ typedef struct ucp_rkey {
         unsigned                  max_put_short;/* Cached value of max_put_short */
         uct_rkey_t                rma_rkey;     /* Key to use for RMAs */
         uct_rkey_t                amo_rkey;     /* Key to use for AMOs */
+        ucp_amo_proto_t           *amo_proto;   /* Protocol for AMOs */
+        ucp_rma_proto_t           *rma_proto;   /* Protocol for RMAs */
     } cache;
     ucp_md_map_t                  md_map;  /* Which *remote* MDs have valid memory handles */
+    uct_memory_type_t             mem_type;/* Memory type of remote key memory */
     uct_rkey_bundle_t             uct[0];  /* Remote key for every MD */
 } ucp_rkey_t;
 
@@ -73,6 +76,7 @@ typedef struct ucp_mem_desc {
 void ucp_rkey_resolve_inner(ucp_rkey_h rkey, ucp_ep_h ep);
 
 ucp_lane_index_t ucp_rkey_get_rma_bw_lane(ucp_rkey_h rkey, ucp_ep_h ep,
+                                          uct_memory_type_t mem_type,
                                           uct_rkey_t *uct_rkey_p,
                                           ucp_lane_map_t ignore);
 
@@ -119,18 +123,29 @@ ucs_status_t ucp_mem_rereg_mds(ucp_context_h context, ucp_md_map_t reg_md_map,
 size_t ucp_rkey_packed_size(ucp_context_h context, ucp_md_map_t md_map);
 
 void ucp_rkey_packed_copy(ucp_context_h context, ucp_md_map_t md_map,
-                          void *rkey_buffer, const void* uct_rkeys[]);
+                          uct_memory_type_t mem_type, void *rkey_buffer,
+                          const void* uct_rkeys[]);
 
 ssize_t ucp_rkey_pack_uct(ucp_context_h context, ucp_md_map_t md_map,
-                          const uct_mem_h *memh, void *rkey_buffer);
+                          const uct_mem_h *memh, uct_memory_type_t mem_type,
+                          void *rkey_buffer);
 
 void ucp_rkey_dump_packed(const void *rkey_buffer, char *buffer, size_t max);
 
+ucs_status_t ucp_mem_type_reg_buffers(ucp_worker_h worker, void *remote_addr,
+                                      size_t length, uct_memory_type_t mem_type,
+                                      unsigned md_index, uct_mem_h *memh,
+                                      ucp_md_map_t *md_map,
+                                      uct_rkey_bundle_t *rkey_bundle);
+
+void ucp_mem_type_unreg_buffers(ucp_worker_h worker, uct_memory_type_t mem_type,
+                                uct_mem_h *memh, ucp_md_map_t *md_map,
+                                uct_rkey_bundle_t *rkey_bundle);
 
 static UCS_F_ALWAYS_INLINE ucp_md_index_t
 ucp_memh_map2idx(ucp_md_map_t md_map, ucp_md_index_t md_idx)
 {
-    return ucs_count_one_bits(md_map & UCS_MASK(md_idx));
+    return ucs_popcount(md_map & UCS_MASK(md_idx));
 }
 
 static UCS_F_ALWAYS_INLINE uct_mem_h
@@ -165,5 +180,6 @@ ucp_memh2uct(ucp_mem_h memh, ucp_md_index_t md_idx)
     })
 
 #define UCP_MEM_IS_HOST(_mem_type) ((_mem_type) == UCT_MD_MEM_TYPE_HOST)
+#define UCP_MEM_IS_CUDA_MANAGED(_mem_type) ((_mem_type) == UCT_MD_MEM_TYPE_CUDA_MANAGED)
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.c
index 169d655f1..cdb62da94 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.c
@@ -68,22 +68,18 @@ UCP_PROXY_EP_DEFINE_OP(ssize_t, am_bcopy, uint8_t, uct_pack_callback_t, void*,
                        unsigned)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, am_zcopy, uint8_t, const void*, unsigned,
                        const uct_iov_t*, size_t, unsigned, uct_completion_t*)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_add64, uint64_t, uint64_t,
-                       uct_rkey_t)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_fadd64, uint64_t, uint64_t,
-                       uct_rkey_t, uint64_t*, uct_completion_t*)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_swap64, uint64_t, uint64_t,
-                       uct_rkey_t, uint64_t*, uct_completion_t*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_cswap64, uint64_t, uint64_t,
                        uint64_t, uct_rkey_t, uint64_t*, uct_completion_t*)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_add32, uint32_t, uint64_t,
-                       uct_rkey_t)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_fadd32, uint32_t, uint64_t,
-                       uct_rkey_t, uint32_t*, uct_completion_t*)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_swap32, uint32_t, uint64_t,
-                       uct_rkey_t, uint32_t*, uct_completion_t*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic_cswap32, uint32_t, uint32_t,
                        uint64_t, uct_rkey_t, uint32_t*, uct_completion_t*)
+UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic64_post, uct_atomic_op_t,
+                       uint64_t, uint64_t, uct_rkey_t)
+UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic32_post, uct_atomic_op_t,
+                       uint32_t, uint64_t, uct_rkey_t)
+UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic64_fetch, uct_atomic_op_t, uint64_t,
+                       uint64_t*, uint64_t, uct_rkey_t, uct_completion_t*)
+UCP_PROXY_EP_DEFINE_OP(ucs_status_t, atomic32_fetch, uct_atomic_op_t, uint32_t,
+                       uint32_t*, uint64_t, uct_rkey_t, uct_completion_t*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, tag_eager_short, uct_tag_t, const void*,
                        size_t)
 UCP_PROXY_EP_DEFINE_OP(ssize_t, tag_eager_bcopy, uct_tag_t, uint64_t,
@@ -96,7 +92,7 @@ UCP_PROXY_EP_DEFINE_OP(ucs_status_ptr_t, tag_rndv_zcopy, uct_tag_t, const void*,
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, tag_rndv_cancel, void*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, tag_rndv_request, uct_tag_t, const void*,
                        unsigned, unsigned)
-UCP_PROXY_EP_DEFINE_OP(ucs_status_t, pending_add, uct_pending_req_t*)
+UCP_PROXY_EP_DEFINE_OP(ucs_status_t, pending_add, uct_pending_req_t*, unsigned)
 UCP_PROXY_EP_DEFINE_OP(void, pending_purge, uct_pending_purge_callback_t, void*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, flush, unsigned, uct_completion_t*)
 UCP_PROXY_EP_DEFINE_OP(ucs_status_t, fence, unsigned)
@@ -133,14 +129,12 @@ UCS_CLASS_INIT_FUNC(ucp_proxy_ep_t, const uct_iface_ops_t *ops, ucp_ep_h ucp_ep,
     UCP_PROXY_EP_SET_OP(ep_am_short);
     UCP_PROXY_EP_SET_OP(ep_am_bcopy);
     UCP_PROXY_EP_SET_OP(ep_am_zcopy);
-    UCP_PROXY_EP_SET_OP(ep_atomic_add64);
-    UCP_PROXY_EP_SET_OP(ep_atomic_fadd64);
-    UCP_PROXY_EP_SET_OP(ep_atomic_swap64);
     UCP_PROXY_EP_SET_OP(ep_atomic_cswap64);
-    UCP_PROXY_EP_SET_OP(ep_atomic_add32);
-    UCP_PROXY_EP_SET_OP(ep_atomic_fadd32);
-    UCP_PROXY_EP_SET_OP(ep_atomic_swap32);
     UCP_PROXY_EP_SET_OP(ep_atomic_cswap32);
+    UCP_PROXY_EP_SET_OP(ep_atomic64_post);
+    UCP_PROXY_EP_SET_OP(ep_atomic32_post);
+    UCP_PROXY_EP_SET_OP(ep_atomic64_fetch);
+    UCP_PROXY_EP_SET_OP(ep_atomic32_fetch);
     UCP_PROXY_EP_SET_OP(ep_tag_eager_short);
     UCP_PROXY_EP_SET_OP(ep_tag_eager_bcopy);
     UCP_PROXY_EP_SET_OP(ep_tag_eager_zcopy);
@@ -183,20 +177,59 @@ static UCS_CLASS_CLEANUP_FUNC(ucp_proxy_ep_t)
     }
 }
 
+int ucp_proxy_ep_test(uct_ep_h uct_ep)
+{
+    return uct_ep->iface->ops.ep_destroy == ucp_proxy_ep_destroy;
+}
+
+uct_ep_h ucp_proxy_ep_extract(uct_ep_h ep)
+{
+    ucp_proxy_ep_t *proxy_ep = ucs_derived_of(ep, ucp_proxy_ep_t);
+    uct_ep_h uct_ep;
+
+    uct_ep = proxy_ep->uct_ep;
+    proxy_ep->uct_ep = NULL;
+    return uct_ep;
+}
+
+static void ucp_proxy_ep_replace_if_owned(uct_ep_h uct_ep, uct_ep_h owned_ep,
+                                          uct_ep_h replacement_ep)
+{
+    ucp_proxy_ep_t *proxy_ep;
+
+    if (ucp_proxy_ep_test(uct_ep)) {
+        proxy_ep = ucs_derived_of(uct_ep, ucp_proxy_ep_t);
+        if (proxy_ep->uct_ep == owned_ep) {
+            proxy_ep->uct_ep = replacement_ep;
+        }
+        ucs_assert(replacement_ep != NULL);
+    }
+}
+
 void ucp_proxy_ep_replace(ucp_proxy_ep_t *proxy_ep)
 {
     ucp_ep_h ucp_ep = proxy_ep->ucp_ep;
     ucp_lane_index_t lane;
+    uct_ep_h tl_ep = NULL;
 
     ucs_assert(proxy_ep->uct_ep != NULL);
     for (lane = 0; lane < ucp_ep_num_lanes(ucp_ep); ++lane) {
         if (ucp_ep->uct_eps[lane] == &proxy_ep->super) {
+            ucs_assert(proxy_ep->uct_ep != NULL);    /* make sure there is only one match */
             ucp_ep->uct_eps[lane] = proxy_ep->uct_ep;
+            tl_ep = ucp_ep->uct_eps[lane];
             proxy_ep->uct_ep = NULL;
-            break;
         }
     }
 
+    /* go through the lanes and check if the proxy ep that is being destroyed,
+     * is pointed to by another proxy ep. if so, redirect that other proxy ep
+     * to point to the underlying uct ep. */
+    for (lane = 0; lane < ucp_ep_num_lanes(ucp_ep); ++lane) {
+        ucp_proxy_ep_replace_if_owned(ucp_ep->uct_eps[lane], &proxy_ep->super,
+                                      tl_ep);
+    }
+
     uct_ep_destroy(&proxy_ep->super);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.h
index a23b4d820..fe15d32a8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_proxy_ep.h
@@ -42,6 +42,10 @@ UCS_CLASS_DECLARE(ucp_proxy_ep_t, const uct_iface_ops_t *ops, ucp_ep_h ucp_ep,
  */
 void ucp_proxy_ep_replace(ucp_proxy_ep_t *proxy_ep);
 
+int ucp_proxy_ep_test(uct_ep_h ep);
+
+uct_ep_h ucp_proxy_ep_extract(uct_ep_h ep);
+
 void ucp_proxy_ep_set_uct_ep(ucp_proxy_ep_t *proxy_ep, uct_ep_h uct_ep,
                              int is_owner);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.c
index f42deaa53..f6b3b1b88 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.c
@@ -66,7 +66,7 @@ ucp_request_release_common(void *request, uint8_t cb_flag, const char *debug_nam
                                                         ucp_worker_t, req_mp);
     uint16_t flags;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     flags = req->flags;
     ucs_trace_req("%s request %p (%p) "UCP_REQUEST_FLAGS_FMT, debug_name,
@@ -81,7 +81,7 @@ ucp_request_release_common(void *request, uint8_t cb_flag, const char *debug_nam
         req->flags = (flags | UCP_REQUEST_FLAG_RELEASED) & ~cb_flag;
     }
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 }
 
 UCS_PROFILE_FUNC_VOID(ucp_request_release, (request), void *request)
@@ -106,7 +106,7 @@ UCS_PROFILE_FUNC_VOID(ucp_request_cancel, (worker, request),
     }
 
     if (req->flags & UCP_REQUEST_FLAG_EXPECTED) {
-        UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+        UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
         ucp_tag_exp_remove(&worker->tm, req);
         /* If tag posted to the transport need to wait its completion */
@@ -114,7 +114,7 @@ UCS_PROFILE_FUNC_VOID(ucp_request_cancel, (worker, request),
             ucp_request_complete_tag_recv(req, UCS_ERR_CANCELED);
         }
 
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+        UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     }
 }
 
@@ -163,7 +163,7 @@ int ucp_request_pending_add(ucp_request_t *req, ucs_status_t *req_status)
                 ucs_debug_get_symbol_name(req->send.uct.func));
 
     uct_ep = req->send.ep->uct_eps[req->send.lane];
-    status = uct_ep_pending_add(uct_ep, &req->send.uct);
+    status = uct_ep_pending_add(uct_ep, &req->send.uct, 0);
     if (status == UCS_OK) {
         ucs_trace_data("ep %p: added pending uct request %p to lane[%d]=%p",
                        req->send.ep, req, req->send.lane, uct_ep);
@@ -195,26 +195,29 @@ static void ucp_request_dt_dereg(ucp_context_t *context, ucp_dt_reg_t *dt_reg,
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_request_memory_reg,
-                 (context, md_map, buffer, length, datatype, state, req_dbg),
+                 (context, md_map, buffer, length, datatype, state, mem_type, req_dbg, uct_flags),
                  ucp_context_t *context, ucp_md_map_t md_map, void *buffer,
                  size_t length, ucp_datatype_t datatype, ucp_dt_state_t *state,
-                 ucp_request_t *req_dbg)
+                 uct_memory_type_t mem_type, ucp_request_t *req_dbg, unsigned uct_flags)
 {
     size_t iov_it, iovcnt;
     const ucp_dt_iov_t *iov;
     ucp_dt_reg_t *dt_reg;
     ucs_status_t status;
+    int flags;
+    int level;
 
     ucs_trace_func("context=%p md_map=0x%lx buffer=%p length=%zu datatype=0x%lu "
                    "state=%p", context, md_map, buffer, length, datatype, state);
 
     status = UCS_OK;
+    flags  = UCT_MD_MEM_ACCESS_RMA | uct_flags;
     switch (datatype & UCP_DATATYPE_CLASS_MASK) {
     case UCP_DATATYPE_CONTIG:
-        ucs_assert(ucs_count_one_bits(md_map) <= UCP_MAX_OP_MDS);
-        status = ucp_mem_rereg_mds(context, md_map, buffer, length,
-                                   UCT_MD_MEM_ACCESS_RMA, NULL, UCT_MD_MEM_TYPE_HOST, NULL,
-                                   state->dt.contig.memh, &state->dt.contig.md_map);
+        ucs_assert(ucs_popcount(md_map) <= UCP_MAX_OP_MDS);
+        status = ucp_mem_rereg_mds(context, md_map, buffer, length, flags,
+                                   NULL, mem_type, NULL, state->dt.contig.memh,
+                                   &state->dt.contig.md_map);
         ucp_trace_req(req_dbg, "mem reg md_map 0x%"PRIx64"/0x%"PRIx64,
                       state->dt.contig.md_map, md_map);
         break;
@@ -230,10 +233,8 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_request_memory_reg,
             dt_reg[iov_it].md_map = 0;
             if (iov[iov_it].length) {
                 status = ucp_mem_rereg_mds(context, md_map, iov[iov_it].buffer,
-                                           iov[iov_it].length,
-                                           UCT_MD_MEM_ACCESS_RMA, NULL,
-                                           UCT_MD_MEM_TYPE_HOST,  NULL,
-                                           dt_reg[iov_it].memh,
+                                           iov[iov_it].length, flags, NULL,
+                                           mem_type, NULL, dt_reg[iov_it].memh,
                                            &dt_reg[iov_it].md_map);
                 if (status != UCS_OK) {
                     /* unregister previously registered memory */
@@ -255,8 +256,11 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_request_memory_reg,
 
 err:
     if (status != UCS_OK) {
-        ucs_error("failed to register user buffer datatype 0x%lx address %p len %zu:"
-                  " %s", datatype, buffer, length, ucs_status_string(status));
+        level = (flags & UCT_MD_MEM_FLAG_HIDE_ERRORS) ?
+                UCS_LOG_LEVEL_DEBUG : UCS_LOG_LEVEL_ERROR;
+        ucs_log(level,
+                "failed to register user buffer datatype 0x%lx address %p len %zu:"
+                " %s", datatype, buffer, length, ucs_status_string(status));
     }
     return status;
 }
@@ -302,11 +306,13 @@ ucs_status_t ucp_request_test(void *request, ucp_tag_recv_info_t *info)
 
 ucs_status_t
 ucp_request_send_start(ucp_request_t *req, ssize_t max_short,
-                       size_t zcopy_thresh, size_t seg_size,
-                       size_t zcopy_max, const ucp_proto_t *proto)
+                       size_t zcopy_thresh, size_t zcopy_max, size_t dt_count,
+                       const ucp_ep_msg_config_t* msg_config,
+                       const ucp_proto_t *proto)
 {
     size_t       length = req->send.length;
     ucs_status_t status;
+    int          multi;
 
     if ((ssize_t)length <= max_short) {
         /* short */
@@ -316,13 +322,13 @@ ucp_request_send_start(ucp_request_t *req, ssize_t max_short,
     } else if (length < zcopy_thresh) {
         /* bcopy */
         ucp_request_send_state_reset(req, NULL, UCP_REQUEST_SEND_PROTO_BCOPY_AM);
-        if (length < seg_size) {
+        if (length <= msg_config->max_bcopy - proto->only_hdr_size) {
             req->send.uct.func   = proto->bcopy_single;
             UCS_PROFILE_REQUEST_EVENT(req, "start_bcopy_single", req->send.length);
         } else {
             req->send.uct.func        = proto->bcopy_multi;
             req->send.tag.message_id  = req->send.ep->worker->tm.am.message_id++;
-            req->send.tag.am_bw_index = 0;
+            req->send.tag.am_bw_index = 1;
             req->send.pending_lane    = UCP_NULL_LANE;
             UCS_PROFILE_REQUEST_EVENT(req, "start_bcopy_multi", req->send.length);
         }
@@ -336,15 +342,28 @@ ucp_request_send_start(ucp_request_t *req, ssize_t max_short,
             return status;
         }
 
-        if (length < seg_size) {
-            req->send.uct.func   = proto->zcopy_single;
-            UCS_PROFILE_REQUEST_EVENT(req, "start_zcopy_single", req->send.length);
+        if (ucs_unlikely(length > msg_config->max_zcopy - proto->only_hdr_size)) {
+            multi = 1;
+        } else if (ucs_unlikely(UCP_DT_IS_IOV(req->send.datatype))) {
+            if (dt_count <= msg_config->max_iov) {
+                multi = 0;
+            } else {
+                multi = ucp_dt_iov_count_nonempty(req->send.buffer, dt_count) >
+                        msg_config->max_iov;
+            }
         } else {
+            multi = 0;
+        }
+
+        if (multi) {
             req->send.uct.func        = proto->zcopy_multi;
             req->send.tag.message_id  = req->send.ep->worker->tm.am.message_id++;
-            req->send.tag.am_bw_index = 0;
+            req->send.tag.am_bw_index = 1;
             req->send.pending_lane    = UCP_NULL_LANE;
             UCS_PROFILE_REQUEST_EVENT(req, "start_zcopy_multi", req->send.length);
+        } else {
+            req->send.uct.func   = proto->zcopy_single;
+            UCS_PROFILE_REQUEST_EVENT(req, "start_zcopy_single", req->send.length);
         }
         return UCS_OK;
     }
@@ -352,3 +371,13 @@ ucp_request_send_start(ucp_request_t *req, ssize_t max_short,
     return UCS_ERR_NO_PROGRESS;
 }
 
+void ucp_request_send_state_ff(ucp_request_t *req, ucs_status_t status)
+{
+    if (req->send.state.uct_comp.func) {
+        req->send.state.dt.offset = req->send.length;
+        req->send.state.uct_comp.count = 0;
+        req->send.state.uct_comp.func(&req->send.state.uct_comp, status);
+    } else {
+        ucp_request_complete_send(req, status);
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.h
index 71be52548..1e0da8ea1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.h
@@ -17,6 +17,7 @@
 #include <ucs/datastruct/mpool.h>
 #include <ucs/datastruct/queue_types.h>
 #include <ucp/dt/dt.h>
+#include <ucp/rma/rma.h>
 #include <ucp/wireup/wireup.h>
 
 
@@ -38,9 +39,10 @@ enum {
     UCP_REQUEST_FLAG_SYNC                 = UCS_BIT(8),
     UCP_REQUEST_FLAG_OFFLOADED            = UCS_BIT(10),
     UCP_REQUEST_FLAG_BLOCK_OFFLOAD        = UCS_BIT(11),
-    UCP_REQUEST_FLAG_STREAM_RECV          = UCS_BIT(13),
+    UCP_REQUEST_FLAG_STREAM_RECV_WAITALL  = UCS_BIT(12),
 
 #if ENABLE_ASSERT
+    UCP_REQUEST_FLAG_STREAM_RECV          = UCS_BIT(14),
     UCP_REQUEST_DEBUG_FLAG_EXTERNAL       = UCS_BIT(15)
 #else
     UCP_REQUEST_DEBUG_FLAG_EXTERNAL       = 0
@@ -90,9 +92,12 @@ struct ucp_request {
     uint16_t                      flags;   /* Request flags */
 
     union {
+
+        /* "send" part - used for tag_send, stream_send,  put, get, and atomic
+         * operations */
         struct {
             ucp_ep_h              ep;
-            const void            *buffer;  /* Send buffer */
+            void                  *buffer;  /* Send buffer */
             ucp_datatype_t        datatype; /* Send type */
             size_t                length;   /* Total length, in bytes */
             uct_memory_type_t     mem_type; /* Memory type */
@@ -107,6 +112,8 @@ struct ucp_request {
                     ucp_tag_t        tag;
                     uint64_t         message_id;  /* message ID used in AM */
                     ucp_lane_index_t am_bw_index; /* AM BW lane index */
+                    uintptr_t        rreq_ptr;    /* receive request ptr on the
+                                                     recv side (used in AM rndv) */
                 } tag;
 
                 struct {
@@ -118,7 +125,6 @@ struct ucp_request {
                     uintptr_t     remote_request; /* pointer to the send request on receiver side */
                     uint8_t       am_id;
                     ucs_status_t  status;
-                    uint64_t      sender_uuid; /* Sender uuid, which is sent back in sync ack */
                     ucp_tag_t     sender_tag;  /* Sender tag, which is sent back in sync ack */
                     ucp_request_callback_t comp_cb; /* Called to complete the request */
                 } proto;
@@ -145,10 +151,6 @@ struct ucp_request {
                     uct_rkey_t       uct_rkey;       /* UCT remote key */
                 } rndv_put;
 
-                struct {
-                    uintptr_t     rreq_ptr;    /* receive request ptr on the recv side */
-                } rndv_data;
-
                 struct {
                     uintptr_t         remote_request; /* pointer to the send request on receiver side */
                     ucp_request_t     *rreq;
@@ -156,10 +158,15 @@ struct ucp_request {
 
                 struct {
                     ucp_request_callback_t flushed_cb;/* Called when flushed */
+                    ucp_request_t          *worker_req;
+                    ucs_queue_elem_t       queue;     /* Queue element in proto_status */
+                    unsigned               uct_flags; /* Flags to pass to @ref uct_ep_flush */
                     uct_worker_cb_id_t     prog_id;   /* Progress callback ID */
+                    uint32_t               cmpl_sn;   /* Sequence number of the remote completion
+                                                         this request is waiting for */
+                    uint8_t                sw_started;
+                    uint8_t                sw_done;
                     ucp_lane_map_t         lanes;     /* Which lanes need to be flushed */
-                    unsigned               uct_flags; /* Flags to pass to
-                                                            @ref uct_ep_flush */
                 } flush;
 
                 struct {
@@ -168,10 +175,9 @@ struct ucp_request {
 
                 struct {
                     uint64_t              remote_addr; /* Remote address */
-                    ucp_atomic_fetch_op_t op; /* Requested AMO */
-                    ucp_rkey_h            rkey;     /* Remote memory key */
-                    uint64_t              value;
-                    void                  *result;
+                    ucp_rkey_h            rkey;        /* Remote memory key */
+                    uint64_t              value;       /* Atomic argument */
+                    uct_atomic_op_t       uct_op;      /* Requested UCT AMO */
                 } amo;
 
                 struct {
@@ -179,7 +185,16 @@ struct ucp_request {
                     ucp_tag_t         ssend_tag; /* Tag in offload sync send */
                     void              *rndv_op;  /* Handler of issued rndv send. Need to cancel
                                                     the operation if it is completed by SW. */
-                 } tag_offload;
+                } tag_offload;
+
+                struct {
+                    uintptr_t              req;  /* Remote get request pointer */
+                } get_reply;
+
+                struct {
+                    uintptr_t              req;  /* Remote atomic request pointer */
+                    ucp_atomic_reply_t     data; /* Atomic reply data */
+                } atomic_reply;
             };
 
             /* This structure holds all mutable fields, and everything else
@@ -198,6 +213,7 @@ struct ucp_request {
             ucp_mem_desc_t        *mdesc;
         } send;
 
+        /* "receive" part - used for tag_recv and stream_recv operations */
         struct {
             ucs_queue_elem_t      queue;    /* Expected queue element */
             void                  *buffer;  /* Buffer to receive data to */
@@ -234,13 +250,27 @@ struct ucp_request {
             ucp_worker_h          worker;   /* Worker to flush */
             ucp_send_callback_t   cb;       /* Completion callback */
             uct_worker_cb_id_t    prog_id;  /* Progress callback ID */
+            int                   comp_count; /* Countdown to request completion */
+            ucp_ep_ext_gen_t      *next_ep; /* Next endpoint to flush */
         } flush_worker;
     };
 };
 
 
 /**
- * Unexpected receive descriptor.
+ * Unexpected receive descriptor. If it is initialized in the headroom of UCT
+ * descriptor, the layout looks like the following:
+ *
+ *
+ * headroom                                    data
+ * |-------------------------------------------|-------------------------|
+ * | unused | ucp_recv_desc |      priv_length |                         |
+ * |        |               |                  |                         |
+ * |-------------------------------------------|-------------------------|
+ *
+ * Some protocols (i. e. tag offload) may need some space right before the
+ * incoming data to add specific headers needed for further message processing.
+ * Note: priv_length value should be in [0, UCP_WORKER_HEADROOM_PRIV_SIZE] range.
  */
 struct ucp_recv_desc {
     union {
@@ -252,6 +282,11 @@ struct ucp_recv_desc {
     uint32_t                payload_offset; /* Offset from end of the descriptor
                                              * to AM data */
     uint16_t                flags;          /* Flags */
+    int16_t                 priv_length;    /* Number of bytes consumed from
+                                               headroom private space, except the
+                                               space needed for ucp_recv_desc itself.
+                                               It is used for releasing descriptor
+                                               back to UCT only */
 };
 
 
@@ -263,13 +298,19 @@ int ucp_request_pending_add(ucp_request_t *req, ucs_status_t *req_status);
 
 ucs_status_t ucp_request_memory_reg(ucp_context_t *context, ucp_md_map_t md_map,
                                     void *buffer, size_t length, ucp_datatype_t datatype,
-                                    ucp_dt_state_t *state, ucp_request_t *req_dbg);
+                                    ucp_dt_state_t *state, uct_memory_type_t mem_type,
+                                    ucp_request_t *req_dbg, unsigned uct_flags);
 
 void ucp_request_memory_dereg(ucp_context_t *context, ucp_datatype_t datatype,
                               ucp_dt_state_t *state, ucp_request_t *req_dbg);
 
 ucs_status_t ucp_request_send_start(ucp_request_t *req, ssize_t max_short,
-                                    size_t zcopy_thresh, size_t multi_thresh,
-                                    size_t rndv_thresh, const ucp_proto_t *proto);
+                                    size_t zcopy_thresh, size_t zcopy_max,
+                                    size_t dt_count,
+                                    const ucp_ep_msg_config_t* msg_config,
+                                    const ucp_proto_t *proto);
+
+/* Fast-forward to data end */
+void ucp_request_send_state_ff(ucp_request_t *req, ucs_status_t status);
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.inl
index 4b58545ae..e73621c0b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_request.inl
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
@@ -13,7 +13,7 @@
 
 #include <ucp/core/ucp_worker.h>
 #include <ucp/dt/dt.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <ucs/datastruct/mpool.inl>
 #include <ucp/dt/dt.inl>
 #include <inttypes.h>
@@ -110,13 +110,12 @@ ucp_request_complete_tag_recv(ucp_request_t *req, ucs_status_t status)
 }
 
 static UCS_F_ALWAYS_INLINE void
-ucp_request_complete_stream_recv(ucp_request_t *req,
-                                 ucp_ep_ext_stream_t* ep_stream,
+ucp_request_complete_stream_recv(ucp_request_t *req, ucp_ep_ext_proto_t* ep_ext,
                                  ucs_status_t status)
 {
     /* dequeue request before complete */
     ucp_request_t *check_req UCS_V_UNUSED =
-            ucs_queue_pull_elem_non_empty(&ep_stream->match_q, ucp_request_t,
+            ucs_queue_pull_elem_non_empty(&ep_ext->stream.match_q, ucp_request_t,
                                           recv.queue);
     ucs_assert(check_req               == req);
     ucs_assert(req->recv.stream.offset >  0);
@@ -139,6 +138,10 @@ ucp_request_can_complete_stream_recv(ucp_request_t *req)
         return 1;
     }
 
+    if (req->flags & UCP_REQUEST_FLAG_STREAM_RECV_WAITALL) {
+        return 0;
+    }
+
     /* 0-length stream recv is meaningless if this was not requested explicitely */
     if (req->recv.stream.offset == 0) {
         return 0;
@@ -334,25 +337,13 @@ ucp_request_send_state_advance(ucp_request_t *req,
                (req->send.state.dt.offset <= req->send.length));
 }
 
-/* Fast-forward to data end */
-static UCS_F_ALWAYS_INLINE void
-ucp_request_send_state_ff(ucp_request_t *req, ucs_status_t status)
-{
-    if (req->send.state.uct_comp.func) {
-        req->send.state.dt.offset = req->send.length;
-        req->send.state.uct_comp.count = 0;
-        req->send.state.uct_comp.func(&req->send.state.uct_comp, status);
-    } else {
-        ucp_request_complete_send(req, status);
-    }
-}
-
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_request_send_buffer_reg(ucp_request_t *req, ucp_md_map_t md_map)
 {
     return ucp_request_memory_reg(req->send.ep->worker->context, md_map,
                                   (void*)req->send.buffer, req->send.length,
-                                  req->send.datatype, &req->send.state.dt, req);
+                                  req->send.datatype, &req->send.state.dt,
+                                  req->send.mem_type, req, 0);
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
@@ -368,16 +359,18 @@ ucp_send_request_add_reg_lane(ucp_request_t *req, ucp_lane_index_t lane)
     /* add new lane to registration map */
     ucp_md_map_t md_map = UCS_BIT(ucp_ep_md_index(req->send.ep, lane)) |
                           req->send.state.dt.dt.contig.md_map;
-    ucs_assert(ucs_count_one_bits(md_map) <= UCP_MAX_OP_MDS);
+    ucs_assert(ucs_popcount(md_map) <= UCP_MAX_OP_MDS);
     return ucp_request_send_buffer_reg(req, md_map);
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_request_recv_buffer_reg(ucp_request_t *req, ucp_md_map_t md_map)
+ucp_request_recv_buffer_reg(ucp_request_t *req, ucp_md_map_t md_map,
+                            size_t length)
 {
     return ucp_request_memory_reg(req->recv.worker->context, md_map,
-                                  req->recv.buffer, req->recv.length,
-                                  req->recv.datatype, &req->recv.state, req);
+                                  req->recv.buffer, length,
+                                  req->recv.datatype, &req->recv.state,
+                                  req->recv.mem_type, req, 0);
 }
 
 static UCS_F_ALWAYS_INLINE void ucp_request_send_buffer_dereg(ucp_request_t *req)
@@ -435,7 +428,8 @@ ucp_request_recv_data_unpack(ucp_request_t *req, const void *data,
 
     switch (req->recv.datatype & UCP_DATATYPE_CLASS_MASK) {
     case UCP_DATATYPE_CONTIG:
-        if (ucs_likely(UCP_MEM_IS_HOST(req->recv.mem_type))) {
+        if ((ucs_likely(UCP_MEM_IS_HOST(req->recv.mem_type))) ||
+            (ucs_likely(UCP_MEM_IS_CUDA_MANAGED(req->recv.mem_type)))) {
             UCS_PROFILE_NAMED_CALL("memcpy_recv", memcpy, req->recv.buffer + offset,
                                    data, length);
         } else {
@@ -477,17 +471,22 @@ ucp_request_recv_data_unpack(ucp_request_t *req, const void *data,
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_recv_desc_init(ucp_worker_h worker, void *data, size_t length,
-                   unsigned am_flags, uint16_t hdr_len, uint16_t rdesc_flags,
+                   int data_offset, unsigned am_flags, uint16_t hdr_len,
+                   uint16_t rdesc_flags, uint16_t priv_length,
                    ucp_recv_desc_t **rdesc_p)
 {
     ucp_recv_desc_t *rdesc;
+    void *data_hdr;
     ucs_status_t status;
 
     if (ucs_unlikely(am_flags & UCT_CB_PARAM_FLAG_DESC)) {
         /* slowpath */
-        rdesc        = (ucp_recv_desc_t *)data - 1;
-        rdesc->flags = rdesc_flags | UCP_RECV_DESC_FLAG_UCT_DESC;
-        status       = UCS_INPROGRESS;
+        ucs_assert(priv_length <= UCP_WORKER_HEADROOM_PRIV_SIZE);
+        data_hdr           = UCS_PTR_BYTE_OFFSET(data, -data_offset);
+        rdesc              = (ucp_recv_desc_t *)data_hdr - 1;
+        rdesc->flags       = rdesc_flags | UCP_RECV_DESC_FLAG_UCT_DESC;
+        rdesc->priv_length = priv_length;
+        status             = UCS_INPROGRESS;
     } else {
         rdesc = (ucp_recv_desc_t*)ucs_mpool_get_inline(&worker->am_mp);
         if (rdesc == NULL) {
@@ -495,17 +494,34 @@ ucp_recv_desc_init(ucp_worker_h worker, void *data, size_t length,
             return UCS_ERR_NO_MEMORY;
         }
 
+        /* No need to initialize rdesc->priv_length here, because it is only
+         * needed for releasing UCT descriptor. */
+
         rdesc->flags = rdesc_flags;
-        memcpy(rdesc + 1, data, length);
-        status = UCS_OK;
+        status       = UCS_OK;
+        memcpy(UCS_PTR_BYTE_OFFSET(rdesc + 1, data_offset), data, length);
     }
 
-    rdesc->length         = length;
+    rdesc->length         = length + data_offset;
     rdesc->payload_offset = hdr_len;
     *rdesc_p              = rdesc;
     return status;
 }
 
+static UCS_F_ALWAYS_INLINE void
+ucp_recv_desc_release(ucp_recv_desc_t *rdesc)
+{
+    ucs_trace_req("release receive descriptor %p", rdesc);
+    if (ucs_unlikely(rdesc->flags & UCP_RECV_DESC_FLAG_UCT_DESC)) {
+        /* uct desc is slowpath */
+        uct_iface_release_desc(UCS_PTR_BYTE_OFFSET(rdesc,
+                                                   -(UCP_WORKER_HEADROOM_PRIV_SIZE -
+                                                     rdesc->priv_length)));
+    } else {
+        ucs_mpool_put_inline(rdesc);
+    }
+}
+
 static UCS_F_ALWAYS_INLINE ucp_lane_index_t
 ucp_send_request_get_next_am_bw_lane(ucp_request_t *req)
 {
@@ -526,4 +542,13 @@ ucp_send_request_get_next_am_bw_lane(ucp_request_t *req)
     }
 }
 
+static UCS_F_ALWAYS_INLINE uintptr_t ucp_request_get_dest_ep_ptr(ucp_request_t *req)
+{
+    /* This function may return 0, but in such cases the message should not be
+     * sent at all because the am_lane would point to a wireup (proxy) endpoint.
+     * So only the receiver side has an assertion that ep_ptr != 0.
+     */
+    return ucp_ep_dest_ep_ptr(req->send.ep);
+}
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_rkey.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_rkey.c
index 126031b21..98caba443 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_rkey.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_rkey.c
@@ -8,16 +8,15 @@
 #include "ucp_request.h"
 #include "ucp_ep.inl"
 
+#include <ucp/rma/rma.h>
 #include <ucs/datastruct/mpool.inl>
 #include <inttypes.h>
 
 
-static ucp_rkey_t ucp_mem_dummy_rkey = {
-                                        // TODO cache?
-    .md_map = 0
-};
-
-static ucp_md_map_t ucp_mem_dummy_buffer = 0;
+static struct {
+    ucp_md_map_t md_map;
+    uint8_t      mem_type;
+} UCS_S_PACKED ucp_mem_dummy_buffer = {0, UCT_MD_MEM_TYPE_HOST};
 
 
 size_t ucp_rkey_packed_size(ucp_context_h context, ucp_md_map_t md_map)
@@ -26,6 +25,7 @@ size_t ucp_rkey_packed_size(ucp_context_h context, ucp_md_map_t md_map)
     unsigned md_index;
 
     size = sizeof(ucp_md_map_t);
+    size += sizeof(uint8_t);
     ucs_for_each_bit (md_index, md_map) {
         md_size = context->tl_mds[md_index].attr.rkey_packed_size;
         ucs_assert_always(md_size <= UINT8_MAX);
@@ -35,7 +35,8 @@ size_t ucp_rkey_packed_size(ucp_context_h context, ucp_md_map_t md_map)
 }
 
 void ucp_rkey_packed_copy(ucp_context_h context, ucp_md_map_t md_map,
-                          void *rkey_buffer, const void* uct_rkeys[])
+                          uct_memory_type_t mem_type, void *rkey_buffer,
+                          const void* uct_rkeys[])
 {
     void *p = rkey_buffer;
     unsigned md_index;
@@ -44,6 +45,8 @@ void ucp_rkey_packed_copy(ucp_context_h context, ucp_md_map_t md_map,
     *(ucp_md_map_t*)p = md_map;
     p += sizeof(ucp_md_map_t);
 
+    *((uint8_t *)p++) = mem_type;
+
     ucs_for_each_bit(md_index, md_map) {
         md_size = context->tl_mds[md_index].attr.rkey_packed_size;
         ucs_assert_always(md_size <= UINT8_MAX);
@@ -55,7 +58,8 @@ void ucp_rkey_packed_copy(ucp_context_h context, ucp_md_map_t md_map,
 }
 
 ssize_t ucp_rkey_pack_uct(ucp_context_h context, ucp_md_map_t md_map,
-                          const uct_mem_h *memh, void *rkey_buffer)
+                          const uct_mem_h *memh, uct_memory_type_t mem_type,
+                          void *rkey_buffer)
 {
     void *p             = rkey_buffer;
     ucs_status_t status = UCS_OK;
@@ -70,6 +74,10 @@ ssize_t ucp_rkey_pack_uct(ucp_context_h context, ucp_md_map_t md_map,
     *(ucp_md_map_t*)p = md_map;
     p += sizeof(ucp_md_map_t);
 
+    /* Write memory type */
+    UCS_STATIC_ASSERT(UCT_MD_MEM_TYPE_LAST <= 255);
+    *((uint8_t*)p++) = mem_type;
+
     /* Write both size and rkey_buffer for each UCT rkey */
     uct_memh_index = 0;
     ucs_for_each_bit (md_index, md_map) {
@@ -100,6 +108,9 @@ ucs_status_t ucp_rkey_pack(ucp_context_h context, ucp_mem_h memh,
     ssize_t packed_size;
     size_t size;
 
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(context, UCP_FEATURE_RMA | UCP_FEATURE_AMO,
+                                    return UCS_ERR_INVALID_PARAM);
+
     /* always acquire context lock */
     UCP_THREAD_CS_ENTER(&context->mt_lock);
 
@@ -123,7 +134,8 @@ ucs_status_t ucp_rkey_pack(ucp_context_h context, ucp_mem_h memh,
 
     p = rkey_buffer;
 
-    packed_size = ucp_rkey_pack_uct(context, memh->md_map, memh->uct, p);
+    packed_size = ucp_rkey_pack_uct(context, memh->md_map, memh->uct,
+                                    memh->mem_type, p);
     if (packed_size < 0) {
         status = (ucs_status_t)packed_size;
         goto err_destroy;
@@ -162,6 +174,7 @@ ucs_status_t ucp_ep_rkey_unpack(ucp_ep_h ep, const void *rkey_buffer,
     unsigned md_count;
     ucs_status_t status;
     ucp_rkey_h rkey;
+    uct_memory_type_t mem_type;
     uint8_t md_size;
     const void *p;
 
@@ -173,14 +186,8 @@ ucs_status_t ucp_ep_rkey_unpack(ucp_ep_h ep, const void *rkey_buffer,
     ucs_trace("unpacking rkey with md_map 0x%lx", remote_md_map);
 
     /* MD map for the unpacked rkey */
-    md_map = remote_md_map & ucp_ep_config(ep)->key.reachable_md_map;
-    if (md_map == 0) {
-        /* Dummy key return ok */
-        *rkey_p = &ucp_mem_dummy_rkey;
-        return UCS_OK;
-    }
-
-    md_count = ucs_count_one_bits(md_map);
+    md_map   = remote_md_map & ucp_ep_config(ep)->key.reachable_md_map;
+    md_count = ucs_popcount(md_map);
     p       += sizeof(ucp_md_map_t);
 
     /* Allocate rkey handle which holds UCT rkeys for all remote MDs. Small key
@@ -200,7 +207,11 @@ ucs_status_t ucp_ep_rkey_unpack(ucp_ep_h ep, const void *rkey_buffer,
         goto err;
     }
 
+    /* Read memory type */
+    mem_type = *((uint8_t*)p++);
+
     rkey->md_map = md_map;
+    rkey->mem_type = mem_type;
 
     /* Unpack rkey of each UCT MD */
     remote_md_index = 0; /* Index of remote MD */
@@ -221,23 +232,26 @@ ucs_status_t ucp_ep_rkey_unpack(ucp_ep_h ep, const void *rkey_buffer,
             ucs_assert(rkey_index < md_count);
 
             status = uct_rkey_unpack(p, &rkey->uct[rkey_index]);
-            if (status != UCS_OK) {
+
+            if (status == UCS_OK) {
+                ucs_trace("rkey[%d] for remote md %d is 0x%lx", rkey_index,
+                          remote_md_index, rkey->uct[rkey_index].rkey);
+                rkey->md_map |= UCS_BIT(remote_md_index);
+                ++rkey_index;
+            } else if (status == UCS_ERR_UNREACHABLE) {
+                rkey->md_map &= ~UCS_BIT(remote_md_index);
+                ucs_trace("rkey[%d] for remote md %d is 0x%lx not reachable", rkey_index,
+                          remote_md_index, rkey->uct[rkey_index].rkey);
+            } else {
                 ucs_error("Failed to unpack remote key from remote md[%d]: %s",
                           remote_md_index, ucs_status_string(status));
                 goto err_destroy;
             }
-
-            ucs_trace("rkey[%d] for remote md %d is 0x%lx", rkey_index,
-                      remote_md_index, rkey->uct[rkey_index].rkey);
-            rkey->md_map |= UCS_BIT(remote_md_index);
-            ++rkey_index;
         }
 
         p += md_size;
     }
 
-    ucs_assert(rkey_index == md_count);
-
     ucp_rkey_resolve_inner(rkey, ep);
     *rkey_p = rkey;
     return UCS_OK;
@@ -263,6 +277,8 @@ void ucp_rkey_dump_packed(const void *rkey_buffer, char *buffer, size_t max)
     md_map = *(ucp_md_map_t*)(rkey_buffer);
     rkey_buffer += sizeof(ucp_md_map_t);
 
+    rkey_buffer += sizeof(uint8_t);
+
     first = 1;
     ucs_for_each_bit(md_index, md_map) {
          md_size      = *((uint8_t*)rkey_buffer);
@@ -293,11 +309,7 @@ ucs_status_t ucp_rkey_ptr(ucp_rkey_h rkey, uint64_t raddr, void **addr_p)
     unsigned i;
     ucs_status_t status;
 
-    if (rkey == &ucp_mem_dummy_rkey) {
-        return UCS_ERR_UNREACHABLE;
-    }
-
-    num_rkeys = ucs_count_one_bits(rkey->md_map);
+    num_rkeys = ucs_popcount(rkey->md_map);
 
     for (i = 0; i < num_rkeys; ++i) {
         status = uct_rkey_ptr(&rkey->uct[i], raddr, addr_p);
@@ -316,17 +328,13 @@ void ucp_rkey_destroy(ucp_rkey_h rkey)
     unsigned num_rkeys;
     unsigned i;
 
-    if (rkey == &ucp_mem_dummy_rkey) {
-        return;
-    }
-
-    num_rkeys = ucs_count_one_bits(rkey->md_map);
+    num_rkeys = ucs_popcount(rkey->md_map);
 
     for (i = 0; i < num_rkeys; ++i) {
         uct_rkey_release(&rkey->uct[i]);
     }
 
-    if (ucs_count_one_bits(rkey->md_map) <= UCP_RKEY_MPOOL_MAX_MD) {
+    if (ucs_popcount(rkey->md_map) <= UCP_RKEY_MPOOL_MAX_MD) {
         context = ucs_container_of(ucs_mpool_obj_owner(rkey), ucp_context_t,
                                    rkey_mp);
         UCP_THREAD_CS_ENTER_CONDITIONAL(&context->mt_lock);
@@ -339,6 +347,7 @@ void ucp_rkey_destroy(ucp_rkey_h rkey)
 
 static ucp_lane_index_t ucp_config_find_rma_lane(ucp_context_h context,
                                                  const ucp_ep_config_t *config,
+                                                 uct_memory_type_t mem_type,
                                                  const ucp_lane_index_t *lanes,
                                                  ucp_rkey_h rkey,
                                                  ucp_lane_map_t ignore,
@@ -348,6 +357,7 @@ static ucp_lane_index_t ucp_config_find_rma_lane(ucp_context_h context,
     ucp_lane_index_t lane;
     ucp_md_map_t dst_md_mask;
     ucp_md_index_t md_index;
+    uct_md_attr_t *md_attr;
     uint8_t rkey_index;
     int prio;
 
@@ -360,19 +370,29 @@ static ucp_lane_index_t ucp_config_find_rma_lane(ucp_context_h context,
         }
 
         md_index = config->md_index[lane];
+        md_attr  = &context->tl_mds[md_index].attr;
+
         if ((md_index != UCP_NULL_RESOURCE) &&
-            (!(context->tl_mds[md_index].attr.cap.flags & UCT_MD_FLAG_NEED_RKEY)))
+            (!(md_attr->cap.flags & UCT_MD_FLAG_NEED_RKEY)))
         {
             /* Lane does not need rkey, can use the lane with invalid rkey  */
-            *uct_rkey_p = UCT_INVALID_RKEY;
-            return lane;
+            if (!rkey || ((mem_type == md_attr->cap.mem_type) &&
+                          (mem_type == rkey->mem_type))) {
+                *uct_rkey_p = UCT_INVALID_RKEY;
+                return lane;
+            }
+        }
+
+        if ((md_index != UCP_NULL_RESOURCE) &&
+            (!(md_attr->cap.reg_mem_types & UCS_BIT(mem_type)))) {
+            continue;
         }
 
         dst_md_index = config->key.lanes[lane].dst_md_index;
         dst_md_mask  = UCS_BIT(dst_md_index);
         if (rkey->md_map & dst_md_mask) {
             /* Return first matching lane */
-            rkey_index  = ucs_count_one_bits(rkey->md_map & (dst_md_mask - 1));
+            rkey_index  = ucs_popcount(rkey->md_map & (dst_md_mask - 1));
             *uct_rkey_p = rkey->uct[rkey_index].rkey;
             return lane;
         }
@@ -385,34 +405,75 @@ void ucp_rkey_resolve_inner(ucp_rkey_h rkey, ucp_ep_h ep)
 {
     ucp_context_h context   = ep->worker->context;
     ucp_ep_config_t *config = ucp_ep_config(ep);
+    ucs_status_t status;
     uct_rkey_t uct_rkey;
+    int rma_sw, amo_sw;
 
     rkey->cache.rma_lane = ucp_config_find_rma_lane(context, config,
+                                                    UCT_MD_MEM_TYPE_HOST,
                                                     config->key.rma_lanes, rkey,
                                                     0, &uct_rkey);
-    if (rkey->cache.rma_lane != UCP_NULL_LANE) {
+    rma_sw = (rkey->cache.rma_lane == UCP_NULL_LANE);
+    if (rma_sw) {
+        rkey->cache.rma_proto     = &ucp_rma_sw_proto;
+        rkey->cache.rma_rkey      = UCT_INVALID_RKEY;
+        rkey->cache.max_put_short = 0;
+    } else {
+        rkey->cache.rma_proto     = &ucp_rma_basic_proto;
         rkey->cache.rma_rkey      = uct_rkey;
+        rkey->cache.rma_proto     = &ucp_rma_basic_proto;
         rkey->cache.max_put_short = config->rma[rkey->cache.rma_lane].max_put_short;
     }
 
     rkey->cache.amo_lane = ucp_config_find_rma_lane(context, config,
+                                                    UCT_MD_MEM_TYPE_HOST,
                                                     config->key.amo_lanes, rkey,
                                                     0, &uct_rkey);
-    if (rkey->cache.amo_lane != UCP_NULL_LANE) {
+    amo_sw = (rkey->cache.amo_lane == UCP_NULL_LANE);
+    if (amo_sw) {
+        rkey->cache.amo_proto     = &ucp_amo_sw_proto;
+        rkey->cache.amo_rkey      = UCT_INVALID_RKEY;
+    } else {
+        rkey->cache.amo_proto     = &ucp_amo_basic_proto;
         rkey->cache.amo_rkey      = uct_rkey;
     }
 
+    /* If we use sw rma/amo need to resolve destination endpoint in order to
+     * receive responses and completion messages
+     */
+    if ((amo_sw || rma_sw) && (config->key.am_lane != UCP_NULL_LANE)) {
+        status = ucp_ep_resolve_dest_ep_ptr(ep, config->key.am_lane);
+        if (status != UCS_OK) {
+            ucs_debug("ep %p: failed to resolve destination ep, "
+                      "sw rma cannot be used", ep);
+        } else {
+            /* if we can resolve destination ep, save the active message lane
+             * as the rma/amo lane in the rkey cache
+             */
+            if (amo_sw) {
+                rkey->cache.amo_lane = config->key.am_lane;
+            }
+            if (rma_sw) {
+                rkey->cache.rma_lane = config->key.am_lane;
+            }
+        }
+    }
+
     rkey->cache.ep_cfg_index  = ep->cfg_index;
-    ucs_trace("rkey %p ep %p @ cfg[%d] rma_lane %d amo_lane %d", rkey, ep,
-              ep->cfg_index, rkey->cache.rma_lane, rkey->cache.amo_lane);
+
+    ucs_trace("rkey %p ep %p @ cfg[%d] %s lane %d %s lane %d",
+              rkey, ep, ep->cfg_index,
+              rkey->cache.rma_proto->name, rkey->cache.rma_lane,
+              rkey->cache.amo_proto->name, rkey->cache.amo_lane);
 }
 
 ucp_lane_index_t ucp_rkey_get_rma_bw_lane(ucp_rkey_h rkey, ucp_ep_h ep,
+                                          uct_memory_type_t mem_type,
                                           uct_rkey_t *uct_rkey_p,
                                           ucp_lane_map_t ignore)
 {
     ucp_ep_config_t *config = ucp_ep_config(ep);
-    return ucp_config_find_rma_lane(ep->worker->context, config,
+    return ucp_config_find_rma_lane(ep->worker->context, config, mem_type,
                                     config->key.rma_bw_lanes, rkey,
                                     ignore, uct_rkey_p);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_thread.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_thread.h
index 0bc22f11c..e5510bda0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_thread.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_thread.h
@@ -11,6 +11,8 @@
 #  include "config.h"
 #endif
 
+#include <ucs/async/async.h>
+#include <ucs/async/thread.h>
 #include <ucs/type/spinlock.h>
 
 
@@ -98,4 +100,14 @@ typedef struct ucp_mt_lock {
         }                                                               \
     }
 
+
+#ifdef ENABLE_ASSERT
+
+/* Debug macro */
+#define UCP_THREAD_CS_IS_RECURSIVELY_LOCKED(_lock_ptr)                  \
+    (((_lock_ptr)->mt_type == UCP_MT_TYPE_MUTEX) ? 0 :                  \
+     ucs_spin_is_owner(&((_lock_ptr)->lock.mt_spinlock), pthread_self()))
+
+#endif
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_types.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_types.h
index 853cdf636..ae13821d6 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_types.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_types.h
@@ -15,6 +15,7 @@
 
 #define UCP_WORKER_NAME_MAX          32   /* Worker name for debugging */
 #define UCP_MIN_BCOPY                64   /* Minimal size for bcopy */
+#define UCP_FEATURE_AMO              (UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)
 
 /* Resources */
 #define UCP_MAX_RESOURCES            UINT8_MAX
@@ -26,24 +27,29 @@ typedef uint8_t                      ucp_rsc_index_t;
 #define UCP_MD_INDEX_BITS            64  /* How many bits are in MD index */
 typedef ucp_rsc_index_t              ucp_md_index_t;
 #define UCP_MAX_MDS                  ucs_min(UCP_MD_INDEX_BITS, UCP_MAX_RESOURCES)
-#define UCP_MAX_OP_MDS               3  /* maximal number of MDs per single op */
+#define UCP_MAX_OP_MDS               4  /* maximal number of MDs per single op */
 UCP_UINT_TYPE(UCP_MD_INDEX_BITS)     ucp_md_map_t;
 
 /* Lanes */
-#define UCP_MAX_LANES                8
+#define UCP_MAX_LANES                6
 #define UCP_NULL_LANE                ((ucp_lane_index_t)-1)
 typedef uint8_t                      ucp_lane_index_t;
-UCP_UINT_TYPE(UCP_MAX_LANES)         ucp_lane_map_t;
+typedef uint8_t                      ucp_lane_map_t;
 
+/* Connection sequence number */
+typedef uint16_t                     ucp_ep_conn_sn_t;
 
 /* Forward declarations */
 typedef struct ucp_request              ucp_request_t;
 typedef struct ucp_recv_desc            ucp_recv_desc_t;
 typedef struct ucp_address_iface_attr   ucp_address_iface_attr_t;
 typedef struct ucp_address_entry        ucp_address_entry_t;
+typedef struct ucp_unpacked_address     ucp_unpacked_address_t;
 typedef struct ucp_wireup_ep            ucp_wireup_ep_t;
 typedef struct ucp_proto                ucp_proto_t;
 typedef struct ucp_worker_iface         ucp_worker_iface_t;
+typedef struct ucp_rma_proto            ucp_rma_proto_t;
+typedef struct ucp_amo_proto            ucp_amo_proto_t;
 
 
 /**
@@ -72,6 +78,13 @@ enum {
 
     UCP_AM_ID_RNDV_ATP          =  16, /* Ack-to-put complete after finishing a put_zcopy */
 
+    UCP_AM_ID_PUT               =  17, /* Remote memory write */
+    UCP_AM_ID_GET_REQ           =  18, /* Remote memory read request */
+    UCP_AM_ID_GET_REP           =  19, /* Remote memory read reply */
+    UCP_AM_ID_ATOMIC_REQ        =  20, /* Remote memory atomic request */
+    UCP_AM_ID_ATOMIC_REP        =  21, /* Remote memory atomic reply */
+    UCP_AM_ID_CMPL              =  22, /* Remote memory operation completion */
+
     UCP_AM_ID_LAST
 };
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.c
index 9bdf66fc4..d94ed3bef 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.c
@@ -14,6 +14,7 @@
 #include <ucp/tag/eager.h>
 #include <ucp/tag/offload.h>
 #include <ucp/stream/stream.h>
+#include <ucs/config/parser.h>
 #include <ucs/datastruct/mpool.inl>
 #include <ucs/datastruct/queue.h>
 #include <ucs/type/cpu_set.h>
@@ -21,6 +22,11 @@
 #include <sys/poll.h>
 #include <sys/eventfd.h>
 
+
+#define UCP_WORKER_HEADROOM_SIZE \
+    (sizeof(ucp_recv_desc_t) + UCP_WORKER_HEADROOM_PRIV_SIZE)
+
+
 #if ENABLE_STATS
 static ucs_stats_class_t ucp_worker_stats_class = {
     .name           = "ucp_worker",
@@ -128,7 +134,7 @@ static void ucp_worker_set_am_handlers(ucp_worker_iface_t *wiface, int is_proxy)
             continue;
         }
 
-        if ((ucp_am_handlers[am_id].flags & UCT_CB_FLAG_SYNC) &&
+        if (!(ucp_am_handlers[am_id].flags & UCT_CB_FLAG_ASYNC) &&
             !(wiface->attr.cap.flags & UCT_IFACE_FLAG_CB_SYNC))
         {
             /* Do not register a sync callback on interface which does not
@@ -142,7 +148,7 @@ static void ucp_worker_set_am_handlers(ucp_worker_iface_t *wiface, int is_proxy)
             /* we care only about sync active messages, and this also makes sure
              * the counter is not accessed from another thread.
              */
-            ucs_assert(ucp_am_handlers[am_id].flags & UCT_CB_FLAG_SYNC);
+            ucs_assert(!(ucp_am_handlers[am_id].flags & UCT_CB_FLAG_ASYNC));
             status = uct_iface_set_am_handler(wiface->iface, am_id,
                                               ucp_am_handlers[am_id].proxy_cb,
                                               wiface,
@@ -341,41 +347,23 @@ void ucp_worker_signal_internal(ucp_worker_h worker)
     }
 }
 
-static ucs_status_t
-ucp_worker_iface_error_handler(void *arg, uct_ep_h uct_ep, ucs_status_t status)
+static unsigned ucp_worker_iface_err_handle_progress(void *arg)
 {
-    ucp_worker_h            worker           = (ucp_worker_h)arg;
-    ucp_ep_h                ucp_ep           = NULL;
-    uct_tl_resource_desc_t* tl_rsc;
-    uint64_t                dest_uuid UCS_V_UNUSED;
-    khiter_t                ucp_ep_errh_iter;
-    ucp_err_handler_cb_t    err_cb;
-    ucp_lane_index_t        lane, failed_lane;
-    ucp_rsc_index_t         rsc_index;
+    ucp_worker_err_handle_arg_t *err_handle_arg = arg;
+    ucp_worker_h worker                         = err_handle_arg->worker;
+    ucp_ep_h ucp_ep                             = err_handle_arg->ucp_ep;
+    uct_ep_h uct_ep                             = err_handle_arg->uct_ep;
+    ucs_status_t status                         = err_handle_arg->status;
+    ucp_lane_index_t failed_lane                = err_handle_arg->failed_lane;
+    ucp_lane_index_t lane;
+    ucp_ep_config_key_t key;
 
-    /* TODO: need to optimize uct_ep -> ucp_ep lookup */
-    kh_foreach(&worker->ep_hash, dest_uuid, ucp_ep, {
-        for (lane = 0; lane < ucp_ep_num_lanes(ucp_ep); ++lane) {
-            if ((uct_ep == ucp_ep->uct_eps[lane]) ||
-                ucp_wireup_ep_is_owner(ucp_ep->uct_eps[lane], uct_ep)) {
-                failed_lane = lane;
-                goto found_ucp_ep;
-            }
-        }
-    });
+    UCS_ASYNC_BLOCK(&worker->async);
 
-    ucs_fatal("no uct_ep_h %p associated with ucp_ep_h on ucp_worker_h %p",
-              uct_ep, worker);
+    ucs_debug("ep %p: handle error on lane[%d]=%p: %s",
+              ucp_ep, failed_lane, uct_ep, ucs_status_string(status));
 
-found_ucp_ep:
-    if (ucp_ep_config(ucp_ep)->key.err_mode == UCP_ERR_HANDLING_MODE_NONE) {
-        /* NOTE: if user has not requested error handling on the endpoint,
-         *       the failure is considered unhandled */
-        return status;
-    }
-
-    rsc_index   = ucp_ep_get_rsc_index(ucp_ep, lane);
-    tl_rsc      = &worker->context->tl_rscs[rsc_index].tl_rsc;
+    ucs_assert(ucp_ep->flags & UCP_EP_FLAG_FAILED);
 
     /* Destroy all lanes except failed one since ucp_ep becomes unusable as well */
     for (lane = 0; lane < ucp_ep_num_lanes(ucp_ep); ++lane) {
@@ -398,7 +386,7 @@ found_ucp_ep:
     }
 
     /* Move failed lane to index 0 */
-    if (failed_lane != 0) {
+    if ((failed_lane != 0) && (failed_lane != UCP_NULL_LANE)) {
         ucp_ep->uct_eps[0] = ucp_ep->uct_eps[failed_lane];
         ucp_ep->uct_eps[failed_lane] = NULL;
     }
@@ -415,7 +403,7 @@ found_ucp_ep:
     }
 
     /* Redirect all lanes to failed one */
-    ucp_ep_config_key_t key = ucp_ep_config(ucp_ep)->key;
+    key                    = ucp_ep_config(ucp_ep)->key;
     key.am_lane            = 0;
     key.wireup_lane        = 0;
     key.tag_lane           = 0;
@@ -427,23 +415,137 @@ found_ucp_ep:
     key.status             = status;
 
     ucp_ep->cfg_index = ucp_worker_get_ep_config(worker, &key);
-    ucp_ep->flags    |= UCP_EP_FLAG_FAILED;
     ucp_ep->am_lane   = 0;
 
-    ucp_ep_errh_iter = kh_get(ucp_ep_errh_hash, &worker->ep_errh_hash,
-                              (uintptr_t)ucp_ep);
-    if (ucp_ep_errh_iter == kh_end(&worker->ep_errh_hash)) {
-        ucs_error("Error %s was not handled for ep %p - "
-                  UCT_TL_RESOURCE_DESC_FMT,
-                  ucs_status_string(status), ucp_ep,
-                  UCT_TL_RESOURCE_DESC_ARG(tl_rsc));
-        return status;
+    if (ucp_ep_ext_gen(ucp_ep)->err_cb != NULL) {
+        ucs_assert(ucp_ep->flags & UCP_EP_FLAG_USED);
+        ucs_debug("ep %p: calling user error callback %p with arg %p", ucp_ep,
+                  ucp_ep_ext_gen(ucp_ep)->err_cb,  ucp_ep_ext_gen(ucp_ep)->user_data);
+        ucp_ep_ext_gen(ucp_ep)->err_cb(ucp_ep_ext_gen(ucp_ep)->user_data, ucp_ep,
+                                       status);
+    } else if (!(ucp_ep->flags & UCP_EP_FLAG_USED)) {
+        ucs_debug("ep %p: destroy internal endpoint due to peer failure", ucp_ep);
+        ucp_ep_disconnected(ucp_ep, 1);
     }
 
-    err_cb = kh_val(&worker->ep_errh_hash, ucp_ep_errh_iter);
-    err_cb(ucp_ep->user_data, ucp_ep, status);
+    ucs_free(err_handle_arg);
+    UCS_ASYNC_UNBLOCK(&worker->async);
+    return 1;
+}
+
+int ucp_worker_err_handle_remove_filter(const ucs_callbackq_elem_t *elem,
+                                        void *arg)
+{
+    ucp_worker_err_handle_arg_t *err_handle_arg = elem->arg;
 
-    return UCS_OK;
+    return (elem->cb == ucp_worker_iface_err_handle_progress) &&
+           (err_handle_arg->ucp_ep == arg);
+}
+
+ucs_status_t ucp_worker_set_ep_failed(ucp_worker_h worker, ucp_ep_h ucp_ep,
+                                      uct_ep_h uct_ep, ucp_lane_index_t lane,
+                                      ucs_status_t status)
+{
+    uct_worker_cb_id_t          prog_id    = UCS_CALLBACKQ_ID_NULL;
+    ucs_status_t                ret_status = UCS_OK;
+    ucp_rsc_index_t             rsc_index;
+    uct_tl_resource_desc_t      *tl_rsc;
+    ucp_worker_err_handle_arg_t *err_handle_arg;
+
+    if (ucp_ep->flags & UCP_EP_FLAG_FAILED) {
+        goto out_ok;
+    }
+
+    /* set endpoint to failed to prevent wireup_ep switch */
+    ucp_ep->flags |= UCP_EP_FLAG_FAILED;
+
+    if (ucp_ep_config(ucp_ep)->key.err_mode == UCP_ERR_HANDLING_MODE_NONE) {
+        /* NOTE: if user has not requested error handling on the endpoint,
+         *       the failure is considered unhandled */
+        ret_status = status;
+        goto out;
+    }
+
+    err_handle_arg = ucs_malloc(sizeof(*err_handle_arg), "ucp_worker_err_handle_arg");
+    if (err_handle_arg == NULL) {
+        ucs_error("failed to allocate ucp_worker_err_handle_arg");
+        ret_status = UCS_ERR_NO_MEMORY;
+        goto out;
+    }
+
+    err_handle_arg->worker      = worker;
+    err_handle_arg->ucp_ep      = ucp_ep;
+    err_handle_arg->uct_ep      = uct_ep;
+    err_handle_arg->status      = status;
+    err_handle_arg->failed_lane = lane;
+
+    /* invoke the rest of the error handling flow from the main thread */
+    uct_worker_progress_register_safe(worker->uct,
+                                      ucp_worker_iface_err_handle_progress,
+                                      err_handle_arg, UCS_CALLBACKQ_FLAG_ONESHOT,
+                                      &prog_id);
+
+    if ((ucp_ep_ext_gen(ucp_ep)->err_cb == NULL) &&
+        (ucp_ep->flags & UCP_EP_FLAG_USED)) {
+        if (lane != UCP_NULL_LANE) {
+            rsc_index = ucp_ep_get_rsc_index(ucp_ep, lane);
+            tl_rsc    = &worker->context->tl_rscs[rsc_index].tl_rsc;
+            ucs_error("error '%s' will not be handled for ep %p - "
+                      UCT_TL_RESOURCE_DESC_FMT, ucs_status_string(status), ucp_ep,
+                      UCT_TL_RESOURCE_DESC_ARG(tl_rsc));
+        } else {
+            ucs_assert(uct_ep == NULL);
+            ucs_error("error '%s' occurred on wireup will not be handled for ep %p",
+                      ucs_status_string(status), ucp_ep);
+        }
+        ret_status = status;
+        goto out;
+    }
+
+out_ok:
+    ret_status = UCS_OK;
+
+out:
+    /* If the worker supports the UCP_FEATURE_WAKEUP feature, signal the user so
+     * that he can wake-up on this event */
+    ucp_worker_signal_internal(worker);
+
+    return ret_status;
+}
+
+static ucs_status_t
+ucp_worker_iface_error_handler(void *arg, uct_ep_h uct_ep, ucs_status_t status)
+{
+    ucp_worker_h worker         = (ucp_worker_h)arg;
+    ucp_lane_index_t lane;
+    ucs_status_t ret_status;
+    ucp_ep_ext_gen_t *ep_ext;
+    ucp_ep_h ucp_ep;
+
+    UCS_ASYNC_BLOCK(&worker->async);
+
+    ucs_debug("worker %p: error handler called for uct_ep %p: %s",
+              worker, uct_ep, ucs_status_string(status));
+
+    /* TODO: need to optimize uct_ep -> ucp_ep lookup */
+    ucs_list_for_each(ep_ext, &worker->all_eps, ep_list) {
+        ucp_ep = ucp_ep_from_ext_gen(ep_ext);
+        for (lane = 0; lane < ucp_ep_num_lanes(ucp_ep); ++lane) {
+            if ((uct_ep == ucp_ep->uct_eps[lane]) ||
+                ucp_wireup_ep_is_owner(ucp_ep->uct_eps[lane], uct_ep)) {
+                ret_status = ucp_worker_set_ep_failed(worker, ucp_ep, uct_ep,
+                                                      lane, status);
+                UCS_ASYNC_UNBLOCK(&worker->async);
+                return ret_status;
+            }
+        }
+    }
+
+    ucs_error("no uct_ep_h %p associated with ucp_ep_h on ucp_worker_h %p",
+              uct_ep, worker);
+    UCS_ASYNC_UNBLOCK(&worker->async);
+
+    return UCS_ERR_NO_ELEM;
 }
 
 void ucp_worker_iface_activate(ucp_worker_iface_t *wiface, unsigned uct_flags)
@@ -451,8 +553,8 @@ void ucp_worker_iface_activate(ucp_worker_iface_t *wiface, unsigned uct_flags)
     ucp_worker_h worker = wiface->worker;
     ucs_status_t status;
 
-    ucs_trace("activate iface %p acount=%u", wiface->iface,
-              wiface->activate_count);
+    ucs_trace("activate iface %p acount=%u aifaces=%u", wiface->iface,
+              wiface->activate_count, worker->num_active_ifaces);
 
     if (wiface->activate_count++ > 0) {
         return; /* was already activated */
@@ -472,20 +574,24 @@ void ucp_worker_iface_activate(ucp_worker_iface_t *wiface, unsigned uct_flags)
         ucs_list_add_tail(&worker->arm_ifaces, &wiface->arm_list);
     }
 
+    ++worker->num_active_ifaces;
+
     uct_iface_progress_enable(wiface->iface,
                               UCT_PROGRESS_SEND | UCT_PROGRESS_RECV | uct_flags);
 }
 
 static void ucp_worker_iface_deactivate(ucp_worker_iface_t *wiface, int force)
 {
-    ucs_trace("deactivate iface %p force=%d acount=%u", wiface->iface, force,
-              wiface->activate_count);
+    ucs_trace("deactivate iface %p force=%d acount=%u aifaces=%u",
+              wiface->iface, force, wiface->activate_count,
+              wiface->worker->num_active_ifaces);
 
     if (!force) {
         ucs_assert(wiface->activate_count > 0);
         if (--wiface->activate_count > 0) {
             return; /* not completely deactivated yet */
         }
+        --wiface->worker->num_active_ifaces;
     }
 
     /* Avoid progress on the interface to reduce overhead */
@@ -693,11 +799,6 @@ static void ucp_worker_close_ifaces(ucp_worker_h worker)
     UCS_ASYNC_UNBLOCK(&worker->async);
 }
 
-static size_t ucp_worker_rx_headroom(ucp_worker_h worker)
-{
-    return sizeof(ucp_recv_desc_t) + sizeof(ucp_eager_sync_hdr_t);
-}
-
 ucs_status_t ucp_worker_iface_init(ucp_worker_h worker, ucp_rsc_index_t tl_id,
                                    uct_iface_params_t *iface_params,
                                    ucp_worker_iface_t *wiface)
@@ -729,15 +830,18 @@ ucs_status_t ucp_worker_iface_init(ucp_worker_h worker, ucp_rsc_index_t tl_id,
         return status;
     }
 
+    UCS_STATIC_ASSERT(UCP_WORKER_HEADROOM_PRIV_SIZE >= sizeof(ucp_eager_sync_hdr_t));
+
     /* Fill rest of uct_iface params (caller should fill specific mode fields) */
-    iface_params->stats_root      = UCS_STATS_RVAL(worker->stats);
-    iface_params->rx_headroom     = ucp_worker_rx_headroom(worker);
-    iface_params->err_handler_arg = worker;
-    iface_params->err_handler     = ucp_worker_iface_error_handler;
-    iface_params->eager_arg       = iface_params->rndv_arg = wiface;
-    iface_params->eager_cb        = ucp_tag_offload_unexp_eager;
-    iface_params->rndv_cb         = ucp_tag_offload_unexp_rndv;
-    iface_params->cpu_mask        = worker->cpu_mask;
+    iface_params->stats_root        = UCS_STATS_RVAL(worker->stats);
+    iface_params->rx_headroom       = UCP_WORKER_HEADROOM_SIZE;
+    iface_params->err_handler_arg   = worker;
+    iface_params->err_handler       = ucp_worker_iface_error_handler;
+    iface_params->err_handler_flags = UCT_CB_FLAG_ASYNC;
+    iface_params->eager_arg         = iface_params->rndv_arg = wiface;
+    iface_params->eager_cb          = ucp_tag_offload_unexp_eager;
+    iface_params->rndv_cb           = ucp_tag_offload_unexp_rndv;
+    iface_params->cpu_mask          = worker->cpu_mask;
 
     /* Open UCT interface */
     status = uct_iface_open(md, worker->uct, iface_params, iface_config,
@@ -749,8 +853,8 @@ ucs_status_t ucp_worker_iface_init(ucp_worker_h worker, ucp_rsc_index_t tl_id,
     }
 
     ucs_debug("created interface[%d]=%p using "UCT_TL_RESOURCE_DESC_FMT" on worker %p",
-               tl_id, wiface->iface, UCT_TL_RESOURCE_DESC_ARG(&resource->tl_rsc),
-               worker);
+              tl_id, wiface->iface, UCT_TL_RESOURCE_DESC_ARG(&resource->tl_rsc),
+              worker);
 
     VALGRIND_MAKE_MEM_UNDEFINED(&wiface->attr, sizeof(wiface->attr));
     status = uct_iface_query(wiface->iface, &wiface->attr);
@@ -842,6 +946,8 @@ static void ucp_worker_init_cpu_atomics(ucp_worker_h worker)
     ucp_context_h context = worker->context;
     ucp_rsc_index_t rsc_index;
 
+    ucs_debug("worker %p: using cpu atomics", worker);
+
     /* Enable all interfaces which have host-based atomics */
     for (rsc_index = 0; rsc_index < context->num_tls; ++rsc_index) {
         if (worker->ifaces[rsc_index].attr.cap.flags & UCT_IFACE_FLAG_ATOMIC_CPU) {
@@ -863,9 +969,11 @@ static void ucp_worker_init_device_atomics(ucp_worker_h worker)
     uct_md_attr_t *md_attr;
     uint64_t supp_tls;
     uint8_t priority, best_priority;
+    ucp_tl_iface_atomic_flags_t atomic;
+
+    ucp_context_uct_atomic_iface_flags(context, &atomic);
 
-    iface_cap_flags = ucp_context_uct_atomic_iface_flags(context) |
-                      UCT_IFACE_FLAG_ATOMIC_DEVICE;
+    iface_cap_flags             = UCT_IFACE_FLAG_ATOMIC_DEVICE;
 
     dummy_iface_attr.bandwidth  = 1e12;
     dummy_iface_attr.cap_flags  = -1;
@@ -886,7 +994,11 @@ static void ucp_worker_init_device_atomics(ucp_worker_h worker)
         iface_attr = &worker->ifaces[rsc_index].attr;
 
         if (!(md_attr->cap.flags & UCT_MD_FLAG_REG) ||
-            !ucs_test_all_flags(iface_attr->cap.flags, iface_cap_flags))
+            !ucs_test_all_flags(iface_attr->cap.flags, iface_cap_flags)                        ||
+            !ucs_test_all_flags(iface_attr->cap.atomic32.op_flags, atomic.atomic32.op_flags)   ||
+            !ucs_test_all_flags(iface_attr->cap.atomic32.fop_flags, atomic.atomic32.fop_flags) ||
+            !ucs_test_all_flags(iface_attr->cap.atomic64.op_flags, atomic.atomic64.op_flags)   ||
+            !ucs_test_all_flags(iface_attr->cap.atomic64.fop_flags, atomic.atomic64.fop_flags))
         {
             continue;
         }
@@ -910,6 +1022,8 @@ static void ucp_worker_init_device_atomics(ucp_worker_h worker)
         return;
     }
 
+    ucs_debug("worker %p: using device atomics", worker);
+
     /* Enable atomics on all resources using same device as the "best" resource */
     for (rsc_index = 0; rsc_index < context->num_tls; ++rsc_index) {
         rsc = &context->tl_rscs[rsc_index];
@@ -946,7 +1060,7 @@ static void ucp_worker_init_atomic_tls(ucp_worker_h worker)
 
     worker->atomic_tls = 0;
 
-    if (context->config.features & (UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {
+    if (context->config.features & UCP_FEATURE_AMO) {
         switch(context->config.ext.atomic_mode) {
         case UCP_ATOMIC_MODE_CPU:
             ucp_worker_init_cpu_atomics(worker);
@@ -966,7 +1080,6 @@ static void ucp_worker_init_atomic_tls(ucp_worker_h worker)
 
 static ucs_status_t ucp_worker_init_mpools(ucp_worker_h worker)
 {
-    size_t           rx_headroom       = ucp_worker_rx_headroom(worker);
     size_t           max_mp_entry_size = 0;
     ucp_context_t    *context          = worker->context;
     uct_iface_attr_t *if_attr;
@@ -984,7 +1097,7 @@ static ucs_status_t ucp_worker_init_mpools(ucp_worker_h worker)
     }
 
     status = ucs_mpool_init(&worker->am_mp, 0,
-                            max_mp_entry_size + rx_headroom,
+                            max_mp_entry_size + UCP_WORKER_HEADROOM_SIZE,
                             0, UCS_SYS_CACHE_LINE_SIZE, 128, UINT_MAX,
                             &ucp_am_mpool_ops, "ucp_am_bufs");
     if (status != UCS_OK) {
@@ -1000,9 +1113,9 @@ static ucs_status_t ucp_worker_init_mpools(ucp_worker_h worker)
     }
 
     status = ucs_mpool_init(&worker->rndv_frag_mp, 0,
-                            context->config.ext.rndv_frag_size,
-                            0, UCS_SYS_CACHE_LINE_SIZE, 128, UINT_MAX,
-                            &ucp_frag_mpool_ops, "ucp_rndv_frags");
+                            context->config.ext.rndv_frag_size + sizeof(ucp_mem_desc_t),
+                            sizeof(ucp_mem_desc_t), UCS_SYS_CACHE_LINE_SIZE, 128,
+                            UINT_MAX, &ucp_frag_mpool_ops, "ucp_rndv_frags");
     if (status != UCS_OK) {
         goto err_release_reg_mpool;
     }
@@ -1057,11 +1170,12 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
                                const ucp_worker_params_t *params,
                                ucp_worker_h *worker_p)
 {
-    ucp_worker_h worker;
-    ucs_status_t status;
+    ucs_thread_mode_t uct_thread_mode;
+    ucs_thread_mode_t thread_mode;
     unsigned config_count;
     unsigned name_length;
-    ucs_thread_mode_t thread_mode;
+    ucp_worker_h worker;
+    ucs_status_t status;
 
     config_count = ucs_min((context->num_tls + 1) * (context->num_tls + 1) * context->num_tls,
                            UINT8_MAX);
@@ -1074,7 +1188,14 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
     }
 
     if (params->field_mask & UCP_WORKER_PARAM_FIELD_THREAD_MODE) {
+#if !ENABLE_MT
+        thread_mode = UCS_THREAD_MODE_SINGLE;
+        if (params->thread_mode != UCS_THREAD_MODE_SINGLE) {
+            ucs_debug("forced single thread mode on worker create");
+        }
+#else
         thread_mode = params->thread_mode;
+#endif
     } else {
         thread_mode = UCS_THREAD_MODE_SINGLE;
     }
@@ -1087,17 +1208,35 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
         worker->mt_lock.mt_type = UCP_MT_TYPE_SPINLOCK;
     }
 
+    if (thread_mode == UCS_THREAD_MODE_SINGLE) {
+        uct_thread_mode = UCS_THREAD_MODE_SINGLE;
+    } else {
+        /* UCT is serialized by UCP lock or by UCP user */
+        uct_thread_mode = UCS_THREAD_MODE_SERIALIZED;
+    }
+
     UCP_THREAD_LOCK_INIT(&worker->mt_lock);
 
     worker->context           = context;
     worker->uuid              = ucs_generate_uuid((uintptr_t)worker);
-    worker->wireup_pend_count = 0;
+    worker->flush_ops_count   = 0;
     worker->flags             = 0;
     worker->inprogress        = 0;
     worker->ep_config_max     = config_count;
     worker->ep_config_count   = 0;
+    worker->num_active_ifaces = 0;
     ucs_list_head_init(&worker->arm_ifaces);
-    ucs_list_head_init(&worker->stream_eps);
+    ucs_list_head_init(&worker->stream_ready_eps);
+    ucs_list_head_init(&worker->all_eps);
+    ucp_ep_match_init(&worker->ep_match_ctx);
+
+    UCS_STATIC_ASSERT(sizeof(ucp_ep_ext_gen_t) <= sizeof(ucp_ep_t));
+    if (context->config.features & UCP_FEATURE_STREAM) {
+        UCS_STATIC_ASSERT(sizeof(ucp_ep_ext_proto_t) <= sizeof(ucp_ep_t));
+        ucs_strided_alloc_init(&worker->ep_alloc, sizeof(ucp_ep_t), 3);
+    } else {
+        ucs_strided_alloc_init(&worker->ep_alloc, sizeof(ucp_ep_t), 2);
+    }
 
     if (params->field_mask & UCP_WORKER_PARAM_FIELD_USER_DATA) {
         worker->user_data = params->user_data;
@@ -1110,9 +1249,6 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
     ucs_snprintf_zero(worker->name, name_length, "%s:%d", ucs_get_host_name(),
                       getpid());
 
-    kh_init_inplace(ucp_worker_ep_hash, &worker->ep_hash);
-    kh_init_inplace(ucp_ep_errh_hash,   &worker->ep_errh_hash);
-
     worker->ifaces = ucs_calloc(context->num_tls, sizeof(ucp_worker_iface_t),
                                 "ucp iface");
     if (worker->ifaces == NULL) {
@@ -1140,7 +1276,7 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
     }
 
     /* Create the underlying UCT worker */
-    status = uct_worker_create(&worker->async, thread_mode, &worker->uct);
+    status = uct_worker_create(&worker->async, uct_thread_mode, &worker->uct);
     if (status != UCS_OK) {
         goto err_destroy_async;
     }
@@ -1193,6 +1329,11 @@ ucs_status_t ucp_worker_create(ucp_context_h context,
     /* Select atomic resources */
     ucp_worker_init_atomic_tls(worker);
 
+    /* At this point all UCT memory domains and interfaces are already created
+     * so warn about unused environment variables.
+     */
+    ucs_config_parser_warn_unused_env_vars();
+
     *worker_p = worker;
     return UCS_OK;
 
@@ -1214,6 +1355,7 @@ err_free_stats:
 err_free_ifaces:
     ucs_free(worker->ifaces);
 err_free:
+    ucs_strided_alloc_cleanup(&worker->ep_alloc);
     UCP_THREAD_LOCK_FINALIZE(&worker->mt_lock);
     ucs_free(worker);
     return status;
@@ -1221,17 +1363,23 @@ err_free:
 
 static void ucp_worker_destroy_eps(ucp_worker_h worker)
 {
-    ucp_ep_h ep;
+    ucp_ep_ext_gen_t *ep_ext, *tmp;
 
     ucs_debug("worker %p: destroy all endpoints", worker);
-    kh_foreach_value(&worker->ep_hash, ep, ucp_ep_destroy_internal(ep));
+    ucs_list_for_each_safe(ep_ext, tmp, &worker->all_eps, ep_list) {
+        ucp_ep_disconnected(ucp_ep_from_ext_gen(ep_ext), 1);
+    }
 }
 
 void ucp_worker_destroy(ucp_worker_h worker)
 {
     ucs_trace_func("worker=%p", worker);
-    ucp_worker_remove_am_handlers(worker);
+
+    UCS_ASYNC_BLOCK(&worker->async);
     ucp_worker_destroy_eps(worker);
+    ucp_worker_remove_am_handlers(worker);
+    UCS_ASYNC_UNBLOCK(&worker->async);
+
     ucs_mpool_cleanup(&worker->am_mp, 1);
     ucs_mpool_cleanup(&worker->reg_mp, 1);
     ucs_mpool_cleanup(&worker->rndv_frag_mp, 1);
@@ -1242,8 +1390,8 @@ void ucp_worker_destroy(ucp_worker_h worker)
     uct_worker_destroy(worker->uct);
     ucs_async_context_cleanup(&worker->async);
     ucs_free(worker->ifaces);
-    kh_destroy_inplace(ucp_worker_ep_hash, &worker->ep_hash);
-    kh_destroy_inplace(ucp_ep_errh_hash, &worker->ep_errh_hash);
+    ucp_ep_match_cleanup(&worker->ep_match_ctx);
+    ucs_strided_alloc_cleanup(&worker->ep_alloc);
     UCP_THREAD_LOCK_FINALIZE(&worker->mt_lock);
     UCS_STATS_NODE_FREE(worker->tm_offload_stats);
     UCS_STATS_NODE_FREE(worker->stats);
@@ -1271,8 +1419,9 @@ unsigned ucp_worker_progress(ucp_worker_h worker)
     /* worker->inprogress is used only for assertion check.
      * coverity[assert_side_effect]
      */
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
+    /* check that ucp_worker_progress is not called from within ucp_worker_progress */
     ucs_assert(worker->inprogress++ == 0);
     count = uct_worker_progress(worker->uct);
     ucs_async_check_miss(&worker->async);
@@ -1280,7 +1429,7 @@ unsigned ucp_worker_progress(ucp_worker_h worker)
     /* coverity[assert_side_effect] */
     ucs_assert(--worker->inprogress == 0);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return count;
 }
@@ -1289,19 +1438,24 @@ ssize_t ucp_stream_worker_poll(ucp_worker_h worker,
                                ucp_stream_poll_ep_t *poll_eps,
                                size_t max_eps, unsigned flags)
 {
-    ucp_ep_ext_stream_t *ep;
-    ssize_t             count = 0;
+    ssize_t            count = 0;
+    ucp_ep_ext_proto_t *ep_ext;
+    ucp_ep_h           ep;
+
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_STREAM,
+                                    return UCS_ERR_INVALID_PARAM);
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
-    while ((count < max_eps) && !ucs_list_is_empty(&worker->stream_eps)) {
-        ep                        = ucp_stream_worker_dequeue_ep_head(worker);
-        poll_eps[count].ep        = ep->ucp_ep;
-        poll_eps[count].user_data = ep->ucp_ep->user_data;
+    while ((count < max_eps) && !ucs_list_is_empty(&worker->stream_ready_eps)) {
+        ep_ext                    = ucp_stream_worker_dequeue_ep_head(worker);
+        ep                        = ucp_ep_from_ext_proto(ep_ext);
+        poll_eps[count].ep        = ep;
+        poll_eps[count].user_data = ucp_ep_ext_gen(ep)->user_data;
         ++count;
     }
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return count;
 }
@@ -1310,14 +1464,17 @@ ucs_status_t ucp_worker_get_efd(ucp_worker_h worker, int *fd)
 {
     ucs_status_t status;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_WAKEUP,
+                                    return UCS_ERR_INVALID_PARAM);
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
     if (worker->flags & UCP_WORKER_FLAG_EXTERNAL_EVENT_FD) {
         status = UCS_ERR_UNSUPPORTED;
     } else {
         *fd = worker->epfd;
         status = UCS_OK;
     }
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return status;
 }
 
@@ -1330,6 +1487,9 @@ ucs_status_t ucp_worker_arm(ucp_worker_h worker)
 
     ucs_trace_func("worker=%p", worker);
 
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_WAKEUP,
+                                    return UCS_ERR_INVALID_PARAM);
+
     /* Read from event pipe. If some events are found, return BUSY,
      * Otherwise, continue to arm the transport interfaces.
      */
@@ -1351,7 +1511,7 @@ ucs_status_t ucp_worker_arm(ucp_worker_h worker)
         }
     } while (ret != 0);
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     /* Go over arm_list of active interfaces which support events and arm them */
     ucs_list_for_each(wiface, &worker->arm_ifaces, arm_list) {
@@ -1367,7 +1527,7 @@ ucs_status_t ucp_worker_arm(ucp_worker_h worker)
     status = UCS_OK;
 
 out_unlock:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 out:
     ucs_trace("ucp_worker_arm returning %s", ucs_status_string(status));
     return status;
@@ -1375,7 +1535,7 @@ out:
 
 void ucp_worker_wait_mem(ucp_worker_h worker, void *address)
 {
-   ucs_arch_wait_mem(address);
+    ucs_arch_wait_mem(address);
 }
 
 ucs_status_t ucp_worker_wait(ucp_worker_h worker)
@@ -1388,7 +1548,10 @@ ucs_status_t ucp_worker_wait(ucp_worker_h worker)
 
     ucs_trace_func("worker %p", worker);
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_WAKEUP,
+                                    return UCS_ERR_INVALID_PARAM);
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     status = ucp_worker_arm(worker);
     if (status == UCS_ERR_BUSY) { /* if UCS_ERR_BUSY returned - no poll() must called */
@@ -1429,21 +1592,16 @@ ucs_status_t ucp_worker_wait(ucp_worker_h worker)
     }
 
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return status;
 }
 
 ucs_status_t ucp_worker_signal(ucp_worker_h worker)
 {
-    ucs_status_t status;
-
     ucs_trace_func("worker %p", worker);
-
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
-    status = ucp_worker_wakeup_signal_fd(worker);
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
-
-    return status;
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_WAKEUP,
+                                    return UCS_ERR_INVALID_PARAM);
+    return ucp_worker_wakeup_signal_fd(worker);
 }
 
 ucs_status_t ucp_worker_get_address(ucp_worker_h worker, ucp_address_t **address_p,
@@ -1451,12 +1609,12 @@ ucs_status_t ucp_worker_get_address(ucp_worker_h worker, ucp_address_t **address
 {
     ucs_status_t status;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     status = ucp_address_pack(worker, NULL, -1, NULL, address_length_p,
                               (void**)address_p);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return status;
 }
@@ -1466,47 +1624,6 @@ void ucp_worker_release_address(ucp_worker_h worker, ucp_address_t *address)
     ucs_free(address);
 }
 
-ucp_ep_h ucp_worker_get_reply_ep(ucp_worker_h worker, uint64_t dest_uuid)
-{
-    ucs_status_t status;
-    ucp_ep_h ep;
-
-    UCS_ASYNC_BLOCK(&worker->async);
-
-    ep = ucp_worker_ep_find(worker, dest_uuid);
-    if (ep == NULL) {
-        status = ucp_ep_create_stub(worker, dest_uuid, NULL, "??",
-                                    "for-sending-reply", &ep);
-        if (status != UCS_OK) {
-            goto err;
-        }
-        ep->flags |= UCP_EP_FLAG_DEST_UUID_PEER;
-    } else {
-        ucs_debug("found reply ep %p to uuid %"PRIx64, ep, dest_uuid);
-    }
-
-    UCS_ASYNC_UNBLOCK(&worker->async);
-    return ep;
-
-err:
-    UCS_ASYNC_UNBLOCK(&worker->async);
-    ucs_fatal("failed to create reply endpoint: %s", ucs_status_string(status));
-}
-
-ucp_request_t *ucp_worker_allocate_reply(ucp_worker_h worker, uint64_t dest_uuid)
-{
-    ucp_request_t *req;
-
-    req = ucp_request_get(worker);
-    if (req == NULL) {
-        ucs_fatal("could not allocate request");
-    }
-
-    req->flags   = 0;
-    req->send.ep = ucp_worker_get_reply_ep(worker, dest_uuid);
-    req->send.mdesc = NULL;
-    return req;
-}
 
 void ucp_worker_print_info(ucp_worker_h worker, FILE *stream)
 {
@@ -1517,7 +1634,7 @@ void ucp_worker_print_info(ucp_worker_h worker, FILE *stream)
     ucp_rsc_index_t rsc_index;
     int first;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     fprintf(stream, "#\n");
     fprintf(stream, "# UCP worker '%s'\n", ucp_worker_get_name(worker));
@@ -1531,7 +1648,7 @@ void ucp_worker_print_info(ucp_worker_h worker, FILE *stream)
         fprintf(stream, "# <failed to get address>\n");
     }
 
-    if (context->config.features & (UCP_FEATURE_AMO32|UCP_FEATURE_AMO64)) {
+    if (context->config.features & UCP_FEATURE_AMO) {
         fprintf(stream, "#                 atomics: ");
         first = 1;
         for (rsc_index = 0; rsc_index < worker->context->num_tls; ++rsc_index) {
@@ -1549,5 +1666,5 @@ void ucp_worker_print_info(ucp_worker_h worker, FILE *stream)
 
     fprintf(stream, "#\n");
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.h
index 28c2872f8..5417c0c2a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/core/ucp_worker.h
@@ -11,28 +11,31 @@
 #include "ucp_ep.h"
 #include "ucp_thread.h"
 
+#include <ucp/proto/proto.h>
 #include <ucp/tag/tag_match.h>
+#include <ucp/wireup/ep_match.h>
 #include <ucs/datastruct/mpool.h>
-#include <ucs/datastruct/khash.h>
 #include <ucs/datastruct/queue_types.h>
-#include <ucs/async/async.h>
+#include <ucs/datastruct/strided_alloc.h>
 
-KHASH_MAP_INIT_INT64(ucp_worker_ep_hash, ucp_ep_t *);
-KHASH_MAP_INIT_INT64(ucp_ep_errh_hash,   ucp_err_handler_cb_t);
 
+/* The size of the private buffer in UCT descriptor headroom, which UCP may
+ * use for its own needs. This size does not include ucp_recv_desc_t length,
+ * because it is common for all cases and protocols (TAG, STREAM). */
+#define UCP_WORKER_HEADROOM_PRIV_SIZE 24
 
-enum {
-    UCP_UCT_IFACE_ATOMIC32_FLAGS =
-        UCT_IFACE_FLAG_ATOMIC_ADD32  |
-        UCT_IFACE_FLAG_ATOMIC_FADD32 |
-        UCT_IFACE_FLAG_ATOMIC_SWAP32 |
-        UCT_IFACE_FLAG_ATOMIC_CSWAP32,
-    UCP_UCT_IFACE_ATOMIC64_FLAGS =
-        UCT_IFACE_FLAG_ATOMIC_ADD64  |
-        UCT_IFACE_FLAG_ATOMIC_FADD64 |
-        UCT_IFACE_FLAG_ATOMIC_SWAP64 |
-        UCT_IFACE_FLAG_ATOMIC_CSWAP64
-};
+
+#define UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(_worker)                 \
+    do {                                                                   \
+        ucs_assert(!UCP_THREAD_IS_REQUIRED(&(_worker)->mt_lock) ||      \
+                   UCP_THREAD_CS_IS_RECURSIVELY_LOCKED(&(_worker)->mt_lock) || \
+                   !UCS_ASYNC_IS_RECURSIVELY_BLOCKED(&(_worker)->async)); \
+        UCP_THREAD_CS_ENTER_CONDITIONAL(&(_worker)->mt_lock);           \
+    } while (0)
+
+
+#define UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(_worker)                  \
+    UCP_THREAD_CS_EXIT_CONDITIONAL(&(_worker)->mt_lock)
 
 
 /**
@@ -172,7 +175,7 @@ typedef struct ucp_worker {
     int                           inprogress;
     char                          name[UCP_WORKER_NAME_MAX]; /* Worker name */
 
-    unsigned                      wireup_pend_count;/* Number of pending requests on wireup endpoints*/
+    unsigned                      flush_ops_count;/* Number of pending operations */
 
     unsigned                      flags;         /* Worker flags */
     int                           epfd;          /* Allocated (on-demand) epoll fd for wakeup */
@@ -181,10 +184,12 @@ typedef struct ucp_worker {
     ucs_list_link_t               arm_ifaces;    /* List of interfaces to arm */
 
     void                          *user_data;    /* User-defined data */
-    ucs_list_link_t               stream_eps;    /* List of EPs with received stream data */
-    khash_t(ucp_worker_ep_hash)   ep_hash;       /* Hash table of all endpoints */
-    khash_t(ucp_ep_errh_hash)     ep_errh_hash;  /* Hash table of error handlers associated with endpoints */
+    ucs_strided_alloc_t           ep_alloc;      /* Endpoint allocator */
+    ucs_list_link_t               stream_ready_eps; /* List of EPs with received stream data */
+    ucs_list_link_t               all_eps;       /* List of all endpoints */
+    ucp_ep_match_ctx_t            ep_match_ctx;  /* Endpoint-to-endpoint matching context */
     ucp_worker_iface_t            *ifaces;       /* Array of interfaces, one for each resource */
+    unsigned                      num_active_ifaces; /* Number of activated ifaces  */
     ucs_mpool_t                   am_mp;         /* Memory pool for AM receives */
     ucs_mpool_t                   reg_mp;        /* Registered memory pool */
     ucs_mpool_t                   rndv_frag_mp;  /* Memory pool for RNDV fragments */
@@ -202,9 +207,17 @@ typedef struct ucp_worker {
 } ucp_worker_t;
 
 
-ucp_ep_h ucp_worker_get_reply_ep(ucp_worker_h worker, uint64_t dest_uuid);
+/**
+ * UCP worker argument for the error handling callback
+ */
+typedef struct ucp_worker_err_handle_arg {
+    ucp_worker_h     worker;
+    ucp_ep_h         ucp_ep;
+    uct_ep_h         uct_ep;
+    ucp_lane_index_t failed_lane;
+    ucs_status_t     status;
+} ucp_worker_err_handle_arg_t;
 
-ucp_request_t *ucp_worker_allocate_reply(ucp_worker_h worker, uint64_t dest_uuid);
 
 unsigned ucp_worker_get_ep_config(ucp_worker_h worker,
                                   const ucp_ep_config_key_t *key);
@@ -223,21 +236,27 @@ void ucp_worker_signal_internal(ucp_worker_h worker);
 
 void ucp_worker_iface_activate(ucp_worker_iface_t *wiface, unsigned uct_flags);
 
+int ucp_worker_err_handle_remove_filter(const ucs_callbackq_elem_t *elem,
+                                        void *arg);
+ucs_status_t ucp_worker_set_ep_failed(ucp_worker_h worker, ucp_ep_h ucp_ep,
+                                      uct_ep_h uct_ep, ucp_lane_index_t lane,
+                                      ucs_status_t status);
+
 static inline const char* ucp_worker_get_name(ucp_worker_h worker)
 {
     return worker->name;
 }
 
-static inline ucp_ep_h ucp_worker_ep_find(ucp_worker_h worker, uint64_t dest_uuid)
+/* get ep by pointer received from remote side, do some debug checks */
+static inline ucp_ep_h ucp_worker_get_ep_by_ptr(ucp_worker_h worker,
+                                                uintptr_t ep_ptr)
 {
-    khiter_t hash_it;
-
-    hash_it = kh_get(ucp_worker_ep_hash, &worker->ep_hash, dest_uuid);
-    if (ucs_unlikely(hash_it == kh_end(&worker->ep_hash))) {
-        return NULL;
-    }
+    ucp_ep_h ep = (ucp_ep_h)ep_ptr;
 
-    return kh_value(&worker->ep_hash, hash_it);
+    ucs_assert(ep != NULL);
+    ucs_assertv(ep->worker == worker, "worker=%p ep=%p ep->worker=%p", worker,
+                ep, ep->worker);
+    return ep;
 }
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.c
index 9759a0899..1c0fa8484 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.c
@@ -8,13 +8,93 @@
 
 #include <ucp/core/ucp_ep.inl>
 #include <ucp/core/ucp_request.h>
-#include <ucs/debug/profile.h>
+#include <ucp/core/ucp_mm.h>
+#include <ucs/profile/profile.h>
 
-size_t ucp_dt_pack(ucp_datatype_t datatype, void *dest, const void *src,
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_mem_type_unpack,
+                 (worker, buffer, recv_data, recv_length, mem_type),
+                 ucp_worker_h worker, void *buffer, const void *recv_data,
+                 size_t recv_length, uct_memory_type_t mem_type)
+{
+    ucp_ep_h ep         = worker->mem_type_ep[mem_type];
+    ucp_md_map_t md_map = 0;
+    ucp_lane_index_t lane;
+    unsigned md_index;
+    uct_mem_h memh[1];
+    ucs_status_t status;
+    uct_rkey_bundle_t rkey_bundle;
+
+    if (recv_length == 0) {
+        return UCS_OK;
+    }
+
+    lane     = ucp_ep_config(ep)->key.rma_lanes[0];
+    md_index = ucp_ep_md_index(ep, lane);
+
+    status = ucp_mem_type_reg_buffers(worker, buffer, recv_length,
+                                      mem_type, md_index, memh, &md_map,
+                                      &rkey_bundle);
+    if (status != UCS_OK) {
+        ucs_error("failed to register buffer with mem type domian");
+        return status;
+    }
+
+    status = uct_ep_put_short(ep->uct_eps[lane], recv_data, recv_length,
+                              (uint64_t)buffer, rkey_bundle.rkey);
+    if (status != UCS_OK) {
+        ucs_error("uct_ep_put_short() failed %s", ucs_status_string(status));
+    }
+
+    ucp_mem_type_unreg_buffers(worker, mem_type, memh,
+                               &md_map, &rkey_bundle);
+    return status;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_mem_type_pack,
+                 (worker, dest, src, length, mem_type),
+                 ucp_worker_h worker, void *dest, const void *src, size_t length,
+                 uct_memory_type_t mem_type)
+{
+    ucp_ep_h ep         = worker->mem_type_ep[mem_type];
+    ucp_md_map_t md_map = 0;
+    ucp_lane_index_t lane;
+    unsigned md_index;
+    ucs_status_t status;
+    uct_mem_h memh[1];
+    uct_rkey_bundle_t rkey_bundle;
+
+    if (length == 0) {
+        return UCS_OK;
+    }
+
+    lane     = ucp_ep_config(ep)->key.rma_lanes[0];
+    md_index = ucp_ep_md_index(ep, lane);
+
+    status = ucp_mem_type_reg_buffers(worker, (void *)src, length, mem_type, md_index,
+                                      memh, &md_map, &rkey_bundle);
+    if (status != UCS_OK) {
+        ucs_error("failed to register buffer with mem type domian");
+        return status;
+    }
+
+    status = uct_ep_get_short(ep->uct_eps[lane], dest, length,
+                              (uint64_t)src, rkey_bundle.rkey);
+    if (status != UCS_OK) {
+        ucs_error("uct_ep_put_short() failed %s", ucs_status_string(status));
+    }
+
+    ucp_mem_type_unreg_buffers(worker, mem_type, memh,
+                               &md_map, &rkey_bundle);
+    return status;
+}
+
+size_t ucp_dt_pack(ucp_worker_h worker, ucp_datatype_t datatype,
+                   uct_memory_type_t mem_type, void *dest, const void *src,
                    ucp_dt_state_t *state, size_t length)
 {
-    ucp_dt_generic_t *dt;
     size_t result_len = 0;
+    ucp_dt_generic_t *dt;
 
     if (!length) {
         return length;
@@ -22,7 +102,12 @@ size_t ucp_dt_pack(ucp_datatype_t datatype, void *dest, const void *src,
 
     switch (datatype & UCP_DATATYPE_CLASS_MASK) {
     case UCP_DATATYPE_CONTIG:
-        UCS_PROFILE_CALL(memcpy, dest, src + state->offset, length);
+        if ((ucs_likely(UCP_MEM_IS_HOST(mem_type))) ||
+            (ucs_likely(UCP_MEM_IS_CUDA_MANAGED(mem_type)))) {
+            UCS_PROFILE_CALL(memcpy, dest, src + state->offset, length);
+        } else {
+            ucp_mem_type_pack(worker, dest, src + state->offset, length, mem_type);
+        }
         result_len = length;
         break;
 
@@ -47,64 +132,3 @@ size_t ucp_dt_pack(ucp_datatype_t datatype, void *dest, const void *src,
     state->offset += result_len;
     return result_len;
 }
-
-UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_mem_type_unpack(ucp_worker_h worker, void *buffer, const void *recv_data,
-                    size_t recv_length, uct_memory_type_t mem_type)
-{
-    ucp_context_h context = worker->context;
-    ucp_ep_h ep = worker->mem_type_ep[mem_type];
-    ucp_md_map_t md_map = 0;
-    ucp_lane_index_t lane;
-    uct_md_h md;
-    unsigned md_index;
-    uct_mem_h memh[1];
-    ucs_status_t status;
-    char *rkey_buffer;
-    uct_rkey_bundle_t rkey_bundle;
-
-    if (recv_length == 0) {
-        return UCS_OK;
-    }
-
-    lane = ucp_ep_config(ep)->key.rma_lanes[0];
-    md_index = ucp_ep_md_index(ep, lane);
-    md = context->tl_mds[md_index].md;
-
-    status = ucp_mem_rereg_mds(context, UCS_BIT(md_index), buffer,
-                               recv_length, UCT_MD_MEM_ACCESS_ALL, NULL, mem_type,
-                               NULL, memh, &md_map);
-    if (status != UCS_OK) {
-        goto err;
-    }
-
-    rkey_buffer = ucs_alloca(context->tl_mds[md_index].attr.rkey_packed_size);
-
-    status = uct_md_mkey_pack(md, memh[0], rkey_buffer);
-    if (status != UCS_OK) {
-        ucs_error("failed to pack key from md[%d]: %s",
-                  md_index, ucs_status_string(status));
-        goto err_dreg_mem;
-    }
-
-    status = uct_rkey_unpack(rkey_buffer, &rkey_bundle);
-    if (status != UCS_OK) {
-        ucs_error("failed to unpack key from md[%d]: %s",
-                   md_index, ucs_status_string(status));
-        goto err_dreg_mem;
-    }
-
-    status = uct_ep_put_short(ep->uct_eps[lane], recv_data, recv_length,
-                              (uint64_t)buffer, rkey_bundle.rkey);
-    if (status != UCS_OK) {
-        ucs_error("uct_ep_put_short() failed %s", ucs_status_string(status));
-        goto err_destroy_rkey;
-    }
-
-err_destroy_rkey:
-    uct_rkey_release(&rkey_bundle);
-err_dreg_mem:
-    ucp_mem_rereg_mds(context, 0, NULL, 0, 0, NULL, mem_type, NULL, memh, &md_map);
-err:
-    return status;
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.h
index 1156e9029..bd034fcfb 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.h
@@ -46,7 +46,8 @@ typedef struct ucp_dt_state {
 } ucp_dt_state_t;
 
 
-size_t ucp_dt_pack(ucp_datatype_t datatype, void *dest, const void *src,
+size_t ucp_dt_pack(ucp_worker_h worker, ucp_datatype_t datatype,
+                   uct_memory_type_t mem_type, void *dest, const void *src,
                    ucp_dt_state_t *state, size_t length);
 
 ucs_status_t ucp_mem_type_unpack(ucp_worker_h worker, void *buffer,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.inl
index b8d47cb55..99f8f31d2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt.inl
@@ -7,7 +7,7 @@
 #ifndef UCP_DT_INL_
 #define UCP_DT_INL_
 
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 
 /**
  * Get the total length of the data
@@ -56,7 +56,8 @@ ucp_dt_unpack_only(ucp_worker_h worker, void *buffer, size_t count,
             ucs_unlikely(length > (buffer_size = ucp_contig_dt_length(datatype, count)))) {
             goto err_truncated;
         }
-        if (ucs_likely(UCP_MEM_IS_HOST(mem_type))) {
+        if (ucs_likely(UCP_MEM_IS_HOST(mem_type)) ||
+            (ucs_likely(UCP_MEM_IS_CUDA_MANAGED(mem_type)))) {
             UCS_PROFILE_NAMED_CALL("memcpy_recv", memcpy, buffer, data, length);
         } else {
             ucp_mem_type_unpack(worker, buffer, data, length, mem_type);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_contig.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_contig.c
index 2c233b9fb..024de95c3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_contig.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_contig.c
@@ -6,7 +6,7 @@
 
 #include "dt_contig.h"
 
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <string.h>
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_generic.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_generic.h
index db49261d2..bf0647b8a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_generic.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_generic.h
@@ -5,8 +5,8 @@
  */
 
 
-#ifndef UCP_DATATYPE_H_
-#define UCP_DATATYPE_H_
+#ifndef UCP_DT_GENERIC_H_
+#define UCP_DT_GENERIC_H_
 
 #include <ucp/api/ucp.h>
 
@@ -28,4 +28,4 @@ static inline ucp_dt_generic_t* ucp_dt_generic(ucp_datatype_t datatype)
 #define UCP_DT_IS_GENERIC(_datatype) \
           (((_datatype) & UCP_DATATYPE_CLASS_MASK) == UCP_DATATYPE_GENERIC)
 
-#endif
+#endif /* UCP_DT_GENERIC_H_ */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.c
index c072cde9f..2ae6b6049 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.c
@@ -92,3 +92,14 @@ void ucp_dt_iov_seek(ucp_dt_iov_t *iov, size_t iovcnt, ptrdiff_t distance,
 
     *iov_offset = new_iov_offset;
 }
+
+size_t ucp_dt_iov_count_nonempty(const ucp_dt_iov_t *iov, size_t iovcnt)
+{
+    size_t iov_it, count;
+
+    count = 0;
+    for (iov_it = 0; iov_it < iovcnt; ++iov_it) {
+        count += iov[iov_it].length != 0;
+    }
+    return count;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.h
index 43fc81f8e..c19a7bae7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/dt/dt_iov.h
@@ -88,4 +88,14 @@ void ucp_dt_iov_seek(ucp_dt_iov_t *iov, size_t iovcnt, ptrdiff_t distance,
                      size_t *iov_offset, size_t *iovcnt_offset);
 
 
+/**
+ * Count non-empty buffers in the iov
+ *
+ * @param [in]     iov            @ref ucp_dt_iov_t buffer to count
+ * @param [in]     iovcnt         Number of entries the @a iov buffer
+ *
+ * @return Number of non-empty buffers in the iovec
+ */
+size_t ucp_dt_iov_count_nonempty(const ucp_dt_iov_t *iov, size_t iovcnt);
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto.h
index 5653f37be..290efa735 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto.h
@@ -8,9 +8,6 @@
 #define UCP_PROTO_H_
 
 #include <ucp/core/ucp_ep.h>
-#include <ucp/wireup/wireup.h>
-#include <ucp/core/ucp_context.h>
-#include <ucp/core/ucp_worker.h>
 #include <ucs/sys/compiler.h>
 #include <ucs/sys/sys.h>
 
@@ -19,7 +16,7 @@
  * Header segment for a transaction
  */
 typedef struct {
-    uint64_t                  sender_uuid;
+    uintptr_t                 ep_ptr;
     uintptr_t                 reqptr;
 } UCS_S_PACKED ucp_request_hdr_t;
 
@@ -55,17 +52,4 @@ void ucp_proto_am_zcopy_completion(uct_completion_t *self, ucs_status_t status);
 
 void ucp_proto_am_zcopy_req_complete(ucp_request_t *req, ucs_status_t status);
 
-/*
- * Make sure the remote worker would be able to send replies to our endpoint.
- * Should be used before sending a message which requires a reply.
- */
-static inline void ucp_ep_connect_remote(ucp_ep_h ep)
-{
-    if (ucs_unlikely(!(ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED))) {
-        ucs_assert(ep->flags & UCP_EP_FLAG_DEST_UUID_PEER);
-        ucp_wireup_send_request(ep, ep->dest_uuid);
-    }
-}
-
-
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.c
index ac9efa583..f5f5de66a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.c
@@ -26,8 +26,8 @@ static size_t ucp_proto_pack(void *dest, void *arg)
         return sizeof(*rep_hdr);
     case UCP_AM_ID_OFFLOAD_SYNC_ACK:
         off_rep_hdr = dest;
-        off_rep_hdr->sender_tag  = req->send.proto.sender_tag;
-        off_rep_hdr->sender_uuid = req->send.proto.sender_uuid;
+        off_rep_hdr->sender_tag = req->send.proto.sender_tag;
+        off_rep_hdr->ep_ptr     = ucp_request_get_dest_ep_ptr(req);
         return sizeof(*off_rep_hdr);
     }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.inl
index b35b85c81..ab6917262 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/proto/proto_am.inl
@@ -9,7 +9,7 @@
 #include <ucp/core/ucp_request.inl>
 #include <ucp/core/ucp_ep.inl>
 #include <ucp/dt/dt.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 
 #define UCP_STATUS_PENDING_SWITCH (UCS_ERR_LAST - 1)
 
@@ -36,17 +36,16 @@ ucp_do_am_bcopy_single(uct_pending_req_t *self, uint8_t am_id,
 
 static UCS_F_ALWAYS_INLINE
 ucs_status_t ucp_do_am_bcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
-                                   uint8_t am_id_middle, uint8_t am_id_last,
+                                   uint8_t am_id_middle,
                                    size_t hdr_size_middle,
                                    uct_pack_callback_t pack_first,
                                    uct_pack_callback_t pack_middle,
-                                   uct_pack_callback_t pack_last,
                                    int enable_am_bw)
 {
     ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_ep_t *ep       = req->send.ep;
     ucs_status_t status;
-    size_t max_middle;
+    size_t UCS_V_UNUSED max_middle;
     ssize_t packed_len;
     uct_ep_h uct_ep;
     size_t offset;
@@ -65,20 +64,21 @@ ucs_status_t ucp_do_am_bcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
             packed_len = uct_ep_am_bcopy(uct_ep, am_id_first, pack_first, req, 0);
             UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_bcopy_first", packed_len,
                                                    packed_len);
-        } else if (offset + max_middle < req->send.length) {
-            /* Middle */
+            ucs_assertv(req->send.state.dt.offset < req->send.length,
+                        "offset=%zd", req->send.state.dt.offset);
+        } else {
+            ucs_assert(offset < req->send.length);
+            /* Middle or last */
             packed_len = uct_ep_am_bcopy(uct_ep, am_id_middle, pack_middle, req, 0);
             ucs_assertv((packed_len < 0) || (packed_len <= max_middle + hdr_size_middle),
                         "packed_len=%zd max_middle=%zu hdr_size_middle=%zu",
                         packed_len, max_middle, hdr_size_middle);
             UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_bcopy_middle",
                                                    packed_len, packed_len);
-        } else {
-            /* Last */
-            packed_len = uct_ep_am_bcopy(uct_ep, am_id_last, pack_last, req, 0);
-            UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_bcopy_last", packed_len,
-                                                   packed_len);
-            if (packed_len >= 0) {
+            ucs_assert((packed_len < 0) ||
+                       (offset + packed_len - hdr_size_middle <= req->send.length));
+            if ((packed_len > 0) && (offset + packed_len - hdr_size_middle == req->send.length)) {
+                /* Last */
                 return UCS_OK;
             }
         }
@@ -188,7 +188,7 @@ ucs_status_t ucp_do_am_zcopy_single(uct_pending_req_t *self, uint8_t am_id,
 
     ucp_dt_iov_copy_uct(ep->worker->context,iov, &iovcnt, max_iov,
                         &state, req->send.buffer, req->send.datatype,
-                        req->send.length, 0, NULL);
+                        req->send.length, ucp_ep_md_index(ep, req->send.lane), NULL);
 
     status = uct_ep_am_zcopy(ep->uct_eps[req->send.lane], am_id, (void*)hdr,
                              hdr_size, iov, iovcnt, 0,
@@ -205,7 +205,7 @@ ucs_status_t ucp_do_am_zcopy_single(uct_pending_req_t *self, uint8_t am_id,
 
 static UCS_F_ALWAYS_INLINE
 ucs_status_t ucp_do_am_zcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
-                                   uint8_t am_id_middle, uint8_t am_id_last,
+                                   uint8_t am_id_middle,
                                    const void *hdr_first, size_t hdr_size_first,
                                    const void *hdr_middle, size_t hdr_size_middle,
                                    ucp_req_complete_func_t complete, int enable_am_bw)
@@ -219,6 +219,7 @@ ucs_status_t ucp_do_am_zcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
     size_t max_iov;
     uct_iov_t *iov;
     size_t offset;
+    size_t mid_len;
     ucs_status_t status;
     uct_ep_h uct_ep;
     int pending_adde_res;
@@ -262,6 +263,9 @@ ucs_status_t ucp_do_am_zcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
                                 req->send.buffer,  req->send.datatype,
                                 max_middle - hdr_size_first + hdr_size_middle,
                                 ucp_ep_md_index(ep, req->send.lane), NULL);
+            ucs_assertv(state.offset != 0, "state must be changed on 1st stage");
+            ucs_assertv(state.offset < req->send.length, "state.offset=%zu",
+                        state.offset);
 
             status = uct_ep_am_zcopy(uct_ep, am_id_first, (void*)hdr_first,
                                      hdr_size_first, iov, iovcnt, 0,
@@ -269,39 +273,45 @@ ucs_status_t ucp_do_am_zcopy_multi(uct_pending_req_t *self, uint8_t am_id_first,
 
             UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_zcopy_first",
                                                    iov[0].length, status);
-        } else if ((offset + max_middle < req->send.length) || flag_iov_mid) {
-            /* Middle stage */
-            ucp_dt_iov_copy_uct(ep->worker->context, iov, &iovcnt, max_iov,
-                                &state, req->send.buffer, req->send.datatype,
-                                max_middle, ucp_ep_md_index(ep, req->send.lane), NULL);
-
-            status = uct_ep_am_zcopy(uct_ep, am_id_middle, (void*)hdr_middle,
-                                     hdr_size_middle, iov, iovcnt, 0,
-                                     &req->send.state.uct_comp);
-
-            UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_zcopy_middle",
-                                                   iov[0].length, status);
         } else {
-            /* Last stage */
+            /* Middle or last stage */
+            mid_len = ucs_min(max_middle, req->send.length - offset);
+            ucs_assert(offset + mid_len <= req->send.length);
             ucp_dt_iov_copy_uct(ep->worker->context, iov, &iovcnt, max_iov, &state,
-                                req->send.buffer, req->send.datatype,
-                                req->send.length - offset,
+                                req->send.buffer, req->send.datatype, mid_len,
                                 ucp_ep_md_index(ep, req->send.lane), NULL);
 
-            status = uct_ep_am_zcopy(uct_ep, am_id_last, (void*)hdr_middle,
-                                     hdr_size_middle, iov, iovcnt, 0,
-                                     &req->send.state.uct_comp);
-            UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_zcopy_last",
-                                                   iov[0].length, status);
-            if (status == UCS_OK) {
-                complete(req, UCS_OK);
+            if (offset < state.offset) {
+                status = uct_ep_am_zcopy(uct_ep, am_id_middle, (void*)hdr_middle,
+                                         hdr_size_middle, iov, iovcnt, 0,
+                                         &req->send.state.uct_comp);
+            } else if (state.offset == req->send.length) {
+                /* Empty IOVs on last stage */
                 return UCS_OK;
+            } else {
+                ucs_assert(offset == state.offset);
+                /* Empty IOVs in the middle */
+                ucp_request_send_state_advance(req, &state,
+                                               UCP_REQUEST_SEND_PROTO_ZCOPY_AM,
+                                               UCS_OK);
+                continue;
             }
-            ucp_request_send_state_advance(req, &state,
-                                           UCP_REQUEST_SEND_PROTO_ZCOPY_AM,
-                                           status);
-            if (!UCS_STATUS_IS_ERR(status)) {
-                return UCS_OK;
+
+            UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(req, "am_zcopy_middle",
+                                                   iov[0].length, status);
+
+            if (!flag_iov_mid && (offset + mid_len == req->send.length)) {
+                /* Last stage */
+                if (status == UCS_OK) {
+                    complete(req, UCS_OK);
+                    return UCS_OK;
+                }
+                ucp_request_send_state_advance(req, &state,
+                                               UCP_REQUEST_SEND_PROTO_ZCOPY_AM,
+                                               status);
+                if (!UCS_STATUS_IS_ERR(status)) {
+                    return UCS_OK;
+                }
             }
         }
 
@@ -340,7 +350,7 @@ ucp_proto_get_zcopy_threshold(const ucp_request_t *req,
     }
 
     if (ucs_unlikely(!UCP_MEM_IS_HOST(req->send.mem_type))) {
-        return 1;
+        return ucs_min(max_zcopy, msg_config->mem_type_zcopy_thresh[req->send.mem_type]);
     }
 
     if (ucs_likely(UCP_DT_IS_CONTIG(req->send.datatype))) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_basic.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_basic.c
new file mode 100644
index 000000000..e48030ca4
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_basic.c
@@ -0,0 +1,100 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+* Copyright (C) UT-Battelle, LLC. 2016.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "rma.h"
+#include "rma.inl"
+
+#include <ucs/profile/profile.h>
+
+
+static UCS_F_ALWAYS_INLINE
+ucs_status_t ucp_amo_check_send_status(ucp_request_t *req, ucs_status_t status)
+{
+    if (status == UCS_INPROGRESS) {
+        return UCS_OK;
+    }
+    /* Complete for UCS_OK and unexpected errors */
+    if (status != UCS_ERR_NO_RESOURCE) {
+        ucp_request_complete_send(req, status);
+    }
+    return status;
+}
+
+static ucs_status_t ucp_amo_basic_progress_post(uct_pending_req_t *self)
+{
+    ucp_request_t *req   = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_rkey_h rkey      = req->send.amo.rkey;
+    ucp_ep_t *ep         = req->send.ep;
+    uint64_t value       = req->send.amo.value;
+    uint64_t remote_addr = req->send.amo.remote_addr;
+    uct_atomic_op_t op   = req->send.amo.uct_op;
+    ucs_status_t status;
+
+    req->send.lane = rkey->cache.amo_lane;
+    if (req->send.length == sizeof(uint64_t)) {
+        status = UCS_PROFILE_CALL(uct_ep_atomic64_post,
+                                  ep->uct_eps[req->send.lane], op, value,
+                                  remote_addr, rkey->cache.amo_rkey);
+    } else {
+        ucs_assert(req->send.length == sizeof(uint32_t));
+        status = UCS_PROFILE_CALL(uct_ep_atomic32_post,
+                                  ep->uct_eps[req->send.lane], op, value,
+                                  remote_addr, rkey->cache.amo_rkey);
+    }
+
+    return ucp_amo_check_send_status(req, status);
+}
+
+static ucs_status_t ucp_amo_basic_progress_fetch(uct_pending_req_t *self)
+{
+    ucp_request_t *req    = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_rkey_h rkey       = req->send.amo.rkey;
+    ucp_ep_t *ep          = req->send.ep;
+    uint64_t value        = req->send.amo.value;
+    uint64_t *result      = req->send.buffer;
+    uint64_t remote_addr  = req->send.amo.remote_addr;
+    uct_atomic_op_t op    = req->send.amo.uct_op;
+    ucs_status_t status;
+
+    req->send.lane = rkey->cache.amo_lane;
+    if (req->send.length == sizeof(uint64_t)) {
+        if (op != UCT_ATOMIC_OP_CSWAP) {
+            status = uct_ep_atomic64_fetch(ep->uct_eps[req->send.lane],
+                                           op, value, result,
+                                           remote_addr,
+                                           rkey->cache.amo_rkey,
+                                           &req->send.state.uct_comp);
+        } else {
+            status = uct_ep_atomic_cswap64(ep->uct_eps[req->send.lane],
+                                           value, *result,
+                                           remote_addr, rkey->cache.amo_rkey, result,
+                                           &req->send.state.uct_comp);
+        }
+    } else {
+        ucs_assert(req->send.length == sizeof(uint32_t));
+        if (op != UCT_ATOMIC_OP_CSWAP) {
+            status = uct_ep_atomic32_fetch(ep->uct_eps[req->send.lane],
+                                           op, value, (uint32_t*)result,
+                                           remote_addr,
+                                           rkey->cache.amo_rkey,
+                                           &req->send.state.uct_comp);
+        } else {
+            status = uct_ep_atomic_cswap32(ep->uct_eps[req->send.lane],
+                                           value, *result, remote_addr,
+                                           rkey->cache.amo_rkey, (uint32_t*)result,
+                                           &req->send.state.uct_comp);
+        }
+    }
+
+    return ucp_amo_check_send_status(req, status);
+}
+
+ucp_amo_proto_t ucp_amo_basic_proto = {
+    .name           = "basic_amo",
+    .progress_fetch = ucp_amo_basic_progress_fetch,
+    .progress_post  = ucp_amo_basic_progress_post
+};
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_send.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_send.c
new file mode 100644
index 000000000..48985f16b
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_send.c
@@ -0,0 +1,278 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+* Copyright (C) UT-Battelle, LLC. 2016.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "rma.h"
+#include "rma.inl"
+
+#include <ucp/core/ucp_mm.h>
+#include <ucp/core/ucp_ep.inl>
+#include <ucs/profile/profile.h>
+#include <ucs/debug/log.h>
+#include <inttypes.h>
+
+
+#define UCP_AMO_CHECK_PARAM(_context, _remote_addr, _size, _opcode, \
+                            _last_opcode, _action) \
+    { \
+        if (ENABLE_PARAMS_CHECK && \
+            ucs_unlikely(((_remote_addr) % (_size)) != 0)) { \
+            ucs_error("atomic variable must be naturally aligned " \
+                      "(remote address 0x%"PRIx64", size %zu)", (_remote_addr), \
+                      (_size)); \
+            _action; \
+        } \
+        \
+        if (ENABLE_PARAMS_CHECK && \
+            ucs_unlikely(((_size) != 4) && (_size != 8))) { \
+            ucs_error("invalid atomic operation size: %zu", (_size)); \
+            _action; \
+        } \
+        \
+        UCP_CONTEXT_CHECK_FEATURE_FLAGS((_context), ((_size) == 4) ? \
+                                        UCP_FEATURE_AMO32 : UCP_FEATURE_AMO64, \
+                                        _action); \
+        \
+        if (ENABLE_PARAMS_CHECK && \
+            (ucs_unlikely((_opcode) >= (_last_opcode)))) { \
+            ucs_error("invalid atomic opcode %d ", _opcode); \
+            _action; \
+        } \
+    }
+
+
+static uct_atomic_op_t ucp_uct_op_table[] = {
+    [UCP_ATOMIC_POST_OP_ADD]    = UCT_ATOMIC_OP_ADD,
+    [UCP_ATOMIC_POST_OP_AND]    = UCT_ATOMIC_OP_AND,
+    [UCP_ATOMIC_POST_OP_OR]     = UCT_ATOMIC_OP_OR,
+    [UCP_ATOMIC_POST_OP_XOR]    = UCT_ATOMIC_OP_XOR
+};
+
+static uct_atomic_op_t ucp_uct_fop_table[] = {
+    [UCP_ATOMIC_FETCH_OP_FADD]  = UCT_ATOMIC_OP_ADD,
+    [UCP_ATOMIC_FETCH_OP_FAND]  = UCT_ATOMIC_OP_AND,
+    [UCP_ATOMIC_FETCH_OP_FOR]   = UCT_ATOMIC_OP_OR,
+    [UCP_ATOMIC_FETCH_OP_FXOR]  = UCT_ATOMIC_OP_XOR,
+    [UCP_ATOMIC_FETCH_OP_SWAP]  = UCT_ATOMIC_OP_SWAP,
+    [UCP_ATOMIC_FETCH_OP_CSWAP] = UCT_ATOMIC_OP_CSWAP,
+};
+
+
+static void ucp_amo_completed_single(uct_completion_t *self,
+                                     ucs_status_t status)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
+                                          send.state.uct_comp);
+    ucs_trace("invoking completion on AMO request %p", req);
+    ucp_request_complete_send(req, status);
+}
+
+static UCS_F_ALWAYS_INLINE void
+ucp_amo_init_common(ucp_request_t *req, ucp_ep_h ep, uct_atomic_op_t op,
+                    uint64_t remote_addr, ucp_rkey_h rkey, uint64_t value,
+                    size_t size)
+{
+    req->flags                = 0;
+    req->send.ep              = ep;
+    req->send.length          = size;
+    req->send.amo.uct_op      = op;
+    req->send.amo.remote_addr = remote_addr;
+    req->send.amo.rkey        = rkey;
+    req->send.amo.value       = value;
+#if ENABLE_ASSERT
+    req->send.lane            = UCP_NULL_LANE;
+#endif
+}
+
+static UCS_F_ALWAYS_INLINE void
+ucp_amo_init_fetch(ucp_request_t *req, ucp_ep_h ep, void *buffer,
+                   uct_atomic_op_t op, size_t op_size, uint64_t remote_addr,
+                   ucp_rkey_h rkey, uint64_t value, const ucp_amo_proto_t *proto)
+{
+    ucp_amo_init_common(req, ep, op, remote_addr, rkey, value, op_size);
+    req->send.state.uct_comp.count  = 1;
+    req->send.state.uct_comp.func   = ucp_amo_completed_single;
+    req->send.uct.func              = proto->progress_fetch;
+    req->send.buffer                = buffer;
+}
+
+static UCS_F_ALWAYS_INLINE
+void ucp_amo_init_post(ucp_request_t *req, ucp_ep_h ep, uct_atomic_op_t op,
+                       size_t op_size, uint64_t remote_addr, ucp_rkey_h rkey,
+                       uint64_t value, const ucp_amo_proto_t *proto)
+{
+    ucp_amo_init_common(req, ep, op, remote_addr, rkey, value, op_size);
+    req->send.uct.func = proto->progress_post;
+}
+
+ucs_status_ptr_t ucp_atomic_fetch_nb(ucp_ep_h ep, ucp_atomic_fetch_op_t opcode,
+                                     uint64_t value, void *result, size_t op_size,
+                                     uint64_t remote_addr, ucp_rkey_h rkey,
+                                     ucp_send_callback_t cb)
+{
+    ucs_status_ptr_t status_p;
+    ucs_status_t status;
+    ucp_request_t *req;
+
+    UCP_AMO_CHECK_PARAM(ep->worker->context, remote_addr, op_size, opcode,
+                        UCP_ATOMIC_FETCH_OP_LAST,
+                        return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, amo);
+    if (status != UCS_OK) {
+        status_p = UCS_STATUS_PTR(UCS_ERR_UNREACHABLE);
+        goto out;
+    }
+
+    req = ucp_request_get(ep->worker);
+    if (ucs_unlikely(NULL == req)) {
+        status_p = UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
+        goto out;
+    }
+
+    ucp_amo_init_fetch(req, ep, result, ucp_uct_fop_table[opcode], op_size,
+                       remote_addr, rkey, value, rkey->cache.amo_proto);
+
+    status_p = ucp_rma_send_request_cb(req, cb);
+
+out:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return status_p;
+}
+
+ucs_status_t ucp_atomic_post(ucp_ep_h ep, ucp_atomic_post_op_t opcode, uint64_t value,
+                             size_t op_size, uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    ucs_status_ptr_t status_p;
+    ucs_status_t status;
+    ucp_request_t *req;
+
+    UCP_AMO_CHECK_PARAM(ep->worker->context, remote_addr, op_size, opcode,
+                        UCP_ATOMIC_POST_OP_LAST, return UCS_ERR_INVALID_PARAM);
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, amo);
+    if (status != UCS_OK) {
+        goto out;
+    }
+
+    req = ucp_request_get(ep->worker);
+    if (ucs_unlikely(NULL == req)) {
+        status = UCS_ERR_NO_MEMORY;
+        goto out;
+    }
+
+    ucp_amo_init_post(req, ep, ucp_uct_op_table[opcode], op_size, remote_addr,
+                      rkey, value, rkey->cache.amo_proto);
+
+    status_p = ucp_rma_send_request_cb(req, (ucp_send_callback_t)ucs_empty_function);
+    if (UCS_PTR_IS_PTR(status_p)) {
+        ucp_request_release(status_p);
+        status = UCS_OK;
+    } else {
+        status = UCS_PTR_STATUS(status_p);
+    }
+
+out:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return status;
+}
+
+static inline ucs_status_t
+ucp_atomic_fetch_b(ucp_ep_h ep, ucp_atomic_fetch_op_t opcode, uint64_t value,
+                   void *result, size_t size, uint64_t remote_addr,
+                   ucp_rkey_h rkey, const char *op_name)
+{
+    void *request;
+
+    request = ucp_atomic_fetch_nb(ep, opcode, value, result, size, remote_addr,
+                                  rkey, (void*)ucs_empty_function);
+    return ucp_rma_wait(ep->worker, request, op_name);
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_add32, (ep, add, remote_addr, rkey),
+                 ucp_ep_h ep, uint32_t add, uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    return ucp_atomic_post(ep, UCP_ATOMIC_POST_OP_ADD, add, sizeof(add),
+                           remote_addr, rkey);
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_add64, (ep, add, remote_addr, rkey),
+                 ucp_ep_h ep, uint64_t add, uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    return ucp_atomic_post(ep, UCP_ATOMIC_POST_OP_ADD, add, sizeof(add),
+                           remote_addr, rkey);
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_fadd32, (ep, add, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint32_t add, uint64_t remote_addr, ucp_rkey_h rkey,
+                 uint32_t *result)
+{
+    return ucp_atomic_fetch_b(ep, UCP_ATOMIC_FETCH_OP_FADD, add, result,
+                              sizeof(add), remote_addr, rkey, "atomic_fadd32");
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_fadd64, (ep, add, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint64_t add, uint64_t remote_addr, ucp_rkey_h rkey,
+                 uint64_t *result)
+{
+    return ucp_atomic_fetch_b(ep, UCP_ATOMIC_FETCH_OP_FADD, add, result,
+                              sizeof(add), remote_addr, rkey, "atomic_fadd64");
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_swap32, (ep, swap, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint32_t swap, uint64_t remote_addr, ucp_rkey_h rkey,
+                 uint32_t *result)
+{
+    return ucp_atomic_fetch_b(ep, UCP_ATOMIC_FETCH_OP_SWAP, swap, result,
+                              sizeof(swap), remote_addr, rkey, "atomic_swap32");
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_swap64, (ep, swap, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint64_t swap, uint64_t remote_addr, ucp_rkey_h rkey,
+                 uint64_t *result)
+{
+    return ucp_atomic_fetch_b(ep, UCP_ATOMIC_FETCH_OP_SWAP, swap, result,
+                              sizeof(swap), remote_addr, rkey, "atomic_swap64");
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+ucp_atomic_cswap_b(ucp_ep_h ep, uint64_t compare, uint64_t swap, size_t size,
+                   uint64_t remote_addr, ucp_rkey_h rkey, void *result,
+                   const char *op_name)
+{
+    char tmp[sizeof(swap)]; /* sufficient storage for maximal operand size */
+    ucs_status_t status;
+
+    memcpy(tmp, &swap, size);
+    status = ucp_atomic_fetch_b(ep, UCP_ATOMIC_FETCH_OP_CSWAP, compare, &tmp,
+                                size, remote_addr, rkey, op_name);
+    if (status == UCS_OK) {
+        memcpy(result, tmp, size);
+    }
+    return status;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_cswap32,
+                 (ep, compare, swap, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint32_t compare, uint32_t swap,
+                 uint64_t remote_addr, ucp_rkey_h rkey, uint32_t *result)
+{
+    return ucp_atomic_cswap_b(ep, compare, swap, sizeof(swap), remote_addr,
+                              rkey, result, "atomic_cswap32");
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_cswap64,
+                 (ep, compare, swap, remote_addr, rkey, result),
+                 ucp_ep_h ep, uint64_t compare, uint64_t swap,
+                 uint64_t remote_addr, ucp_rkey_h rkey, uint64_t *result)
+{
+    return ucp_atomic_cswap_b(ep, compare, swap, sizeof(swap), remote_addr,
+                              rkey, result, "atomic_cswap64");
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_sw.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_sw.c
new file mode 100644
index 000000000..f9ec1f434
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/amo_sw.c
@@ -0,0 +1,292 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "rma.h"
+#include "rma.inl"
+
+#include <ucs/arch/atomic.h>
+#include <ucs/profile/profile.h>
+
+
+static size_t ucp_amo_sw_pack(void *dest, void *arg, uint8_t fetch)
+{
+    ucp_request_t *req            = arg;
+    ucp_atomic_req_hdr_t *atomich = dest;
+    ucp_ep_t *ep                  = req->send.ep;
+    size_t size                   = req->send.length;
+    size_t length;
+
+    atomich->address    = req->send.rma.remote_addr;
+    atomich->req.ep_ptr = ucp_ep_dest_ep_ptr(ep);
+    atomich->req.reqptr = fetch ? (uintptr_t)req : 0;
+    atomich->length     = size;
+    atomich->opcode     = req->send.amo.uct_op;
+
+    memcpy(atomich + 1, &req->send.amo.value, size);
+    length = sizeof(*atomich) + size;
+
+    if (req->send.amo.uct_op == UCT_ATOMIC_OP_CSWAP) {
+        /* compare-swap has two arguments */
+        memcpy((void*)(atomich + 1) + size, req->send.buffer, size);
+        length += size;
+    }
+
+    return length;
+}
+
+static size_t ucp_amo_sw_post_pack_cb(void *dest, void *arg)
+{
+    return ucp_amo_sw_pack(dest, arg, 0);
+}
+
+static size_t ucp_amo_sw_fetch_pack_cb(void *dest, void *arg)
+{
+    return ucp_amo_sw_pack(dest, arg, 1);
+}
+
+static ucs_status_t ucp_amo_sw_progress(uct_pending_req_t *self,
+                                        uct_pack_callback_t pack_cb, int fetch)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ucs_status_t status;
+    ssize_t packed_len;
+
+    req->send.lane = ucp_ep_get_am_lane(ep);
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane],
+                                 UCP_AM_ID_ATOMIC_REQ, pack_cb, req, 0);
+    if (packed_len > 0) {
+        ucp_ep_rma_remote_request_sent(ep);
+        if (!fetch) {
+            ucp_request_complete_send(req, UCS_OK);
+        }
+        return UCS_OK;
+    } else {
+        status = (ucs_status_t)packed_len;
+        if (status != UCS_ERR_NO_RESOURCE) {
+            /* failure */
+            ucp_request_complete_send(req, status);
+        }
+        return status;
+    }
+}
+
+static ucs_status_t ucp_amo_sw_progress_post(uct_pending_req_t *self)
+{
+    return ucp_amo_sw_progress(self, ucp_amo_sw_post_pack_cb, 0);
+}
+
+static ucs_status_t ucp_amo_sw_progress_fetch(uct_pending_req_t *self)
+{
+    return ucp_amo_sw_progress(self, ucp_amo_sw_fetch_pack_cb, 1);
+}
+
+ucp_amo_proto_t ucp_amo_sw_proto = {
+    .name           = "sw_amo",
+    .progress_fetch = ucp_amo_sw_progress_fetch,
+    .progress_post  = ucp_amo_sw_progress_post
+};
+
+static size_t ucp_amo_sw_pack_atomic_reply(void *dest, void *arg)
+{
+    ucp_rma_rep_hdr_t *hdr = dest;
+    ucp_request_t *req     = arg;
+
+    hdr->req = req->send.get_reply.req;
+
+    switch (req->send.length) {
+    case sizeof(uint32_t):
+        *(uint32_t*)(hdr + 1) = req->send.atomic_reply.data.reply32;
+        break;
+    case sizeof(uint64_t):
+        *(uint64_t*)(hdr + 1) = req->send.atomic_reply.data.reply64;
+        break;
+    default:
+        ucs_fatal("invalid atomic length: %zu", req->send.length);
+    }
+
+    return sizeof(*hdr) + req->send.length;
+}
+
+static ucs_status_t ucp_progress_atomic_reply(uct_pending_req_t *self)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ssize_t packed_len;
+
+    req->send.lane = ucp_ep_get_am_lane(ep);
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_ATOMIC_REP,
+                                 ucp_amo_sw_pack_atomic_reply, req, 0);
+
+    if (packed_len < 0) {
+        return (ucs_status_t)packed_len;
+    }
+
+    ucs_assert(packed_len == sizeof(ucp_rma_rep_hdr_t) + req->send.length);
+    ucp_request_put(req);
+    return UCS_OK;
+}
+
+#define DEFINE_AMO_SW_OP(_bits) \
+    static void ucp_amo_sw_do_op##_bits(const ucp_atomic_req_hdr_t *atomicreqh) \
+    { \
+        uint##_bits##_t *ptr  = (void*)atomicreqh->address; \
+        uint##_bits##_t *args = (void*)(atomicreqh + 1); \
+        \
+       switch (atomicreqh->opcode) { \
+        case UCT_ATOMIC_OP_ADD: \
+            ucs_atomic_add##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_AND: \
+            ucs_atomic_and##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_OR: \
+            ucs_atomic_or##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_XOR: \
+            ucs_atomic_xor##_bits(ptr, args[0]); \
+            break; \
+        default: \
+            ucs_fatal("invalid opcode: %d", atomicreqh->opcode); \
+        } \
+    }
+
+#define DEFINE_AMO_SW_FOP(_bits) \
+    static void ucp_amo_sw_do_fop##_bits(const ucp_atomic_req_hdr_t *atomicreqh, \
+                                         ucp_atomic_reply_t *result) \
+    { \
+        uint##_bits##_t *ptr  = (void*)atomicreqh->address; \
+        uint##_bits##_t *args = (void*)(atomicreqh + 1); \
+        \
+        switch (atomicreqh->opcode) { \
+        case UCT_ATOMIC_OP_ADD: \
+            result->reply##_bits = ucs_atomic_fadd##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_AND: \
+            result->reply##_bits = ucs_atomic_fand##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_OR: \
+            result->reply##_bits = ucs_atomic_for##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_XOR: \
+            result->reply##_bits = ucs_atomic_fxor##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_SWAP: \
+            result->reply##_bits = ucs_atomic_swap##_bits(ptr, args[0]); \
+            break; \
+        case UCT_ATOMIC_OP_CSWAP: \
+            result->reply##_bits = ucs_atomic_cswap##_bits(ptr, args[0], args[1]); \
+            break; \
+        default: \
+            ucs_fatal("invalid opcode: %d", atomicreqh->opcode); \
+        } \
+    }
+
+DEFINE_AMO_SW_OP(32)
+DEFINE_AMO_SW_OP(64)
+DEFINE_AMO_SW_FOP(32)
+DEFINE_AMO_SW_FOP(64)
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_req_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_atomic_req_hdr_t *atomicreqh = data;
+    ucp_worker_h worker              = arg;
+    ucp_ep_h ep                      = ucp_worker_get_ep_by_ptr(worker,
+                                                                atomicreqh->req.ep_ptr);
+    ucp_request_t *req;
+
+    if (atomicreqh->req.reqptr == 0) {
+        /* atomic operation without result */
+        switch (atomicreqh->length) {
+        case sizeof(uint32_t):
+            ucp_amo_sw_do_op32(atomicreqh);
+            break;
+        case sizeof(uint64_t):
+            ucp_amo_sw_do_op64(atomicreqh);
+            break;
+        default:
+            ucs_fatal("invalid atomic length: %u", atomicreqh->length);
+        }
+        ucp_rma_sw_send_cmpl(ep);
+    } else {
+        /* atomic operation with result */
+        req = ucp_request_get(worker);
+        ucs_assert(req != NULL);
+
+        switch (atomicreqh->length) {
+        case sizeof(uint32_t):
+            ucp_amo_sw_do_fop32(atomicreqh, &req->send.atomic_reply.data);
+            break;
+        case sizeof(uint64_t):
+            ucp_amo_sw_do_fop64(atomicreqh, &req->send.atomic_reply.data);
+            break;
+        default:
+            ucs_fatal("invalid atomic length: %u", atomicreqh->length);
+        }
+
+        req->send.ep               = ep;
+        req->send.atomic_reply.req = atomicreqh->req.reqptr;
+        req->send.length           = atomicreqh->length;
+        req->send.uct.func         = ucp_progress_atomic_reply;
+        ucp_request_send(req);
+    }
+
+    return UCS_OK;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_atomic_rep_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_rma_rep_hdr_t *hdr = data;
+    size_t frag_length     = length - sizeof(*hdr);
+    ucp_request_t *req     = (ucp_request_t*)hdr->req;
+    ucp_ep_h ep            = req->send.ep;
+
+    memcpy(req->send.buffer, hdr + 1, frag_length);
+    ucp_request_complete_send(req, UCS_OK);
+    ucp_ep_rma_remote_request_completed(ep);
+    return UCS_OK;
+}
+
+static void ucp_amo_sw_dump_packet(ucp_worker_h worker, uct_am_trace_type_t type,
+                                   uint8_t id, const void *data, size_t length,
+                                   char *buffer, size_t max)
+{
+    const ucp_atomic_req_hdr_t *atomich;
+    const ucp_rma_rep_hdr_t *reph;
+    size_t header_len;
+    char *p;
+
+    switch (id) {
+    case UCP_AM_ID_ATOMIC_REQ:
+        atomich = data;
+        snprintf(buffer, max,
+                 "ATOMIC_REQ [addr 0x%lx len %u reqptr 0x%lx ep 0x%lx op %d]",
+                 atomich->address, atomich->length, atomich->req.reqptr,
+                 atomich->req.ep_ptr, atomich->opcode);
+        header_len = sizeof(*atomich);;
+        break;
+    case UCP_AM_ID_ATOMIC_REP:
+        reph = data;
+        snprintf(buffer, max, "ATOMIC_REP [reqptr 0x%lx]", reph->req);
+        header_len = sizeof(*reph);
+        break;
+    default:
+        return;
+    }
+
+    p = buffer + strlen(buffer);
+    ucp_dump_payload(worker->context, p, buffer + max - p, data + header_len,
+                     length - header_len);
+}
+
+UCP_DEFINE_AM(UCP_FEATURE_AMO, UCP_AM_ID_ATOMIC_REQ, ucp_atomic_req_handler,
+              ucp_amo_sw_dump_packet, 0);
+UCP_DEFINE_AM(UCP_FEATURE_AMO, UCP_AM_ID_ATOMIC_REP, ucp_atomic_rep_handler,
+              ucp_amo_sw_dump_packet, 0);
+
+UCP_DEFINE_AM_PROXY(UCP_AM_ID_ATOMIC_REQ);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/basic_rma.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/basic_rma.c
deleted file mode 100644
index 68b28cf0c..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/basic_rma.c
+++ /dev/null
@@ -1,425 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
-* Copyright (c) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#include <ucp/core/ucp_mm.h>
-
-#include <ucp/core/ucp_ep.h>
-#include <ucp/core/ucp_worker.h>
-#include <ucp/core/ucp_context.h>
-#include <ucp/dt/dt_contig.h>
-#include <ucs/debug/profile.h>
-
-#include <ucp/proto/proto_am.inl>
-#include <ucs/datastruct/mpool.inl>
-
-
-#define UCP_RMA_CHECK_PARAMS(_buffer, _length) \
-    if ((_length) == 0) { \
-        return UCS_OK; \
-    } \
-    if (ENABLE_PARAMS_CHECK && ((_buffer) == NULL)) { \
-        return UCS_ERR_INVALID_PARAM; \
-    }
-
-/* request can be released if 
- *  - all fragments were sent (length == 0) (bcopy & zcopy mix)
- *  - all zcopy fragments are done (uct_comp.count == 0)
- *  - and request was allocated from the mpool 
- *    (checked in ucp_request_complete_send)
- *
- * Request can be released either immediately or in the completion callback.
- * We must check req length in the completion callback to avoid the following
- * scenario:
- *  partial_send;no_resos;progress;
- *  send_completed;cb called;req free(ooops);
- *  next_partial_send; (oops req already freed)
- */
-static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_rma_request_advance(ucp_request_t *req, ssize_t frag_length,
-                        ucs_status_t status)
-{
-    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
-        if (status != UCS_ERR_NO_RESOURCE) {
-            ucp_request_send_buffer_dereg(req);
-            ucp_request_complete_send(req, status);
-        }
-        return status;
-    }
-
-    req->send.length -= frag_length;
-    if (req->send.length == 0) {
-        /* bcopy is the fast path */
-        if (ucs_likely(req->send.state.uct_comp.count == 0)) {
-            ucp_request_send_buffer_dereg(req);
-            ucp_request_complete_send(req, UCS_OK);
-        }
-        return UCS_OK;
-    }
-    req->send.buffer          += frag_length;
-    req->send.rma.remote_addr += frag_length;
-    return UCS_INPROGRESS;
-}
-
-static void ucp_rma_request_bcopy_completion(uct_completion_t *self,
-                                             ucs_status_t status)
-{
-    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
-                                          send.state.uct_comp);
-
-    if (ucs_likely(req->send.length == 0)) {
-        ucp_request_complete_send(req, UCS_OK);
-    }
-}
-
-static void ucp_rma_request_zcopy_completion(uct_completion_t *self,
-                                             ucs_status_t status)
-{
-    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
-                                          send.state.uct_comp);
-
-    if (ucs_likely(req->send.length == 0)) {
-        ucp_request_send_buffer_dereg(req);
-        ucp_request_complete_send(req, UCS_OK);
-    }
-}
-
-static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_rma_request_init(ucp_request_t *req, ucp_ep_h ep, const void *buffer, 
-                     size_t length, uint64_t remote_addr, ucp_rkey_h rkey,
-                     uct_pending_callback_t cb, size_t zcopy_thresh, int flags)
-{
-    req->flags                = flags; /* Implicit release */
-    req->send.ep              = ep;
-    req->send.buffer          = buffer;
-    req->send.datatype        = ucp_dt_make_contig(1);
-    req->send.length          = length;
-    req->send.rma.remote_addr = remote_addr;
-    req->send.rma.rkey        = rkey;
-    req->send.uct.func        = cb;
-    req->send.lane            = rkey->cache.rma_lane;
-    ucp_request_send_state_init(req, ucp_dt_make_contig(1), length);
-    ucp_request_send_state_reset(req,
-                                 (length < zcopy_thresh) ?
-                                 ucp_rma_request_bcopy_completion :
-                                 ucp_rma_request_zcopy_completion,
-                                 UCP_REQUEST_SEND_PROTO_RMA);
-#if ENABLE_ASSERT
-    req->send.cb              = NULL;
-#endif
-    if (length < zcopy_thresh) {
-        return UCS_OK;
-    }
-
-    return ucp_request_send_buffer_reg_lane(req, req->send.lane);
-}
-
-static ucs_status_t ucp_progress_put(uct_pending_req_t *self)
-{
-    ucp_request_t *req              = ucs_container_of(self, ucp_request_t, send.uct);
-    ucp_ep_t *ep                    = req->send.ep;
-    ucp_rkey_h rkey                 = req->send.rma.rkey;
-    ucp_lane_index_t lane           = req->send.lane;
-    ucp_ep_rma_config_t *rma_config = &ucp_ep_config(ep)->rma[lane];
-    ucs_status_t status;
-    ssize_t packed_len;
-
-    ucs_assert(rkey->cache.ep_cfg_index == ep->cfg_index);
-    ucs_assert(rkey->cache.rma_lane == lane);
-
-    if (req->send.length <= ucp_ep_config(ep)->bcopy_thresh) {
-        packed_len = ucs_min(req->send.length, rma_config->max_put_short);
-        status = UCS_PROFILE_CALL(uct_ep_put_short,
-                                  ep->uct_eps[lane],
-                                  req->send.buffer,
-                                  packed_len,
-                                  req->send.rma.remote_addr,
-                                  rkey->cache.rma_rkey);
-    } else if (ucs_likely(req->send.length < rma_config->put_zcopy_thresh)) {
-        ucp_memcpy_pack_context_t pack_ctx;
-        pack_ctx.src    = req->send.buffer;
-        pack_ctx.length = ucs_min(req->send.length, rma_config->max_put_bcopy);
-        packed_len = UCS_PROFILE_CALL(uct_ep_put_bcopy,
-                                      ep->uct_eps[lane],
-                                      ucp_memcpy_pack,
-                                      &pack_ctx,
-                                      req->send.rma.remote_addr,
-                                      rkey->cache.rma_rkey);
-        status = (packed_len > 0) ? UCS_OK : (ucs_status_t)packed_len;
-    } else {
-        uct_iov_t iov;
-
-        /* TODO: leave last fragment for bcopy */
-        packed_len = ucs_min(req->send.length, rma_config->max_put_zcopy);
-        /* TODO: use ucp_dt_iov_copy_uct */
-        iov.buffer = (void *)req->send.buffer;
-        iov.length = packed_len;
-        iov.count  = 1;
-        iov.memh   = req->send.state.dt.dt.contig.memh[0];
-
-        status = UCS_PROFILE_CALL(uct_ep_put_zcopy,
-                                  ep->uct_eps[lane],
-                                  &iov, 1, 
-                                  req->send.rma.remote_addr,
-                                  rkey->cache.rma_rkey,
-                                  &req->send.state.uct_comp);
-        ucp_request_send_state_advance(req, NULL, UCP_REQUEST_SEND_PROTO_RMA,
-                                       status);
-    }
-
-    return ucp_rma_request_advance(req, packed_len, status);
-}
-
-static ucs_status_t ucp_progress_get(uct_pending_req_t *self)
-{
-    ucp_request_t *req              = ucs_container_of(self, ucp_request_t, send.uct);
-    ucp_ep_t *ep                    = req->send.ep;
-    ucp_rkey_h rkey                 = req->send.rma.rkey;
-    ucp_lane_index_t lane           = req->send.lane;
-    ucp_ep_rma_config_t *rma_config = &ucp_ep_config(ep)->rma[lane];
-    ucs_status_t status;
-    size_t frag_length;
-
-    ucs_assert(rkey->cache.ep_cfg_index == ep->cfg_index);
-    ucs_assert(rkey->cache.rma_lane == lane);
-
-    if (ucs_likely(req->send.length < rma_config->get_zcopy_thresh)) {
-        frag_length = ucs_min(rma_config->max_get_bcopy, req->send.length);
-        status = UCS_PROFILE_CALL(uct_ep_get_bcopy,
-                                  ep->uct_eps[lane],
-                                  (uct_unpack_callback_t)memcpy,
-                                  (void*)req->send.buffer,
-                                  frag_length,
-                                  req->send.rma.remote_addr,
-                                  rkey->cache.rma_rkey,
-                                  &req->send.state.uct_comp);
-    } else {
-        uct_iov_t iov;
-        frag_length = ucs_min(req->send.length, rma_config->max_get_zcopy);
-        iov.buffer  = (void *)req->send.buffer;
-        iov.length  = frag_length;
-        iov.count   = 1;
-        iov.memh    = req->send.state.dt.dt.contig.memh[0];
-
-        status = UCS_PROFILE_CALL(uct_ep_get_zcopy,
-                                  ep->uct_eps[lane],
-                                  &iov, 1, 
-                                  req->send.rma.remote_addr,
-                                  rkey->cache.rma_rkey,
-                                  &req->send.state.uct_comp);
-    }
-
-    if (status == UCS_INPROGRESS) {
-        ucp_request_send_state_advance(req, 0, UCP_REQUEST_SEND_PROTO_RMA,
-                                       UCS_INPROGRESS);
-    }
-
-    return ucp_rma_request_advance(req, frag_length, status);
-}
-
-static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_rma_blocking(ucp_ep_h ep, const void *buffer, size_t length,
-                 uint64_t remote_addr, ucp_rkey_h rkey,
-                 uct_pending_callback_t progress_cb, size_t zcopy_thresh)
-{
-    ucs_status_t status;
-    ucp_request_t req;
-
-    status = ucp_rma_request_init(&req, ep, buffer, length, remote_addr, rkey,
-                                  NULL, zcopy_thresh, 0);
-    if (ucs_unlikely(status != UCS_OK)) {
-        return status;
-    }
-
-    /* Loop until all message has been sent.
-     * We re-check the configuration on every iteration except for zcopy, 
-     * because it can be * changed by transport switch.
-     */
-    for (;;) {
-        /* coverity[callee_ptr_arith] */
-        status = progress_cb(&req.send.uct);
-        if (ucs_likely(status == UCS_OK)) {
-            break;
-        } else if (status == UCS_INPROGRESS) {
-            continue;
-        } else if (status != UCS_ERR_NO_RESOURCE) {
-            break;
-        } else {
-            ucp_worker_progress(ep->worker);
-        }
-    }
-
-    ucp_request_wait_uct_comp(&req);
-    return status;
-}
-
-static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_rma_nonblocking(ucp_ep_h ep, const void *buffer, size_t length,
-                    uint64_t remote_addr, ucp_rkey_h rkey,
-                    uct_pending_callback_t progress_cb, size_t zcopy_thresh)
-{
-    ucs_status_t status;
-    ucp_request_t *req;
-
-    req = ucp_request_get(ep->worker);
-    if (req == NULL) {
-        return UCS_ERR_NO_MEMORY;
-    }
-
-    status = ucp_rma_request_init(req, ep, buffer, length, remote_addr, rkey,
-                                  progress_cb, zcopy_thresh,
-                                  UCP_REQUEST_FLAG_RELEASED);
-    if (ucs_unlikely(status != UCS_OK)) {
-        return status;
-    }
-
-    return ucp_request_send(req);
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_put, (ep, buffer, length, remote_addr, rkey),
-                 ucp_ep_h ep, const void *buffer, size_t length,
-                 uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    ucp_ep_rma_config_t *rma_config;
-    ucs_status_t status;
-
-    UCP_RMA_CHECK_PARAMS(buffer, length);
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
-    if (status != UCS_OK) {
-        goto out_unlock;
-    }
-
-    if (ucs_likely(length <= rkey->cache.max_put_short)) {
-        do {
-            /* testing shows that for put message rate it is better to finish
-             * put_short here instead of doing it once, getting NO_RESOURCE 
-             * and continuing to ucp_rma_blocking()
-             */
-            status = UCS_PROFILE_CALL(uct_ep_put_short, ep->uct_eps[rkey->cache.rma_lane],
-                                      buffer, length, remote_addr, rkey->cache.rma_rkey);
-            if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
-                goto out_unlock;
-            }
-
-            ucp_worker_progress(ep->worker);
-
-            status = UCP_RKEY_RESOLVE(rkey, ep, rma);
-            if (status != UCS_OK) {
-                goto out_unlock;
-            }
-        } while (1);
-    }
-
-    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
-    status = ucp_rma_blocking(ep, buffer, length, remote_addr, rkey,
-                              ucp_progress_put, rma_config->put_zcopy_thresh);
-out_unlock:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_get, (ep, buffer, length, remote_addr, rkey),
-                 ucp_ep_h ep, void *buffer, size_t length,
-                 uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    ucp_ep_rma_config_t *rma_config;
-    ucs_status_t status;
-
-    UCP_RMA_CHECK_PARAMS(buffer, length);
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
-    if (status != UCS_OK) {
-        goto out_unlock;
-    }
-
-    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
-    status = ucp_rma_blocking(ep, buffer, length, remote_addr, rkey, 
-                              ucp_progress_get, rma_config->get_zcopy_thresh);
-out_unlock:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
-
-ucs_status_t ucp_put_nbi(ucp_ep_h ep, const void *buffer, size_t length,
-                         uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    ucp_ep_rma_config_t *rma_config;
-    ucs_status_t status;
-
-    UCP_RMA_CHECK_PARAMS(buffer, length);
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
-    if (status != UCS_OK) {
-        goto out_unlock;
-    }
-
-    /* Fast path for a single short message */
-    if (ucs_likely(length <= rkey->cache.max_put_short)) {
-        status = UCS_PROFILE_CALL(uct_ep_put_short, ep->uct_eps[rkey->cache.rma_lane],
-                                  buffer, length, remote_addr, rkey->cache.rma_rkey);
-        if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
-            goto out_unlock;
-        }
-    }
-
-    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
-    status = ucp_rma_nonblocking(ep, buffer, length, remote_addr, rkey,
-                                 ucp_progress_put, rma_config->put_zcopy_thresh);
-out_unlock:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
-
-ucs_status_t ucp_get_nbi(ucp_ep_h ep, void *buffer, size_t length,
-                         uint64_t remote_addr, ucp_rkey_h rkey)
-{
-    ucp_ep_rma_config_t *rma_config;
-    ucs_status_t status;
-
-    UCP_RMA_CHECK_PARAMS(buffer, length);
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
-    if (status != UCS_OK) {
-        goto out_unlock;
-    }
-
-    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
-    status = ucp_rma_nonblocking(ep, buffer, length, remote_addr, rkey,
-                         ucp_progress_get, rma_config->get_zcopy_thresh);
-out_unlock:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
-    return status;
-}
-
-UCS_PROFILE_FUNC(ucs_status_t, ucp_worker_fence, (worker), ucp_worker_h worker)
-{
-    unsigned rsc_index;
-    ucs_status_t status;
-
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
-
-    for (rsc_index = 0; rsc_index < worker->context->num_tls; ++rsc_index) {
-        if (worker->ifaces[rsc_index].iface == NULL) {
-            continue;
-        }
-
-        status = uct_iface_fence(worker->ifaces[rsc_index].iface, 0);
-        if (status != UCS_OK) {
-            goto out;
-        }
-    }
-    status = UCS_OK;
-
-out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
-    return status;
-}
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/flush.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/flush.c
index 71b6b7452..05c8ceabc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/flush.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/flush.c
@@ -8,6 +8,8 @@
 #include <ucp/core/ucp_ep.inl>
 #include <ucp/core/ucp_request.inl>
 
+#include "rma.inl"
+
 
 static void ucp_ep_flush_error(ucp_request_t *req, ucs_status_t status)
 {
@@ -19,9 +21,15 @@ static void ucp_ep_flush_error(ucp_request_t *req, ucs_status_t status)
     --req->send.state.uct_comp.count;
 }
 
+static int ucp_ep_flush_is_completed(ucp_request_t *req)
+{
+    return (req->send.state.uct_comp.count == 0) && req->send.flush.sw_done;
+}
+
 static void ucp_ep_flush_progress(ucp_request_t *req)
 {
     ucp_ep_h ep = req->send.ep;
+    ucp_ep_flush_state_t *flush_state;
     ucp_lane_index_t lane;
     ucs_status_t status;
     uct_ep_h uct_ep;
@@ -52,6 +60,8 @@ static void ucp_ep_flush_progress(ucp_request_t *req)
         if (status == UCS_OK) {
             req->send.flush.lanes &= ~UCS_BIT(lane);
             --req->send.state.uct_comp.count;
+            ucs_trace("ep %p: flush comp %p count reduced to %d", ep,
+                      &req->send.state.uct_comp, req->send.state.uct_comp.count);
         } else if (status == UCS_INPROGRESS) {
             req->send.flush.lanes &= ~UCS_BIT(lane);
         } else if (status == UCS_ERR_NO_RESOURCE) {
@@ -62,7 +72,7 @@ static void ucp_ep_flush_progress(ucp_request_t *req)
                 break;
             }
 
-            status = uct_ep_pending_add(uct_ep, &req->send.uct);
+            status = uct_ep_pending_add(uct_ep, &req->send.uct, 0);
             ucs_trace("adding pending flush on ep %p lane[%d]: %s", ep, lane,
                       ucs_status_string(status));
             if (status == UCS_OK) {
@@ -77,6 +87,36 @@ static void ucp_ep_flush_progress(ucp_request_t *req)
             break;
         }
     }
+
+    if (!req->send.flush.sw_started && (req->send.state.uct_comp.count == 0)) {
+        /* Start waiting for remote completions only after all lanes are flushed
+         * on the transport level, so we are sure all pending requests were sent.
+         * We don't need to wait for remote completions in these cases:
+         * - The flush operation is in 'cancel' mode
+         * - The endpoint is either not used or did not resolve the peer endpoint,
+         *   which means we didn't have any user operations which require remote
+         *   completion. In this case, the flush state may not even be initialized.
+         */
+        if ((req->send.flush.uct_flags & UCT_FLUSH_FLAG_CANCEL) ||
+            !ucs_test_all_flags(ep->flags, UCP_EP_FLAG_USED|UCP_EP_FLAG_DEST_EP)) {
+            ucs_trace_req("flush request %p not waiting for remote completions",
+                          req);
+            req->send.flush.sw_done = 1;
+        } else {
+            /* All pending requests were sent, so 'send_sn' value is up-to-date */
+            flush_state = ucp_ep_flush_state(ep);
+            if (flush_state->send_sn == flush_state->cmpl_sn) {
+                req->send.flush.sw_done = 1;
+                ucs_trace_req("flush request %p remote completions done", req);
+            } else {
+                req->send.flush.cmpl_sn = flush_state->send_sn;
+                ucs_queue_push(&flush_state->reqs, &req->send.flush.queue);
+                ucs_trace_req("added flush request %p to ep remote completion queue"
+                              " with sn %d", req, req->send.flush.cmpl_sn);
+            }
+        }
+        req->send.flush.sw_started = 1;
+    }
 }
 
 static void ucp_ep_flush_slow_path_remove(ucp_request_t *req)
@@ -89,10 +129,11 @@ static void ucp_ep_flush_slow_path_remove(ucp_request_t *req)
 static int ucp_flush_check_completion(ucp_request_t *req)
 {
     /* Check if flushed all lanes */
-    if (req->send.state.uct_comp.count != 0) {
+    if (!ucp_ep_flush_is_completed(req)) {
         return 0;
     }
 
+    ucs_trace_req("flush req %p completed", req);
     ucp_ep_flush_slow_path_remove(req);
     req->send.flush.flushed_cb(req);
     return 1;
@@ -141,7 +182,10 @@ static ucs_status_t ucp_ep_flush_progress_pending(uct_pending_req_t *self)
     }
 
     if ((status == UCS_OK) || (status == UCS_INPROGRESS)) {
-        req->send.lane = UCP_NULL_LANE;
+        /* flushed callback might release the request */
+        if (!completed) {
+            req->send.lane = UCP_NULL_LANE;
+        }
         return UCS_OK;
     } else if (status == UCS_ERR_NO_RESOURCE) {
         return UCS_ERR_NO_RESOURCE;
@@ -156,7 +200,7 @@ static void ucp_ep_flush_completion(uct_completion_t *self, ucs_status_t status)
     ucp_request_t *req = ucs_container_of(self, ucp_request_t,
                                           send.state.uct_comp);
 
-    ucs_trace("flush completion req=%p status=%d", req, status);
+    ucs_trace_req("flush completion req=%p status=%d", req, status);
 
     ucs_assert(!(req->flags & UCP_REQUEST_FLAG_COMPLETED));
 
@@ -165,27 +209,43 @@ static void ucp_ep_flush_completion(uct_completion_t *self, ucs_status_t status)
     if (status == UCS_OK) {
         ucp_ep_flush_progress(req);
     } else {
+        /* force flush completion in case of error */
+        req->send.flush.sw_done        = 1;
         req->send.state.uct_comp.count = 0;
     }
 
+
+    ucs_trace_req("flush completion req=%p comp_count=%d", req, req->send.state.uct_comp.count);
     ucp_flush_check_completion(req);
 }
 
+void ucp_ep_flush_remote_completed(ucp_request_t *req)
+{
+    ucs_trace_req("flush remote ops completed req=%p", req);
+
+    if (!req->send.flush.sw_done) {
+        req->send.flush.sw_done = 1;
+        ucp_flush_check_completion(req);
+    }
+}
+
 ucs_status_ptr_t ucp_ep_flush_internal(ucp_ep_h ep, unsigned uct_flags,
                                        ucp_send_callback_t req_cb,
                                        unsigned req_flags,
-                                       ucp_request_callback_t flushed_cb)
+                                       ucp_request_t *worker_req,
+                                       ucp_request_callback_t flushed_cb,
+                                       const char *debug_name)
 {
     ucs_status_t status;
     ucp_request_t *req;
 
-    ucs_debug("disconnect ep %p", ep);
+    ucs_debug("%s ep %p", debug_name, ep);
 
     if (ep->flags & UCP_EP_FLAG_FAILED) {
         return NULL;
     }
 
-    req = ucs_mpool_get(&ep->worker->req_mp);
+    req = ucp_request_get(ep->worker);
     if (req == NULL) {
         return UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
     }
@@ -206,6 +266,9 @@ ucs_status_ptr_t ucp_ep_flush_internal(ucp_ep_h ep, unsigned uct_flags,
     req->send.flush.lanes       = UCS_MASK(ucp_ep_num_lanes(ep));
     req->send.flush.prog_id     = UCS_CALLBACKQ_ID_NULL;
     req->send.flush.uct_flags   = uct_flags;
+    req->send.flush.worker_req  = worker_req;
+    req->send.flush.sw_started  = 0;
+    req->send.flush.sw_done     = 0;
 
     req->send.lane              = UCP_NULL_LANE;
     req->send.uct.func          = ucp_ep_flush_progress_pending;
@@ -214,11 +277,11 @@ ucs_status_ptr_t ucp_ep_flush_internal(ucp_ep_h ep, unsigned uct_flags,
 
     ucp_ep_flush_progress(req);
 
-    if (req->send.state.uct_comp.count == 0) {
+    if (ucp_ep_flush_is_completed(req)) {
         status = req->status;
         ucs_trace_req("ep %p: releasing flush request %p, returning status %s",
                       ep, req, ucs_status_string(status));
-        ucs_mpool_put(req);
+        ucp_request_put(req);
         return UCS_STATUS_PTR(status);
     }
 
@@ -237,13 +300,13 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_ep_flush_nb, (ep, flags, cb),
 {
     void *request;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
-    request = ucp_ep_flush_internal(ep, UCT_FLUSH_FLAG_LOCAL,
-                                    cb, UCP_REQUEST_FLAG_CALLBACK,
-                                    ucp_ep_flushed_callback);
+    request = ucp_ep_flush_internal(ep, UCT_FLUSH_FLAG_LOCAL, cb,
+                                    UCP_REQUEST_FLAG_CALLBACK, NULL,
+                                    ucp_ep_flushed_callback, "flush_nb");
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
 
     return request;
 }
@@ -253,7 +316,7 @@ static ucs_status_t ucp_worker_flush_check(ucp_worker_h worker)
     ucs_status_t status;
     unsigned rsc_index;
 
-    if (worker->wireup_pend_count > 0) {
+    if (worker->flush_ops_count) {
         return UCS_INPROGRESS;
     }
 
@@ -264,6 +327,12 @@ static ucs_status_t ucp_worker_flush_check(ucp_worker_h worker)
 
         status = uct_iface_flush(worker->ifaces[rsc_index].iface, 0, NULL);
         if (status != UCS_OK) {
+            if (UCS_STATUS_IS_ERR(status)) {
+                ucs_error("iface[%d] "UCT_TL_RESOURCE_DESC_FMT" flush failed: %s",
+                          rsc_index,
+                          UCT_TL_RESOURCE_DESC_ARG(&worker->context->tl_rscs[rsc_index].tl_rsc),
+                          ucs_status_string(status));
+            }
             return status;
         }
     }
@@ -271,19 +340,73 @@ static ucs_status_t ucp_worker_flush_check(ucp_worker_h worker)
     return UCS_OK;
 }
 
-static unsigned ucp_worker_flush_progress(void *arg)
+static void ucp_worker_flush_complete_one(ucp_request_t *req, ucs_status_t status,
+                                          int force_progress_unreg)
 {
-    ucp_request_t *req  = arg;
     ucp_worker_h worker = req->flush_worker.worker;
+    int complete;
+
+    --req->flush_worker.comp_count;
+    complete = (req->flush_worker.comp_count == 0) || (status != UCS_OK);
+
+    if (complete || force_progress_unreg) {
+        uct_worker_progress_unregister_safe(worker->uct,
+                                            &req->flush_worker.prog_id);
+    }
+
+    if (complete) {
+        ucs_assert(status != UCS_INPROGRESS);
+        ucp_request_complete(req, flush_worker.cb, status);
+    }
+}
+
+static void ucp_worker_flush_ep_flushed_cb(ucp_request_t *req)
+{
+    ucp_worker_flush_complete_one(req->send.flush.worker_req, UCS_OK, 0);
+    ucp_request_put(req);
+}
+
+static unsigned ucp_worker_flush_progress(void *arg)
+{
+    ucp_request_t *req        = arg;
+    ucp_worker_h worker       = req->flush_worker.worker;
+    ucp_ep_ext_gen_t *next_ep = req->flush_worker.next_ep;
+    void *ep_flush_request;
     ucs_status_t status;
+    ucp_ep_h ep;
 
     status = ucp_worker_flush_check(worker);
-    if ((status == UCS_INPROGRESS) || (status == UCS_ERR_NO_RESOURCE)) {
-        return 0;
+    if ((status == UCS_OK) || (&next_ep->ep_list == &worker->all_eps)) {
+        /* If all ifaces are flushed, or we finished going over all endpoints,
+         * no need to progress this request actively any more. Just wait until
+         * all associated endpoint flush requests are completed.
+         */
+        ucp_worker_flush_complete_one(req, UCS_OK, 1);
+    } else if (status != UCS_INPROGRESS) {
+        /* Error returned from uct iface flush */
+        ucp_worker_flush_complete_one(req, status, 1);
+    } else if (worker->context->config.ext.flush_worker_eps) {
+        /* Some endpoints are not flushed yet. Take next endpoint from the list
+         * and start flush operation on it.
+         */
+        ep                        = ucp_ep_from_ext_gen(next_ep);
+        req->flush_worker.next_ep = ucs_list_next(&next_ep->ep_list,
+                                                  ucp_ep_ext_gen_t, ep_list);
+
+        ep_flush_request = ucp_ep_flush_internal(ep, UCT_FLUSH_FLAG_LOCAL, NULL,
+                                                 UCP_REQUEST_FLAG_RELEASED, req,
+                                                 ucp_worker_flush_ep_flushed_cb,
+                                                 "flush_worker");
+        if (UCS_PTR_IS_ERR(ep_flush_request)) {
+            /* endpoint flush resulted in an error */
+            status = UCS_PTR_STATUS(ep_flush_request);
+            ucs_warn("ucp_ep_flush_internal() failed: %s", ucs_status_string(status));
+        } else if (ep_flush_request != NULL) {
+            /* endpoint flush started, increment refcount */
+            ++req->flush_worker.comp_count;
+        }
     }
 
-    uct_worker_progress_unregister_safe(worker->uct, &req->flush_worker.prog_id);
-    ucp_request_complete(req, flush_worker.cb, status);
     return 0;
 }
 
@@ -299,16 +422,20 @@ static ucs_status_ptr_t ucp_worker_flush_nb_internal(ucp_worker_h worker,
         return UCS_STATUS_PTR(status);
     }
 
-    req = ucs_mpool_get(&worker->req_mp);
+    req = ucp_request_get(worker);
     if (req == NULL) {
         return UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
     }
 
-    req->flags                = req_flags;
-    req->status               = UCS_OK;
-    req->flush_worker.worker  = worker;
-    req->flush_worker.cb      = cb;
-    req->flush_worker.prog_id = UCS_CALLBACKQ_ID_NULL;
+    req->flags                   = req_flags;
+    req->status                  = UCS_OK;
+    req->flush_worker.worker     = worker;
+    req->flush_worker.cb         = cb;
+    req->flush_worker.comp_count = 1; /* counting starts from 1, and decremented
+                                         when finished going over all endpoints */
+    req->flush_worker.prog_id    = UCS_CALLBACKQ_ID_NULL;
+    req->flush_worker.next_ep    = ucs_list_head(&worker->all_eps,
+                                                 ucp_ep_ext_gen_t, ep_list);
 
     uct_worker_progress_register_safe(worker->uct, ucp_worker_flush_progress,
                                       req, 0, &req->flush_worker.prog_id);
@@ -320,33 +447,19 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_worker_flush_nb, (worker, flags, cb),
 {
     void *request;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     request = ucp_worker_flush_nb_internal(worker, cb,
                                            UCP_REQUEST_FLAG_CALLBACK);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return request;
 }
 
 static ucs_status_t ucp_flush_wait(ucp_worker_h worker, void *request)
 {
-    ucs_status_t status;
-
-    if (request == NULL) {
-        return UCS_OK;
-    } else if (UCS_PTR_IS_ERR(request)) {
-        ucs_warn("flush failed: %s", ucs_status_string(UCS_PTR_STATUS(request)));
-        return UCS_PTR_STATUS(request);
-    } else {
-        do {
-            ucp_worker_progress(worker);
-            status = ucp_request_check_status(request);
-        } while (status == UCS_INPROGRESS);
-        ucp_request_release(request);
-        return status;
-    }
+    return ucp_rma_wait(worker, request, "flush");
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_worker_flush, (worker), ucp_worker_h worker)
@@ -354,12 +467,12 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_worker_flush, (worker), ucp_worker_h worker)
     ucs_status_t status;
     void *request;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     request = ucp_worker_flush_nb_internal(worker, NULL, 0);
     status = ucp_flush_wait(worker, request);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return status;
 }
@@ -369,12 +482,36 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_ep_flush, (ep), ucp_ep_h ep)
     ucs_status_t status;
     void *request;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
-    request = ucp_ep_flush_internal(ep, UCT_FLUSH_FLAG_LOCAL, NULL, 0,
-                                    ucp_ep_flushed_callback);
+    request = ucp_ep_flush_internal(ep, UCT_FLUSH_FLAG_LOCAL, NULL, 0, NULL,
+                                    ucp_ep_flushed_callback, "flush");
     status = ucp_flush_wait(ep->worker, request);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return status;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_worker_fence, (worker), ucp_worker_h worker)
+{
+    unsigned rsc_index;
+    ucs_status_t status;
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
+
+    for (rsc_index = 0; rsc_index < worker->context->num_tls; ++rsc_index) {
+        if (worker->ifaces[rsc_index].iface == NULL) {
+            continue;
+        }
+
+        status = uct_iface_fence(worker->ifaces[rsc_index].iface, 0);
+        if (status != UCS_OK) {
+            goto out;
+        }
+    }
+    status = UCS_OK;
+
+out:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return status;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.h
new file mode 100644
index 000000000..84b9608b3
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.h
@@ -0,0 +1,86 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCP_RMA_H_
+#define UCP_RMA_H_
+
+#include <ucp/proto/proto.h>
+
+
+/**
+ * Defines functions for RMA protocol
+ */
+struct ucp_rma_proto {
+    const char                 *name;
+    uct_pending_callback_t     progress_put;
+    uct_pending_callback_t     progress_get;
+};
+
+
+/**
+ * Defines functions for AMO protocol
+ */
+struct ucp_amo_proto {
+    const char                 *name;
+    uct_pending_callback_t     progress_fetch;
+    uct_pending_callback_t     progress_post;
+};
+
+
+/**
+ * Atomic reply data
+ */
+typedef union {
+    uint32_t           reply32; /* 32-bit reply */
+    uint64_t           reply64; /* 64-bit reply */
+} ucp_atomic_reply_t;
+
+
+typedef struct {
+    uint64_t                  address;
+    uintptr_t                 ep_ptr;
+} UCS_S_PACKED ucp_put_hdr_t;
+
+
+typedef struct {
+    uintptr_t                 ep_ptr;
+} UCS_S_PACKED ucp_cmpl_hdr_t;
+
+
+typedef struct {
+    uint64_t                  address;
+    uint64_t                  length;
+    ucp_request_hdr_t         req;
+} UCS_S_PACKED ucp_get_req_hdr_t;
+
+
+typedef struct {
+    uintptr_t                 req;
+} UCS_S_PACKED ucp_rma_rep_hdr_t;
+
+
+typedef struct {
+    uint64_t                  address;
+    ucp_request_hdr_t         req; // NULL if no reply
+    uint8_t                   length;
+    uint8_t                   opcode;
+} UCS_S_PACKED ucp_atomic_req_hdr_t;
+
+
+extern ucp_rma_proto_t ucp_rma_basic_proto;
+extern ucp_rma_proto_t ucp_rma_sw_proto;
+extern ucp_amo_proto_t ucp_amo_basic_proto;
+extern ucp_amo_proto_t ucp_amo_sw_proto;
+
+
+ucs_status_t ucp_rma_request_advance(ucp_request_t *req, ssize_t frag_length,
+                                     ucs_status_t status);
+
+void ucp_ep_flush_remote_completed(ucp_request_t *req);
+
+void ucp_rma_sw_send_cmpl(ucp_ep_h ep);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.inl
new file mode 100644
index 000000000..d1828dd1f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma.inl
@@ -0,0 +1,80 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCP_RMA_INL_
+#define UCP_RMA_INL_
+
+#include "rma.h"
+
+#include <ucp/api/ucp.h>
+#include <ucp/core/ucp_request.inl>
+#include <ucs/debug/log.h>
+
+
+static UCS_F_ALWAYS_INLINE ucs_status_ptr_t
+ucp_rma_send_request_cb(ucp_request_t *req, ucp_send_callback_t cb)
+{
+    ucs_status_t status = ucp_request_send(req);
+
+    if (req->flags & UCP_REQUEST_FLAG_COMPLETED) {
+        ucs_trace_req("releasing send request %p, returning status %s", req,
+                      ucs_status_string(status));
+        ucs_mpool_put(req);
+        return UCS_STATUS_PTR(status);
+    }
+
+    ucs_trace_req("returning request %p, status %s", req,
+                  ucs_status_string(status));
+    ucp_request_set_callback(req, send.cb, cb);
+    return req + 1;
+}
+
+static inline ucs_status_t ucp_rma_wait(ucp_worker_h worker, void *user_req,
+                                        const char *op_name)
+{
+    ucs_status_t status;
+    ucp_request_t *req;
+
+    if (ucs_likely(user_req == NULL)) {
+        return UCS_OK;
+    } else if (ucs_unlikely(UCS_PTR_IS_ERR(user_req))) {
+        ucs_warn("%s failed: %s", op_name,
+                 ucs_status_string(UCS_PTR_STATUS(user_req)));
+        return UCS_PTR_STATUS(user_req);
+    } else {
+        req = (ucp_request_t*)user_req - 1;
+        do {
+            ucp_worker_progress(worker);
+            status = ucp_request_check_status(user_req);
+        } while (!(req->flags & UCP_REQUEST_FLAG_COMPLETED));
+        status = req->status;
+        ucp_request_release(user_req);
+        return status;
+    }
+}
+
+static inline void ucp_ep_rma_remote_request_sent(ucp_ep_t *ep)
+{
+    ++ucp_ep_flush_state(ep)->send_sn;
+    ++ep->worker->flush_ops_count;
+}
+
+static inline void ucp_ep_rma_remote_request_completed(ucp_ep_t *ep)
+{
+    ucp_ep_flush_state_t *flush_state = ucp_ep_flush_state(ep);
+    ucp_request_t *req;
+
+    --ep->worker->flush_ops_count;
+    ++flush_state->cmpl_sn;
+
+    ucs_queue_for_each_extract(req, &flush_state->reqs, send.flush.queue,
+                               UCS_CIRCULAR_COMPARE32(req->send.flush.cmpl_sn,
+                                                      <= ,flush_state->cmpl_sn)) {
+        ucp_ep_flush_remote_completed(req);
+    }
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_basic.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_basic.c
new file mode 100644
index 000000000..e101d2322
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_basic.c
@@ -0,0 +1,123 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+* Copyright (c) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
+* Copyright (C) Los Alamos National Security, LLC. 2018. ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "rma.h"
+
+#include <ucp/proto/proto_am.inl>
+
+
+static ucs_status_t ucp_rma_basic_progress_put(uct_pending_req_t *self)
+{
+    ucp_request_t *req              = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep                    = req->send.ep;
+    ucp_rkey_h rkey                 = req->send.rma.rkey;
+    ucp_lane_index_t lane           = req->send.lane;
+    ucp_ep_rma_config_t *rma_config = &ucp_ep_config(ep)->rma[lane];
+    ucs_status_t status;
+    ssize_t packed_len;
+
+    ucs_assert(rkey->cache.ep_cfg_index == ep->cfg_index);
+    ucs_assert(rkey->cache.rma_lane == lane);
+
+    if ((req->send.length <= rma_config->max_put_short) ||
+        (req->send.length <= ucp_ep_config(ep)->bcopy_thresh))
+    {
+        packed_len = ucs_min(req->send.length, rma_config->max_put_short);
+        status = UCS_PROFILE_CALL(uct_ep_put_short,
+                                  ep->uct_eps[lane],
+                                  req->send.buffer,
+                                  packed_len,
+                                  req->send.rma.remote_addr,
+                                  rkey->cache.rma_rkey);
+    } else if (ucs_likely(req->send.length < rma_config->put_zcopy_thresh)) {
+        ucp_memcpy_pack_context_t pack_ctx;
+        pack_ctx.src    = req->send.buffer;
+        pack_ctx.length = ucs_min(req->send.length, rma_config->max_put_bcopy);
+        packed_len = UCS_PROFILE_CALL(uct_ep_put_bcopy,
+                                      ep->uct_eps[lane],
+                                      ucp_memcpy_pack,
+                                      &pack_ctx,
+                                      req->send.rma.remote_addr,
+                                      rkey->cache.rma_rkey);
+        status = (packed_len > 0) ? UCS_OK : (ucs_status_t)packed_len;
+    } else {
+        uct_iov_t iov;
+
+        /* TODO: leave last fragment for bcopy */
+        packed_len = ucs_min(req->send.length, rma_config->max_put_zcopy);
+        /* TODO: use ucp_dt_iov_copy_uct */
+        iov.buffer = (void *)req->send.buffer;
+        iov.length = packed_len;
+        iov.count  = 1;
+        iov.memh   = req->send.state.dt.dt.contig.memh[0];
+
+        status = UCS_PROFILE_CALL(uct_ep_put_zcopy,
+                                  ep->uct_eps[lane],
+                                  &iov, 1,
+                                  req->send.rma.remote_addr,
+                                  rkey->cache.rma_rkey,
+                                  &req->send.state.uct_comp);
+        ucp_request_send_state_advance(req, NULL, UCP_REQUEST_SEND_PROTO_RMA,
+                                       status);
+    }
+
+    return ucp_rma_request_advance(req, packed_len, status);
+}
+
+static ucs_status_t ucp_rma_basic_progress_get(uct_pending_req_t *self)
+{
+    ucp_request_t *req              = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep                    = req->send.ep;
+    ucp_rkey_h rkey                 = req->send.rma.rkey;
+    ucp_lane_index_t lane           = req->send.lane;
+    ucp_ep_rma_config_t *rma_config = &ucp_ep_config(ep)->rma[lane];
+    ucs_status_t status;
+    size_t frag_length;
+
+    ucs_assert(rkey->cache.ep_cfg_index == ep->cfg_index);
+    ucs_assert(rkey->cache.rma_lane == lane);
+
+    if (ucs_likely(req->send.length < rma_config->get_zcopy_thresh)) {
+        frag_length = ucs_min(rma_config->max_get_bcopy, req->send.length);
+        status = UCS_PROFILE_CALL(uct_ep_get_bcopy,
+                                  ep->uct_eps[lane],
+                                  (uct_unpack_callback_t)memcpy,
+                                  (void*)req->send.buffer,
+                                  frag_length,
+                                  req->send.rma.remote_addr,
+                                  rkey->cache.rma_rkey,
+                                  &req->send.state.uct_comp);
+    } else {
+        uct_iov_t iov;
+        frag_length = ucs_min(req->send.length, rma_config->max_get_zcopy);
+        iov.buffer  = (void *)req->send.buffer;
+        iov.length  = frag_length;
+        iov.count   = 1;
+        iov.memh    = req->send.state.dt.dt.contig.memh[0];
+
+        status = UCS_PROFILE_CALL(uct_ep_get_zcopy,
+                                  ep->uct_eps[lane],
+                                  &iov, 1,
+                                  req->send.rma.remote_addr,
+                                  rkey->cache.rma_rkey,
+                                  &req->send.state.uct_comp);
+    }
+
+    if (status == UCS_INPROGRESS) {
+        ucp_request_send_state_advance(req, 0, UCP_REQUEST_SEND_PROTO_RMA,
+                                       UCS_INPROGRESS);
+    }
+
+    return ucp_rma_request_advance(req, frag_length, status);
+}
+
+ucp_rma_proto_t ucp_rma_basic_proto = {
+    .name         = "basic_rma",
+    .progress_put = ucp_rma_basic_progress_put,
+    .progress_get = ucp_rma_basic_progress_get
+};
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_send.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_send.c
new file mode 100644
index 000000000..aaca06286
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_send.c
@@ -0,0 +1,341 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "rma.h"
+#include "rma.inl"
+
+#include <ucp/core/ucp_mm.h>
+
+#include <ucp/dt/dt_contig.h>
+#include <ucs/profile/profile.h>
+
+
+#define UCP_RMA_CHECK_BUFFER(_buffer, _action) \
+    do { \
+        if (ENABLE_PARAMS_CHECK && ucs_unlikely((_buffer) == NULL)) { \
+            _action; \
+        } \
+    } while (0)
+
+
+#define UCP_RMA_CHECK_ZERO_LENGTH(_length, _action) \
+    do { \
+        if ((_length) == 0) { \
+            _action; \
+        } \
+    } while (0)
+
+
+#define UCP_RMA_CHECK(_context, _buffer, _length) \
+    do { \
+        UCP_CONTEXT_CHECK_FEATURE_FLAGS(_context, UCP_FEATURE_RMA, \
+                                        return UCS_ERR_INVALID_PARAM); \
+        UCP_RMA_CHECK_ZERO_LENGTH(_length, return UCS_OK); \
+        UCP_RMA_CHECK_BUFFER(_buffer, return UCS_ERR_INVALID_PARAM); \
+    } while (0)
+
+
+#define UCP_RMA_CHECK_PTR(_context, _buffer, _length) \
+    do { \
+        UCP_CONTEXT_CHECK_FEATURE_FLAGS(_context, UCP_FEATURE_RMA, \
+                                        return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM)); \
+        UCP_RMA_CHECK_ZERO_LENGTH(_length, return NULL); \
+        UCP_RMA_CHECK_BUFFER(_buffer, \
+                             return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM)); \
+    } while (0)
+
+
+/* request can be released if
+ *  - all fragments were sent (length == 0) (bcopy & zcopy mix)
+ *  - all zcopy fragments are done (uct_comp.count == 0)
+ *  - and request was allocated from the mpool
+ *    (checked in ucp_request_complete_send)
+ *
+ * Request can be released either immediately or in the completion callback.
+ * We must check req length in the completion callback to avoid the following
+ * scenario:
+ *  partial_send;no_resos;progress;
+ *  send_completed;cb called;req free(ooops);
+ *  next_partial_send; (oops req already freed)
+ */
+ucs_status_t ucp_rma_request_advance(ucp_request_t *req, ssize_t frag_length,
+                                     ucs_status_t status)
+{
+    ucs_assert(status != UCS_ERR_NOT_IMPLEMENTED);
+
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        if (status != UCS_ERR_NO_RESOURCE) {
+            ucp_request_send_buffer_dereg(req);
+            ucp_request_complete_send(req, status);
+        }
+        return status;
+    }
+
+    ucs_assert(frag_length >= 0);
+    ucs_assert(req->send.length >= frag_length);
+    req->send.length -= frag_length;
+    if (req->send.length == 0) {
+        /* bcopy is the fast path */
+        if (ucs_likely(req->send.state.uct_comp.count == 0)) {
+            ucp_request_send_buffer_dereg(req);
+            ucp_request_complete_send(req, UCS_OK);
+        }
+        return UCS_OK;
+    }
+    req->send.buffer          += frag_length;
+    req->send.rma.remote_addr += frag_length;
+    return UCS_INPROGRESS;
+}
+
+static void ucp_rma_request_bcopy_completion(uct_completion_t *self,
+                                             ucs_status_t status)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
+                                          send.state.uct_comp);
+
+    if (ucs_likely(req->send.length == req->send.state.dt.offset)) {
+        ucp_request_complete_send(req, status);
+    }
+}
+
+static void ucp_rma_request_zcopy_completion(uct_completion_t *self,
+                                             ucs_status_t status)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t,
+                                          send.state.uct_comp);
+
+    if (ucs_likely(req->send.length == req->send.state.dt.offset)) {
+        ucp_request_send_buffer_dereg(req);
+        ucp_request_complete_send(req, status);
+    }
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+ucp_rma_request_init(ucp_request_t *req, ucp_ep_h ep, const void *buffer,
+                     size_t length, uint64_t remote_addr, ucp_rkey_h rkey,
+                     uct_pending_callback_t cb, size_t zcopy_thresh, int flags)
+{
+    req->flags                = flags; /* Implicit release */
+    req->send.ep              = ep;
+    req->send.buffer          = (void*)buffer;
+    req->send.datatype        = ucp_dt_make_contig(1);
+    req->send.mem_type        = UCT_MD_MEM_TYPE_HOST;
+    req->send.length          = length;
+    req->send.rma.remote_addr = remote_addr;
+    req->send.rma.rkey        = rkey;
+    req->send.uct.func        = cb;
+    req->send.lane            = rkey->cache.rma_lane;
+    ucp_request_send_state_init(req, ucp_dt_make_contig(1), length);
+    ucp_request_send_state_reset(req,
+                                 (length < zcopy_thresh) ?
+                                 ucp_rma_request_bcopy_completion :
+                                 ucp_rma_request_zcopy_completion,
+                                 UCP_REQUEST_SEND_PROTO_RMA);
+#if ENABLE_ASSERT
+    req->send.cb              = NULL;
+#endif
+    if (length < zcopy_thresh) {
+        return UCS_OK;
+    }
+
+    return ucp_request_send_buffer_reg_lane(req, req->send.lane);
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+ucp_rma_nonblocking(ucp_ep_h ep, const void *buffer, size_t length,
+                    uint64_t remote_addr, ucp_rkey_h rkey,
+                    uct_pending_callback_t progress_cb, size_t zcopy_thresh)
+{
+    ucs_status_t status;
+    ucp_request_t *req;
+
+    req = ucp_request_get(ep->worker);
+    if (req == NULL) {
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    status = ucp_rma_request_init(req, ep, buffer, length, remote_addr, rkey,
+                                  progress_cb, zcopy_thresh,
+                                  UCP_REQUEST_FLAG_RELEASED);
+    if (ucs_unlikely(status != UCS_OK)) {
+        return status;
+    }
+
+    return ucp_request_send(req);
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_ptr_t
+ucp_rma_nonblocking_cb(ucp_ep_h ep, const void *buffer, size_t length,
+                       uint64_t remote_addr, ucp_rkey_h rkey,
+                       uct_pending_callback_t progress_cb, size_t zcopy_thresh,
+                       ucp_send_callback_t cb)
+{
+    ucs_status_t status;
+    ucp_request_t *req;
+
+    req = ucp_request_get(ep->worker);
+    if (req == NULL) {
+        return UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
+    }
+
+    status = ucp_rma_request_init(req, ep, buffer, length, remote_addr, rkey,
+                                  progress_cb, zcopy_thresh, 0);
+    if (ucs_unlikely(status != UCS_OK)) {
+        return UCS_STATUS_PTR(status);
+    }
+
+    return ucp_rma_send_request_cb(req, cb);
+}
+
+ucs_status_t ucp_put_nbi(ucp_ep_h ep, const void *buffer, size_t length,
+                         uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    ucp_ep_rma_config_t *rma_config;
+    ucs_status_t status;
+
+    UCP_RMA_CHECK(ep->worker->context, buffer, length);
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    ucs_trace_req("put_nbi buffer %p length %zu remote_addr %"PRIx64" rkey %p to %s",
+                   buffer, length, remote_addr, rkey, ucp_ep_peer_name(ep));
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
+    if (status != UCS_OK) {
+        goto out_unlock;
+    }
+
+    /* Fast path for a single short message */
+    if (ucs_likely((ssize_t)length <= (int)rkey->cache.max_put_short)) {
+        status = UCS_PROFILE_CALL(uct_ep_put_short, ep->uct_eps[rkey->cache.rma_lane],
+                                  buffer, length, remote_addr, rkey->cache.rma_rkey);
+        if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
+            goto out_unlock;
+        }
+    }
+
+    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
+    status = ucp_rma_nonblocking(ep, buffer, length, remote_addr, rkey,
+                                 rkey->cache.rma_proto->progress_put,
+                                 rma_config->put_zcopy_thresh);
+out_unlock:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return status;
+}
+
+ucs_status_ptr_t ucp_put_nb(ucp_ep_h ep, const void *buffer, size_t length,
+                            uint64_t remote_addr, ucp_rkey_h rkey,
+                            ucp_send_callback_t cb)
+{
+    ucp_ep_rma_config_t *rma_config;
+    ucs_status_ptr_t ptr_status;
+    ucs_status_t status;
+
+    UCP_RMA_CHECK_PTR(ep->worker->context, buffer, length);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    ucs_trace_req("put_nb buffer %p length %zu remote_addr %"PRIx64" rkey %p to %s cb %p",
+                   buffer, length, remote_addr, rkey, ucp_ep_peer_name(ep), cb);
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
+    if (status != UCS_OK) {
+        ptr_status = UCS_STATUS_PTR(status);
+        goto out_unlock;
+    }
+
+    /* Fast path for a single short message */
+    if (ucs_likely((ssize_t)length <= (int)rkey->cache.max_put_short)) {
+        status = UCS_PROFILE_CALL(uct_ep_put_short, ep->uct_eps[rkey->cache.rma_lane],
+                                  buffer, length, remote_addr, rkey->cache.rma_rkey);
+        if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
+            ptr_status = UCS_STATUS_PTR(status);
+            goto out_unlock;
+        }
+    }
+
+    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
+    ptr_status = ucp_rma_nonblocking_cb(ep, buffer, length, remote_addr, rkey,
+                                        rkey->cache.rma_proto->progress_put,
+                                        rma_config->put_zcopy_thresh, cb);
+out_unlock:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return ptr_status;
+}
+
+ucs_status_t ucp_get_nbi(ucp_ep_h ep, void *buffer, size_t length,
+                         uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    ucp_ep_rma_config_t *rma_config;
+    ucs_status_t status;
+
+    UCP_RMA_CHECK(ep->worker->context, buffer, length);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    ucs_trace_req("get_nbi buffer %p length %zu remote_addr %"PRIx64" rkey %p from %s",
+                   buffer, length, remote_addr, rkey, ucp_ep_peer_name(ep));
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
+    if (status != UCS_OK) {
+        goto out_unlock;
+    }
+
+    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
+    status = ucp_rma_nonblocking(ep, buffer, length, remote_addr, rkey,
+                                 rkey->cache.rma_proto->progress_get,
+                                 rma_config->get_zcopy_thresh);
+out_unlock:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return status;
+}
+
+ucs_status_ptr_t ucp_get_nb(ucp_ep_h ep, void *buffer, size_t length,
+                            uint64_t remote_addr, ucp_rkey_h rkey,
+                            ucp_send_callback_t cb)
+{
+    ucp_ep_rma_config_t *rma_config;
+    ucs_status_ptr_t ptr_status;
+    ucs_status_t status;
+
+    UCP_RMA_CHECK_PTR(ep->worker->context, buffer, length);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    ucs_trace_req("get_nb buffer %p length %zu remote_addr %"PRIx64" rkey %p from %s cb %p",
+                   buffer, length, remote_addr, rkey, ucp_ep_peer_name(ep), cb);
+
+    status = UCP_RKEY_RESOLVE(rkey, ep, rma);
+    if (status != UCS_OK) {
+        ptr_status = UCS_STATUS_PTR(status);
+        goto out_unlock;
+    }
+
+    rma_config = &ucp_ep_config(ep)->rma[rkey->cache.rma_lane];
+    ptr_status = ucp_rma_nonblocking_cb(ep, buffer, length, remote_addr, rkey,
+                                        rkey->cache.rma_proto->progress_get,
+                                        rma_config->get_zcopy_thresh, cb);
+out_unlock:
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+    return ptr_status;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_put, (ep, buffer, length, remote_addr, rkey),
+                 ucp_ep_h ep, const void *buffer, size_t length,
+                 uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    return ucp_rma_wait(ep->worker,
+                        ucp_put_nb(ep, buffer, length, remote_addr, rkey,
+                                   (void*)ucs_empty_function),
+                        "put");
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_get, (ep, buffer, length, remote_addr, rkey),
+                 ucp_ep_h ep, void *buffer, size_t length,
+                 uint64_t remote_addr, ucp_rkey_h rkey)
+{
+    return ucp_rma_wait(ep->worker,
+                        ucp_get_nb(ep, buffer, length, remote_addr, rkey,
+                                   (void*)ucs_empty_function),
+                        "get");
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_sw.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_sw.c
new file mode 100644
index 000000000..bd40e8515
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/rma/rma_sw.c
@@ -0,0 +1,294 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "rma.h"
+#include "rma.inl"
+
+#include <ucs/profile/profile.h>
+#include <ucp/core/ucp_request.inl>
+
+
+static size_t ucp_rma_sw_put_pack_cb(void *dest, void *arg)
+{
+    ucp_request_t *req  = arg;
+    ucp_ep_t *ep        = req->send.ep;
+    ucp_put_hdr_t *puth = dest;
+    size_t length;
+
+    puth->address = req->send.rma.remote_addr;
+    puth->ep_ptr  = ucp_ep_dest_ep_ptr(ep);
+
+    ucs_assert(puth->ep_ptr != 0);
+
+    length = ucs_min(req->send.length,
+                     ucp_ep_config(ep)->am.max_bcopy - sizeof(*puth));
+    memcpy(puth + 1, req->send.buffer, length);
+
+    return sizeof(*puth) + length;
+}
+
+static ucs_status_t ucp_rma_sw_progress_put(uct_pending_req_t *self)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ssize_t packed_len;
+    ucs_status_t status;
+
+    ucs_assert(req->send.lane == ucp_ep_get_am_lane(ep));
+
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_PUT,
+                                 ucp_rma_sw_put_pack_cb, req, 0);
+    if (packed_len > 0) {
+        status = UCS_OK;
+        ucp_ep_rma_remote_request_sent(ep);
+    } else {
+        status = (ucs_status_t)packed_len;
+    }
+
+    return ucp_rma_request_advance(req, packed_len - sizeof(ucp_put_hdr_t),
+                                   status);
+}
+
+static size_t ucp_rma_sw_get_req_pack_cb(void *dest, void *arg)
+{
+    ucp_request_t *req         = arg;
+    ucp_get_req_hdr_t *getreqh = dest;
+
+    getreqh->address      = req->send.rma.remote_addr;
+    getreqh->length       = req->send.length;
+    getreqh->req.ep_ptr   = ucp_ep_dest_ep_ptr(req->send.ep);
+    getreqh->req.reqptr  = (uintptr_t)req;
+    ucs_assert(getreqh->req.ep_ptr != 0);
+
+    return sizeof(*getreqh);
+}
+
+static ucs_status_t ucp_rma_sw_progress_get(uct_pending_req_t *self)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ucs_status_t status;
+    ssize_t packed_len;
+
+    ucs_assert(req->send.lane == ucp_ep_get_am_lane(ep));
+
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_GET_REQ,
+                                 ucp_rma_sw_get_req_pack_cb, req, 0);
+    if (packed_len < 0) {
+        status = (ucs_status_t)packed_len;
+        if (status != UCS_ERR_NO_RESOURCE) {
+            ucp_request_complete_send(req, status);
+        }
+        return status;
+    }
+
+    /* get request packet sent, complete the request object when all data arrives */
+    ucs_assert(packed_len == sizeof(ucp_get_req_hdr_t));
+    ucp_ep_rma_remote_request_sent(ep);
+    return UCS_OK;
+}
+
+ucp_rma_proto_t ucp_rma_sw_proto = {
+    .name         = "sw_rma",
+    .progress_put = ucp_rma_sw_progress_put,
+    .progress_get = ucp_rma_sw_progress_get
+};
+
+static size_t ucp_rma_sw_pack_rma_ack(void *dest, void *arg)
+{
+    ucp_cmpl_hdr_t *hdr = dest;
+    ucp_request_t *req = arg;
+
+    hdr->ep_ptr = ucp_ep_dest_ep_ptr(req->send.ep);
+    return sizeof(*hdr);
+}
+
+static ucs_status_t ucp_progress_rma_cmpl(uct_pending_req_t *self)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ssize_t packed_len;
+
+    req->send.lane = ucp_ep_get_am_lane(ep);
+
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_CMPL,
+                                 ucp_rma_sw_pack_rma_ack, req, 0);
+    if (packed_len < 0) {
+        return (ucs_status_t)packed_len;
+    }
+
+    ucs_assert(packed_len == sizeof(ucp_cmpl_hdr_t));
+    ucp_request_put(req);
+    return UCS_OK;
+}
+
+void ucp_rma_sw_send_cmpl(ucp_ep_h ep)
+{
+    ucp_request_t *req;
+
+    req = ucp_request_get(ep->worker);
+    ucs_assert(req != NULL);
+
+    req->send.ep       = ep;
+    req->send.uct.func = ucp_progress_rma_cmpl;
+    ucp_request_send(req);
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_put_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_put_hdr_t *puth = data;
+    ucp_worker_h worker = arg;
+
+    memcpy((void*)puth->address, puth + 1, length - sizeof(*puth));
+    ucp_rma_sw_send_cmpl(ucp_worker_get_ep_by_ptr(worker, puth->ep_ptr));
+    return UCS_OK;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_rma_cmpl_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_cmpl_hdr_t *putackh = data;
+    ucp_worker_h worker     = arg;
+    ucp_ep_h ep             = ucp_worker_get_ep_by_ptr(worker, putackh->ep_ptr);
+
+    ucp_ep_rma_remote_request_completed(ep);
+    return UCS_OK;
+}
+
+static size_t ucp_rma_sw_pack_get_reply(void *dest, void *arg)
+{
+    ucp_rma_rep_hdr_t *hdr = dest;
+    ucp_request_t *req    = arg;
+    size_t length;
+
+    length   = ucs_min(req->send.length,
+                       ucp_ep_config(req->send.ep)->am.max_bcopy - sizeof(*hdr));
+    hdr->req = req->send.get_reply.req;
+    memcpy(hdr + 1, req->send.buffer, length);
+
+    return sizeof(*hdr) + length;
+}
+
+static ucs_status_t ucp_progress_get_reply(uct_pending_req_t *self)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucp_ep_t *ep       = req->send.ep;
+    ssize_t packed_len, payload_len;
+
+    req->send.lane = ucp_ep_get_am_lane(ep);
+    packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_GET_REP,
+                                 ucp_rma_sw_pack_get_reply, req, 0);
+    if (packed_len < 0) {
+        return (ucs_status_t)packed_len;
+    }
+
+    payload_len = packed_len - sizeof(ucp_rma_rep_hdr_t);
+    ucs_assert(payload_len >= 0);
+
+    req->send.buffer += payload_len;
+    req->send.length -= payload_len;
+
+    if (req->send.length == 0) {
+        ucp_request_put(req);
+        return UCS_OK;
+    } else {
+        return UCS_INPROGRESS;
+    }
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_get_req_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_get_req_hdr_t *getreqh = data;
+    ucp_worker_h worker        = arg;
+    ucp_ep_h ep                = ucp_worker_get_ep_by_ptr(worker,
+                                                          getreqh->req.ep_ptr);
+    ucp_request_t *req;
+
+    req = ucp_request_get(worker);
+    ucs_assert(req != NULL);
+
+    req->send.ep            = ep;
+    req->send.buffer        = (void*)getreqh->address;
+    req->send.length        = getreqh->length;
+    req->send.get_reply.req = getreqh->req.reqptr;
+    req->send.uct.func      = ucp_progress_get_reply;
+
+    ucp_request_send(req);
+    return UCS_OK;
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucp_get_rep_handler, (arg, data, length, am_flags),
+                 void *arg, void *data, size_t length, unsigned am_flags)
+{
+    ucp_rma_rep_hdr_t *getreph = data;
+    size_t frag_length         = length - sizeof(*getreph);
+    ucp_request_t *req         = (ucp_request_t*)getreph->req;
+    ucp_ep_h ep                = req->send.ep;
+
+    memcpy(req->send.buffer, getreph + 1, frag_length);
+
+    /* complete get request on last fragment of the reply */
+    if (ucp_rma_request_advance(req, frag_length, UCS_OK) == UCS_OK) {
+        ucp_ep_rma_remote_request_completed(ep);
+    }
+
+    return UCS_OK;
+}
+
+static void ucp_rma_sw_dump_packet(ucp_worker_h worker, uct_am_trace_type_t type,
+                                   uint8_t id, const void *data, size_t length,
+                                   char *buffer, size_t max)
+{
+    const ucp_get_req_hdr_t *geth;
+    const ucp_rma_rep_hdr_t *reph;
+    const ucp_cmpl_hdr_t *cmplh;
+    const ucp_put_hdr_t *puth;
+    size_t header_len;
+    char *p;
+
+    switch (id) {
+    case UCP_AM_ID_PUT:
+        puth = data;
+        snprintf(buffer, max, "PUT [addr 0x%lx ep_ptr 0x%lx]", puth->address,
+                 puth->ep_ptr);
+        header_len = sizeof(*puth);
+        break;
+    case UCP_AM_ID_GET_REQ:
+        geth = data;
+        snprintf(buffer, max, "GET_REQ [addr 0x%lx len %zu reqptr 0x%lx ep 0x%lx]",
+                 geth->address, geth->length, geth->req.reqptr, geth->req.ep_ptr);
+        return;
+    case UCP_AM_ID_GET_REP:
+        reph = data;
+        snprintf(buffer, max, "GET_REP [reqptr 0x%lx]", reph->req);
+        header_len = sizeof(*reph);
+        break;
+    case UCP_AM_ID_CMPL:
+        cmplh = data;
+        snprintf(buffer, max, "CMPL [ep_ptr 0x%lx]", cmplh->ep_ptr);
+        return;
+    default:
+        return;
+    }
+
+    p = buffer + strlen(buffer);
+    ucp_dump_payload(worker->context, p, buffer + max - p, data + header_len,
+                     length - header_len);
+}
+
+UCP_DEFINE_AM(UCP_FEATURE_RMA, UCP_AM_ID_PUT, ucp_put_handler,
+              ucp_rma_sw_dump_packet, 0);
+UCP_DEFINE_AM(UCP_FEATURE_RMA, UCP_AM_ID_GET_REQ, ucp_get_req_handler,
+              ucp_rma_sw_dump_packet, 0);
+UCP_DEFINE_AM(UCP_FEATURE_RMA, UCP_AM_ID_GET_REP, ucp_get_rep_handler,
+              ucp_rma_sw_dump_packet, 0);
+UCP_DEFINE_AM(UCP_FEATURE_RMA|UCP_FEATURE_AMO, UCP_AM_ID_CMPL,
+              ucp_rma_cmpl_handler, ucp_rma_sw_dump_packet, 0);
+
+UCP_DEFINE_AM_PROXY(UCP_AM_ID_PUT);
+UCP_DEFINE_AM_PROXY(UCP_AM_ID_GET_REQ);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream.h
index 5e98a0d75..06c5bb238 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream.h
@@ -8,10 +8,12 @@
 #define UCP_STREAM_H_
 
 #include <ucp/core/ucp_ep.h>
+#include <ucp/core/ucp_ep.inl>
 #include <ucp/core/ucp_worker.h>
 
+
 typedef struct {
-    uint64_t    sender_uuid;
+    uintptr_t                ep_ptr;
 } UCS_S_PACKED ucp_stream_am_hdr_t;
 
 
@@ -23,51 +25,44 @@ typedef struct {
 } UCS_S_PACKED ucp_stream_am_data_t;
 
 
-/**
- * Stream specific endpoint flags
- */
-enum {
-    UCP_EP_STREAM_FLAG_IS_QUEUED = UCS_BIT(0), /* EP is queued in stream list of
-                                                  worker */
-    UCP_EP_STREAM_FLAG_HAS_DATA  = UCS_BIT(1), /* EP has data in the match_q */
-    UCP_EP_STREAM_FLAG_VALID     = UCS_BIT(2)  /* EP is valid. EP can be
-                                                  invalidated by ucp_ep_close_cb
-                                                  (all incoming data will be
-                                                  dropped) then returned back by
-                                                  ucp_ep_create if internal
-                                                  connection is still alive */
-};
-
-
-static UCS_F_ALWAYS_INLINE void
-ucp_stream_ep_enqueue(ucp_ep_ext_stream_t *ep, ucp_worker_h worker)
+void ucp_stream_ep_init(ucp_ep_h ep);
+
+void ucp_stream_ep_cleanup(ucp_ep_h ep);
+
+void ucp_stream_ep_activate(ucp_ep_h ep);
+
+
+static UCS_F_ALWAYS_INLINE int ucp_stream_ep_is_queued(ucp_ep_ext_proto_t *ep_ext)
+{
+    return ep_ext->stream.ready_list.next != NULL;
+}
+
+static UCS_F_ALWAYS_INLINE int ucp_stream_ep_has_data(ucp_ep_ext_proto_t *ep_ext)
 {
-    ucs_assert(!(ep->flags & UCP_EP_STREAM_FLAG_IS_QUEUED));
-    ucs_list_add_tail(&worker->stream_eps, &ep->list);
-    ep->flags |= UCP_EP_STREAM_FLAG_IS_QUEUED;
+    return ucp_ep_from_ext_proto(ep_ext)->flags & UCP_EP_FLAG_STREAM_HAS_DATA;
 }
 
-static UCS_F_ALWAYS_INLINE int
-ucp_stream_ep_is_queued(ucp_ep_ext_stream_t *ep)
+static UCS_F_ALWAYS_INLINE
+void ucp_stream_ep_enqueue(ucp_ep_ext_proto_t *ep_ext, ucp_worker_h worker)
 {
-    return ep->flags & UCP_EP_STREAM_FLAG_IS_QUEUED;
+    ucs_assert(!ucp_stream_ep_is_queued(ep_ext));
+    ucs_list_add_tail(&worker->stream_ready_eps, &ep_ext->stream.ready_list);
 }
 
-static UCS_F_ALWAYS_INLINE void
-ucp_stream_ep_dequeue(ucp_ep_ext_stream_t *ep)
+static UCS_F_ALWAYS_INLINE void ucp_stream_ep_dequeue(ucp_ep_ext_proto_t *ep_ext)
 {
-    ucs_assert(ep->flags & UCP_EP_STREAM_FLAG_IS_QUEUED);
-    ep->flags &= ~UCP_EP_STREAM_FLAG_IS_QUEUED;
-    ucs_list_del(&ep->list);
+    ucs_list_del(&ep_ext->stream.ready_list);
+    ep_ext->stream.ready_list.next = NULL;
 }
 
-static UCS_F_ALWAYS_INLINE ucp_ep_ext_stream_t *
+static UCS_F_ALWAYS_INLINE ucp_ep_ext_proto_t*
 ucp_stream_worker_dequeue_ep_head(ucp_worker_h worker)
 {
-    ucp_ep_ext_stream_t *ep = ucs_list_head(&worker->stream_eps,
-                                            ucp_ep_ext_stream_t, list);
-    ucp_stream_ep_dequeue(ep);
-    return ep;
+    ucp_ep_ext_proto_t *ep_ext = ucs_list_head(&worker->stream_ready_eps,
+                                               ucp_ep_ext_proto_t,
+                                               stream.ready_list);
+    ucp_stream_ep_dequeue(ep_ext);
+    return ep_ext;
 }
 
 #endif /* UCP_STREAM_H_ */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_recv.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_recv.c
index dbbf08288..e2d082843 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_recv.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_recv.c
@@ -12,9 +12,7 @@
 #include <ucp/stream/stream.h>
 
 #include <ucs/datastruct/mpool.inl>
-#include <ucs/debug/profile.h>
-
-#include <ucp/tag/eager.h> /* TODO: remove ucp_eager_sync_hdr_t usage */
+#include <ucs/profile/profile.h>
 
 
 /* @verbatim
@@ -59,17 +57,16 @@
 
 
 static UCS_F_ALWAYS_INLINE ucp_recv_desc_t *
-ucp_stream_rdesc_dequeue(ucp_ep_ext_stream_t *ep_stream)
+ucp_stream_rdesc_dequeue(ucp_ep_ext_proto_t *ep_ext)
 {
-    ucp_recv_desc_t *rdesc = ucs_queue_pull_elem_non_empty(&ep_stream->match_q,
+    ucp_recv_desc_t *rdesc = ucs_queue_pull_elem_non_empty(&ep_ext->stream.match_q,
                                                            ucp_recv_desc_t,
                                                            stream_queue);
-    ucs_assert(ep_stream->flags & UCP_EP_STREAM_FLAG_HAS_DATA);
-
-    if (ucs_unlikely(ucs_queue_is_empty(&ep_stream->match_q))) {
-        ep_stream->flags &= ~UCP_EP_STREAM_FLAG_HAS_DATA;
-        if (ucp_stream_ep_is_queued(ep_stream)) {
-            ucp_stream_ep_dequeue(ep_stream);
+    ucs_assert(ucp_stream_ep_has_data(ep_ext));
+    if (ucs_unlikely(ucs_queue_is_empty(&ep_ext->stream.match_q))) {
+        ucp_ep_from_ext_proto(ep_ext)->flags &= ~UCP_EP_FLAG_STREAM_HAS_DATA;
+        if (ucp_stream_ep_is_queued(ep_ext)) {
+            ucp_stream_ep_dequeue(ep_ext);
         }
     }
 
@@ -77,35 +74,31 @@ ucp_stream_rdesc_dequeue(ucp_ep_ext_stream_t *ep_stream)
 }
 
 static UCS_F_ALWAYS_INLINE ucp_recv_desc_t *
-ucp_stream_rdesc_get(ucp_ep_ext_stream_t *ep_stream)
+ucp_stream_rdesc_get(ucp_ep_ext_proto_t *ep_ext)
 {
-    ucp_recv_desc_t *rdesc = ucs_queue_head_elem_non_empty(&ep_stream->match_q,
+    ucp_recv_desc_t *rdesc = ucs_queue_head_elem_non_empty(&ep_ext->stream.match_q,
                                                            ucp_recv_desc_t,
                                                            stream_queue);
 
-    ucs_assert(ep_stream->flags & UCP_EP_STREAM_FLAG_HAS_DATA);
+    ucs_assert(ucp_stream_ep_has_data(ep_ext));
     ucs_trace_data("ep %p, rdesc %p with %u stream bytes",
-                   ep_stream->ucp_ep, rdesc, rdesc->length);
+                   ucp_ep_from_ext_proto(ep_ext), rdesc, rdesc->length);
 
     return rdesc;
 }
 
-UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_data_nb,
-                 (ep, length), ucp_ep_h ep, size_t *length)
+static UCS_F_ALWAYS_INLINE ucs_status_ptr_t
+ucp_stream_recv_data_nb_nolock(ucp_ep_h ep, size_t *length)
 {
+    ucp_ep_ext_proto_t   *ep_ext = ucp_ep_ext_proto(ep);
     ucp_recv_desc_t      *rdesc;
     ucp_stream_am_data_t *am_data;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    if (ucs_unlikely(!(ep->ext.stream->flags & UCP_EP_STREAM_FLAG_HAS_DATA))) {
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    if (ucs_unlikely(!ucp_stream_ep_has_data(ep_ext))) {
         return UCS_STATUS_PTR(UCS_OK);
     }
 
-    rdesc = ucp_stream_rdesc_dequeue(ep->ext.stream);
-
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    rdesc = ucp_stream_rdesc_dequeue(ep_ext);
 
     *length         = rdesc->length;
     am_data         = ucp_stream_rdesc_am_data(rdesc);
@@ -113,27 +106,31 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_data_nb,
     return am_data + 1;
 }
 
-static UCS_F_ALWAYS_INLINE void
-ucp_stream_rdesc_release(ucp_recv_desc_t *rdesc)
+UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_data_nb, (ep, length),
+                 ucp_ep_h ep, size_t *length)
 {
-    if (ucs_unlikely(rdesc->flags & UCP_RECV_DESC_FLAG_UCT_DESC)) {
-        uct_iface_release_desc(UCS_PTR_BYTE_OFFSET(rdesc,
-                                                   -sizeof(ucp_eager_sync_hdr_t)));
-    } else {
-        ucs_mpool_put_inline(rdesc);
-    }
+    ucs_status_ptr_t status_ptr;
+
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_STREAM,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+    status_ptr = ucp_stream_recv_data_nb_nolock(ep, length);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
+
+    return status_ptr;
 }
 
 static UCS_F_ALWAYS_INLINE void
 ucp_stream_rdesc_dequeue_and_release(ucp_recv_desc_t *rdesc,
-                                     ucp_ep_ext_stream_t *ep)
+                                     ucp_ep_ext_proto_t *ep_ext)
 {
-    ucs_assert(ep->flags & UCP_EP_STREAM_FLAG_HAS_DATA);
-    ucs_assert(rdesc == ucs_queue_head_elem_non_empty(&ep->match_q,
+    ucs_assert(ucp_stream_ep_has_data(ep_ext));
+    ucs_assert(rdesc == ucs_queue_head_elem_non_empty(&ep_ext->stream.match_q,
                                                       ucp_recv_desc_t,
                                                       stream_queue));
-    ucp_stream_rdesc_dequeue(ep);
-    ucp_stream_rdesc_release(rdesc);
+    ucp_stream_rdesc_dequeue(ep_ext);
+    ucp_recv_desc_release(rdesc);
 }
 
 UCS_PROFILE_FUNC_VOID(ucp_stream_data_release, (ep, data),
@@ -141,23 +138,31 @@ UCS_PROFILE_FUNC_VOID(ucp_stream_data_release, (ep, data),
 {
     ucp_recv_desc_t *rdesc = ucp_stream_rdesc_from_data(data);
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
-    ucp_stream_rdesc_release(rdesc);
+    ucp_recv_desc_release(rdesc);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
 }
 
 static UCS_F_ALWAYS_INLINE ssize_t
 ucp_stream_rdata_unpack(const void *rdata, size_t length, ucp_request_t *dst_req)
 {
-    /* Truncated error is not actual for stream, need to adjust */
-    size_t       valid_len = ucs_min((dst_req->recv.length -
-                                      dst_req->recv.stream.offset), length);
+    size_t       valid_len;
+    int          last;
     ucs_status_t status;
 
+    /* Truncated error is not actual for stream, need to adjust */
+    valid_len = dst_req->recv.length - dst_req->recv.stream.offset;
+    if (valid_len <= length) {
+        last = (valid_len == length);
+    } else {
+        valid_len = length;
+        last      = !(dst_req->flags & UCP_REQUEST_FLAG_STREAM_RECV_WAITALL);
+    }
+
     status = ucp_request_recv_data_unpack(dst_req, rdata, valid_len,
-                                          dst_req->recv.stream.offset, 1);
+                                          dst_req->recv.stream.offset, last);
     if (ucs_likely(status == UCS_OK)) {
         dst_req->recv.stream.offset += valid_len;
         ucs_trace_data("unpacked %zd bytes of stream data %p",
@@ -171,14 +176,14 @@ ucp_stream_rdata_unpack(const void *rdata, size_t length, ucp_request_t *dst_req
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_stream_rdesc_advance(ucp_recv_desc_t *rdesc, ssize_t offset,
-                         ucp_ep_ext_stream_t *ep)
+                         ucp_ep_ext_proto_t *ep_ext)
 {
     ucs_assert(offset <= rdesc->length);
 
     if (ucs_unlikely(offset < 0)) {
         return offset;
     } else if (ucs_likely(offset == rdesc->length)) {
-        ucp_stream_rdesc_dequeue_and_release(rdesc, ep);
+        ucp_stream_rdesc_dequeue_and_release(rdesc, ep_ext);
     } else {
         rdesc->length         -= offset;
         rdesc->payload_offset += offset;
@@ -190,22 +195,27 @@ ucp_stream_rdesc_advance(ucp_recv_desc_t *rdesc, ssize_t offset,
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_stream_process_rdesc_inplace(ucp_recv_desc_t *rdesc, ucp_datatype_t dt,
                                  void *buffer, size_t count, size_t length,
-                                 ucp_ep_ext_stream_t *ep_stream)
+                                 ucp_ep_ext_proto_t *ep_ext)
 {
     ucs_status_t status;
     ssize_t unpacked;
+    uct_memory_type_t mem_type;
+
+
+    ucp_memory_type_detect_mds(ucp_ep_from_ext_proto(ep_ext)->worker->context, buffer,
+                               length, &mem_type);
 
-    status   = ucp_dt_unpack_only(ep_stream->ucp_ep->worker, buffer, count, dt,
-                                  UCT_MD_MEM_TYPE_HOST, ucp_stream_rdesc_payload(rdesc),
-                                  length, 0);
+    status   = ucp_dt_unpack_only(ucp_ep_from_ext_proto(ep_ext)->worker, buffer,
+                                  count, dt, mem_type,
+                                  ucp_stream_rdesc_payload(rdesc), length, 0);
 
     unpacked = ucs_likely(status == UCS_OK) ? length : status;
 
-    return ucp_stream_rdesc_advance(rdesc, unpacked, ep_stream);
+    return ucp_stream_rdesc_advance(rdesc, unpacked, ep_ext);
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_stream_process_rdesc(ucp_recv_desc_t *rdesc, ucp_ep_ext_stream_t *ep_stream,
+ucp_stream_process_rdesc(ucp_recv_desc_t *rdesc, ucp_ep_ext_proto_t *ep_ext,
                          ucp_request_t *req)
 {
     ssize_t unpacked;
@@ -214,36 +224,41 @@ ucp_stream_process_rdesc(ucp_recv_desc_t *rdesc, ucp_ep_ext_stream_t *ep_stream,
                                        rdesc->length, req);
     ucs_assert(req->recv.stream.offset <= req->recv.length);
 
-    return ucp_stream_rdesc_advance(rdesc, unpacked, ep_stream);
+    return ucp_stream_rdesc_advance(rdesc, unpacked, ep_ext);
 }
 
 static UCS_F_ALWAYS_INLINE void
-ucp_stream_recv_request_init(ucp_request_t *req, void *buffer, size_t count,
-                             size_t length, ucp_datatype_t datatype,
-                             ucp_stream_recv_callback_t cb)
+ucp_stream_recv_request_init(ucp_request_t *req, ucp_ep_h ep, void *buffer,
+                             size_t count, size_t length,
+                             ucp_datatype_t datatype,
+                             ucp_stream_recv_callback_t cb,
+                             uint16_t request_flags)
 {
-    req->flags              = UCP_REQUEST_FLAG_CALLBACK |
-                              UCP_REQUEST_FLAG_STREAM_RECV;
+    req->flags              = UCP_REQUEST_FLAG_CALLBACK | request_flags;
 #if ENABLE_ASSERT
+    req->flags             |= UCP_REQUEST_FLAG_STREAM_RECV;
     req->status             = UCS_OK; /* for ucp_request_recv_data_unpack() */
 #endif
     req->recv.stream.cb     = cb;
     req->recv.stream.length = 0;
     req->recv.stream.offset = 0;
 
+    ucp_dt_recv_state_init(&req->recv.state, buffer, datatype, count);
+
+    req->recv.worker   = ep->worker;
     req->recv.buffer   = buffer;
     req->recv.datatype = datatype;
-    req->recv.length   = length;
-    req->recv.mem_type = UCT_MD_MEM_TYPE_HOST;
-
-    ucp_dt_recv_state_init(&req->recv.state, buffer, datatype, count);
+    req->recv.length   = ucs_likely(!UCP_DT_IS_GENERIC(datatype)) ? length :
+                         ucp_dt_length(datatype, count, NULL, &req->recv.state);
+    ucp_memory_type_detect_mds(ep->worker->context, (void *)buffer,
+                               req->recv.length, &req->recv.mem_type);
 }
 
 static UCS_F_ALWAYS_INLINE int
-ucp_stream_recv_nb_is_inplace(ucp_ep_ext_stream_t *ep, size_t dt_length)
+ucp_stream_recv_nb_is_inplace(ucp_ep_ext_proto_t *ep_ext, size_t dt_length)
 {
-    return (ep->flags & UCP_EP_STREAM_FLAG_HAS_DATA) &&
-           (ucp_stream_rdesc_get(ep)->length >= dt_length);
+    return ucp_stream_ep_has_data(ep_ext) &&
+           (ucp_stream_rdesc_get(ep_ext)->length >= dt_length);
 }
 
 UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_nb,
@@ -253,27 +268,26 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_nb,
                  size_t *length, unsigned flags)
 {
     ucs_status_t        status     = UCS_OK;
-    ucp_ep_ext_stream_t *ep_stream = ep->ext.stream;
+    ucp_ep_ext_proto_t  *ep_ext    = ucp_ep_ext_proto(ep);
     size_t              dt_length;
     ucp_request_t       *req;
-    ucs_status_ptr_t    rdesc;
-    ucp_dt_state_t      dt_state;
-
-    if (UCP_DT_IS_GENERIC(datatype)) {
-        ucs_error("ucp_stream_recv_nb doesn't support generic datatype");
-        return UCS_STATUS_PTR(UCS_ERR_NOT_IMPLEMENTED);
-    }
-
-    dt_length = ucp_dt_length(datatype, count, buffer, &dt_state);
-
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
-
-    if (ucs_likely(ucp_stream_recv_nb_is_inplace(ep_stream, dt_length))) {
-        status = ucp_stream_process_rdesc_inplace(ucp_stream_rdesc_get(ep_stream),
-                                                  datatype, buffer, count,
-                                                  dt_length, ep_stream);
-        *length = dt_length;
-        goto out_status;
+    ucp_recv_desc_t     *rdesc;
+
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_STREAM,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
+
+    if (ucs_likely(!UCP_DT_IS_GENERIC(datatype))) {
+        dt_length = ucp_dt_length(datatype, count, buffer, NULL);
+        if (ucs_likely(ucp_stream_recv_nb_is_inplace(ep_ext, dt_length))) {
+            status = ucp_stream_process_rdesc_inplace(ucp_stream_rdesc_get(ep_ext),
+                                                      datatype, buffer, count,
+                                                      dt_length, ep_ext);
+            *length = dt_length;
+            goto out_status;
+        }
+    } else {
+        dt_length = 0; /* Suppress warnings of paranoid compilers */
     }
 
     req = ucp_request_get(ep->worker);
@@ -282,17 +296,29 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_nb,
         goto out_status;
     }
 
-    ucp_stream_recv_request_init(req, buffer, count, dt_length, datatype, cb);
+    ucp_stream_recv_request_init(req, ep, buffer, count, dt_length, datatype,
+                                 cb, (flags & UCP_STREAM_RECV_FLAG_WAITALL) ?
+                                 UCP_REQUEST_FLAG_STREAM_RECV_WAITALL : 0);
 
     /* OK, lets obtain all arrived data which matches the recv size */
     while ((req->recv.stream.offset < req->recv.length) &&
-           (ep_stream->flags & UCP_EP_STREAM_FLAG_HAS_DATA)) {
+           ucp_stream_ep_has_data(ep_ext)) {
 
-        rdesc  = ucp_stream_rdesc_get(ep_stream);
-        status = ucp_stream_process_rdesc(rdesc, ep_stream, req);
+        rdesc  = ucp_stream_rdesc_get(ep_ext);
+        status = ucp_stream_process_rdesc(rdesc, ep_ext, req);
         if (ucs_unlikely(status != UCS_OK)) {
             goto out_put_request;
         }
+
+        /*
+         * NOTE: generic datatype can be completed with any amount of data to
+         *       avoid extra logic in ucp_stream_process_rdesc, exception is
+         *       WAITALL flag
+         */
+        if (ucs_unlikely(UCP_DT_IS_GENERIC(req->recv.datatype)) &&
+            !(req->flags & UCP_REQUEST_FLAG_STREAM_RECV_WAITALL)) {
+            break;
+        }
     }
 
     ucs_assert(req->recv.stream.offset <= req->recv.length);
@@ -300,8 +326,8 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_recv_nb,
     if (ucp_request_can_complete_stream_recv(req)) {
         *length = req->recv.stream.offset;
     } else {
-        ucs_assert(!(ep_stream->flags & UCP_EP_STREAM_FLAG_HAS_DATA));
-        ucs_queue_push(&ep_stream->match_q, &req->recv.queue);
+        ucs_assert(!ucp_stream_ep_has_data(ep_ext));
+        ucs_queue_push(&ep_ext->stream.match_q, &req->recv.queue);
         req += 1;
         goto out;
     }
@@ -313,12 +339,12 @@ out_status:
     req = UCS_STATUS_PTR(status);
 
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
     return req;
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
-ucp_stream_am_data_process(ucp_worker_t *worker, ucp_ep_ext_stream_t *ep,
+ucp_stream_am_data_process(ucp_worker_t *worker, ucp_ep_ext_proto_t *ep_ext,
                            ucp_stream_am_data_t *am_data, size_t length,
                            unsigned am_flags)
 {
@@ -334,10 +360,10 @@ ucp_stream_am_data_process(ucp_worker_t *worker, ucp_ep_ext_stream_t *ep,
                                                     place */
 
     /* First, process expected requests */
-    if (!(ep->flags & UCP_EP_STREAM_FLAG_HAS_DATA)) {
-        while (!ucs_queue_is_empty(&ep->match_q)) {
-            req      = ucs_queue_head_elem_non_empty(&ep->match_q, ucp_request_t,
-                                                     recv.queue);
+    if (!ucp_stream_ep_has_data(ep_ext)) {
+        while (!ucs_queue_is_empty(&ep_ext->stream.match_q)) {
+            req      = ucs_queue_head_elem_non_empty(&ep_ext->stream.match_q,
+                                                     ucp_request_t, recv.queue);
             payload  = UCS_PTR_BYTE_OFFSET(am_data, rdesc_tmp.payload_offset);
             unpacked = ucp_stream_rdata_unpack(payload, rdesc_tmp.length, req);
             if (ucs_unlikely(unpacked < 0)) {
@@ -345,14 +371,14 @@ ucp_stream_am_data_process(ucp_worker_t *worker, ucp_ep_ext_stream_t *ep,
                           am_data, rdesc_tmp.payload_offset, req);
             } else if (unpacked == rdesc_tmp.length) {
                 if (ucp_request_can_complete_stream_recv(req)) {
-                    ucp_request_complete_stream_recv(req, ep, UCS_OK);
+                    ucp_request_complete_stream_recv(req, ep_ext, UCS_OK);
                 }
                 return UCS_OK;
             }
-            ucp_stream_rdesc_advance(&rdesc_tmp, unpacked, ep);
+            ucp_stream_rdesc_advance(&rdesc_tmp, unpacked, ep_ext);
             /* This request is full, try next one */
             ucs_assert(ucp_request_can_complete_stream_recv(req));
-            ucp_request_complete_stream_recv(req, ep, UCS_OK);
+            ucp_request_complete_stream_recv(req, ep_ext, UCS_OK);
         }
     }
 
@@ -375,43 +401,76 @@ ucp_stream_am_data_process(ucp_worker_t *worker, ucp_ep_ext_stream_t *ep,
         rdesc                 = (ucp_recv_desc_t *)am_data - 1;
         rdesc->length         = rdesc_tmp.length;
         rdesc->payload_offset = rdesc_tmp.payload_offset + sizeof(*rdesc);
+        rdesc->priv_length    = 0;
         rdesc->flags          = UCP_RECV_DESC_FLAG_UCT_DESC;
     }
 
-    ep->flags |= UCP_EP_STREAM_FLAG_HAS_DATA;
-    ucs_queue_push(&ep->match_q, &rdesc->stream_queue);
+    ucp_ep_from_ext_proto(ep_ext)->flags |= UCP_EP_FLAG_STREAM_HAS_DATA;
+    ucs_queue_push(&ep_ext->stream.match_q, &rdesc->stream_queue);
 
     return UCS_INPROGRESS;
 }
 
+void ucp_stream_ep_init(ucp_ep_h ep)
+{
+    ucp_ep_ext_proto_t *ep_ext = ucp_ep_ext_proto(ep);
+
+    if (ep->worker->context->config.features & UCP_FEATURE_STREAM) {
+        ep_ext->stream.ready_list.prev = NULL;
+        ep_ext->stream.ready_list.next = NULL;
+        ucs_queue_head_init(&ep_ext->stream.match_q);
+    }
+}
+
+void ucp_stream_ep_cleanup(ucp_ep_h ep)
+{
+    size_t length;
+    void *data;
+
+    if (ep->worker->context->config.features & UCP_FEATURE_STREAM) {
+        while ((data = ucp_stream_recv_data_nb_nolock(ep, &length)) != NULL) {
+            ucs_assert_always(!UCS_PTR_IS_ERR(data));
+            ucp_stream_data_release(ep, data);
+        }
+
+        if (ucp_stream_ep_is_queued(ucp_ep_ext_proto(ep))) {
+            ucp_stream_ep_dequeue(ucp_ep_ext_proto(ep));
+        }
+    }
+}
+
+void ucp_stream_ep_activate(ucp_ep_h ep)
+{
+    ucp_ep_ext_proto_t *ep_ext = ucp_ep_ext_proto(ep);
+
+    if ((ep->worker->context->config.features & UCP_FEATURE_STREAM) &&
+        ucp_stream_ep_has_data(ep_ext) && !ucp_stream_ep_is_queued(ep_ext)) {
+        ucp_stream_ep_enqueue(ep_ext, ep->worker);
+    }
+}
+
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_stream_am_handler(void *am_arg, void *am_data, size_t am_length,
                       unsigned am_flags)
 {
     ucp_worker_h          worker    = am_arg;
     ucp_stream_am_data_t *data      = am_data;
-    ucp_ep_ext_stream_t  *ep_stream;
     ucp_ep_h              ep;
+    ucp_ep_ext_proto_t    *ep_ext;
     ucs_status_t          status;
 
     ucs_assert(am_length >= sizeof(ucp_stream_am_hdr_t));
 
-    ep = ucp_worker_ep_find(worker, data->hdr.sender_uuid);
-    if (ucs_unlikely(ep == NULL)) {
-        ucs_trace_data("ep not found for uuid %"PRIx64, data->hdr.sender_uuid);
-        /* drop the data */
-        return UCS_OK;
-    }
+    ep     = ucp_worker_get_ep_by_ptr(worker, data->hdr.ep_ptr);
+    ep_ext = ucp_ep_ext_proto(ep);
 
-    ep_stream = ep->ext.stream;
-    if (ucs_unlikely(!(ep_stream->flags & UCP_EP_STREAM_FLAG_VALID))) {
-        ucs_trace_data("stream ep with uuid %"PRIx64" is invalid",
-                       data->hdr.sender_uuid);
+    if (ucs_unlikely(ep->flags & UCP_EP_FLAG_CLOSED)) {
+        ucs_trace_data("ep %p: stream is invalid", ep);
         /* drop the data */
         return UCS_OK;
     }
 
-    status = ucp_stream_am_data_process(worker, ep_stream, data,
+    status = ucp_stream_am_data_process(worker, ep_ext, data,
                                         am_length - sizeof(data->hdr),
                                         am_flags);
     if (status == UCS_OK) {
@@ -421,8 +480,8 @@ ucp_stream_am_handler(void *am_arg, void *am_data, size_t am_length,
 
     ucs_assert(status == UCS_INPROGRESS);
 
-    if (!ucp_stream_ep_is_queued(ep_stream)) {
-        ucp_stream_ep_enqueue(ep_stream, worker);
+    if (!ucp_stream_ep_is_queued(ep_ext) && (ep->flags & UCP_EP_FLAG_USED)) {
+        ucp_stream_ep_enqueue(ep_ext, worker);
     }
 
     return (am_flags & UCT_CB_PARAM_FLAG_DESC) ? UCS_INPROGRESS : UCS_OK;
@@ -436,15 +495,15 @@ static void ucp_stream_am_dump(ucp_worker_h worker, uct_am_trace_type_t type,
     size_t                    hdr_len = sizeof(*hdr);
     char                      *p;
 
-    snprintf(buffer, max, "STREAM ep uuid %"PRIx64, hdr->sender_uuid);
+    snprintf(buffer, max, "STREAM ep_ptr 0x%lx", hdr->ep_ptr);
     p = buffer + strlen(buffer);
 
+    ucs_assert(hdr->ep_ptr != 0);
     ucp_dump_payload(worker->context, p, buffer + max - p, data + hdr_len,
                      length - hdr_len);
 }
 
-UCP_DEFINE_AM(UCP_FEATURE_STREAM, UCP_AM_ID_STREAM_DATA,
-              ucp_stream_am_handler, ucp_stream_am_dump,
-              UCT_CB_FLAG_SYNC);
+UCP_DEFINE_AM(UCP_FEATURE_STREAM, UCP_AM_ID_STREAM_DATA, ucp_stream_am_handler,
+              ucp_stream_am_dump, 0);
 
 UCP_DEFINE_AM_PROXY(UCP_AM_ID_STREAM_DATA);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_send.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_send.c
index 6be01cd8d..c0779b0e3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_send.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/stream/stream_send.c
@@ -21,7 +21,7 @@ ucp_stream_send_am_short(ucp_ep_t *ep, const void *buffer, size_t length)
     UCS_STATIC_ASSERT(sizeof(ep->worker->uuid) == sizeof(uint64_t));
 
     return uct_ep_am_short(ucp_ep_get_am_uct_ep(ep), UCP_AM_ID_STREAM_DATA,
-                           ep->worker->uuid, buffer, length);
+                           ucp_ep_dest_ep_ptr(ep), buffer, length);
 }
 
 static void ucp_stream_send_req_init(ucp_request_t* req, ucp_ep_h ep,
@@ -30,14 +30,15 @@ static void ucp_stream_send_req_init(ucp_request_t* req, ucp_ep_h ep,
 {
     req->flags             = flags;
     req->send.ep           = ep;
-    req->send.buffer       = buffer;
+    req->send.buffer       = (void*)buffer;
     req->send.datatype     = datatype;
-    req->send.mem_type     = UCT_MD_MEM_TYPE_HOST;
     req->send.lane         = ep->am_lane;
     ucp_request_send_state_init(req, datatype, count);
     req->send.length       = ucp_dt_length(req->send.datatype, count,
                                            req->send.buffer,
                                            &req->send.state.dt);
+    ucp_memory_type_detect_mds(ep->worker->context, (void *)buffer,
+                               req->send.length, &req->send.mem_type);
     VALGRIND_MAKE_MEM_UNDEFINED(&req->send.tag, sizeof(req->send.tag));
 }
 
@@ -48,11 +49,11 @@ ucp_stream_send_req(ucp_request_t *req, size_t count,
 {
     size_t zcopy_thresh = ucp_proto_get_zcopy_threshold(req, msg_config,
                                                         count, SIZE_MAX);
-    size_t seg_size     = msg_config->max_bcopy - proto->only_hdr_size;
     ssize_t max_short   = ucp_proto_get_short_max(req, msg_config);
 
     ucs_status_t status = ucp_request_send_start(req, max_short, zcopy_thresh,
-                                                 seg_size, SIZE_MAX, proto);
+                                                 SIZE_MAX, count, msg_config,
+                                                 proto);
     if (status != UCS_OK) {
         return UCS_STATUS_PTR(status);
     }
@@ -85,9 +86,11 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_send_nb,
     ucs_status_t     status;
     ucs_status_ptr_t ret;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_STREAM,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
-    ucs_trace_req("send_nb buffer %p count %zu to %s cb %p flags %u",
+    ucs_trace_req("stream_send_nb buffer %p count %zu to %s cb %p flags %u",
                   buffer, count, ucp_ep_peer_name(ep), cb, flags);
 
     if (ucs_unlikely(flags != 0)) {
@@ -95,6 +98,12 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_send_nb,
         goto out;
     }
 
+    status = ucp_ep_resolve_dest_ep_ptr(ep, ep->am_lane);
+    if (status != UCS_OK) {
+        ret = UCS_STATUS_PTR(status);
+        goto out;
+    }
+
     if (ucs_likely(UCP_DT_IS_CONTIG(datatype))) {
         length = ucp_contig_dt_length(datatype, count);
         if (ucs_likely((ssize_t)length <= ucp_ep_config(ep)->am.max_short)) {
@@ -120,7 +129,7 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_stream_send_nb,
                               ucp_ep_config(ep)->stream.proto);
 
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
     return ret;
 }
 
@@ -142,11 +151,12 @@ static size_t ucp_stream_pack_am_single_dt(void *dest, void *arg)
     ucp_request_t       *req = arg;
     size_t              length;
 
-    hdr->sender_uuid = req->send.ep->worker->uuid;
+    hdr->ep_ptr = ucp_request_get_dest_ep_ptr(req);
 
     ucs_assert(req->send.state.dt.offset == 0);
 
-    length = ucp_dt_pack(req->send.datatype, hdr + 1, req->send.buffer,
+    length = ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                         req->send.mem_type, hdr + 1, req->send.buffer,
                          &req->send.state.dt, req->send.length);
     ucs_assert(length == req->send.length);
     return sizeof(*hdr) + length;
@@ -172,15 +182,14 @@ static size_t ucp_stream_pack_am_first_dt(void *dest, void *arg)
     ucp_request_t       *req = arg;
     size_t              length;
 
-    hdr->sender_uuid = req->send.ep->worker->uuid;
-    length           = ucp_ep_config(req->send.ep)->am.max_bcopy - sizeof(*hdr);
+    hdr->ep_ptr = ucp_request_get_dest_ep_ptr(req);
+    length      = ucp_ep_config(req->send.ep)->am.max_bcopy - sizeof(*hdr);
 
-    ucs_debug("pack stream_am_first paylen %zu", length);
     ucs_assert(req->send.state.dt.offset == 0);
     ucs_assert(req->send.length > length);
-    return sizeof(*hdr) + ucp_dt_pack(req->send.datatype, hdr + 1,
-                                      req->send.buffer, &req->send.state.dt,
-                                      length);
+    return sizeof(*hdr) + ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                                      req->send.mem_type, hdr + 1, req->send.buffer,
+                                      &req->send.state.dt, length);
 }
 
 static size_t ucp_stream_pack_am_middle_dt(void *dest, void *arg)
@@ -189,44 +198,22 @@ static size_t ucp_stream_pack_am_middle_dt(void *dest, void *arg)
     ucp_request_t       *req = arg;
     size_t              length;
 
-    hdr->sender_uuid = req->send.ep->worker->uuid;
-    length           = ucp_ep_config(req->send.ep)->am.max_bcopy - sizeof(*hdr);
-    ucs_debug("pack stream_am_middle paylen %zu offset %zu", length,
-              req->send.state.dt.offset);
-    return sizeof(*hdr) + ucp_dt_pack(req->send.datatype, hdr + 1,
-                                      req->send.buffer, &req->send.state.dt,
-                                      length);
-}
-
-static size_t ucp_stream_pack_am_last_dt(void *dest, void *arg)
-{
-    size_t              ret_length;
-    ucp_stream_am_hdr_t *hdr   = dest;
-    ucp_request_t       *req   = arg;
-    size_t              length = req->send.length - req->send.state.dt.offset;
-
-    hdr->sender_uuid = req->send.ep->worker->uuid;
-    ret_length       = ucp_dt_pack(req->send.datatype, hdr + 1,
-                                   req->send.buffer, &req->send.state.dt,
-                                   length);
-
-    ucs_debug("pack stream_am_last paylen %zu offset %zu", length,
-              req->send.state.dt.offset);
-    ucs_assertv(ret_length == length, "length=%zu, max_length=%zu",
-                ret_length, length);
-    return sizeof(*hdr) + ret_length;
+    hdr->ep_ptr = ucp_request_get_dest_ep_ptr(req);
+    length      = ucs_min(ucp_ep_config(req->send.ep)->am.max_bcopy - sizeof(*hdr),
+                          req->send.length - req->send.state.dt.offset);
+    return sizeof(*hdr) + ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                                      req->send.mem_type, hdr + 1, req->send.buffer,
+                                      &req->send.state.dt, length);
 }
 
 static ucs_status_t ucp_stream_bcopy_multi(uct_pending_req_t *self)
 {
     ucs_status_t status = ucp_do_am_bcopy_multi(self,
-                                                UCP_AM_ID_STREAM_DATA,
                                                 UCP_AM_ID_STREAM_DATA,
                                                 UCP_AM_ID_STREAM_DATA,
                                                 sizeof(ucp_stream_am_hdr_t),
                                                 ucp_stream_pack_am_first_dt,
-                                                ucp_stream_pack_am_middle_dt,
-                                                ucp_stream_pack_am_last_dt, 0);
+                                                ucp_stream_pack_am_middle_dt, 0);
     if (status == UCS_OK) {
         ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
         ucp_request_send_generic_dt_finish(req);
@@ -242,7 +229,7 @@ static ucs_status_t ucp_stream_eager_zcopy_single(uct_pending_req_t *self)
     ucp_request_t       *req = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_stream_am_hdr_t hdr;
 
-    hdr.sender_uuid = req->send.ep->worker->uuid;
+    hdr.ep_ptr = ucp_request_get_dest_ep_ptr(req);
     return ucp_do_am_zcopy_single(self, UCP_AM_ID_STREAM_DATA, &hdr,
                                   sizeof(hdr), ucp_proto_am_zcopy_req_complete);
 }
@@ -252,9 +239,8 @@ static ucs_status_t ucp_stream_eager_zcopy_multi(uct_pending_req_t *self)
     ucp_request_t       *req = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_stream_am_hdr_t hdr;
 
-    hdr.sender_uuid = req->send.ep->worker->uuid;
+    hdr.ep_ptr = ucp_request_get_dest_ep_ptr(req);
     return ucp_do_am_zcopy_multi(self,
-                                 UCP_AM_ID_STREAM_DATA,
                                  UCP_AM_ID_STREAM_DATA,
                                  UCP_AM_ID_STREAM_DATA,
                                  &hdr, sizeof(hdr), &hdr, sizeof(hdr),
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager.h
index 636eb16f6..a7656bfd4 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager.h
@@ -65,7 +65,7 @@ typedef struct {
 extern const ucp_proto_t ucp_tag_eager_proto;
 extern const ucp_proto_t ucp_tag_eager_sync_proto;
 
-void ucp_tag_eager_sync_send_ack(ucp_worker_h worker, void *hdr, uint16_t flags);
+void ucp_tag_eager_sync_send_ack(ucp_worker_h worker, void *hdr, uint16_t recv_flags);
 
 void ucp_tag_eager_sync_completion(ucp_request_t *req, uint16_t flag,
                                    ucs_status_t status);
@@ -78,18 +78,4 @@ void ucp_tag_eager_sync_zcopy_req_complete(ucp_request_t *req, ucs_status_t stat
 
 void ucp_tag_eager_sync_zcopy_completion(uct_completion_t *self, ucs_status_t status);
 
-static inline ucs_status_t ucp_tag_send_eager_short(ucp_ep_t *ep, ucp_tag_t tag,
-                                                    const void *buffer, size_t length)
-{
-    if (ep->flags & UCP_EP_FLAG_TAG_OFFLOAD_ENABLED) {
-        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(uct_tag_t));
-        return uct_ep_tag_eager_short(ucp_ep_get_tag_uct_ep(ep), tag, buffer, length);
-    } else {
-        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(ucp_eager_hdr_t));
-        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(uint64_t));
-        return uct_ep_am_short(ucp_ep_get_am_uct_ep(ep), UCP_AM_ID_EAGER_ONLY, tag,
-                               buffer, length);
-    }
-}
-
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_rcv.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_rcv.c
index 9ae1ca325..9e6439906 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_rcv.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_rcv.c
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
@@ -13,10 +13,63 @@
 #include <ucs/datastruct/queue.h>
 #include <ucp/core/ucp_request.inl>
 
+static UCS_F_ALWAYS_INLINE void
+ucp_eager_expected_handler(ucp_worker_t *worker, ucp_request_t *req,
+                           void *data, size_t recv_len, ucp_tag_t recv_tag,
+                           uint16_t flags)
+{
+    ucs_trace_req("found req %p", req);
+    UCS_PROFILE_REQUEST_EVENT(req, "eager_recv", recv_len);
+
+    /* First fragment fills the receive information */
+    UCP_WORKER_STAT_EAGER_MSG(worker, flags);
+    UCP_WORKER_STAT_EAGER_CHUNK(worker, EXP);
+
+    req->recv.tag.info.sender_tag = recv_tag;
+
+    /* Cancel req in transport if it was offloaded,
+     * because it arrived either:
+     * 1) via SW TM (e. g. peer doesn't support offload)
+     * 2) as unexpected via HW TM */
+    ucp_tag_offload_try_cancel(worker, req,
+                               UCP_TAG_OFFLOAD_CANCEL_FORCE |
+                               UCP_TAG_OFFLOAD_CANCEL_DEREG);
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+ucp_eager_offload_handler(void *arg, void *data, size_t length,
+                          unsigned tl_flags, uint16_t flags, ucp_tag_t recv_tag)
+{
+    ucp_worker_t *worker = arg;
+    ucp_request_t *req;
+    ucp_recv_desc_t *rdesc;
+    ucp_tag_t *rdesc_hdr;
+    ucs_status_t status;
+
+    req = ucp_tag_exp_search(&worker->tm, recv_tag);
+    if (req != NULL) {
+        ucp_eager_expected_handler(worker, req, data, length, recv_tag, flags);
+        req->recv.tag.info.length = length;
+        status = ucp_request_recv_data_unpack(req, data, length, 0, 1);
+        ucp_request_complete_tag_recv(req, status);
+        status = UCS_OK;
+    } else {
+        status = ucp_recv_desc_init(worker, data, length, sizeof(ucp_tag_t),
+                                    tl_flags, sizeof(ucp_tag_t), flags,
+                                    sizeof(ucp_tag_t), &rdesc);
+        if (!UCS_STATUS_IS_ERR(status)) {
+            rdesc_hdr  = (ucp_tag_t*)(rdesc + 1);
+            *rdesc_hdr = recv_tag;
+            ucp_tag_unexp_recv(&worker->tm, rdesc, recv_tag);
+        }
+    }
+
+    return status;
+}
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
 ucp_eager_tagged_handler(void *arg, void *data, size_t length, unsigned am_flags,
-                         uint16_t flags, uint16_t hdr_len)
+                         uint16_t flags, uint16_t hdr_len, uint16_t priv_length)
 {
     ucp_worker_h worker        = arg;
     ucp_eager_hdr_t *eager_hdr = data;
@@ -35,22 +88,7 @@ ucp_eager_tagged_handler(void *arg, void *data, size_t length, unsigned am_flags
 
     req = ucp_tag_exp_search(&worker->tm, recv_tag);
     if (req != NULL) {
-        ucs_trace_req("found req %p", req);
-        UCS_PROFILE_REQUEST_EVENT(req, "eager_recv", recv_len);
-
-        /* First fragment fills the receive information */
-        UCP_WORKER_STAT_EAGER_MSG(worker, flags);
-        UCP_WORKER_STAT_EAGER_CHUNK(worker, EXP);
-
-        req->recv.tag.info.sender_tag = recv_tag;
-
-        /* Cancel req in transport if it was offloaded,
-         * because it arrived either:
-         * 1) via SW TM (e. g. peer doesn't support offload)
-         * 2) as unexpected via HW TM */
-        ucp_tag_offload_try_cancel(worker, req,
-                                   UCP_TAG_OFFLOAD_CANCEL_FORCE |
-                                   UCP_TAG_OFFLOAD_CANCEL_DEREG);
+        ucp_eager_expected_handler(worker, req, data, recv_len, recv_tag, flags);
 
         if (flags & UCP_RECV_DESC_FLAG_EAGER_SYNC) {
             ucp_tag_eager_sync_send_ack(worker, data, flags);
@@ -76,8 +114,8 @@ ucp_eager_tagged_handler(void *arg, void *data, size_t length, unsigned am_flags
 
         status = UCS_OK;
     } else {
-        status = ucp_recv_desc_init(worker, data, length, am_flags, hdr_len,
-                                    flags, &rdesc);
+        status = ucp_recv_desc_init(worker, data, length, 0, am_flags, hdr_len,
+                                    flags, priv_length, &rdesc);
         if (!UCS_STATUS_IS_ERR(status)) {
             ucp_tag_unexp_recv(&worker->tm, rdesc, recv_tag);
         }
@@ -93,7 +131,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_only_handler,
     return ucp_eager_tagged_handler(arg, data, length, am_flags,
                                     UCP_RECV_DESC_FLAG_EAGER |
                                     UCP_RECV_DESC_FLAG_EAGER_ONLY,
-                                    sizeof(ucp_eager_hdr_t));
+                                    sizeof(ucp_eager_hdr_t), 0);
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_first_handler,
@@ -102,7 +140,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_first_handler,
 {
     return ucp_eager_tagged_handler(arg, data, length, am_flags,
                                     UCP_RECV_DESC_FLAG_EAGER,
-                                    sizeof(ucp_eager_first_hdr_t));
+                                    sizeof(ucp_eager_first_hdr_t), 0);
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_middle_handler,
@@ -128,8 +166,9 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_middle_handler,
 
     if (ucp_tag_frag_match_is_unexp(matchq)) {
         /* add new received descriptor to the queue */
-        status = ucp_recv_desc_init(worker, data, length, am_flags, sizeof(*hdr),
-                                    UCP_RECV_DESC_FLAG_EAGER, &rdesc);
+        status = ucp_recv_desc_init(worker, data, length, 0, am_flags,
+                                    sizeof(*hdr), UCP_RECV_DESC_FLAG_EAGER, 0,
+                                    &rdesc);
         if (!UCS_STATUS_IS_ERR(status)) {
             ucp_tag_frag_match_add_unexp(matchq, rdesc, hdr->offset);
         }
@@ -160,7 +199,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_sync_only_handler,
                                     UCP_RECV_DESC_FLAG_EAGER|
                                     UCP_RECV_DESC_FLAG_EAGER_ONLY|
                                     UCP_RECV_DESC_FLAG_EAGER_SYNC,
-                                    sizeof(ucp_eager_sync_hdr_t));
+                                    sizeof(ucp_eager_sync_hdr_t), 0);
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_sync_first_handler,
@@ -170,7 +209,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_sync_first_handler,
     return ucp_eager_tagged_handler(arg, data, length, am_flags,
                                     UCP_RECV_DESC_FLAG_EAGER|
                                     UCP_RECV_DESC_FLAG_EAGER_SYNC,
-                                    sizeof(ucp_eager_sync_first_hdr_t));
+                                    sizeof(ucp_eager_sync_first_hdr_t), 0);
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_offload_sync_ack_handler,
@@ -185,15 +224,15 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_eager_offload_sync_ack_handler,
 
     ucs_queue_for_each_safe(sreq, iter, queue, send.tag_offload.queue) {
         if ((sreq->send.tag_offload.ssend_tag == rep_hdr->sender_tag) &&
-            (worker->uuid == rep_hdr->sender_uuid)) {
+            ((uintptr_t)sreq->send.ep == rep_hdr->ep_ptr)) {
             ucp_tag_eager_sync_completion(sreq, UCP_REQUEST_FLAG_REMOTE_COMPLETED,
                                           UCS_OK);
             ucs_queue_del_iter(queue, iter);
             return UCS_OK;
         }
     }
-    ucs_error("Unexpected sync ack received: tag %"PRIx64" uuid %"PRIx64"",
-              rep_hdr->sender_tag, rep_hdr->sender_uuid);
+    ucs_error("unexpected sync ack received: tag %"PRIx64" ep_ptr 0x%lx",
+              rep_hdr->sender_tag, rep_hdr->ep_ptr);
     return UCS_OK;
 }
 
@@ -219,37 +258,36 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_offload_unexp_eager,
     uint16_t flags             = UCP_RECV_DESC_FLAG_EAGER |
                                  UCP_RECV_DESC_FLAG_EAGER_ONLY |
                                  UCP_RECV_DESC_FLAG_EAGER_OFFLOAD;
-    ucp_eager_hdr_t *hdr;
-    ucp_eager_sync_hdr_t *sync_hdr;
-    unsigned hdr_len;
+    ucp_eager_sync_hdr_t *hdr;
+    int hdr_len;
+
+    UCP_WORKER_STAT_TAG_OFFLOAD(wiface->worker, RX_UNEXP_EGR);
 
-    hdr_len = ucs_unlikely(imm) ? sizeof(ucp_eager_sync_hdr_t) :
-                                  sizeof(ucp_eager_hdr_t);
+    ucp_tag_offload_unexp(wiface, stag, length);
+
+    if (ucs_likely(!imm)) {
+        return ucp_eager_offload_handler(wiface->worker, data, length, tl_flags,
+                                         flags, stag);
+    }
+
+    /* It is a sync send, imm data contains sender uuid */
+    hdr_len = sizeof(ucp_eager_sync_hdr_t);
 
     if (ucs_unlikely(tl_flags & UCT_CB_PARAM_FLAG_DESC)) {
-        hdr = (ucp_eager_hdr_t*)((char*)data - hdr_len);
+        hdr = (ucp_eager_sync_hdr_t*)(UCS_PTR_BYTE_OFFSET(data, -hdr_len));
     } else {
         /* Can not shift back, no headroom */
         hdr = ucs_alloca(length + hdr_len);
-        memcpy((char*)hdr + hdr_len, data, length);
-    }
-
-    hdr->super.tag = stag;
-
-    if (ucs_unlikely(imm)) {
-        /* It is a sync send, imm data contains sender uuid */
-        sync_hdr = ucs_derived_of(hdr, ucp_eager_sync_hdr_t);
-        flags   |= UCP_RECV_DESC_FLAG_EAGER_SYNC;
-        sync_hdr->req.reqptr      = 0ul;
-        sync_hdr->req.sender_uuid = imm;
+        memcpy(UCS_PTR_BYTE_OFFSET(hdr, hdr_len), data, length);
     }
 
-    UCP_WORKER_STAT_TAG_OFFLOAD(wiface->worker, RX_UNEXP_EGR);
-
-    ucp_tag_offload_unexp(wiface, stag);
+    hdr->super.super.tag = stag;
+    hdr->req.reqptr      = 0ul;
+    hdr->req.ep_ptr      = imm;
+    flags               |= UCP_RECV_DESC_FLAG_EAGER_SYNC;
 
     return ucp_eager_tagged_handler(wiface->worker, hdr, length + hdr_len,
-                                    tl_flags, flags, hdr_len);
+                                    tl_flags, flags, hdr_len, hdr_len);
 }
 
 static void ucp_eager_dump(ucp_worker_h worker, uct_am_trace_type_t type,
@@ -283,18 +321,19 @@ static void ucp_eager_dump(ucp_worker_h worker, uct_am_trace_type_t type,
         header_len = sizeof(*eager_mid_hdr);
         break;
     case UCP_AM_ID_EAGER_SYNC_ONLY:
-        snprintf(buffer, max, "EGRS tag %"PRIx64" uuid %"PRIx64" request 0x%lx",
-                 eagers_hdr->super.super.tag, eagers_hdr->req.sender_uuid,
+        ucs_assert(eagers_hdr->req.ep_ptr != 0);
+        snprintf(buffer, max, "EGRS tag %"PRIx64" ep_ptr 0x%lx request 0x%lx",
+                 eagers_hdr->super.super.tag, eagers_hdr->req.ep_ptr,
                  eagers_hdr->req.reqptr);
         header_len = sizeof(*eagers_hdr);
         break;
     case UCP_AM_ID_EAGER_SYNC_FIRST:
         snprintf(buffer, max, "EGRS_F tag %"PRIx64" msgid %"PRIx64" len %zu "
-                 "uuid %"PRIx64" request 0x%lx",
+                 "ep_ptr 0x%lx request 0x%lx",
                  eagers_first_hdr->super.super.super.tag,
                  eagers_first_hdr->super.msg_id,
                  eagers_first_hdr->super.total_len,
-                 eagers_first_hdr->req.sender_uuid,
+                 eagers_first_hdr->req.ep_ptr,
                  eagers_first_hdr->req.reqptr);
         header_len = sizeof(*eagers_first_hdr);
         break;
@@ -304,8 +343,8 @@ static void ucp_eager_dump(ucp_worker_h worker, uct_am_trace_type_t type,
         header_len = sizeof(*rep_hdr);
         break;
     case UCP_AM_ID_OFFLOAD_SYNC_ACK:
-        snprintf(buffer, max, "EGRS_A_O tag %"PRIx64" uuid %"PRIx64"",
-                 off_rep_hdr->sender_tag, off_rep_hdr->sender_uuid);
+        snprintf(buffer, max, "EGRS_A_O tag %"PRIx64" ep_ptr 0x%lx",
+                 off_rep_hdr->sender_tag, off_rep_hdr->ep_ptr);
         header_len = sizeof(*rep_hdr);
         break;
     default:
@@ -318,19 +357,19 @@ static void ucp_eager_dump(ucp_worker_h worker, uct_am_trace_type_t type,
 }
 
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_ONLY, ucp_eager_only_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
+              ucp_eager_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_FIRST, ucp_eager_first_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
+              ucp_eager_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_MIDDLE, ucp_eager_middle_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
-UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_ONLY, ucp_eager_sync_only_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
-UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_FIRST, ucp_eager_sync_first_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
-UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_ACK, ucp_eager_sync_ack_handler,
-              ucp_eager_dump, UCT_CB_FLAG_SYNC);
+              ucp_eager_dump, 0);
+UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_ONLY,
+              ucp_eager_sync_only_handler, ucp_eager_dump, 0);
+UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_FIRST,
+              ucp_eager_sync_first_handler, ucp_eager_dump, 0);
+UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_EAGER_SYNC_ACK,
+              ucp_eager_sync_ack_handler, ucp_eager_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_OFFLOAD_SYNC_ACK,
-              ucp_eager_offload_sync_ack_handler, ucp_eager_dump, UCT_CB_FLAG_SYNC);
+              ucp_eager_offload_sync_ack_handler, ucp_eager_dump, 0);
 
 UCP_DEFINE_AM_PROXY(UCP_AM_ID_EAGER_ONLY);
 UCP_DEFINE_AM_PROXY(UCP_AM_ID_EAGER_FIRST);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_snd.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_snd.c
index 14469c9f2..8cbab1b8b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_snd.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/eager_snd.c
@@ -23,7 +23,8 @@ static size_t ucp_tag_pack_eager_only_dt(void *dest, void *arg)
     hdr->super.tag = req->send.tag.tag;
 
     ucs_assert(req->send.state.dt.offset == 0);
-    length = ucp_dt_pack(req->send.datatype, hdr + 1, req->send.buffer,
+    length = ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                         req->send.mem_type, hdr + 1, req->send.buffer,
                          &req->send.state.dt, req->send.length);
     ucs_assert(length == req->send.length);
     return sizeof(*hdr) + length;
@@ -36,11 +37,12 @@ static size_t ucp_tag_pack_eager_sync_only_dt(void *dest, void *arg)
     size_t length;
 
     hdr->super.super.tag = req->send.tag.tag;
-    hdr->req.sender_uuid = req->send.ep->worker->uuid;
+    hdr->req.ep_ptr      = ucp_request_get_dest_ep_ptr(req);
     hdr->req.reqptr      = (uintptr_t)req;
 
     ucs_assert(req->send.state.dt.offset == 0);
-    length = ucp_dt_pack(req->send.datatype, hdr + 1, req->send.buffer,
+    length = ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                         req->send.mem_type, hdr + 1, req->send.buffer,
                          &req->send.state.dt, req->send.length);
     ucs_assert(length == req->send.length);
     return sizeof(*hdr) + length;
@@ -62,9 +64,9 @@ static size_t ucp_tag_pack_eager_first_dt(void *dest, void *arg)
 
     ucs_assert(req->send.state.dt.offset == 0);
     ucs_assert(req->send.length > length);
-    return sizeof(*hdr) + ucp_dt_pack(req->send.datatype, hdr + 1,
-                                      req->send.buffer, &req->send.state.dt,
-                                      length); 
+    return sizeof(*hdr) + ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                                      req->send.mem_type, hdr + 1, req->send.buffer,
+                                      &req->send.state.dt, length);
 }
 
 static size_t ucp_tag_pack_eager_sync_first_dt(void *dest, void *arg)
@@ -79,15 +81,15 @@ static size_t ucp_tag_pack_eager_sync_first_dt(void *dest, void *arg)
                                     sizeof(*hdr);
     hdr->super.super.super.tag    = req->send.tag.tag;
     hdr->super.total_len          = req->send.length;
-    hdr->req.sender_uuid          = req->send.ep->worker->uuid;
+    hdr->req.ep_ptr               = ucp_request_get_dest_ep_ptr(req);
     hdr->super.msg_id             = req->send.tag.message_id;
     hdr->req.reqptr               = (uintptr_t)req;
 
     ucs_assert(req->send.state.dt.offset == 0);
     ucs_assert(req->send.length > length);
-    return sizeof(*hdr) + ucp_dt_pack(req->send.datatype, hdr + 1,
-                                      req->send.buffer, &req->send.state.dt,
-                                      length);
+    return sizeof(*hdr) + ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                                      req->send.mem_type, hdr + 1, req->send.buffer,
+                                      &req->send.state.dt, length);
 }
 
 static size_t ucp_tag_pack_eager_middle_dt(void *dest, void *arg)
@@ -96,29 +98,14 @@ static size_t ucp_tag_pack_eager_middle_dt(void *dest, void *arg)
     ucp_request_t *req          = arg;
     size_t length;
 
-    length          = ucp_ep_get_max_bcopy(req->send.ep, req->send.lane) -
-                      sizeof(*hdr);
+    length          = ucs_min(ucp_ep_get_max_bcopy(req->send.ep, req->send.lane) -
+                              sizeof(*hdr),
+                              req->send.length - req->send.state.dt.offset);
     hdr->msg_id     = req->send.tag.message_id;
     hdr->offset     = req->send.state.dt.offset;
-    return sizeof(*hdr) + ucp_dt_pack(req->send.datatype, hdr + 1,
-                                      req->send.buffer, &req->send.state.dt,
-                                      length);
-}
-
-static size_t ucp_tag_pack_eager_last_dt(void *dest, void *arg)
-{
-    ucp_eager_middle_hdr_t *hdr = dest;
-    ucp_request_t *req          = arg;
-    size_t length, ret_length;
-
-    length         = req->send.length - req->send.state.dt.offset;
-    hdr->msg_id    = req->send.tag.message_id;
-    hdr->offset    = req->send.state.dt.offset;
-    ret_length     = ucp_dt_pack(req->send.datatype, hdr + 1, req->send.buffer,
-                                 &req->send.state.dt, length);
-    ucs_assertv(ret_length == length, "length=%zu, max_length=%zu",
-                ret_length, length);
-    return sizeof(*hdr) + ret_length;
+    return sizeof(*hdr) + ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                                      req->send.mem_type, hdr + 1, req->send.buffer,
+                                      &req->send.state.dt, length);
 }
 
 /* eager */
@@ -157,11 +144,9 @@ static ucs_status_t ucp_tag_eager_bcopy_multi(uct_pending_req_t *self)
     ucs_status_t status = ucp_do_am_bcopy_multi(self,
                                                 UCP_AM_ID_EAGER_FIRST,
                                                 UCP_AM_ID_EAGER_MIDDLE,
-                                                UCP_AM_ID_EAGER_MIDDLE,
                                                 sizeof(ucp_eager_middle_hdr_t),
                                                 ucp_tag_pack_eager_first_dt,
-                                                ucp_tag_pack_eager_middle_dt,
-                                                ucp_tag_pack_eager_last_dt, 1);
+                                                ucp_tag_pack_eager_middle_dt, 1);
     if (status == UCS_OK) {
         ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
         ucp_request_send_generic_dt_finish(req);
@@ -197,7 +182,6 @@ static ucs_status_t ucp_tag_eager_zcopy_multi(uct_pending_req_t *self)
     return ucp_do_am_zcopy_multi(self,
                                  UCP_AM_ID_EAGER_FIRST,
                                  UCP_AM_ID_EAGER_MIDDLE,
-                                 UCP_AM_ID_EAGER_MIDDLE,
                                  &first_hdr, sizeof(first_hdr),
                                  &middle_hdr, sizeof(middle_hdr),
                                  ucp_proto_am_zcopy_req_complete, 1);
@@ -252,11 +236,9 @@ static ucs_status_t ucp_tag_eager_sync_bcopy_multi(uct_pending_req_t *self)
     ucs_status_t status = ucp_do_am_bcopy_multi(self,
                                                 UCP_AM_ID_EAGER_SYNC_FIRST,
                                                 UCP_AM_ID_EAGER_MIDDLE,
-                                                UCP_AM_ID_EAGER_MIDDLE,
                                                 sizeof(ucp_eager_middle_hdr_t),
                                                 ucp_tag_pack_eager_sync_first_dt,
-                                                ucp_tag_pack_eager_middle_dt,
-                                                ucp_tag_pack_eager_last_dt, 1);
+                                                ucp_tag_pack_eager_middle_dt, 1);
     if (status == UCS_OK) {
         ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
         ucp_request_send_generic_dt_finish(req);
@@ -286,7 +268,7 @@ static ucs_status_t ucp_tag_eager_sync_zcopy_single(uct_pending_req_t *self)
     ucp_eager_sync_hdr_t hdr;
 
     hdr.super.super.tag = req->send.tag.tag;
-    hdr.req.sender_uuid = req->send.ep->worker->uuid;
+    hdr.req.ep_ptr      = ucp_request_get_dest_ep_ptr(req);
     hdr.req.reqptr      = (uintptr_t)req;
 
     return ucp_do_am_zcopy_single(self, UCP_AM_ID_EAGER_SYNC_ONLY, &hdr, sizeof(hdr),
@@ -301,7 +283,7 @@ static ucs_status_t ucp_tag_eager_sync_zcopy_multi(uct_pending_req_t *self)
 
     first_hdr.super.super.super.tag = req->send.tag.tag;
     first_hdr.super.total_len       = req->send.length;
-    first_hdr.req.sender_uuid       = req->send.ep->worker->uuid;
+    first_hdr.req.ep_ptr            = ucp_request_get_dest_ep_ptr(req);
     first_hdr.req.reqptr            = (uintptr_t)req;
     first_hdr.super.msg_id          = req->send.tag.message_id;
     middle_hdr.msg_id               = req->send.tag.message_id;
@@ -310,7 +292,6 @@ static ucs_status_t ucp_tag_eager_sync_zcopy_multi(uct_pending_req_t *self)
     return ucp_do_am_zcopy_multi(self,
                                  UCP_AM_ID_EAGER_SYNC_FIRST,
                                  UCP_AM_ID_EAGER_MIDDLE,
-                                 UCP_AM_ID_EAGER_MIDDLE,
                                  &first_hdr, sizeof(first_hdr),
                                  &middle_hdr, sizeof(middle_hdr),
                                  ucp_tag_eager_sync_zcopy_req_complete, 1);
@@ -336,38 +317,41 @@ const ucp_proto_t ucp_tag_eager_sync_proto = {
     .mid_hdr_size            = sizeof(ucp_eager_hdr_t)
 };
 
-void ucp_tag_eager_sync_send_ack(ucp_worker_h worker, void *hdr, uint16_t flags)
+void ucp_tag_eager_sync_send_ack(ucp_worker_h worker, void *hdr, uint16_t recv_flags)
 {
-    ucp_eager_sync_hdr_t *eagers_hdr;
     ucp_request_hdr_t *reqhdr;
     ucp_request_t *req;
 
-    ucs_assert(flags & UCP_RECV_DESC_FLAG_EAGER_SYNC);
+    ucs_assert(recv_flags & UCP_RECV_DESC_FLAG_EAGER_SYNC);
 
-    if (flags & UCP_RECV_DESC_FLAG_EAGER_OFFLOAD) {
-        eagers_hdr = hdr;
-        ucp_tag_offload_eager_sync_send_ack(worker,
-                                            eagers_hdr->req.sender_uuid,
-                                            eagers_hdr->super.super.tag);
-        return;
+    if (recv_flags & UCP_RECV_DESC_FLAG_EAGER_ONLY) {
+        reqhdr = &((ucp_eager_sync_hdr_t*)hdr)->req;       /* only */
+    } else {
+        reqhdr = &((ucp_eager_sync_first_hdr_t*)hdr)->req; /* first */
     }
 
-    if (flags & UCP_RECV_DESC_FLAG_EAGER_ONLY) {
-        reqhdr = &((ucp_eager_sync_hdr_t*)hdr)->req;
-    } else /* first */ {
-        reqhdr = &((ucp_eager_sync_first_hdr_t*)hdr)->req;
+    req = ucp_request_get(worker);
+    if (req == NULL) {
+        ucs_fatal("could not allocate request");
     }
 
-    ucs_assert(reqhdr->reqptr != 0);
-    ucs_trace_req("send_sync_ack sender_uuid %"PRIx64" remote_request 0x%lx",
-                  reqhdr->sender_uuid, reqhdr->reqptr);
-
-    req = ucp_worker_allocate_reply(worker, reqhdr->sender_uuid);
-    req->send.uct.func             = ucp_proto_progress_am_bcopy_single;
-    req->send.proto.am_id          = UCP_AM_ID_EAGER_SYNC_ACK;
-    req->send.proto.remote_request = reqhdr->reqptr;
-    req->send.proto.status         = UCS_OK;
-    req->send.proto.comp_cb        = ucp_request_put;
+    req->flags              = 0;
+    req->send.ep            = ucp_worker_get_ep_by_ptr(worker, reqhdr->ep_ptr);
+    req->send.uct.func      = ucp_proto_progress_am_bcopy_single;
+    req->send.proto.comp_cb = ucp_request_put;
+    req->send.proto.status  = UCS_OK;
+
+    ucs_trace_req("send_sync_ack req %p ep %p", req, req->send.ep);
+
+    if (recv_flags & UCP_RECV_DESC_FLAG_EAGER_OFFLOAD) {
+        ucs_assert(recv_flags & UCP_RECV_DESC_FLAG_EAGER_ONLY);
+        req->send.proto.am_id          = UCP_AM_ID_OFFLOAD_SYNC_ACK;
+        req->send.proto.sender_tag     = ((ucp_eager_sync_hdr_t*)hdr)->super.super.tag;
+    } else {
+        ucs_assert(reqhdr->reqptr != 0);
+        req->send.proto.am_id          = UCP_AM_ID_EAGER_SYNC_ACK;
+        req->send.proto.remote_request = reqhdr->reqptr;
+    }
 
     ucp_request_send(req);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.c
index d0bf5123a..3fe7fe7f9 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.c
@@ -21,28 +21,27 @@ int ucp_tag_offload_iface_activate(ucp_worker_iface_t *iface)
     ucp_worker_t *worker   = iface->worker;
     ucp_context_t *context = worker->context;
 
-    if (!worker->tm.offload.num_ifaces) {
+    if (worker->tm.offload.iface == NULL) {
         ucs_assert(worker->tm.offload.thresh       == SIZE_MAX);
         ucs_assert(worker->tm.offload.zcopy_thresh == SIZE_MAX);
         ucs_assert(worker->tm.offload.iface        == NULL);
 
         worker->tm.offload.thresh       = context->config.ext.tm_thresh;
-        worker->tm.offload.zcopy_thresh = context->config.ext.tm_max_bcopy;
+        worker->tm.offload.zcopy_thresh = context->config.ext.tm_max_bb_size;
 
-        /* Cache active offload iface, in most cases only one iface should be
-         * used for offload. If more ifaces will be activated, they will be
-         * added to offload hash table. */
+        /* Cache active offload iface. Can use it if this will be the only
+         * active iface on the worker. Otherwise would need to retrieve
+         * offload-capable iface from the offload hash table. */
         worker->tm.offload.iface        = iface;
 
         ucs_debug("Enable TM offload: thresh %zu, zcopy_thresh %zu",
                   worker->tm.offload.thresh, worker->tm.offload.zcopy_thresh);
     }
 
-    ++worker->tm.offload.num_ifaces;
     iface->flags |= UCP_WORKER_IFACE_FLAG_OFFLOAD_ACTIVATED;
 
-    ucs_debug("Activate tag offload iface %p, num of offload ifaces %d",
-              iface, worker->tm.offload.num_ifaces);
+    ucs_debug("Activate tag offload iface %p", iface);
+
     return 1;
 }
 
@@ -52,7 +51,7 @@ ucp_tag_offload_iface(ucp_worker_t *worker, ucp_tag_t tag)
     khiter_t hash_it;
     ucp_tag_t key_tag;
 
-    if (ucs_likely(worker->tm.offload.num_ifaces == 1)) {
+    if (worker->num_active_ifaces == 1) {
         ucs_assert(worker->tm.offload.iface != NULL);
         return worker->tm.offload.iface;
     }
@@ -76,7 +75,8 @@ ucp_tag_offload_release_buf(ucp_request_t *req, int dereg)
 }
 
 /* Tag consumed by the transport - need to remove it from expected queue */
-void ucp_tag_offload_tag_consumed(uct_tag_context_t *self)
+UCS_PROFILE_FUNC_VOID(ucp_tag_offload_tag_consumed, (self),
+                      uct_tag_context_t *self)
 {
     ucp_request_t *req = ucs_container_of(self, ucp_request_t, recv.uct_ctx);
     ucs_queue_head_t *queue;
@@ -86,10 +86,13 @@ void ucp_tag_offload_tag_consumed(uct_tag_context_t *self)
 }
 
 /* Message is scattered to user buffer by the transport, complete the request */
-void ucp_tag_offload_completed(uct_tag_context_t *self, uct_tag_t stag,
-                               uint64_t imm, size_t length, ucs_status_t status)
+UCS_PROFILE_FUNC_VOID(ucp_tag_offload_completed,
+                      (self, stag, imm, length, status),
+                      uct_tag_context_t *self, uct_tag_t stag,
+                      uint64_t imm, size_t length, ucs_status_t status)
 {
-    ucp_request_t *req   = ucs_container_of(self, ucp_request_t, recv.uct_ctx);
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, recv.uct_ctx);
+    ucp_eager_sync_hdr_t hdr;
 
     req->recv.tag.info.sender_tag = stag;
     req->recv.tag.info.length     = length;
@@ -100,8 +103,15 @@ void ucp_tag_offload_completed(uct_tag_context_t *self, uct_tag_t stag,
     }
 
     if (ucs_unlikely(imm)) {
+        hdr.req.ep_ptr      = imm;
+        hdr.req.reqptr      = 0;   /* unused */
+        hdr.super.super.tag = stag;
+
         /* Sync send - need to send a reply */
-        ucp_tag_offload_eager_sync_send_ack(req->recv.worker, imm, stag);
+        ucp_tag_eager_sync_send_ack(req->recv.worker, &hdr,
+                                    UCP_RECV_DESC_FLAG_EAGER_ONLY |
+                                    UCP_RECV_DESC_FLAG_EAGER_SYNC |
+                                    UCP_RECV_DESC_FLAG_EAGER_OFFLOAD);
     }
 
     if (req->recv.tag.rdesc != NULL) {
@@ -119,9 +129,11 @@ out:
 }
 
 /* RNDV request matched by the transport. Need to proceed with SW based RNDV */
-void ucp_tag_offload_rndv_cb(uct_tag_context_t *self, uct_tag_t stag,
-                             const void *header, unsigned header_length,
-                             ucs_status_t status)
+UCS_PROFILE_FUNC_VOID(ucp_tag_offload_rndv_cb,
+                      (self, stag, header, header_length, status),
+                      uct_tag_context_t *self, uct_tag_t stag,
+                      const void *header, unsigned header_length,
+                      ucs_status_t status)
 {
     ucp_request_t *req = ucs_container_of(self, ucp_request_t, recv.uct_ctx);
 
@@ -169,13 +181,13 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_offload_unexp_rndv,
          */
         dummy_rts                   = ucs_alloca(dummy_rts_size);
         dummy_rts->super.tag        = stag;
-        dummy_rts->sreq.sender_uuid = rndv_hdr->sender_uuid;
+        dummy_rts->sreq.ep_ptr      = rndv_hdr->ep_ptr;
         dummy_rts->sreq.reqptr      = rndv_hdr->reqptr;
         dummy_rts->address          = remote_addr;
         dummy_rts->size             = length;
 
-        ucp_rkey_packed_copy(worker->context, UCS_BIT(md_index), dummy_rts + 1,
-                             uct_rkeys);
+        ucp_rkey_packed_copy(worker->context, UCS_BIT(md_index),
+                             UCT_MD_MEM_TYPE_HOST, dummy_rts + 1, uct_rkeys);
 
         UCP_WORKER_STAT_TAG_OFFLOAD(worker, RX_UNEXP_RNDV);
         ucp_rndv_process_rts(worker, dummy_rts, dummy_rts_size, 0);
@@ -190,12 +202,16 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_offload_unexp_rndv,
         ucp_rndv_process_rts(worker, (void*)hdr, hdr_length, 0);
     }
 
-    ucp_tag_offload_unexp(iface, stag);
+    /* Unexpected RNDV (both SW and HW) need to enable offload capabilities.
+     * Pass TM_THRESH value as a length to make sure tag is added to the
+     * hash table if there is a need (i.e. we have several active ifaces). */
+    ucp_tag_offload_unexp(iface, stag, worker->tm.offload.thresh);
 
     return UCS_OK;
 }
 
-void ucp_tag_offload_cancel(ucp_worker_t *worker, ucp_request_t *req, unsigned mode)
+UCS_PROFILE_FUNC_VOID(ucp_tag_offload_cancel, (worker, req, mode),
+                      ucp_worker_t *worker, ucp_request_t *req, unsigned mode)
 {
 
     ucp_worker_iface_t *wiface = req->recv.tag.wiface;
@@ -219,7 +235,7 @@ void ucp_tag_offload_cancel(ucp_worker_t *worker, ucp_request_t *req, unsigned m
 }
 
 static UCS_F_ALWAYS_INLINE int
-ucp_tag_offload_do_post(ucp_request_t *req, ucp_request_queue_t *req_queue)
+ucp_tag_offload_do_post(ucp_request_t *req)
 {
     ucp_worker_t *worker   = req->recv.worker;
     ucp_context_t *context = worker->context;
@@ -252,7 +268,8 @@ ucp_tag_offload_do_post(ucp_request_t *req, ucp_request_queue_t *req_queue)
         /* register the whole buffer to support SW RNDV fallback */
         status = ucp_request_memory_reg(context, UCS_BIT(mdi), req->recv.buffer,
                                         req->recv.length, req->recv.datatype,
-                                        &req->recv.state, req);
+                                        &req->recv.state, req->recv.mem_type,
+                                        req, UCT_MD_MEM_FLAG_HIDE_ERRORS);
         if (status != UCS_OK) {
             return status;
         }
@@ -319,7 +336,6 @@ static UCS_F_ALWAYS_INLINE int
 ucp_tag_offload_post_sw_reqs(ucp_request_t *req, ucp_request_queue_t *req_queue)
 {
     ucp_worker_t *worker = req->recv.worker;
-    ucs_queue_iter_t iter;
     ucs_status_t status;
     ucp_request_t *req_exp;
     ucp_worker_iface_t *wiface;
@@ -350,12 +366,12 @@ ucp_tag_offload_post_sw_reqs(ucp_request_t *req, ucp_request_queue_t *req_queue)
         return 0;
     }
 
-    ucs_queue_for_each_safe(req_exp, iter, &req_queue->queue, recv.queue) {
+    ucs_queue_for_each(req_exp, &req_queue->queue, recv.queue) {
         if (req_exp->flags & UCP_REQUEST_FLAG_OFFLOADED) {
             continue;
         }
         ucs_assert(req_exp != req);
-        status = ucp_tag_offload_do_post(req_exp, req_queue);
+        status = ucp_tag_offload_do_post(req_exp);
         if (status != UCS_OK) {
             return 0;
         }
@@ -366,7 +382,8 @@ ucp_tag_offload_post_sw_reqs(ucp_request_t *req, ucp_request_queue_t *req_queue)
     return 1;
 }
 
-int ucp_tag_offload_post(ucp_request_t *req, ucp_request_queue_t *req_queue)
+UCS_PROFILE_FUNC(int, ucp_tag_offload_post, (req, req_queue),
+                 ucp_request_t *req, ucp_request_queue_t *req_queue)
 {
     ucp_worker_t *worker   = req->recv.worker;
     ucp_context_t *context = worker->context;
@@ -395,7 +412,7 @@ int ucp_tag_offload_post(ucp_request_t *req, ucp_request_queue_t *req_queue)
         return 0;
     }
 
-    if (ucp_tag_offload_do_post(req, req_queue) != UCS_OK) {
+    if (ucp_tag_offload_do_post(req) != UCS_OK) {
         return 0;
     }
 
@@ -407,7 +424,8 @@ static size_t ucp_tag_offload_pack_eager(void *dest, void *arg)
     ucp_request_t *req = arg;
     size_t length;
 
-    length = ucp_dt_pack(req->send.datatype, dest, req->send.buffer,
+    length = ucp_dt_pack(req->send.ep->worker, req->send.datatype,
+                         req->send.mem_type, dest, req->send.buffer,
                          &req->send.state.dt, req->send.length);
     ucs_assert(length == req->send.length);
     return length;
@@ -462,7 +480,8 @@ ucp_do_tag_offload_zcopy(uct_pending_req_t *self, uint64_t imm_data,
     req->send.lane = ucp_ep_get_tag_lane(ep);
 
     ucp_dt_iov_copy_uct(ep->worker->context, iov, &iovcnt, max_iov, &dt_state,
-                        req->send.buffer, req->send.datatype, req->send.length, 0, NULL);
+                        req->send.buffer, req->send.datatype, req->send.length,
+                        ucp_ep_md_index(ep, req->send.lane), NULL);
 
     status = uct_ep_tag_eager_zcopy(ep->uct_eps[req->send.lane], req->send.tag.tag,
                                     imm_data, iov, iovcnt, 0,
@@ -539,7 +558,7 @@ ucs_status_t ucp_tag_offload_rndv_zcopy(uct_pending_req_t *self)
     md_index = ucp_ep_md_index(ep, req->send.lane);
 
     ucp_tag_offload_unexp_rndv_hdr_t rndv_hdr = {
-        .sender_uuid   = ep->worker->uuid,
+        .ep_ptr        = ucp_request_get_dest_ep_ptr(req),
         .reqptr        = (uintptr_t)req,
         .md_index      = md_index
     };
@@ -550,7 +569,8 @@ ucs_status_t ucp_tag_offload_rndv_zcopy(uct_pending_req_t *self)
     ucs_assert_always(UCP_DT_IS_CONTIG(req->send.datatype));
 
     ucp_dt_iov_copy_uct(ep->worker->context, iov, &iovcnt, max_iov, &dt_state,
-                        req->send.buffer, req->send.datatype, req->send.length, 0, NULL);
+                        req->send.buffer, req->send.datatype, req->send.length,
+                        ucp_ep_md_index(ep, req->send.lane), NULL);
 
     rndv_op = uct_ep_tag_rndv_zcopy(ep->uct_eps[req->send.lane], req->send.tag.tag,
                                     &rndv_hdr, sizeof(rndv_hdr), iov, iovcnt, 0,
@@ -637,7 +657,7 @@ static ucs_status_t ucp_tag_offload_eager_sync_bcopy(uct_pending_req_t *self)
     ucp_worker_t *worker = req->send.ep->worker;
     ucs_status_t status;
 
-    status = ucp_do_tag_offload_bcopy(self, worker->uuid,
+    status = ucp_do_tag_offload_bcopy(self, ucp_request_get_dest_ep_ptr(req),
                                       ucp_tag_offload_pack_eager);
     if (status == UCS_OK) {
         ucp_tag_offload_sync_posted(worker, req);
@@ -654,7 +674,7 @@ static ucs_status_t ucp_tag_offload_eager_sync_zcopy(uct_pending_req_t *self)
     ucp_worker_t *worker = req->send.ep->worker;
     ucs_status_t status;
 
-    status = ucp_do_tag_offload_zcopy(self, worker->uuid,
+    status = ucp_do_tag_offload_zcopy(self, ucp_request_get_dest_ep_ptr(req),
                                       ucp_tag_eager_sync_zcopy_req_complete);
     if (status == UCS_OK) {
         ucp_tag_offload_sync_posted(worker, req);
@@ -662,24 +682,6 @@ static ucs_status_t ucp_tag_offload_eager_sync_zcopy(uct_pending_req_t *self)
     return status;
 }
 
-void ucp_tag_offload_eager_sync_send_ack(ucp_worker_h worker,
-                                         uint64_t sender_uuid,
-                                         ucp_tag_t sender_tag)
-{
-    ucp_request_t *req;
-
-    ucs_trace_req("offload_send_sync_ack sender_uuid %"PRIx64" sender_tag %"PRIx64"",
-                  sender_uuid, sender_tag);
-
-    req = ucp_worker_allocate_reply(worker, sender_uuid);
-    req->send.uct.func          = ucp_proto_progress_am_bcopy_single;
-    req->send.proto.am_id       = UCP_AM_ID_OFFLOAD_SYNC_ACK;
-    req->send.proto.sender_uuid = sender_uuid;
-    req->send.proto.sender_tag  = sender_tag;
-    req->send.proto.comp_cb     = ucp_request_put;
-    ucp_request_send(req);
-}
-
 const ucp_proto_t ucp_tag_offload_sync_proto = {
     .contig_short     = NULL,
     .bcopy_single     = ucp_tag_offload_eager_sync_bcopy,
@@ -691,5 +693,3 @@ const ucp_proto_t ucp_tag_offload_sync_proto = {
     .first_hdr_size   = 0,
     .mid_hdr_size     = 0
 };
-
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.h
index 29e921d25..fbdc4be75 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/offload.h
@@ -22,7 +22,7 @@ enum {
  * Header for unexpected rendezvous
  */
 typedef struct {
-    uint64_t       sender_uuid;  /* Sender worker uuid */
+    uintptr_t      ep_ptr;
     uintptr_t      reqptr;       /* Request pointer */
     uint8_t        md_index;     /* md index */
 } UCS_S_PACKED ucp_tag_offload_unexp_rndv_hdr_t;
@@ -32,7 +32,7 @@ typedef struct {
  * Header for sync send acknowledgment
  */
 typedef struct {
-    uint64_t          sender_uuid;
+    uintptr_t         ep_ptr;
     ucp_tag_t         sender_tag;
 } UCS_S_PACKED ucp_offload_ssend_hdr_t;
 
@@ -49,10 +49,6 @@ void ucp_tag_offload_cancel_rndv(ucp_request_t *req);
 
 ucs_status_t ucp_tag_offload_start_rndv(ucp_request_t *sreq);
 
-void ucp_tag_offload_eager_sync_send_ack(ucp_worker_h worker,
-                                         uint64_t sender_uuid,
-                                         ucp_tag_t sender_tag);
-
 ucs_status_t ucp_tag_offload_unexp_eager(void *arg, void *data, size_t length,
                                          unsigned flags, uct_tag_t stag, uint64_t imm);
 
@@ -106,18 +102,21 @@ ucp_tag_offload_try_cancel(ucp_worker_t *worker, ucp_request_t *req, unsigned mo
  * message received on this interface. Also it maintains hash of tags, if
  * more than one interface is active. Then, when expected receive request needs
  * to be offloaded, the corresponding offload-capable interface is retrieved
- * from the hash. Having just one offload-capable interface is supposed to be
- * a fast path, because it matches homogeneous cluster configurations. So, no
- * hashing is done, while only one offload-capable interface is active.
+ * from the hash.
  *
  * @note Hash key is a tag masked with 'tag_sender_mask', because it needs to
  *       identify a particular sender, rather than every single tag.
  *
+ * @note Tag is added to the hash table for messages bigger than TM_THRESH.
+ *       Smaller messages are not supposed to be matched in HW, thus no need
+ *       to waste time on hashing for them.
+ *
+ *
  * @param [in]  wiface        UCP worker interface.
  * @param [in]  tag           Tag of the arrived unexpected message.
  */
 static UCS_F_ALWAYS_INLINE void
-ucp_tag_offload_unexp(ucp_worker_iface_t *wiface, ucp_tag_t tag)
+ucp_tag_offload_unexp(ucp_worker_iface_t *wiface, ucp_tag_t tag, size_t length)
 {
     ucp_worker_t *worker = wiface->worker;
     ucp_tag_t tag_key;
@@ -132,7 +131,8 @@ ucp_tag_offload_unexp(ucp_worker_iface_t *wiface, ucp_tag_t tag)
         }
     }
 
-    if (ucs_unlikely(worker->tm.offload.num_ifaces > 1)) {
+    if (ucs_unlikely((length >= worker->tm.offload.thresh) &&
+                     (worker->num_active_ifaces > 1))) {
         tag_key = worker->context->config.tag_sender_mask & tag;
         hash_it = kh_put(ucp_tag_offload_hash, &worker->tm.offload.tag_hash,
                          tag_key, &ret);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/probe.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/probe.c
index a097b8f76..ec80139fa 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/probe.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/probe.c
@@ -21,7 +21,9 @@ ucp_tag_message_h ucp_tag_probe_nb(ucp_worker_h worker, ucp_tag_t tag,
     ucp_recv_desc_t *rdesc;
     uint16_t flags;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_TAG,
+                                    return NULL);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     ucs_trace_req("probe_nb tag %"PRIx64"/%"PRIx64" remove=%d", tag, tag_mask,
                   remove);
@@ -41,7 +43,7 @@ ucp_tag_message_h ucp_tag_probe_nb(ucp_worker_h worker, ucp_tag_t tag,
         }
     }
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
 
     return rdesc;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/rndv.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/rndv.c
index 41c194060..64ee63431 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/rndv.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/rndv.c
@@ -19,6 +19,26 @@ static int ucp_rndv_is_get_zcopy(ucp_request_t *sreq, ucp_rndv_mode_t rndv_mode)
              UCP_MEM_IS_HOST(sreq->send.mem_type)));
 }
 
+static int ucp_rndv_is_pipeline_needed(ucp_request_t *sreq) {
+    uct_md_attr_t *md_attr;
+    unsigned md_index;
+
+    if (UCP_MEM_IS_HOST(sreq->send.mem_type)) {
+        return 0;
+    }
+
+    if (sreq->send.ep->worker->context->config.ext.rndv_mode ==
+        UCP_RNDV_MODE_PUT_ZCOPY) {
+        return 0;
+    }
+
+    md_index  = ucp_ep_md_index(sreq->send.ep, sreq->send.lane);
+    md_attr   = &sreq->send.ep->worker->context->tl_mds[md_index].attr;
+
+    /*  check if lane support only mem type */
+    return md_attr->cap.reg_mem_types & UCS_BIT(UCT_MD_MEM_TYPE_HOST);
+}
+
 size_t ucp_tag_rndv_rts_pack(void *dest, void *arg)
 {
     ucp_request_t *sreq              = arg;   /* send request */
@@ -28,7 +48,7 @@ size_t ucp_tag_rndv_rts_pack(void *dest, void *arg)
 
     rndv_rts_hdr->super.tag        = sreq->send.tag.tag;
     rndv_rts_hdr->sreq.reqptr      = (uintptr_t)sreq;
-    rndv_rts_hdr->sreq.sender_uuid = worker->uuid;
+    rndv_rts_hdr->sreq.ep_ptr      = ucp_request_get_dest_ep_ptr(sreq);
     rndv_rts_hdr->size             = sreq->send.length;
 
     /* Pack remote keys (which can be empty list) */
@@ -39,6 +59,7 @@ size_t ucp_tag_rndv_rts_pack(void *dest, void *arg)
         packed_rkey_size = ucp_rkey_pack_uct(worker->context,
                                              sreq->send.state.dt.dt.contig.md_map,
                                              sreq->send.state.dt.dt.contig.memh,
+                                             sreq->send.mem_type,
                                              rndv_rts_hdr + 1);
         if (packed_rkey_size < 0) {
             ucs_fatal("failed to pack rendezvous remote key: %s",
@@ -75,6 +96,7 @@ static size_t ucp_tag_rndv_rtr_pack(void *dest, void *arg)
         packed_rkey_size = ucp_rkey_pack_uct(rndv_req->send.ep->worker->context,
                                              rreq->recv.state.dt.contig.md_map,
                                              rreq->recv.state.dt.contig.memh,
+                                             rreq->recv.mem_type,
                                              rndv_rtr_hdr + 1);
         if (packed_rkey_size < 0) {
             return packed_rkey_size;
@@ -113,7 +135,12 @@ ucs_status_t ucp_tag_send_start_rndv(ucp_request_t *sreq)
                   sreq->send.length);
     UCS_PROFILE_REQUEST_EVENT(sreq, "start_rndv", sreq->send.length);
 
-    if (ep->flags & UCP_EP_FLAG_TAG_OFFLOAD_ENABLED) {
+    status = ucp_ep_resolve_dest_ep_ptr(ep, sreq->send.lane);
+    if (status != UCS_OK) {
+        return status;
+    }
+
+    if (ucp_ep_is_tag_offload_enabled(ucp_ep_config(ep))) {
         status = ucp_tag_offload_start_rndv(sreq);
         if (status != UCS_OK) {
             return status;
@@ -133,7 +160,6 @@ ucs_status_t ucp_tag_send_start_rndv(ucp_request_t *sreq)
         sreq->send.uct.func = ucp_proto_progress_rndv_rts;
     }
 
-    ucp_ep_connect_remote(ep);
     return UCS_OK;
 }
 
@@ -166,7 +192,6 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_complete_rma_put_zcopy, (sreq),
     ucp_trace_req(sreq, "rndv_put completed");
     UCS_PROFILE_REQUEST_EVENT(sreq, "complete_rndv_put", 0);
 
-    ucp_rkey_destroy(sreq->send.rndv_put.rkey);
     ucp_request_send_buffer_dereg(sreq);
     ucp_request_complete_send(sreq, UCS_OK);
 }
@@ -180,6 +205,9 @@ static void ucp_rndv_send_atp(ucp_request_t *sreq, uintptr_t remote_request)
     ucp_trace_req(sreq, "send atp remote_request 0x%lx", remote_request);
     UCS_PROFILE_REQUEST_EVENT(sreq, "send_atp", 0);
 
+    /* destroy rkey before it gets overridden by ATP protocol data */
+    ucp_rkey_destroy(sreq->send.rndv_put.rkey);
+
     sreq->send.lane                 = ucp_ep_get_am_lane(sreq->send.ep);
     sreq->send.uct.func             = ucp_proto_progress_am_bcopy_single;
     sreq->send.proto.am_id          = UCP_AM_ID_RNDV_ATP;
@@ -245,7 +273,7 @@ static void ucp_rndv_get_lanes_count(ucp_request_t *req)
         return; /* already resolved */
     }
 
-    while ((lane = ucp_rkey_get_rma_bw_lane(req->send.rndv_get.rkey, ep,
+    while ((lane = ucp_rkey_get_rma_bw_lane(req->send.rndv_get.rkey, ep, req->send.mem_type,
                                             &uct_rkey, map)) != UCP_NULL_LANE) {
         req->send.rndv_get.lane_count++;
         map |= UCS_BIT(lane);
@@ -265,7 +293,7 @@ static ucp_lane_index_t ucp_rndv_get_next_lane(ucp_request_t *rndv_req, uct_rkey
     ucp_ep_h ep = rndv_req->send.ep;
     ucp_lane_index_t lane;
 
-    lane = ucp_rkey_get_rma_bw_lane(rndv_req->send.rndv_get.rkey, ep,
+    lane = ucp_rkey_get_rma_bw_lane(rndv_req->send.rndv_get.rkey, ep, rndv_req->send.mem_type,
                                     uct_rkey, rndv_req->send.rndv_get.lanes_map);
 
     if ((lane == UCP_NULL_LANE) && (rndv_req->send.rndv_get.lanes_map != 0)) {
@@ -273,7 +301,7 @@ static ucp_lane_index_t ucp_rndv_get_next_lane(ucp_request_t *rndv_req, uct_rkey
          * is not NULL - we found at least one lane on previous iteration).
          * reset used lanes map to NULL and iterate it again */
         rndv_req->send.rndv_get.lanes_map = 0;
-        lane = ucp_rkey_get_rma_bw_lane(rndv_req->send.rndv_get.rkey, ep,
+        lane = ucp_rkey_get_rma_bw_lane(rndv_req->send.rndv_get.rkey, ep, rndv_req->send.mem_type,
                                         uct_rkey, rndv_req->send.rndv_get.lanes_map);
     }
 
@@ -285,7 +313,7 @@ static ucp_lane_index_t ucp_rndv_get_next_lane(ucp_request_t *rndv_req, uct_rkey
     rndv_req->send.rndv_get.lanes_map |= UCS_BIT(lane);
     /* in case if masked too much lanes - reset mask to zero
      * to select first lane next time */
-    if (ucs_count_one_bits(rndv_req->send.rndv_get.lanes_map) >=
+    if (ucs_popcount(rndv_req->send.rndv_get.lanes_map) >=
         ep->worker->context->config.ext.max_rndv_lanes) {
         rndv_req->send.rndv_get.lanes_map = 0;
     }
@@ -309,11 +337,6 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_progress_rma_get_zcopy, (self),
     size_t max_zcopy;
     size_t tail;
 
-    if (ucp_ep_is_stub(ep)) {
-        rndv_req->send.lane = 0;
-        return UCS_ERR_NO_RESOURCE;
-    }
-
     ucp_rndv_get_lanes_count(rndv_req);
 
     /* Figure out which lane to use for get operation */
@@ -455,6 +478,7 @@ static void ucp_rndv_req_send_rma_get(ucp_request_t *rndv_req, ucp_request_t *rr
 
     rndv_req->send.uct.func                = ucp_rndv_progress_rma_get_zcopy;
     rndv_req->send.buffer                  = rreq->recv.buffer;
+    rndv_req->send.mem_type                = rreq->recv.mem_type;
     rndv_req->send.datatype                = ucp_dt_make_contig(1);
     rndv_req->send.length                  = rndv_rts_hdr->size;
     rndv_req->send.rndv_get.remote_request = rndv_rts_hdr->sreq.reqptr;
@@ -496,7 +520,17 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_matched, (worker, rreq, rndv_rts_hdr),
 
     /* the internal send request allocated on receiver side (to perform a "get"
      * operation, send "ATS" and "RTR") */
-    rndv_req = ucp_worker_allocate_reply(worker, rndv_rts_hdr->sreq.sender_uuid);
+    rndv_req = ucp_request_get(worker);
+    if (rndv_req == NULL) {
+        ucs_error("failed to allocate rendezvous reply");
+        goto out;
+    }
+
+    rndv_req->send.ep           = ucp_worker_get_ep_by_ptr(worker,
+                                                           rndv_rts_hdr->sreq.ep_ptr);
+    rndv_req->flags             = 0;
+    rndv_req->send.mdesc        = NULL;
+    rndv_req->send.pending_lane = UCP_NULL_LANE;
 
     ucp_trace_req(rreq,
                   "rndv matched remote {address 0x%"PRIx64" size %zu sreq 0x%lx}"
@@ -513,14 +547,9 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_matched, (worker, rreq, rndv_rts_hdr),
         goto out;
     }
 
-    rndv_req->send.pending_lane = UCP_NULL_LANE;
 
     /* if the receive side is not connected yet then the RTS was received on a stub ep */
     ep = rndv_req->send.ep;
-    if (ucp_ep_is_stub(ep)) {
-        ucs_debug("received rts on a stub ep, ep %p am_lane %d", ep,
-                  ucp_ep_get_am_lane(ep));
-    }
 
     rndv_mode = worker->context->config.ext.rndv_mode;
     if (UCP_DT_IS_CONTIG(rreq->recv.datatype)) {
@@ -530,7 +559,8 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_matched, (worker, rreq, rndv_rts_hdr),
             goto out;
         } else if (rndv_mode != UCP_RNDV_MODE_GET_ZCOPY) {
             /* put protocol is allowed - register receive buffer memory for rma */
-            ucp_request_recv_buffer_reg(rreq, ucp_ep_config(ep)->key.rma_bw_md_map);
+            ucp_request_recv_buffer_reg(rreq, ucp_ep_config(ep)->key.rma_bw_md_map,
+                                        ucs_min(rreq->recv.length, rndv_rts_hdr->size));
         }
     }
 
@@ -564,9 +594,9 @@ ucs_status_t ucp_rndv_process_rts(void *arg, void *data, size_t length,
         UCP_WORKER_STAT_RNDV(worker, EXP);
         status = UCS_OK;
     } else {
-        status = ucp_recv_desc_init(worker, data, length, tl_flags,
+        status = ucp_recv_desc_init(worker, data, length, 0, tl_flags,
                                     sizeof(*rndv_rts_hdr),
-                                    UCP_RECV_DESC_FLAG_RNDV, &rdesc);
+                                    UCP_RECV_DESC_FLAG_RNDV, 0, &rdesc);
         if (!UCS_STATUS_IS_ERR(status)) {
             ucp_tag_unexp_recv(&worker->tm, rdesc, rndv_rts_hdr->super.tag);
         }
@@ -612,34 +642,20 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_atp_handler,
 }
 
 static size_t ucp_rndv_pack_data(void *dest, void *arg)
-{
-    ucp_rndv_data_hdr_t *hdr = dest;
-    ucp_request_t *sreq = arg;
-    size_t length;
-
-    hdr->rreq_ptr = sreq->send.rndv_data.rreq_ptr;
-    hdr->offset   = sreq->send.state.dt.offset;
-    length        = ucp_ep_get_max_bcopy(sreq->send.ep, sreq->send.lane) - sizeof(*hdr);
-
-    return sizeof(*hdr) + ucp_dt_pack(sreq->send.datatype, hdr + 1,
-                                      sreq->send.buffer, &sreq->send.state.dt,
-                                      length);
-}
-
-static size_t ucp_rndv_pack_data_last(void *dest, void *arg)
 {
     ucp_rndv_data_hdr_t *hdr = dest;
     ucp_request_t *sreq = arg;
     size_t length, offset;
 
     offset        = sreq->send.state.dt.offset;
-    hdr->rreq_ptr = sreq->send.rndv_data.rreq_ptr;
-    length        = sreq->send.length - offset;
+    hdr->rreq_ptr = sreq->send.tag.rreq_ptr;
     hdr->offset   = offset;
+    length        = ucs_min(sreq->send.length - offset,
+                            ucp_ep_get_max_bcopy(sreq->send.ep, sreq->send.lane) - sizeof(*hdr));
 
-    return sizeof(*hdr) + ucp_dt_pack(sreq->send.datatype, hdr + 1,
-                                      sreq->send.buffer, &sreq->send.state.dt,
-                                      length);
+    return sizeof(*hdr) + ucp_dt_pack(sreq->send.ep->worker, sreq->send.datatype,
+                                      sreq->send.mem_type, hdr + 1, sreq->send.buffer,
+                                      &sreq->send.state.dt, length);
 }
 
 UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_progress_am_bcopy, (self),
@@ -654,16 +670,13 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_progress_am_bcopy, (self),
     if (sreq->send.length <= ucp_ep_config(ep)->am.max_bcopy - sizeof(ucp_rndv_data_hdr_t)) {
         /* send a single bcopy message */
         status = ucp_do_am_bcopy_single(self, UCP_AM_ID_RNDV_DATA,
-                                        ucp_rndv_pack_data_last);
+                                        ucp_rndv_pack_data);
     } else {
-        /* send multiple bcopy messages (fragments of the original send message) */
         status = ucp_do_am_bcopy_multi(self, UCP_AM_ID_RNDV_DATA,
-                                       UCP_AM_ID_RNDV_DATA,
                                        UCP_AM_ID_RNDV_DATA,
                                        sizeof(ucp_rndv_data_hdr_t),
                                        ucp_rndv_pack_data,
-                                       ucp_rndv_pack_data,
-                                       ucp_rndv_pack_data_last, 1);
+                                       ucp_rndv_pack_data, 1);
     }
     if (status == UCS_OK) {
         ucp_rndv_complete_send(sreq);
@@ -712,8 +725,8 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_progress_rma_put_zcopy, (self),
 
     state = sreq->send.state.dt;
     ucp_dt_iov_copy_uct(ep->worker->context, iov, &iovcnt, max_iovcnt, &state,
-                        sreq->send.buffer, ucp_dt_make_contig(1), length, 0,
-                        sreq->send.mdesc);
+                        sreq->send.buffer, ucp_dt_make_contig(1), length,
+                        ucp_ep_md_index(ep, sreq->send.lane), sreq->send.mdesc);
     status = uct_ep_put_zcopy(ep->uct_eps[sreq->send.lane],
                               iov, iovcnt,
                               sreq->send.rndv_put.remote_address + offset,
@@ -724,7 +737,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_progress_rma_put_zcopy, (self),
                                    status);
     if (sreq->send.state.dt.offset == sreq->send.length) {
         if (sreq->send.state.uct_comp.count == 0) {
-            ucp_rndv_send_atp(sreq, sreq->send.rndv_put.remote_request);
+            sreq->send.state.uct_comp.func(&sreq->send.state.uct_comp, status);
         }
         return UCS_OK;
     } else if (!UCS_STATUS_IS_ERR(status)) {
@@ -758,7 +771,7 @@ static ucs_status_t ucp_rndv_progress_am_zcopy_single(uct_pending_req_t *self)
     ucp_request_t *sreq = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_rndv_data_hdr_t hdr;
 
-    hdr.rreq_ptr = sreq->send.rndv_data.rreq_ptr;
+    hdr.rreq_ptr = sreq->send.tag.rreq_ptr;
     hdr.offset   = 0;
     return ucp_do_am_zcopy_single(self, UCP_AM_ID_RNDV_DATA, &hdr, sizeof(hdr),
                                   ucp_rndv_am_zcopy_send_req_complete);
@@ -769,10 +782,9 @@ static ucs_status_t ucp_rndv_progress_am_zcopy_multi(uct_pending_req_t *self)
     ucp_request_t *sreq = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_rndv_data_hdr_t hdr;
 
-    hdr.rreq_ptr = sreq->send.rndv_data.rreq_ptr;
+    hdr.rreq_ptr = sreq->send.tag.rreq_ptr;
     hdr.offset   = sreq->send.state.dt.offset;
     return ucp_do_am_zcopy_multi(self,
-                                 UCP_AM_ID_RNDV_DATA,
                                  UCP_AM_ID_RNDV_DATA,
                                  UCP_AM_ID_RNDV_DATA,
                                  &hdr, sizeof(hdr),
@@ -787,6 +799,7 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_frag_put_completion, (self, status),
     ucp_request_t *sreq     = frag_req->send.rndv_put.sreq;
 
     ucs_mpool_put_inline((void *)frag_req->send.mdesc);
+    sreq->send.state.dt.offset += frag_req->send.length;
     sreq->send.state.uct_comp.count--;
     if (0 == sreq->send.state.uct_comp.count) {
         ucp_rndv_send_atp(sreq, sreq->send.rndv_put.remote_request);
@@ -813,7 +826,6 @@ UCS_PROFILE_FUNC_VOID(ucp_rndv_frag_get_completion, (self, status),
     frag_req->send.state.dt.dt.contig.md_map = 0;
 
     ucp_request_send(frag_req);
-    sreq->send.state.dt.offset += frag_req->send.length;
 }
 
 static ucs_status_t ucp_rndv_pipeline(ucp_request_t *sreq, ucp_rndv_rtr_hdr_t *rndv_rtr_hdr)
@@ -860,10 +872,12 @@ static ucs_status_t ucp_rndv_pipeline(ucp_request_t *sreq, ucp_rndv_rtr_hdr_t *r
         frag_req->send.ep                        = pipeline_ep;
         frag_req->send.buffer                    = mdesc + 1;
         frag_req->send.datatype                  = ucp_dt_make_contig(1);
+        frag_req->send.mem_type                  = sreq->send.mem_type;
         frag_req->send.state.dt.dt.contig.memh[0]= ucp_memh2uct(mdesc->memh, md_index);
         frag_req->send.state.dt.dt.contig.md_map = UCS_BIT(md_index);
         frag_req->send.length                    = length;
         frag_req->send.uct.func                  = ucp_rndv_progress_rma_get_zcopy;
+        frag_req->send.rndv_get.rkey             = NULL;
         frag_req->send.rndv_get.remote_address   = (uint64_t)(sreq->send.buffer + offset);
         frag_req->send.rndv_get.lanes_map        = 0;
         frag_req->send.rndv_get.lane_count       = 0;
@@ -885,7 +899,6 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_rtr_handler,
     ucp_rndv_rtr_hdr_t *rndv_rtr_hdr = data;
     ucp_request_t *sreq              = (ucp_request_t*)rndv_rtr_hdr->sreq_ptr;
     ucp_ep_h ep                      = sreq->send.ep;
-    ucp_rndv_mode_t rndv_mode        = ep->worker->context->config.ext.rndv_mode;
     ucs_status_t status;
 
     ucp_trace_req(sreq, "received rtr address 0x%lx remote rreq 0x%lx",
@@ -907,11 +920,10 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_rtr_handler,
         }
 
         sreq->send.lane = ucp_rkey_get_rma_bw_lane(sreq->send.rndv_put.rkey, ep,
+                                                   sreq->send.mem_type,
                                                    &sreq->send.rndv_put.uct_rkey, 0);
         if (sreq->send.lane != UCP_NULL_LANE) {
-            if ((rndv_mode == UCP_RNDV_MODE_PUT_ZCOPY) ||
-                ((rndv_mode == UCP_RNDV_MODE_AUTO) &&
-                 UCP_MEM_IS_HOST(sreq->send.mem_type))) {
+            if (!ucp_rndv_is_pipeline_needed(sreq)) {
                 ucp_request_send_state_reset(sreq, ucp_rndv_put_completion,
                                              UCP_REQUEST_SEND_PROTO_RNDV_PUT);
                 sreq->send.uct.func                = ucp_rndv_progress_rma_put_zcopy;
@@ -928,10 +940,11 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_rtr_handler,
     }
 
     /* switch to AM */
-    sreq->send.rndv_data.rreq_ptr = rndv_rtr_hdr->rreq_ptr;
+    sreq->send.tag.rreq_ptr = rndv_rtr_hdr->rreq_ptr;
 
     if (UCP_DT_IS_CONTIG(sreq->send.datatype) &&
-        (sreq->send.length >= ucp_ep_config(ep)->am.zcopy_thresh[0]))
+        (sreq->send.length >=
+         ucp_ep_config(ep)->am.mem_type_zcopy_thresh[sreq->send.mem_type]))
     {
         status = ucp_request_send_buffer_reg_lane(sreq, ucp_ep_get_am_lane(ep));
         ucs_assert_always(status == UCS_OK);
@@ -944,12 +957,12 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_rndv_rtr_handler,
             sreq->send.uct.func = ucp_rndv_progress_am_zcopy_single;
         } else {
             sreq->send.uct.func        = ucp_rndv_progress_am_zcopy_multi;
-            sreq->send.tag.am_bw_index = 0;
+            sreq->send.tag.am_bw_index = 1;
         }
     } else {
         ucp_request_send_state_reset(sreq, NULL, UCP_REQUEST_SEND_PROTO_BCOPY_AM);
         sreq->send.uct.func        = ucp_rndv_progress_am_bcopy;
-        sreq->send.tag.am_bw_index = 0;
+        sreq->send.tag.am_bw_index = 1;
     }
 
 out_send:
@@ -996,9 +1009,10 @@ static void ucp_rndv_dump(ucp_worker_h worker, uct_am_trace_type_t type,
 
     switch (id) {
     case UCP_AM_ID_RNDV_RTS:
-        snprintf(buffer, max, "RNDV_RTS tag %"PRIx64" uuid %"PRIx64" sreq 0x%lx "
+        ucs_assert(rndv_rts_hdr->sreq.ep_ptr != 0);
+        snprintf(buffer, max, "RNDV_RTS tag %"PRIx64" ep_ptr %lx sreq 0x%lx "
                  "address 0x%"PRIx64" size %zu", rndv_rts_hdr->super.tag,
-                 rndv_rts_hdr->sreq.sender_uuid, rndv_rts_hdr->sreq.reqptr,
+                 rndv_rts_hdr->sreq.ep_ptr, rndv_rts_hdr->sreq.reqptr,
                  rndv_rts_hdr->address, rndv_rts_hdr->size);
         if (rndv_rts_hdr->address) {
             ucp_rndv_dump_rkey(rndv_rts_hdr + 1, buffer + strlen(buffer),
@@ -1032,15 +1046,15 @@ static void ucp_rndv_dump(ucp_worker_h worker, uct_am_trace_type_t type,
 }
 
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_RNDV_RTS, ucp_rndv_rts_handler,
-              ucp_rndv_dump, UCT_CB_FLAG_SYNC);
+              ucp_rndv_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_RNDV_ATS, ucp_rndv_ats_handler,
-              ucp_rndv_dump, UCT_CB_FLAG_SYNC);
+              ucp_rndv_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_RNDV_ATP, ucp_rndv_atp_handler,
-              ucp_rndv_dump, UCT_CB_FLAG_SYNC);
+              ucp_rndv_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_RNDV_RTR, ucp_rndv_rtr_handler,
-              ucp_rndv_dump, UCT_CB_FLAG_SYNC);
+              ucp_rndv_dump, 0);
 UCP_DEFINE_AM(UCP_FEATURE_TAG, UCP_AM_ID_RNDV_DATA, ucp_rndv_data_handler,
-              ucp_rndv_dump, UCT_CB_FLAG_SYNC);
+              ucp_rndv_dump, 0);
 
 UCP_DEFINE_AM_PROXY(UCP_AM_ID_RNDV_RTS);
 UCP_DEFINE_AM_PROXY(UCP_AM_ID_RNDV_ATS);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.c
index 1cd17fcf9..e8bc72f06 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.c
@@ -45,7 +45,6 @@ ucs_status_t ucp_tag_match_init(ucp_tag_match_t *tm)
     tm->offload.thresh       = SIZE_MAX;
     tm->offload.zcopy_thresh = SIZE_MAX;
     tm->offload.iface        = NULL;
-    tm->offload.num_ifaces   = 0;
     tm->am.message_id        = ucs_generate_uuid(0);
     return UCS_OK;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.h
index ac1759391..9aa2c033c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.h
@@ -82,8 +82,9 @@ typedef struct ucp_tag_match {
     struct {
         ucs_queue_head_t      sync_reqs;        /* Outgoing sync send requests */
         khash_t(ucp_tag_offload_hash) tag_hash; /* Hash table of offload ifaces */
-        ucp_worker_iface_t    *iface;           /* Active offload iface (relevant if num_ifaces
-                                                   is 1, otherwise hash should be used) */
+        ucp_worker_iface_t    *iface;           /* Active offload iface (relevant if just
+                                                   one iface is activated on the worker,
+                                                   otherwise hash should be used) */
         size_t                thresh;           /* Minimal receive buffer size to be
                                                    used with tag-matching offload. */
         size_t                zcopy_thresh;     /* Minimal size of user-provided
@@ -95,8 +96,6 @@ typedef struct ucp_tag_match {
                                                    or not be used with tag-matching
                                                    offload at all, according to
                                                    'thresh' configuration. */
-        unsigned              num_ifaces;       /* Number of active offload
-                                                   capable interfaces */
     } offload;
 
     struct {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.inl b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.inl
index 2accff867..94ba43525 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_match.inl
@@ -212,27 +212,6 @@ ucp_tag_unexp_search(ucp_tag_match_t *tm, ucp_tag_t tag, uint64_t tag_mask,
     return NULL;
 }
 
-static UCS_F_ALWAYS_INLINE void
-ucp_tag_unexp_desc_release(ucp_recv_desc_t *rdesc)
-{
-    ucs_trace_req("release receive descriptor %p", rdesc);
-    if (ucs_unlikely(rdesc->flags & UCP_RECV_DESC_FLAG_UCT_DESC)) {
-        /* uct desc is slowpath */
-        if (ucs_unlikely(rdesc->flags & UCP_RECV_DESC_FLAG_EAGER_OFFLOAD)) {
-            if (rdesc->flags & UCP_RECV_DESC_FLAG_EAGER_SYNC) {
-                uct_iface_release_desc(rdesc);
-            } else {
-                uct_iface_release_desc( (char*)rdesc -
-                  (sizeof(ucp_eager_sync_hdr_t) - sizeof(ucp_eager_hdr_t)) );
-            }
-        } else {
-            uct_iface_release_desc((char*)rdesc - sizeof(ucp_eager_sync_hdr_t));
-        }
-    } else {
-        ucs_mpool_put_inline(rdesc);
-    }
-}
-
 /*
  * process data, complete receive if done
  * @return UCS_OK/ERR - completed, UCS_INPROGRESS - not completed
@@ -280,7 +259,7 @@ ucp_tag_recv_request_process_rdesc(ucp_request_t *req, ucp_recv_desc_t *rdesc,
      recv_len = rdesc->length - hdr_len;
      status = ucp_tag_request_process_recv_data(req, (void*)(rdesc + 1) + hdr_len,
                                                 recv_len, offset, 0);
-     ucp_tag_unexp_desc_release(rdesc);
+     ucp_recv_desc_release(rdesc);
      return status;
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_recv.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_recv.c
index e96fb29e7..d8a3ea20e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_recv.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_recv.c
@@ -69,7 +69,7 @@ ucp_tag_recv_common(ucp_worker_h worker, void *buffer, size_t count,
 
         status = ucp_dt_unpack_only(worker, buffer, count, datatype, mem_type,
                                     (void*)(rdesc + 1) + hdr_len, recv_len, 1);
-        ucp_tag_unexp_desc_release(rdesc);
+        ucp_recv_desc_release(rdesc);
 
         if (req_flags & UCP_REQUEST_FLAG_CALLBACK) {
             cb(req + 1, status, &req->recv.tag.info);
@@ -125,7 +125,7 @@ ucp_tag_recv_common(ucp_worker_h worker, void *buffer, size_t count,
     if (ucs_unlikely(rdesc->flags & UCP_RECV_DESC_FLAG_RNDV)) {
         ucp_rndv_matched(worker, req, (void*)(rdesc + 1));
         UCP_WORKER_STAT_RNDV(worker, UNEXP);
-        ucp_tag_unexp_desc_release(rdesc);
+        ucp_recv_desc_release(rdesc);
         return;
     }
 
@@ -160,14 +160,16 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_recv_nbr,
     ucp_request_t *req = (ucp_request_t *)request - 1;
     ucp_recv_desc_t *rdesc;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_TAG,
+                                    return UCS_ERR_INVALID_PARAM);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     rdesc = ucp_tag_unexp_search(&worker->tm, tag, tag_mask, 1, "recv_nbr");
     ucp_tag_recv_common(worker, buffer, count, datatype, tag, tag_mask,
                         req, UCP_REQUEST_DEBUG_FLAG_EXTERNAL, NULL, rdesc,
                         "recv_nbr");
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return UCS_OK;
 }
 
@@ -181,7 +183,9 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_recv_nb,
     ucs_status_ptr_t ret;
     ucp_request_t *req;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_TAG,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     req = ucp_request_get(worker);
     if (ucs_likely(req != NULL)) {
@@ -193,7 +197,7 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_recv_nb,
         ret = UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
     }
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return ret;
 }
 
@@ -207,7 +211,9 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_msg_recv_nb,
     ucs_status_ptr_t ret;
     ucp_request_t *req;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(worker->context, UCP_FEATURE_TAG,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(worker);
 
     req = ucp_request_get(worker);
     if (ucs_likely(req != NULL)) {
@@ -219,6 +225,6 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_msg_recv_nb,
         ret = UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
     }
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(worker);
     return ret;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_send.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_send.c
index 04357b8db..429450599 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_send.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/tag/tag_send.c
@@ -19,46 +19,45 @@
 static UCS_F_ALWAYS_INLINE size_t
 ucp_tag_get_rndv_threshold(const ucp_request_t *req, size_t count,
                            size_t max_iov, size_t rndv_rma_thresh,
-                           size_t rndv_am_thresh, size_t seg_size)
+                           size_t rndv_am_thresh)
 {
     switch (req->send.datatype & UCP_DATATYPE_CLASS_MASK) {
-    case UCP_DATATYPE_IOV: 
+    case UCP_DATATYPE_IOV:
         if ((count > max_iov) &&
             ucp_ep_is_tag_offload_enabled(ucp_ep_config(req->send.ep))) {
             /* Make sure SW RNDV will be used, because tag offload does
              * not support multi-packet eager protocols. */
-            return seg_size;
+            return 1;
         }
         /* Fall through */
-    case UCP_DATATYPE_CONTIG: 
+    case UCP_DATATYPE_CONTIG:
         return ucs_min(rndv_rma_thresh, rndv_am_thresh);
     case UCP_DATATYPE_GENERIC:
         return rndv_am_thresh;
     default:
         ucs_error("Invalid data type %lx", req->send.datatype);
     }
- 
+
     return SIZE_MAX;
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_ptr_t
-ucp_tag_send_req(ucp_request_t *req, size_t count,
+ucp_tag_send_req(ucp_request_t *req, size_t dt_count,
                  const ucp_ep_msg_config_t* msg_config,
                  size_t rndv_rma_thresh, size_t rndv_am_thresh,
                  ucp_send_callback_t cb, const ucp_proto_t *proto,
                  int enable_zcopy)
 {
-    size_t seg_size     = (msg_config->max_bcopy - proto->only_hdr_size);
-    size_t rndv_thresh  = ucp_tag_get_rndv_threshold(req, count,
+    size_t rndv_thresh  = ucp_tag_get_rndv_threshold(req, dt_count,
                                                      msg_config->max_iov,
                                                      rndv_rma_thresh,
-                                                     rndv_am_thresh, seg_size);
+                                                     rndv_am_thresh);
     ssize_t max_short   = ucp_proto_get_short_max(req, msg_config);
     ucs_status_t status;
     size_t zcopy_thresh;
 
     if (enable_zcopy || ucs_unlikely(!UCP_MEM_IS_HOST(req->send.mem_type))) {
-        zcopy_thresh = ucp_proto_get_zcopy_threshold(req, msg_config, count,
+        zcopy_thresh = ucp_proto_get_zcopy_threshold(req, msg_config, dt_count,
                                                      rndv_thresh);
     } else {
         zcopy_thresh = rndv_thresh;
@@ -70,8 +69,8 @@ ucp_tag_send_req(ucp_request_t *req, size_t count,
                   req, req->send.datatype, req->send.buffer, req->send.length,
                   max_short, rndv_thresh, zcopy_thresh, enable_zcopy);
 
-    status = ucp_request_send_start(req, max_short, zcopy_thresh, seg_size,
-                                    rndv_thresh, proto);
+    status = ucp_request_send_start(req, max_short, zcopy_thresh, rndv_thresh,
+                                    dt_count, msg_config, proto);
     if (ucs_unlikely(status != UCS_OK)) {
         if (status == UCS_ERR_NO_PROGRESS) {
             /* RMA/AM rendezvous */
@@ -123,7 +122,7 @@ ucp_tag_send_req_init(ucp_request_t* req, ucp_ep_h ep, const void* buffer,
 {
     req->flags             = flags;
     req->send.ep           = ep;
-    req->send.buffer       = buffer;
+    req->send.buffer       = (void*)buffer;
     req->send.datatype     = datatype;
     req->send.tag.tag      = tag;
     ucp_request_send_state_init(req, datatype, count);
@@ -148,14 +147,24 @@ ucp_tag_send_inline(ucp_ep_h ep, const void *buffer, size_t count,
     }
 
     length = ucp_contig_dt_length(datatype, count);
-    if (ucs_unlikely((ssize_t)length > ucp_ep_config(ep)->tag.eager.max_short)) {
+
+    if ((ssize_t)length <= ucp_ep_config(ep)->tag.max_eager_short) {
+        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(ucp_eager_hdr_t));
+        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(uint64_t));
+        status = uct_ep_am_short(ucp_ep_get_am_uct_ep(ep), UCP_AM_ID_EAGER_ONLY,
+                                 tag, buffer, length);
+    } else if ((ssize_t)length <= ucp_ep_config(ep)->tag.offload.max_eager_short) {
+        UCS_STATIC_ASSERT(sizeof(ucp_tag_t) == sizeof(uct_tag_t));
+        status = uct_ep_tag_eager_short(ucp_ep_get_tag_uct_ep(ep), tag, buffer,
+                                        length);
+    } else {
         return UCS_ERR_NO_RESOURCE;
     }
 
-    status = UCS_PROFILE_CALL(ucp_tag_send_eager_short, ep, tag, buffer, length);
     if (status != UCS_ERR_NO_RESOURCE) {
         UCP_EP_STAT_TAG_OP(ep, EAGER);
     }
+
     return status;
 }
 
@@ -169,12 +178,15 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_send_nb,
     ucp_request_t *req;
     ucs_status_ptr_t ret;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_TAG,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
     ucs_trace_req("send_nb buffer %p count %zu tag %"PRIx64" to %s cb %p",
                   buffer, count, tag, ucp_ep_peer_name(ep), cb);
 
-    status = ucp_tag_send_inline(ep, buffer, count, datatype, tag);
+    status = UCS_PROFILE_CALL(ucp_tag_send_inline, ep, buffer, count,
+                              datatype, tag);
     if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
         ret = UCS_STATUS_PTR(status); /* UCS_OK also goes here */
         goto out;
@@ -193,7 +205,7 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_send_nb,
                            ucp_ep_config(ep)->tag.rndv.am_thresh,
                            cb, ucp_ep_config(ep)->tag.proto, 1);
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
     return ret;
 }
 
@@ -206,14 +218,17 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_send_nbr,
     ucs_status_t status;
     ucs_status_ptr_t ret;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_TAG,
+                                    return UCS_ERR_INVALID_PARAM);
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
     ucs_trace_req("send_nbr buffer %p count %zu tag %"PRIx64" to %s req %p",
                   buffer, count, tag, ucp_ep_peer_name(ep), request);
 
-    status = ucp_tag_send_inline(ep, buffer, count, datatype, tag);
+    status = UCS_PROFILE_CALL(ucp_tag_send_inline, ep, buffer, count,
+                              datatype, tag);
     if (ucs_likely(status != UCS_ERR_NO_RESOURCE)) {
-        UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+        UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
         return status;
     }
 
@@ -224,7 +239,7 @@ UCS_PROFILE_FUNC(ucs_status_t, ucp_tag_send_nbr,
                            ucp_ep_config(ep)->tag.rndv_send_nbr.am_thresh,
                            NULL, ucp_ep_config(ep)->tag.proto, 0);
 
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
 
     if (ucs_unlikely(UCS_PTR_IS_ERR(ret))) {
         return UCS_PTR_STATUS(ret);
@@ -239,8 +254,11 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_send_sync_nb,
 {
     ucp_request_t *req;
     ucs_status_ptr_t ret;
+    ucs_status_t status;
 
-    UCP_THREAD_CS_ENTER_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_CONTEXT_CHECK_FEATURE_FLAGS(ep->worker->context, UCP_FEATURE_TAG,
+                                    return UCS_STATUS_PTR(UCS_ERR_INVALID_PARAM));
+    UCP_WORKER_THREAD_CS_ENTER_CONDITIONAL(ep->worker);
 
     ucs_trace_req("send_sync_nb buffer %p count %zu tag %"PRIx64" to %s cb %p",
                   buffer, count, tag, ucp_ep_peer_name(ep), cb);
@@ -250,15 +268,18 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_send_sync_nb,
         goto out;
     }
 
+    status = ucp_ep_resolve_dest_ep_ptr(ep, ucp_ep_config(ep)->tag.lane);
+    if (status != UCS_OK) {
+        ret = UCS_STATUS_PTR(status);
+        goto out;
+    }
+
     req = ucp_request_get(ep->worker);
     if (req == NULL) {
         ret = UCS_STATUS_PTR(UCS_ERR_NO_MEMORY);
         goto out;
     }
 
-    /* Remote side needs to send reply, so have it connect to us */
-    ucp_ep_connect_remote(ep);
-
     ucp_tag_send_req_init(req, ep, buffer, datatype, count, tag,
                           UCP_REQUEST_FLAG_SYNC);
 
@@ -267,6 +288,6 @@ UCS_PROFILE_FUNC(ucs_status_ptr_t, ucp_tag_send_sync_nb,
                            ucp_ep_config(ep)->tag.rndv.am_thresh,
                            cb, ucp_ep_config(ep)->tag.sync_proto, 1);
 out:
-    UCP_THREAD_CS_EXIT_CONDITIONAL(&ep->worker->mt_lock);
+    UCP_WORKER_THREAD_CS_EXIT_CONDITIONAL(ep->worker);
     return ret;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.c
index 3e92f45e8..e2cbe599b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.c
@@ -48,9 +48,11 @@ typedef struct {
     float            overhead;
     float            bandwidth;
     float            lat_ovh;
-    uint32_t         prio_cap_flags; /* 8 lsb: prio, 24 msb - cap flags */
+    uint32_t         prio_cap_flags; /* 8 lsb: prio, 22 msb: cap flags, 2 hsb: amo */
 } ucp_address_packed_iface_attr_t;
 
+#define UCT_ADDRESS_FLAG_ATOMIC32     UCS_BIT(30) /* 32bit atomic operations */
+#define UCT_ADDRESS_FLAG_ATOMIC64     UCS_BIT(31) /* 64bit atomic operations */
 
 #define UCP_ADDRESS_FLAG_LAST         0x80   /* Last address in the list */
 #define UCP_ADDRESS_FLAG_EP_ADDR      0x40   /* Indicates that ep addr is packed
@@ -168,6 +170,15 @@ ucp_address_gather_devices(ucp_worker_h worker, uint64_t tl_bitmap, int has_ep,
     return UCS_OK;
 }
 
+static const char *ucp_address_get_worker_name(ucp_worker_h worker)
+{
+    #if ENABLE_DEBUG_DATA
+        return ucp_worker_get_name(worker);
+    #else
+        return "";
+    #endif
+}
+
 static size_t ucp_address_packed_size(ucp_worker_h worker,
                                       const ucp_address_packed_device_t *devices,
                                       ucp_rsc_index_t num_devices)
@@ -176,7 +187,7 @@ static size_t ucp_address_packed_size(ucp_worker_h worker,
     size_t size;
 
     size = sizeof(uint64_t) +
-           ucp_address_string_packed_size(ucp_worker_get_name(worker));
+           ucp_address_string_packed_size(ucp_address_get_worker_name(worker));
 
     if (num_devices == 0) {
         size += 1;                      /* NULL md_index */
@@ -234,9 +245,6 @@ static void ucp_address_pack_iface_attr(ucp_address_packed_iface_attr_t *packed,
     uint64_t bit;
 
     cap_flags = iface_attr->cap.flags;
-    if (!enable_atomics) {
-        cap_flags &= ~(UCP_UCT_IFACE_ATOMIC32_FLAGS | UCP_UCT_IFACE_ATOMIC64_FLAGS);
-    }
 
     packed->prio_cap_flags = ((uint8_t)iface_attr->priority);
     packed->overhead       = iface_attr->overhead;
@@ -255,6 +263,18 @@ static void ucp_address_pack_iface_attr(ucp_address_packed_iface_attr_t *packed,
         }
         bit <<= 1;
     }
+
+    if (enable_atomics) {
+        if (ucs_test_all_flags(iface_attr->cap.atomic32.op_flags, UCP_ATOMIC_OP_MASK) &&
+            ucs_test_all_flags(iface_attr->cap.atomic32.fop_flags, UCP_ATOMIC_FOP_MASK)) {
+            packed->prio_cap_flags |= UCT_ADDRESS_FLAG_ATOMIC32;
+        }
+        if (ucs_test_all_flags(iface_attr->cap.atomic64.op_flags, UCP_ATOMIC_OP_MASK) &&
+            ucs_test_all_flags(iface_attr->cap.atomic64.fop_flags, UCP_ATOMIC_FOP_MASK)) {
+            packed->prio_cap_flags |= UCT_ADDRESS_FLAG_ATOMIC64;
+        }
+    }
+
 }
 
 static void
@@ -281,6 +301,15 @@ ucp_address_unpack_iface_attr(ucp_address_iface_attr_t *iface_attr,
         }
         bit <<= 1;
     }
+
+    if (packed->prio_cap_flags & UCT_ADDRESS_FLAG_ATOMIC32) {
+        iface_attr->atomic.atomic32.op_flags  |= UCP_ATOMIC_OP_MASK;
+        iface_attr->atomic.atomic32.fop_flags |= UCP_ATOMIC_FOP_MASK;
+    }
+    if (packed->prio_cap_flags & UCT_ADDRESS_FLAG_ATOMIC64) {
+        iface_attr->atomic.atomic64.op_flags  |= UCP_ATOMIC_OP_MASK;
+        iface_attr->atomic.atomic64.fop_flags |= UCP_ATOMIC_FOP_MASK;
+    }
 }
 
 static ucs_status_t ucp_address_do_pack(ucp_worker_h worker, ucp_ep_h ep,
@@ -307,7 +336,7 @@ static ucs_status_t ucp_address_do_pack(ucp_worker_h worker, ucp_ep_h ep,
 
     *(uint64_t*)ptr = worker->uuid;
     ptr += sizeof(uint64_t);
-    ptr = ucp_address_pack_string(ucp_worker_get_name(worker), ptr);
+    ptr = ucp_address_pack_string(ucp_address_get_worker_name(worker), ptr);
 
     if (num_devices == 0) {
         *((uint8_t*)ptr) = UCP_NULL_RESOURCE;
@@ -405,7 +434,7 @@ static ucs_status_t ucp_address_do_pack(ucp_worker_h worker, ucp_ep_h ep,
 
             /* Save the address index of this transport */
             if (order != NULL) {
-                order[ucs_count_one_bits(tl_bitmap & UCS_MASK(i))] = index;
+                order[ucs_popcount(tl_bitmap & UCS_MASK(i))] = index;
             }
 
             ucs_trace("pack addr[%d] : "UCT_TL_RESOURCE_DESC_FMT
@@ -476,10 +505,8 @@ out:
     return status;
 }
 
-ucs_status_t ucp_address_unpack(const void *buffer, uint64_t *remote_uuid_p,
-                                char *remote_name, size_t max,
-                                unsigned *address_count_p,
-                                ucp_address_entry_t **address_list_p)
+ucs_status_t ucp_address_unpack(const void *buffer,
+                                ucp_unpacked_address_t *unpacked_address)
 {
     ucp_address_entry_t *address_list, *address;
     const uct_device_addr_t *dev_addr;
@@ -497,10 +524,11 @@ ucs_status_t ucp_address_unpack(const void *buffer, uint64_t *remote_uuid_p,
     const void *aptr;
 
     ptr = buffer;
-    *remote_uuid_p = *(uint64_t*)ptr;
+    unpacked_address->uuid = *(uint64_t*)ptr;
     ptr += sizeof(uint64_t);
 
-    aptr = ucp_address_unpack_string(ptr, remote_name, max);
+    aptr = ucp_address_unpack_string(ptr, unpacked_address->name,
+                                     sizeof(unpacked_address->name));
 
     address_count = 0;
 
@@ -549,6 +577,7 @@ ucs_status_t ucp_address_unpack(const void *buffer, uint64_t *remote_uuid_p,
     address_list = ucs_calloc(address_count, sizeof(*address_list),
                               "ucp_address_list");
     if (address_list == NULL) {
+        ucs_error("failed to allocate address list");
         return UCS_ERR_NO_MEMORY;
     }
 
@@ -622,8 +651,8 @@ ucs_status_t ucp_address_unpack(const void *buffer, uint64_t *remote_uuid_p,
         ++dev_index;
     } while (!last_dev);
 
-    *address_count_p = address_count;
-    *address_list_p  = address_list;
+    unpacked_address->address_count = address_count;
+    unpacked_address->address_list  = address_list;
     return UCS_OK;
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.h
index 675ebe0c6..24529c545 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/address.h
@@ -23,12 +23,11 @@ enum {
          UCT_IFACE_FLAG_PUT_SHORT |
          UCT_IFACE_FLAG_PUT_BCOPY |
          UCT_IFACE_FLAG_PUT_ZCOPY |
+         UCT_IFACE_FLAG_GET_SHORT |
          UCT_IFACE_FLAG_GET_BCOPY |
          UCT_IFACE_FLAG_GET_ZCOPY |
          UCT_IFACE_FLAG_TAG_EAGER_BCOPY |
          UCT_IFACE_FLAG_TAG_RNDV_ZCOPY  |
-         UCP_UCT_IFACE_ATOMIC32_FLAGS |
-         UCP_UCT_IFACE_ATOMIC64_FLAGS |
          UCT_IFACE_FLAG_EVENT_RECV |
          UCT_IFACE_FLAG_EVENT_RECV_SIG |
          UCT_IFACE_FLAG_PENDING
@@ -39,11 +38,12 @@ enum {
  * Remote interface attributes.
  */
 struct ucp_address_iface_attr {
-    uint64_t                   cap_flags;     /* Interface capability flags */
-    double                     overhead;      /* Interface performance - overhead */
-    double                     bandwidth;     /* Interface performance - bandwidth */
-    int                        priority;      /* Priority of device */
-    double                     lat_ovh;       /* latency overhead */
+    uint64_t                    cap_flags;    /* Interface capability flags */
+    double                      overhead;     /* Interface performance - overhead */
+    double                      bandwidth;    /* Interface performance - bandwidth */
+    int                         priority;     /* Priority of device */
+    double                      lat_ovh;      /* Latency overhead */
+    ucp_tl_iface_atomic_flags_t atomic;       /* Atomic operations */
 };
 
 
@@ -62,6 +62,17 @@ struct ucp_address_entry {
 };
 
 
+/**
+ * Unpacked remote address
+ */
+struct ucp_unpacked_address {
+    uint64_t                   uuid;            /* Remote worker UUID */
+    char                       name[UCP_WORKER_NAME_MAX]; /* Remote worker name */
+    unsigned                   address_count;   /* Length of address list */
+    ucp_address_entry_t        *address_list;   /* Pointer to address list */
+};
+
+
 /**
  * Pack multiple addresses into a buffer, of resources specified in rsc_bitmap.
  * For every resource in rcs_bitmap:
@@ -91,22 +102,16 @@ ucs_status_t ucp_address_pack(ucp_worker_h worker, ucp_ep_h ep, uint64_t tl_bitm
  * Unpack a list of addresses.
  *
  * @param [in]  buffer           Buffer with data to unpack.
- * @param [out] remote_uuid_p    Filled with remote worker uuid.
- * @param [out] remote_name      Filled with remote worker name.
- * @param [in]  max              Maximal length on @a remote_name.
- * @param [out] address_count_p  Filled with amount of addresses in the list.
- * @param [out] address_list_p   Filled with pointer to unpacked address list.
- *                                It should be released by ucs_free().
+ * @param [out] unpacked_address Filled with remote address data.
  *
  * @note Entries in the address list could point into the data buffer, so it
- *       should not be released as long as the list is used.
+ *       should not be released as long as the remote address is used.
  *
- * @note The address list should be released by ucs_free().
+ * @note The address list inside @ref ucp_remote_address_t should be released
+ *       by ucs_free().
  */
-ucs_status_t ucp_address_unpack(const void *buffer, uint64_t *remote_uuid_p,
-                                char *remote_name, size_t max,
-                                unsigned *address_count_p,
-                                ucp_address_entry_t **address_list_p);
+ucs_status_t ucp_address_unpack(const void *buffer,
+                                ucp_unpacked_address_t *unpacked_address);
 
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.c
new file mode 100644
index 000000000..b08d160dc
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.c
@@ -0,0 +1,209 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include <ucp/core/ucp_ep.h>
+#include <ucp/core/ucp_ep.inl>
+#include <ucp/wireup/ep_match.h>
+#include <inttypes.h>
+
+
+__KHASH_IMPL(ucp_ep_match, static UCS_F_MAYBE_UNUSED inline, uint64_t,
+             ucp_ep_match_entry_t, 1, kh_int64_hash_func, kh_int64_hash_equal);
+
+
+#define ucp_ep_match_list_for_each(_elem, _head, _member) \
+    for (_elem = ucs_container_of((_head)->next, typeof(*_elem), _member); \
+         (_elem) != ucs_container_of(NULL, typeof(*_elem), _member); \
+         _elem = ucs_container_of((_elem)->_member.next, typeof(*_elem), _member))
+
+static inline void ucp_ep_match_list_add_tail(ucs_list_link_t *head,
+                                              ucs_list_link_t *elem)
+{
+    ucs_list_link_t *last;
+
+    last       = head->prev;
+    elem->next = NULL;
+    head->prev = elem;
+
+    if (last == NULL) {
+        elem->prev = NULL;
+        head->next = elem;
+    } else {
+        elem->prev = last;
+        last->next = elem;
+    }
+}
+
+static inline void ucp_ep_match_list_del(ucs_list_link_t *head,
+                                         ucs_list_link_t *elem)
+{
+    (elem->prev ? elem->prev : head)->next = elem->next;
+    (elem->next ? elem->next : head)->prev = elem->prev;
+}
+
+void ucp_ep_match_init(ucp_ep_match_ctx_t *match_ctx)
+{
+    kh_init_inplace(ucp_ep_match, &match_ctx->hash);
+}
+
+void ucp_ep_match_cleanup(ucp_ep_match_ctx_t *match_ctx)
+{
+    ucp_ep_match_entry_t entry;
+    uint64_t dest_uuid;
+
+    kh_foreach(&match_ctx->hash, dest_uuid, entry, {
+        if (entry.exp_ep_q.next != NULL) {
+            ucs_warn("match_ctx %p: uuid 0x%"PRIx64" expected queue is not empty",
+                     match_ctx, dest_uuid);
+        }
+        if (entry.unexp_ep_q.next != NULL) {
+            ucs_warn("match_ctx %p: uuid 0x%"PRIx64" unexpected queue is not empty",
+                     match_ctx, dest_uuid);
+        }
+    })
+    kh_destroy_inplace(ucp_ep_match, &match_ctx->hash);
+}
+
+static ucp_ep_match_entry_t*
+ucp_ep_match_entry_get(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid)
+{
+    ucp_ep_match_entry_t *entry;
+    khiter_t iter;
+    int ret;
+
+    iter  = kh_put(ucp_ep_match, &match_ctx->hash, dest_uuid, &ret);
+    entry = &kh_value(&match_ctx->hash, iter);
+
+    if (ret != 0) {
+        /* initialize match list on first use */
+        entry->next_conn_sn    = 0;
+        entry->exp_ep_q.next   = NULL;
+        entry->exp_ep_q.prev   = NULL;
+        entry->unexp_ep_q.next = NULL;
+        entry->unexp_ep_q.prev = NULL;
+    }
+
+    return entry;
+}
+
+ucp_ep_conn_sn_t ucp_ep_match_get_next_sn(ucp_ep_match_ctx_t *match_ctx,
+                                          uint64_t dest_uuid)
+{
+    ucp_ep_match_entry_t *entry = ucp_ep_match_entry_get(match_ctx, dest_uuid);
+    return entry->next_conn_sn++;
+}
+
+static void ucp_ep_match_insert_common(ucp_ep_match_ctx_t *match_ctx,
+                                       ucs_list_link_t *list, ucp_ep_h ep,
+                                       uint64_t dest_uuid, const char *title)
+{
+    /* NOTE: protect union */
+    ucs_assert(!(ep->flags & (UCP_EP_FLAG_ON_MATCH_CTX |
+                              UCP_EP_FLAG_FLUSH_STATE_VALID |
+                              UCP_EP_FLAG_LISTENER)));
+
+    ucp_ep_match_list_add_tail(list, &ucp_ep_ext_gen(ep)->ep_match.list);
+    ep->flags                              |= UCP_EP_FLAG_ON_MATCH_CTX;
+    ucp_ep_ext_gen(ep)->ep_match.dest_uuid  = dest_uuid;
+    ucs_trace("match_ctx %p: ep %p added as %s uuid 0x%"PRIx64" conn_sn %d",
+              match_ctx, ep, title, dest_uuid, ep->conn_sn);
+}
+
+void ucp_ep_match_insert_exp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                             ucp_ep_h ep)
+{
+    ucp_ep_match_entry_t *entry = ucp_ep_match_entry_get(match_ctx, dest_uuid);
+
+    ucs_assert(!(ep->flags & UCP_EP_FLAG_DEST_EP));
+    ucp_ep_match_insert_common(match_ctx, &entry->exp_ep_q, ep, dest_uuid,
+                               "expected");
+}
+
+void ucp_ep_match_insert_unexp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                               ucp_ep_h ep)
+{
+    ucp_ep_match_entry_t *entry = ucp_ep_match_entry_get(match_ctx, dest_uuid);
+
+    ucp_ep_match_insert_common(match_ctx, &entry->unexp_ep_q, ep, dest_uuid,
+                               "unexpected");
+}
+
+static ucp_ep_h
+ucp_ep_match_retrieve_common(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                             ucp_ep_conn_sn_t conn_sn, int is_exp,
+                             ucp_ep_flags_t exp_ep_flags, const char *title)
+{
+    ucp_ep_match_entry_t *entry;
+    ucs_list_link_t *list;
+    ucp_ep_ext_gen_t *ep_ext;
+    khiter_t iter;
+    ucp_ep_h ep;
+
+    iter = kh_get(ucp_ep_match, &match_ctx->hash, dest_uuid);
+    if (iter == kh_end(&match_ctx->hash)) {
+        goto notfound; /* no hash entry */
+    }
+
+    entry = &kh_value(&match_ctx->hash, iter);
+    list  = is_exp ? &entry->exp_ep_q : &entry->unexp_ep_q;
+    ucp_ep_match_list_for_each(ep_ext, list, ep_match.list) {
+        ep = ucp_ep_from_ext_gen(ep_ext);
+        if (ep->conn_sn == conn_sn) {
+            ucp_ep_match_list_del(list, &ep_ext->ep_match.list);
+            ucs_trace("match_ctx %p: matched %s ep %p by uuid 0x%"PRIx64" conn_sn %d",
+                      match_ctx, title, ep, dest_uuid, conn_sn);
+            ucs_assertv(ucs_test_all_flags(ep->flags,
+                                           exp_ep_flags | UCP_EP_FLAG_ON_MATCH_CTX),
+                        "ep=%p flags=0x%x exp_flags=0x%x", ep, ep->flags,
+                        exp_ep_flags);
+            ep->flags &= ~UCP_EP_FLAG_ON_MATCH_CTX;
+            return ep;
+        }
+    }
+
+notfound:
+    ucs_trace("match_ctx %p: %s uuid 0x%"PRIx64" conn_sn %d not found",
+              match_ctx, title, dest_uuid, conn_sn);
+    return NULL;
+}
+
+ucp_ep_h ucp_ep_match_retrieve_exp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                                   ucp_ep_conn_sn_t conn_sn)
+{
+    return ucp_ep_match_retrieve_common(match_ctx, dest_uuid, conn_sn, 1, 0,
+                                        "expected");
+}
+
+ucp_ep_h ucp_ep_match_retrieve_unexp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                                     ucp_ep_conn_sn_t conn_sn)
+{
+    return ucp_ep_match_retrieve_common(match_ctx, dest_uuid, conn_sn, 0,
+                                        UCP_EP_FLAG_DEST_EP, "unexpected");
+}
+
+void ucp_ep_match_remove_ep(ucp_ep_match_ctx_t *match_ctx, ucp_ep_h ep)
+{
+    ucp_ep_ext_gen_t *ep_ext = ucp_ep_ext_gen(ep);
+    ucp_ep_match_entry_t *entry;
+    khiter_t iter;
+
+    if (!(ep->flags & UCP_EP_FLAG_ON_MATCH_CTX)) {
+        return;
+    }
+
+    iter = kh_get(ucp_ep_match, &match_ctx->hash, ep_ext->ep_match.dest_uuid);
+    ucs_assertv(iter != kh_end(&match_ctx->hash), "ep %p not found in hash", ep);
+    entry = &kh_value(&match_ctx->hash, iter);
+
+    if (ep->flags & UCP_EP_FLAG_DEST_EP) {
+        ucs_trace("match_ctx %p: remove unexpected ep %p", match_ctx, ep);
+        ucp_ep_match_list_del(&entry->unexp_ep_q, &ep_ext->ep_match.list);
+    } else {
+        ucs_trace("match_ctx %p: remove expected ep %p", match_ctx, ep);
+        ucp_ep_match_list_del(&entry->exp_ep_q, &ep_ext->ep_match.list);
+    }
+    ep->flags &= ~UCP_EP_FLAG_ON_MATCH_CTX;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.h
new file mode 100644
index 000000000..6b424d395
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/ep_match.h
@@ -0,0 +1,74 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCP_EP_MATCH_H_
+#define UCP_EP_MATCH_H_
+
+#include <ucp/core/ucp_types.h>
+#include <ucs/datastruct/khash.h>
+#include <ucs/datastruct/list.h>
+
+
+/*
+ * Structure to embed in a UCP endpoint to support matching with remote endpoints
+ */
+typedef struct {
+    uint64_t                  dest_uuid;     /* Destination worker UUID */
+    ucs_list_link_t           list;          /* List entry into endpoint
+                                                matching structure */
+} ucp_ep_match_t;
+
+
+/**
+ * Endpoint-to-endpoint matching entry - allows *ordered* matching of endpoints
+ * between a pair of workers.
+ * The expected/unexpected lists are *not* circular
+ */
+typedef struct ucp_ep_match_entry {
+    ucs_list_link_t          exp_ep_q;        /* Endpoints created by API and not
+                                                 connected to remote endpoint */
+    ucs_list_link_t          unexp_ep_q;      /* Endpoints created internally as
+                                                 connected a to remote endpoints,
+                                                 but not provided to user yet */
+    ucp_ep_conn_sn_t         next_conn_sn;    /* Sequence number of matching
+                                                 endpoints, since UCT may provide
+                                                 wireup messages which were sent
+                                                 on different endpoint out-of-order */
+} ucp_ep_match_entry_t;
+
+
+__KHASH_TYPE(ucp_ep_match, uint64_t, ucp_ep_match_entry_t)
+
+
+/* Context for matching endpoints */
+typedef struct {
+    khash_t(ucp_ep_match)    hash;
+} ucp_ep_match_ctx_t;
+
+
+void ucp_ep_match_init(ucp_ep_match_ctx_t *match_ctx);
+
+void ucp_ep_match_cleanup(ucp_ep_match_ctx_t *match_ctx);
+
+ucp_ep_conn_sn_t ucp_ep_match_get_next_sn(ucp_ep_match_ctx_t *match_ctx,
+                                          uint64_t dest_uuid);
+
+void ucp_ep_match_insert_exp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                             ucp_ep_h ep);
+
+void ucp_ep_match_insert_unexp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                               ucp_ep_h ep);
+
+ucp_ep_h ucp_ep_match_retrieve_exp(ucp_ep_match_ctx_t *match_ctx, uint64_t dest_uuid,
+                                   ucp_ep_conn_sn_t conn_sn);
+
+ucp_ep_h ucp_ep_match_retrieve_unexp(ucp_ep_match_ctx_t *ep_conn, uint64_t dest_uuid,
+                                     ucp_ep_conn_sn_t conn_sn);
+
+void ucp_ep_match_remove_ep(ucp_ep_match_ctx_t *ep_conn, ucp_ep_h ep);
+
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/select.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/select.c
index bc1635a27..8bb5db3e4 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/select.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/select.c
@@ -16,6 +16,25 @@
 
 #define UCP_WIREUP_RMA_BW_TEST_MSG_SIZE       262144
 
+#define UCP_WIREUP_CHECK_AMO_FLAGS(_ae, _criteria, _context, _addr_index, _op, _size)      \
+    if (!ucs_test_all_flags((_ae)->iface_attr.atomic.atomic##_size._op##_flags,            \
+                            (_criteria)->remote_atomic_flags.atomic##_size._op##_flags)) { \
+        char desc[256];                                                                    \
+        ucs_trace("addr[%d] %s: no %s", (_addr_index),                                     \
+                  ucp_find_tl_name_by_csum((_context), (_ae)->tl_name_csum),               \
+                  ucp_wireup_get_missing_amo_flag_desc_##_op(                              \
+                      (_ae)->iface_attr.atomic.atomic##_size._op##_flags,                  \
+                      (_criteria)->remote_atomic_flags.atomic##_size._op##_flags,          \
+                      (_size), desc, sizeof(desc)));                                       \
+        continue;                                                                          \
+    }
+
+typedef struct ucp_wireup_atomic_flag {
+    const char *name;
+    const char *fetch;
+} ucp_wireup_atomic_flag_t;
+
+
 enum {
     UCP_WIREUP_LANE_USAGE_AM     = UCS_BIT(0), /* Active messages */
     UCP_WIREUP_LANE_USAGE_AM_BW  = UCS_BIT(1), /* High-BW active messages */
@@ -64,14 +83,6 @@ static const char *ucp_wireup_iface_flags[] = {
     [ucs_ilog2(UCT_IFACE_FLAG_GET_SHORT)]        = "get short",
     [ucs_ilog2(UCT_IFACE_FLAG_GET_BCOPY)]        = "get bcopy",
     [ucs_ilog2(UCT_IFACE_FLAG_GET_ZCOPY)]        = "get zcopy",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_ADD32)]     = "32-bit atomic add",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_ADD64)]     = "64-bit atomic add",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_FADD32)]    = "32-bit atomic fetch-add",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_FADD64)]    = "64-bit atomic fetch-add",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_SWAP32)]    = "32-bit atomic swap",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_SWAP64)]    = "64-bit atomic swap",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_CSWAP32)]   = "32-bit atomic compare-swap",
-    [ucs_ilog2(UCT_IFACE_FLAG_ATOMIC_CSWAP64)]   = "64-bit atomic compare-swap",
     [ucs_ilog2(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE)] = "peer failure handler",
     [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_IFACE)] = "connect to iface",
     [ucs_ilog2(UCT_IFACE_FLAG_CONNECT_TO_EP)]    = "connect to ep",
@@ -88,6 +99,16 @@ static const char *ucp_wireup_iface_flags[] = {
     [ucs_ilog2(UCT_IFACE_FLAG_TAG_RNDV_ZCOPY)]   = "tag rndv zcopy"
 };
 
+static ucp_wireup_atomic_flag_t ucp_wireup_atomic_desc[] = {
+     [UCT_ATOMIC_OP_ADD]   = {.name = "add",   .fetch = "fetch-"},
+     [UCT_ATOMIC_OP_AND]   = {.name = "and",   .fetch = "fetch-"},
+     [UCT_ATOMIC_OP_OR]    = {.name = "or",    .fetch = "fetch-"},
+     [UCT_ATOMIC_OP_XOR]   = {.name = "xor",   .fetch = "fetch-"},
+     [UCT_ATOMIC_OP_SWAP]  = {.name = "swap",  .fetch = ""},
+     [UCT_ATOMIC_OP_CSWAP] = {.name = "cscap", .fetch = ""}
+};
+
+
 static double ucp_wireup_aux_score_func(ucp_context_h context,
                                         const uct_md_attr_t *md_attr,
                                         const uct_iface_attr_t *iface_attr,
@@ -101,6 +122,37 @@ ucp_wireup_get_missing_flag_desc(uint64_t flags, uint64_t required_flags,
     return flag_descs[ucs_ffs64(required_flags & (~flags))];
 }
 
+static const char *
+ucp_wireup_get_missing_amo_flag_desc(uint64_t flags, uint64_t required_flags,
+                                     int op_size, int fetch, char *buf, size_t len)
+{
+    int idx;
+
+    ucs_assert((required_flags & (~flags)) != 0);
+
+    idx = ucs_ffs64(required_flags & (~flags));
+
+    snprintf(buf, len, "%d-bit atomic %s%s", op_size,
+             fetch ? ucp_wireup_atomic_desc[idx].fetch : "",
+             ucp_wireup_atomic_desc[idx].name);
+
+    return buf;
+}
+
+static const char *
+ucp_wireup_get_missing_amo_flag_desc_op(uint64_t flags, uint64_t required_flags,
+                                        int op_size, char *buf, size_t len)
+{
+    return ucp_wireup_get_missing_amo_flag_desc(flags, required_flags, op_size, 0, buf, len);
+}
+
+static const char *
+ucp_wireup_get_missing_amo_flag_desc_fop(uint64_t flags, uint64_t required_flags,
+                                         int op_size, char *buf, size_t len)
+{
+    return ucp_wireup_get_missing_amo_flag_desc(flags, required_flags, op_size, 1, buf, len);
+}
+
 static int ucp_wireup_check_flags(const uct_tl_resource_desc_t *resource,
                                   uint64_t flags, uint64_t required_flags,
                                   const char *title, const char ** flag_descs,
@@ -124,6 +176,30 @@ static int ucp_wireup_check_flags(const uct_tl_resource_desc_t *resource,
     return 0;
 }
 
+static int ucp_wireup_check_amo_flags(const uct_tl_resource_desc_t *resource,
+                                      uint64_t flags, uint64_t required_flags,
+                                      int op_size, int fetch,
+                                      const char *title, char *reason, size_t max)
+{
+    char missing_flag_desc[256];
+
+    if (ucs_test_all_flags(flags, required_flags)) {
+        return 1;
+    }
+
+    if (required_flags) {
+        ucp_wireup_get_missing_amo_flag_desc(flags, required_flags,
+                                             op_size, fetch, missing_flag_desc,
+                                             sizeof(missing_flag_desc));
+        ucs_trace(UCT_TL_RESOURCE_DESC_FMT " : not suitable for %s, no %s",
+                  UCT_TL_RESOURCE_DESC_ARG(resource), title,
+                  missing_flag_desc);
+        snprintf(reason, max, UCT_TL_RESOURCE_DESC_FMT" - no %s",
+                 UCT_TL_RESOURCE_DESC_ARG(resource), missing_flag_desc);
+    }
+    return 0;
+}
+
 static int ucp_wireup_is_reachable(ucp_worker_h worker, ucp_rsc_index_t rsc_index,
                                    const ucp_address_entry_t *ae)
 {
@@ -191,7 +267,8 @@ ucp_wireup_select_transport(ucp_ep_h ep, const ucp_address_entry_t *address_list
 
         /* Make sure we are indeed passing all flags required by the criteria in
          * ucp packed address */
-        ucs_assert(ucs_test_all_flags(UCP_ADDRESS_IFACE_FLAGS, criteria->remote_iface_flags));
+        ucs_assert(ucs_test_all_flags(UCP_ADDRESS_IFACE_FLAGS,
+                                      criteria->remote_iface_flags));
 
         if (!ucs_test_all_flags(ae->iface_attr.cap_flags, criteria->remote_iface_flags)) {
             ucs_trace("addr[%d] %s: no %s", addr_index,
@@ -202,6 +279,11 @@ ucp_wireup_select_transport(ucp_ep_h ep, const ucp_address_entry_t *address_list
             continue;
         }
 
+        UCP_WIREUP_CHECK_AMO_FLAGS(ae, criteria, context, addr_index, op, 32);
+        UCP_WIREUP_CHECK_AMO_FLAGS(ae, criteria, context, addr_index, op, 64);
+        UCP_WIREUP_CHECK_AMO_FLAGS(ae, criteria, context, addr_index, fop, 32);
+        UCP_WIREUP_CHECK_AMO_FLAGS(ae, criteria, context, addr_index, fop, 64);
+
         addr_index_map |= UCS_BIT(addr_index);
     }
 
@@ -230,7 +312,19 @@ ucp_wireup_select_transport(ucp_ep_h ep, const ucp_address_entry_t *address_list
                                     ucp_wireup_md_flags, p, endp - p) ||
             !ucp_wireup_check_flags(resource, iface_attr->cap.flags,
                                     criteria->local_iface_flags, criteria->title,
-                                    ucp_wireup_iface_flags, p, endp - p))
+                                    ucp_wireup_iface_flags, p, endp - p) ||
+            !ucp_wireup_check_amo_flags(resource, iface_attr->cap.atomic32.op_flags,
+                                        criteria->local_atomic_flags.atomic32.op_flags,
+                                        32, 0, criteria->title, p, endp - p) ||
+            !ucp_wireup_check_amo_flags(resource, iface_attr->cap.atomic64.op_flags,
+                                        criteria->local_atomic_flags.atomic64.op_flags,
+                                        64, 0, criteria->title, p, endp - p) ||
+            !ucp_wireup_check_amo_flags(resource, iface_attr->cap.atomic32.fop_flags,
+                                        criteria->local_atomic_flags.atomic32.fop_flags,
+                                        32, 1, criteria->title, p, endp - p) ||
+            !ucp_wireup_check_amo_flags(resource, iface_attr->cap.atomic64.fop_flags,
+                                        criteria->local_atomic_flags.atomic64.fop_flags,
+                                        64, 1, criteria->title, p, endp - p))
         {
             p += strlen(p);
             snprintf(p, endp - p, ", ");
@@ -469,7 +563,7 @@ ucp_wireup_add_memaccess_lanes(ucp_ep_h ep, unsigned address_count,
                                ucp_lane_index_t *num_lanes_p,
                                const ucp_wireup_criteria_t *criteria,
                                uint64_t tl_bitmap, uint32_t usage,
-                               int select_best)
+                               int select_best, int show_error)
 {
     ucp_wireup_criteria_t mem_criteria = *criteria;
     ucp_address_entry_t *address_list_copy;
@@ -499,7 +593,7 @@ ucp_wireup_add_memaccess_lanes(ucp_ep_h ep, unsigned address_count,
     mem_criteria.remote_md_flags = UCT_MD_FLAG_REG | criteria->remote_md_flags;
     status = ucp_wireup_select_transport(ep, address_list_copy, address_count,
                                          &mem_criteria, tl_bitmap, remote_md_map,
-                                         -1, -1, select_best,
+                                         -1, -1, show_error,
                                          &rsc_index, &addr_index, &score);
     if (status != UCS_OK) {
         goto out_free_address_list;
@@ -573,11 +667,16 @@ static double ucp_wireup_rma_score_func(ucp_context_h context,
                    (4096.0 / ucs_min(iface_attr->bandwidth, remote_iface_attr->bandwidth)));
 }
 
+static int ucp_wireup_ep_params_is_err_mode_peer(const ucp_ep_params_t *params)
+{
+    return (params->field_mask & UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE) &&
+           (params->err_mode == UCP_ERR_HANDLING_MODE_PEER);
+}
+
 static void ucp_wireup_fill_ep_params_criteria(ucp_wireup_criteria_t *criteria,
                                                const ucp_ep_params_t *params)
 {
-    if ((params->field_mask & UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE) &&
-        (params->err_mode == UCP_ERR_HANDLING_MODE_PEER)) {
+    if (ucp_wireup_ep_params_is_err_mode_peer(params)) {
         criteria->local_iface_flags |= UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE;
     }
 }
@@ -600,22 +699,39 @@ static void ucp_wireup_fill_aux_criteria(ucp_wireup_criteria_t *criteria,
     ucp_wireup_fill_ep_params_criteria(criteria, params);
 }
 
+static void ucp_wireup_clean_amo_criteria(ucp_wireup_criteria_t *criteria)
+{
+    memset(&criteria->remote_atomic_flags, 0,
+           sizeof(criteria->remote_atomic_flags));
+    memset(&criteria->local_atomic_flags, 0,
+           sizeof(criteria->local_atomic_flags));
+}
+
+static int ucp_wireup_allow_am_emulation_layer(const ucp_ep_params_t *params,
+                                               unsigned ep_init_flags)
+{
+    return !(ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE) &&
+           /* disable emulation layer if err handling is required due to lack of
+            * keep alive protocol */
+           !ucp_wireup_ep_params_is_err_mode_peer(params);
+}
+
 static ucs_status_t ucp_wireup_add_rma_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
                                              unsigned ep_init_flags, unsigned address_count,
                                              const ucp_address_entry_t *address_list,
                                              ucp_wireup_lane_desc_t *lane_descs,
-                                             ucp_lane_index_t *num_lanes_p)
+                                             ucp_lane_index_t *num_lanes_p,
+                                             int *need_am)
 {
-    ucp_wireup_criteria_t criteria;
+    ucp_wireup_criteria_t criteria = {0};
+    ucs_status_t status;
+    int allow_am;
 
     if (!(ucp_ep_get_context_features(ep) & UCP_FEATURE_RMA) &&
-        (!(ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE))) {
+        !(ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE)) {
         return UCS_OK;
     }
 
-    criteria.local_md_flags     = 0;
-    criteria.remote_md_flags    = 0;
-
     if (ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE) {
         criteria.title              = "copy across memory types";
         criteria.remote_iface_flags = UCT_IFACE_FLAG_PUT_SHORT;
@@ -632,9 +748,19 @@ static ucs_status_t ucp_wireup_add_rma_lanes(ucp_ep_h ep, const ucp_ep_params_t
     criteria.tl_rsc_flags       = 0;
     ucp_wireup_fill_ep_params_criteria(&criteria, params);
 
-    return ucp_wireup_add_memaccess_lanes(ep, address_count, address_list,
-                                          lane_descs, num_lanes_p, &criteria,
-                                          -1, UCP_WIREUP_LANE_USAGE_RMA, 1);
+    allow_am = ucp_wireup_allow_am_emulation_layer(params, ep_init_flags);
+    status = ucp_wireup_add_memaccess_lanes(ep, address_count, address_list,
+                                            lane_descs, num_lanes_p, &criteria,
+                                            -1, UCP_WIREUP_LANE_USAGE_RMA, 1,
+                                            !allow_am);
+    if (status == UCS_OK) {
+        return status; /* using transport RMA operations */
+    } else if (allow_am) {
+        *need_am = 1;  /* using emulation over active messages */
+        return UCS_OK;
+    } else {
+        return status;
+    }
 }
 
 double ucp_wireup_amo_score_func(ucp_context_h context,
@@ -652,27 +778,29 @@ static ucs_status_t ucp_wireup_add_amo_lanes(ucp_ep_h ep, const ucp_ep_params_t
                                              unsigned address_count,
                                              const ucp_address_entry_t *address_list,
                                              ucp_wireup_lane_desc_t *lane_descs,
-                                             ucp_lane_index_t *num_lanes_p)
+                                             ucp_lane_index_t *num_lanes_p,
+                                             int *need_am)
 {
-    ucp_worker_h worker   = ep->worker;
-    ucp_context_h context = worker->context;
-    ucp_wireup_criteria_t criteria;
+    ucp_worker_h worker            = ep->worker;
+    ucp_context_h context          = worker->context;
+    ucp_wireup_criteria_t criteria = {0};
     ucp_rsc_index_t rsc_index;
+    ucs_status_t status;
     uint64_t tl_bitmap;
+    int allow_am;
 
-    criteria.remote_iface_flags = ucp_context_uct_atomic_iface_flags(context);
-    if ((criteria.remote_iface_flags == 0) ||
+    if (!ucs_test_flags(context->config.features, UCP_FEATURE_AMO32, UCP_FEATURE_AMO64) ||
         (ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE)) {
         return UCS_OK;
     }
 
+    ucp_context_uct_atomic_iface_flags(context, &criteria.remote_atomic_flags);
+
     criteria.title              = "atomic operations on %s memory";
-    criteria.local_md_flags     = 0;
-    criteria.remote_md_flags    = 0;
     criteria.local_iface_flags  = criteria.remote_iface_flags |
                                   UCT_IFACE_FLAG_PENDING;
+    criteria.local_atomic_flags = criteria.remote_atomic_flags;
     criteria.calc_score         = ucp_wireup_amo_score_func;
-    criteria.tl_rsc_flags       = 0;
     ucp_wireup_fill_ep_params_criteria(&criteria, params);
 
     /* We can use only non-p2p resources or resources which are explicitly
@@ -686,9 +814,19 @@ static ucs_status_t ucp_wireup_add_amo_lanes(ucp_ep_h ep, const ucp_ep_params_t
         }
     }
 
-    return ucp_wireup_add_memaccess_lanes(ep, address_count, address_list,
-                                          lane_descs, num_lanes_p, &criteria,
-                                          tl_bitmap, UCP_WIREUP_LANE_USAGE_AMO, 1);
+    allow_am = ucp_wireup_allow_am_emulation_layer(params, ep_init_flags);
+    status = ucp_wireup_add_memaccess_lanes(ep, address_count, address_list,
+                                            lane_descs, num_lanes_p, &criteria,
+                                            tl_bitmap, UCP_WIREUP_LANE_USAGE_AMO,
+                                            1, !allow_am);
+    if (status == UCS_OK) {
+        return status; /* using transport AMO operations */
+    } else if (allow_am) {
+        *need_am = 1;  /* using emulation over active messages */
+        return UCS_OK;
+    } else {
+        return status;
+    }
 }
 
 static double ucp_wireup_am_score_func(ucp_context_h context,
@@ -724,44 +862,61 @@ static int ucp_wireup_is_lane_proxy(ucp_ep_h ep, ucp_rsc_index_t rsc_index,
             UCT_IFACE_FLAG_EVENT_RECV_SIG);
 }
 
+static inline int ucp_wireup_is_am_required(ucp_ep_h ep,
+                                            const ucp_ep_params_t *params,
+                                            unsigned ep_init_flags,
+                                            ucp_wireup_lane_desc_t *lane_descs,
+                                            int num_lanes_p)
+{
+    ucp_lane_index_t lane;
+
+    /* Check if we need active messages from the configurations, for wireup.
+     * If not, check if am is required due to p2p transports */
+
+    if ((ep_init_flags & UCP_EP_CREATE_AM_LANE) ||
+        (params->field_mask & UCP_EP_PARAM_FIELD_SOCK_ADDR)) {
+        return 1;
+    }
+
+    if (!(ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE) &&
+        (ucp_ep_get_context_features(ep) & (UCP_FEATURE_TAG | UCP_FEATURE_STREAM))) {
+        return 1;
+    }
+
+    for (lane = 0; lane < num_lanes_p; ++lane) {
+        if (ucp_worker_is_tl_p2p(ep->worker, lane_descs[lane].rsc_index)) {
+            return 1;
+        }
+    }
+
+    return 0;
+}
+
 static ucs_status_t ucp_wireup_add_am_lane(ucp_ep_h ep, const ucp_ep_params_t *params,
                                            unsigned ep_init_flags, unsigned address_count,
                                            const ucp_address_entry_t *address_list,
                                            ucp_wireup_lane_desc_t *lane_descs,
                                            ucp_lane_index_t *num_lanes_p,
+                                           double *am_score,
                                            ucp_err_handling_mode_t err_mode)
 {
-    ucp_wireup_criteria_t criteria;
+    ucp_wireup_criteria_t criteria = {0};
     ucp_rsc_index_t rsc_index;
-    ucp_lane_index_t lane;
     ucs_status_t status;
     unsigned addr_index;
     int is_proxy;
-    double score;
-    int need_am;
 
-    /* Check if we need active messages, for wireup */
-    if (!(ucp_ep_get_context_features(ep) & (UCP_FEATURE_TAG | UCP_FEATURE_STREAM)) ||
-        (ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE)) {
-        need_am = 0;
-        for (lane = 0; lane < *num_lanes_p; ++lane) {
-            need_am = need_am || ucp_worker_is_tl_p2p(ep->worker,
-                                                      lane_descs[lane].rsc_index);
-        }
-        if (!need_am) {
-            return UCS_OK;
-        }
+    if (!ucp_wireup_is_am_required(ep, params, ep_init_flags, lane_descs,
+                                   *num_lanes_p)) {
+        return UCS_OK;
     }
 
     /* Select one lane for active messages */
     criteria.title              = "active messages";
-    criteria.local_md_flags     = 0;
-    criteria.remote_md_flags    = 0;
     criteria.remote_iface_flags = UCT_IFACE_FLAG_AM_BCOPY |
                                   UCT_IFACE_FLAG_CB_SYNC;
     criteria.local_iface_flags  = UCT_IFACE_FLAG_AM_BCOPY;
     criteria.calc_score         = ucp_wireup_am_score_func;
-    criteria.tl_rsc_flags       = 0;
     ucp_wireup_fill_ep_params_criteria(&criteria, params);
 
     if (ucs_test_all_flags(ucp_ep_get_context_features(ep), UCP_FEATURE_TAG |
@@ -770,7 +925,8 @@ static ucs_status_t ucp_wireup_add_am_lane(ucp_ep_h ep, const ucp_ep_params_t *p
     }
 
     status = ucp_wireup_select_transport(ep, address_list, address_count, &criteria,
-                                         -1, -1, -1, -1, 1, &rsc_index, &addr_index, &score);
+                                         -1, -1, -1, -1, 1, &rsc_index, &addr_index,
+                                         am_score);
     if (status != UCS_OK) {
         return status;
     }
@@ -784,7 +940,7 @@ static ucs_status_t ucp_wireup_add_am_lane(ucp_ep_h ep, const ucp_ep_params_t *p
                                         address_list[addr_index].iface_attr.cap_flags);
 
     ucp_wireup_add_lane_desc(lane_descs, num_lanes_p, rsc_index, addr_index,
-                             address_list[addr_index].md_index, score,
+                             address_list[addr_index].md_index, *am_score,
                              UCP_WIREUP_LANE_USAGE_AM, is_proxy);
 
     return UCS_OK;
@@ -797,22 +953,25 @@ static double ucp_wireup_am_bw_score_func(ucp_context_h context,
 {
     /* best single MTU bandwidth */
     double size = iface_attr->cap.am.max_bcopy;
-    double time = (size / iface_attr->bandwidth) + iface_attr->overhead +
-                  remote_iface_attr->overhead;
+    double time = (size / ucs_min(iface_attr->bandwidth,
+                                  remote_iface_attr->bandwidth)) +
+                  iface_attr->overhead + remote_iface_attr->overhead +
+                  ucp_wireup_tl_iface_latency(context, iface_attr, remote_iface_attr);
+
     return size / time * 1e-5;
 }
 
-static int ucp_wireup_is_ud_lane(ucp_context_h context, ucp_rsc_index_t rsc_index)
+static int ucp_wireup_is_ep_single_lane(ucp_ep_h ep, ucp_rsc_index_t rsc_index)
 {
-    return !strcmp(context->tl_rscs[rsc_index].tl_rsc.tl_name, "ud") ||
-           !strcmp(context->tl_rscs[rsc_index].tl_rsc.tl_name, "ud_mlx5");
+    return (ep->worker->context->tl_rscs[rsc_index].tl_rsc.dev_type == UCT_DEVICE_TYPE_SHM) ||
+           (ep->worker->context->tl_rscs[rsc_index].tl_rsc.dev_type == UCT_DEVICE_TYPE_SELF);
 }
 
 static ucs_status_t ucp_wireup_add_bw_lanes(ucp_ep_h ep,
                                             unsigned address_count,
                                             const ucp_address_entry_t *address_list,
                                             const ucp_wireup_select_bw_info_t *bw_info,
-                                            int allow_proxy,
+                                            int allow_proxy, uint64_t tl_bitmap,
                                             ucp_wireup_lane_desc_t *lane_descs,
                                             ucp_lane_index_t *num_lanes_p)
 {
@@ -837,9 +996,9 @@ static ucs_status_t ucp_wireup_add_bw_lanes(ucp_ep_h ep,
      * (we have to limit MD's number to avoid malloc in
      * memory registration) */
     while ((num_lanes < bw_info->max_lanes) &&
-           (ucs_count_one_bits(md_map) < UCP_MAX_OP_MDS)) {
+           (ucs_popcount(md_map) < UCP_MAX_OP_MDS)) {
         status = ucp_wireup_select_transport(ep, address_list, address_count,
-                                             &bw_info->criteria, -1, -1,
+                                             &bw_info->criteria, tl_bitmap, -1,
                                              local_dev_bitmap, remote_dev_bitmap,
                                              0, &rsc_index, &addr_index, &score);
         if (status != UCS_OK) {
@@ -850,19 +1009,16 @@ static ucs_status_t ucp_wireup_add_bw_lanes(ucp_ep_h ep,
                    ucp_wireup_is_lane_proxy(ep, rsc_index,
                                             address_list[addr_index].iface_attr.cap_flags);
 
-        /* FIXME a temporary workaround to prevent the UD uct from using multilane. */
-        if (!ucp_wireup_is_ud_lane(context, rsc_index)) {
-            ucp_wireup_add_lane_desc(lane_descs, num_lanes_p, rsc_index, addr_index,
-                                     address_list[addr_index].md_index, score,
-                                     bw_info->usage, is_proxy);
-            md_map |= UCS_BIT(context->tl_rscs[rsc_index].md_index);
-            num_lanes++;
-        }
+        ucp_wireup_add_lane_desc(lane_descs, num_lanes_p, rsc_index, addr_index,
+                                 address_list[addr_index].md_index, score,
+                                 bw_info->usage, is_proxy);
+        md_map |= UCS_BIT(context->tl_rscs[rsc_index].md_index);
+        num_lanes++;
 
         local_dev_bitmap  &= ~UCS_BIT(context->tl_rscs[rsc_index].dev_index);
         remote_dev_bitmap &= ~UCS_BIT(address_list[addr_index].dev_index);
 
-        if (ep->worker->context->tl_rscs[rsc_index].tl_rsc.dev_type == UCT_DEVICE_TYPE_SHM) {
+        if (ucp_wireup_is_ep_single_lane(ep, rsc_index)) {
             /* special case for SHM: do not try to lookup additional lanes when
              * SHM transport detected (another transport will be significantly
              * slower) */
@@ -872,6 +1028,7 @@ static ucs_status_t ucp_wireup_add_bw_lanes(ucp_ep_h ep,
 
     return UCS_OK;
 }
+
 static ucs_status_t ucp_wireup_add_am_bw_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
                                                unsigned ep_init_flags, unsigned address_count,
                                                const ucp_address_entry_t *address_list,
@@ -900,6 +1057,7 @@ static ucs_status_t ucp_wireup_add_am_bw_lanes(ucp_ep_h ep, const ucp_ep_params_
     bw_info.criteria.local_iface_flags  = UCT_IFACE_FLAG_AM_BCOPY;
     bw_info.criteria.calc_score         = ucp_wireup_am_bw_score_func;
     bw_info.criteria.tl_rsc_flags       = 0;
+    ucp_wireup_clean_amo_criteria(&bw_info.criteria);
     ucp_wireup_fill_ep_params_criteria(&bw_info.criteria, params);
 
     if (ucs_test_all_flags(ucp_ep_get_context_features(ep), UCP_FEATURE_TAG |
@@ -918,20 +1076,21 @@ static ucs_status_t ucp_wireup_add_am_bw_lanes(ucp_ep_h ep, const ucp_ep_params_
         if (lane_descs[lane_desc_idx].usage & UCP_WIREUP_LANE_USAGE_AM) {
             addr_index                 = lane_descs[lane_desc_idx].addr_index;
             rsc_index                  = lane_descs[lane_desc_idx].rsc_index;
-            /* FIXME a temporary workaround to prevent the UD uct from using multilane. */
-            if (ucp_wireup_is_ud_lane(context, rsc_index)) {
-                return UCS_OK;
-            }
             bw_info.md_map            |= UCS_BIT(context->tl_rscs[rsc_index].md_index);
             bw_info.local_dev_bitmap  &= ~UCS_BIT(context->tl_rscs[rsc_index].dev_index);
             bw_info.remote_dev_bitmap &= ~UCS_BIT(address_list[addr_index].dev_index);
-            break; /* do not continue searching due to we found
-                      AM lane (and there is only one lane) */
+            if (ucp_wireup_is_ep_single_lane(ep, rsc_index)) {
+                /* if AM lane is SELF or SHMEM - then do not use more lanes */
+                return UCS_OK;
+            } else {
+                break; /* do not continue searching due to we found
+                          AM lane (and there is only one lane) */
+            }
         }
     }
 
     return ucp_wireup_add_bw_lanes(ep, address_count, address_list, &bw_info, 1,
-                                   lane_descs, num_lanes_p);
+                                   -1, lane_descs, num_lanes_p);
 }
 
 static ucs_status_t ucp_wireup_add_rma_bw_lanes(ucp_ep_h ep,
@@ -942,6 +1101,7 @@ static ucs_status_t ucp_wireup_add_rma_bw_lanes(ucp_ep_h ep,
                                                 ucp_lane_index_t *num_lanes_p)
 {
     ucp_wireup_select_bw_info_t bw_info;
+    uct_memory_type_t mem_type;
 
     if ((ucp_ep_get_context_features(ep) & UCP_FEATURE_RMA) ||
         (ep_init_flags & UCP_EP_INIT_FLAG_MEM_TYPE)) {
@@ -955,13 +1115,14 @@ static ucs_status_t ucp_wireup_add_rma_bw_lanes(ucp_ep_h ep,
         bw_info.criteria.remote_md_flags = bw_info.criteria.local_md_flags = UCT_MD_FLAG_REG;
     }
 
-    bw_info.criteria.title              = "high-bw remote %s memory access";
+    bw_info.criteria.title              = "high-bw remote memory access";
     bw_info.criteria.remote_iface_flags = UCT_IFACE_FLAG_GET_ZCOPY |
                                           UCT_IFACE_FLAG_PUT_ZCOPY;
     bw_info.criteria.local_iface_flags  = bw_info.criteria.remote_iface_flags |
                                           UCT_IFACE_FLAG_PENDING;
     bw_info.criteria.calc_score         = ucp_wireup_rma_bw_score_func;
     bw_info.criteria.tl_rsc_flags       = 0;
+    ucp_wireup_clean_amo_criteria(&bw_info.criteria);
     ucp_wireup_fill_ep_params_criteria(&bw_info.criteria, params);
 
     if (ucs_test_all_flags(ucp_ep_get_context_features(ep),
@@ -975,8 +1136,17 @@ static ucs_status_t ucp_wireup_add_rma_bw_lanes(ucp_ep_h ep,
     bw_info.max_lanes         = ep->worker->context->config.ext.max_rndv_lanes;
     bw_info.usage             = UCP_WIREUP_LANE_USAGE_RMA_BW;
 
-    return ucp_wireup_add_bw_lanes(ep, address_count, address_list, &bw_info, 0,
-                                   lane_descs, num_lanes_p);
+    for (mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
+        if (!ep->worker->context->mem_type_tls[mem_type]) {
+            continue;
+        }
+
+        ucp_wireup_add_bw_lanes(ep, address_count, address_list, &bw_info, 0,
+                                ep->worker->context->mem_type_tls[mem_type],
+                                lane_descs, num_lanes_p);
+    }
+
+    return UCS_OK;
 }
 
 /* Lane for transport offloaded tag interface */
@@ -984,9 +1154,10 @@ static ucs_status_t ucp_wireup_add_tag_lane(ucp_ep_h ep, unsigned address_count,
                                             const ucp_address_entry_t *address_list,
                                             ucp_wireup_lane_desc_t *lane_descs,
                                             ucp_lane_index_t *num_lanes_p,
+                                            double am_score,
                                             ucp_err_handling_mode_t err_mode)
 {
-    ucp_wireup_criteria_t criteria;
+    ucp_wireup_criteria_t criteria = {0};
     ucp_rsc_index_t rsc_index;
     ucs_status_t status;
     unsigned addr_index;
@@ -1010,16 +1181,17 @@ static ucs_status_t ucp_wireup_add_tag_lane(ucp_ep_h ep, unsigned address_count,
                                   UCT_IFACE_FLAG_GET_ZCOPY       |
                                   UCT_IFACE_FLAG_PENDING;
     criteria.calc_score         = ucp_wireup_am_score_func;
-    criteria.tl_rsc_flags       = 0;
 
     if (ucs_test_all_flags(ucp_ep_get_context_features(ep), UCP_FEATURE_WAKEUP)) {
         criteria.local_iface_flags |= UCP_WORKER_UCT_UNSIG_EVENT_CAP_FLAGS;
     }
 
+    /* Do not add tag offload lane, if selected tag lane score is lower
+     * than AM score. In this case AM will be used for tag macthing. */
     status = ucp_wireup_select_transport(ep, address_list, address_count, &criteria,
                                          -1, -1, -1, -1, 0, &rsc_index, &addr_index,
                                          &score);
-    if (status != UCS_OK) {
+    if ((status != UCS_OK) || (am_score > score)) {
         goto out;
     }
 
@@ -1046,10 +1218,10 @@ ucp_wireup_select_wireup_msg_lane(ucp_worker_h worker,
                                   const ucp_wireup_lane_desc_t *lane_descs,
                                   ucp_lane_index_t num_lanes)
 {
-    ucp_context_h context     = worker->context;
-    ucp_lane_index_t p2p_lane = UCP_NULL_LANE;
+    ucp_context_h context          = worker->context;
+    ucp_lane_index_t p2p_lane      = UCP_NULL_LANE;
+    ucp_wireup_criteria_t criteria = {0};
     uct_tl_resource_desc_t *resource;
-    ucp_wireup_criteria_t criteria;
     ucp_rsc_index_t rsc_index;
     ucp_lane_index_t lane;
     unsigned addr_index;
@@ -1108,32 +1280,40 @@ ucs_status_t ucp_wireup_select_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
 {
     ucp_worker_h worker   = ep->worker;
     ucp_context_h context = worker->context;
+    double am_score       = 0.0;
     ucp_wireup_lane_desc_t lane_descs[UCP_MAX_LANES];
     ucp_rsc_index_t rsc_index;
     ucp_md_index_t md_index;
     ucp_lane_index_t lane;
     ucp_lane_index_t i;
     ucs_status_t status;
+    int need_am = 0;
 
     memset(lane_descs, 0, sizeof(lane_descs));
     ucp_ep_config_key_reset(key);
     ucp_ep_config_key_set_params(key, params);
 
     status = ucp_wireup_add_rma_lanes(ep, params, ep_init_flags, address_count,
-                                      address_list, lane_descs, &key->num_lanes);
+                                      address_list, lane_descs, &key->num_lanes,
+                                      &need_am);
     if (status != UCS_OK) {
         return status;
     }
 
     status = ucp_wireup_add_amo_lanes(ep, params, ep_init_flags, address_count,
-                                      address_list, lane_descs, &key->num_lanes);
+                                      address_list, lane_descs, &key->num_lanes,
+                                      &need_am);
     if (status != UCS_OK) {
         return status;
     }
 
+    if (need_am) {
+        ep_init_flags |= UCP_EP_CREATE_AM_LANE;
+    }
+
     status = ucp_wireup_add_am_lane(ep, params, ep_init_flags, address_count,
                                     address_list, lane_descs, &key->num_lanes,
-                                    key->err_mode);
+                                    &am_score, key->err_mode);
     if (status != UCS_OK) {
         return status;
     }
@@ -1145,7 +1325,7 @@ ucs_status_t ucp_wireup_select_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
     }
 
     status = ucp_wireup_add_tag_lane(ep, address_count, address_list,
-                                     lane_descs, &key->num_lanes,
+                                     lane_descs, &key->num_lanes, am_score,
                                      key->err_mode);
     if (status != UCS_OK) {
         return status;
@@ -1224,7 +1404,7 @@ ucs_status_t ucp_wireup_select_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
     /* add to map first UCP_MAX_OP_MDS fastest MD's */
     for (i = 0;
          (key->rma_bw_lanes[i] != UCP_NULL_LANE) &&
-         (ucs_count_one_bits(key->rma_bw_md_map) < UCP_MAX_OP_MDS); i++) {
+         (ucs_popcount(key->rma_bw_md_map) < UCP_MAX_OP_MDS); i++) {
         lane = key->rma_bw_lanes[i];
         rsc_index = lane_descs[lane].rsc_index;
         md_index  = worker->context->tl_rscs[rsc_index].md_index;
@@ -1261,7 +1441,7 @@ ucs_status_t ucp_wireup_select_aux_transport(ucp_ep_h ep,
                                              ucp_rsc_index_t *rsc_index_p,
                                              unsigned *addr_index_p)
 {
-    ucp_wireup_criteria_t criteria;
+    ucp_wireup_criteria_t criteria = {0};
     double score;
 
     ucp_wireup_fill_aux_criteria(&criteria, params);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.c
index 53b3f0fb5..a7d75e65d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.c
@@ -12,6 +12,7 @@
 #include <ucp/core/ucp_request.inl>
 #include <ucp/core/ucp_proxy_ep.h>
 #include <ucp/core/ucp_worker.h>
+#include <ucp/core/ucp_listener.h>
 #include <ucp/tag/eager.h>
 #include <ucs/arch/bitops.h>
 #include <ucs/async/async.h>
@@ -35,6 +36,7 @@ ucs_status_t ucp_wireup_msg_progress(uct_pending_req_t *self)
     ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
     ucp_ep_h ep = req->send.ep;
     ssize_t packed_len;
+    unsigned am_flags;
 
     if (req->send.wireup.type == UCP_WIREUP_MSG_REQUEST) {
         if (ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED) {
@@ -42,6 +44,8 @@ ucs_status_t ucp_wireup_msg_progress(uct_pending_req_t *self)
                       ep);
             goto out;
         }
+    } else if (req->send.wireup.type == UCP_WIREUP_MSG_PRE_REQUEST) {
+        ucs_assert (!(ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED));
     }
 
     /* send the active message */
@@ -50,8 +54,18 @@ ucs_status_t ucp_wireup_msg_progress(uct_pending_req_t *self)
     } else {
         req->send.lane = ucp_ep_get_wireup_msg_lane(ep);
     }
+
+    am_flags = 0;
+    if ((req->send.wireup.type == UCP_WIREUP_MSG_REQUEST) ||
+        (req->send.wireup.type == UCP_WIREUP_MSG_PRE_REQUEST)) {
+        am_flags |= UCT_SEND_FLAG_SIGNALED;
+    }
+
+    VALGRIND_CHECK_MEM_IS_DEFINED(&req->send.wireup, sizeof(req->send.wireup));
+    VALGRIND_CHECK_MEM_IS_DEFINED(req->send.buffer, req->send.length);
+
     packed_len = uct_ep_am_bcopy(ep->uct_eps[req->send.lane], UCP_AM_ID_WIREUP,
-                                 ucp_wireup_msg_pack, req, 0);
+                                 ucp_wireup_msg_pack, req, am_flags);
     if (packed_len < 0) {
         if (packed_len != UCS_ERR_NO_RESOURCE) {
             ucs_error("failed to send wireup: %s", ucs_status_string(packed_len));
@@ -60,6 +74,9 @@ ucs_status_t ucp_wireup_msg_progress(uct_pending_req_t *self)
     }
 
     switch (req->send.wireup.type) {
+    case UCP_WIREUP_MSG_PRE_REQUEST:
+        ep->flags |= UCP_EP_FLAG_CONNECT_PRE_REQ_SENT;
+        break;
     case UCP_WIREUP_MSG_REQUEST:
         ep->flags |= UCP_EP_FLAG_CONNECT_REQ_SENT;
         break;
@@ -81,22 +98,27 @@ static unsigned ucp_wireup_address_index(const unsigned *order,
                                          uint64_t tl_bitmap,
                                          ucp_rsc_index_t tl_index)
 {
-    return order[ucs_count_one_bits(tl_bitmap & UCS_MASK(tl_index))];
+    return order[ucs_popcount(tl_bitmap & UCS_MASK(tl_index))];
+}
+
+static inline int ucp_wireup_is_ep_needed(ucp_ep_h ep)
+{
+    return (ep != NULL) && !(ep->flags & UCP_EP_FLAG_LISTENER);
 }
 
 /*
  * @param [in] rsc_tli  Resource index for every lane.
  */
 static ucs_status_t ucp_wireup_msg_send(ucp_ep_h ep, uint8_t type,
-                                        uint64_t ep_uuid, uint64_t tl_bitmap,
+                                        uint64_t tl_bitmap,
                                         const ucp_rsc_index_t *rsc_tli)
 {
     ucp_rsc_index_t rsc_index;
     ucp_lane_index_t lane;
-    unsigned order[UCP_MAX_LANES + 1];
     ucp_request_t* req;
     ucs_status_t status;
     void *address;
+    unsigned *order = ucs_alloca(ep->worker->context->num_tls * sizeof(*order));
 
     ucs_assert(ep->cfg_index != (uint8_t)-1);
 
@@ -112,14 +134,21 @@ static ucs_status_t ucp_wireup_msg_send(ucp_ep_h ep, uint8_t type,
     req->send.ep                 = ep;
     req->send.wireup.type        = type;
     req->send.wireup.err_mode    = ucp_ep_config(ep)->key.err_mode;
-    req->send.wireup.ep_uuid     = ep_uuid;
+    req->send.wireup.conn_sn     = ep->conn_sn;
+    req->send.wireup.src_ep_ptr  = (uintptr_t)ep;
+    if (ep->flags & UCP_EP_FLAG_DEST_EP) {
+        req->send.wireup.dest_ep_ptr = ucp_ep_dest_ep_ptr(ep);
+    } else {
+        req->send.wireup.dest_ep_ptr = 0;
+    }
+
     req->send.uct.func           = ucp_wireup_msg_progress;
     req->send.datatype           = ucp_dt_make_contig(1);
     ucp_request_send_state_init(req, ucp_dt_make_contig(1), 0);
 
     /* pack all addresses */
-    status = ucp_address_pack(ep->worker, ep, tl_bitmap, order,
-                              &req->send.length, &address);
+    status = ucp_address_pack(ep->worker, ucp_wireup_is_ep_needed(ep) ? ep : NULL,
+                              tl_bitmap, order, &req->send.length, &address);
     if (status != UCS_OK) {
         ucs_free(req);
         ucs_error("failed to pack address: %s", ucs_status_string(status));
@@ -175,80 +204,204 @@ static void ucp_wireup_remote_connected(ucp_ep_h ep)
 {
     ucp_lane_index_t lane;
 
+    if (ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED) {
+        return;
+    }
+
     ucs_trace("ep %p: remote connected", ep);
+    ep->flags |= UCP_EP_FLAG_REMOTE_CONNECTED;
+
     for (lane = 0; lane < ucp_ep_num_lanes(ep); ++lane) {
         if (ucp_ep_is_lane_p2p(ep, lane)) {
+            ucs_assert(ucp_wireup_ep_test(ep->uct_eps[lane]));
+        }
+        if (ucp_wireup_ep_test(ep->uct_eps[lane])) {
             ucp_wireup_ep_remote_connected(ep->uct_eps[lane]);
         }
     }
+
+    ucs_assert(ep->flags & UCP_EP_FLAG_DEST_EP);
 }
 
-static void ucp_wireup_process_request(ucp_worker_h worker, const ucp_wireup_msg_t *msg,
-                                       uint64_t uuid, const char *peer_name,
-                                       unsigned address_count,
-                                       const ucp_address_entry_t *address_list)
+
+static ucs_status_t
+ucp_wireup_init_lanes_by_request(ucp_worker_h worker, ucp_ep_h ep,
+                                 const ucp_ep_params_t *params,
+                                 unsigned ep_init_flags, unsigned address_count,
+                                 const ucp_address_entry_t *address_list,
+                                 uint8_t *addr_indices)
 {
+    ucs_status_t status = ucp_wireup_init_lanes(ep, params, ep_init_flags,
+                                                address_count, address_list,
+                                                addr_indices);
+    if (status == UCS_OK) {
+        return UCS_OK;
+    }
+
+    ucp_worker_set_ep_failed(worker, ep, NULL, UCP_NULL_LANE, status);
+    return status;
+}
+
+
+static UCS_F_NOINLINE void
+ucp_wireup_process_pre_request(ucp_worker_h worker, const ucp_wireup_msg_t *msg,
+                               const ucp_unpacked_address_t *remote_address)
+{
+    uint8_t addr_indices[UCP_MAX_LANES];
+    ucp_ep_params_t params;
+    ucs_status_t status;
+    ucp_ep_h ep;
+
+    ucs_assert(msg->type == UCP_WIREUP_MSG_PRE_REQUEST);
+    ucs_assert(msg->dest_ep_ptr != 0);
+    ucs_trace("got wireup pre_request from 0x%"PRIx64" src_ep 0x%lx dst_ep 0x%lx conn_sn %d",
+              remote_address->uuid, msg->src_ep_ptr, msg->dest_ep_ptr, msg->conn_sn);
+
+    /* wireup pre_request for a specific ep */
+    ep = ucp_worker_get_ep_by_ptr(worker, msg->dest_ep_ptr);
+    ucs_assert(ep->flags & UCP_EP_FLAG_SOCKADDR_PARTIAL_ADDR);
+
+    ucp_ep_update_dest_ep_ptr(ep, msg->src_ep_ptr);
+    ucp_ep_flush_state_reset(ep);
+
+    params.field_mask = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
+    params.err_mode   = ucp_ep_config(ep)->key.err_mode;
+
+    /* initialize transport endpoints */
+    status = ucp_wireup_init_lanes_by_request(worker, ep, &params,
+                                              UCP_EP_CREATE_AM_LANE,
+                                              remote_address->address_count,
+                                              remote_address->address_list,
+                                              addr_indices);
+    if (status != UCS_OK) {
+        return;
+    }
+
+    status = ucp_wireup_send_request(ep);
+    if (status != UCS_OK) {
+        ucp_ep_cleanup_lanes(ep);
+    }
+}
+
+static UCS_F_NOINLINE void
+ucp_wireup_process_request(ucp_worker_h worker, const ucp_wireup_msg_t *msg,
+                           const ucp_unpacked_address_t *remote_address)
+{
+    uint64_t remote_uuid   = remote_address->uuid;
+    uint64_t tl_bitmap     = 0;
+    int send_reply         = 0;
+    unsigned ep_init_flags = 0;
     ucp_rsc_index_t rsc_tli[UCP_MAX_LANES];
     uint8_t addr_indices[UCP_MAX_LANES];
     ucp_lane_index_t lane, remote_lane;
     ucp_rsc_index_t rsc_index;
     ucp_ep_params_t params;
     ucs_status_t status;
-    uint64_t tl_bitmap = 0;
+    ucp_ep_flags_t listener_flag;
     ucp_ep_h ep;
 
-    ucs_trace("got wireup request from 0x%"PRIx64, uuid);
-
-    if (msg->ep_uuid == worker->uuid) {
-        /* Request for a new connection to the worker */
-        ep = ucp_worker_ep_find(worker, uuid);
+    ucs_assert(msg->type == UCP_WIREUP_MSG_REQUEST);
+    ucs_trace("got wireup request from 0x%"PRIx64" src_ep 0x%lx dst_ep 0x%lx conn_sn %d",
+              remote_address->uuid, msg->src_ep_ptr, msg->dest_ep_ptr, msg->conn_sn);
+
+    if (msg->dest_ep_ptr != 0) {
+        /* wireup request for a specific ep */
+        ep = ucp_worker_get_ep_by_ptr(worker, msg->dest_ep_ptr);
+        ucp_ep_update_dest_ep_ptr(ep, msg->src_ep_ptr);
+        if (!(ep->flags & UCP_EP_FLAG_LISTENER)) {
+            /* Reset flush state only if it's not a client-server wireup on
+             * server side with long address exchange when listener (united with
+             * flush state) should be valid until user's callback invoking */
+            ucp_ep_flush_state_reset(ep);
+        }
+        ep_init_flags |= UCP_EP_CREATE_AM_LANE;
+    } else {
+        ep = ucp_ep_match_retrieve_exp(&worker->ep_match_ctx, remote_uuid,
+                                       msg->conn_sn ^ (remote_uuid == worker->uuid));
         if (ep == NULL) {
             /* Create a new endpoint if does not exist */
-            status = ucp_ep_new(worker, uuid, peer_name, "remote-request", &ep);
+            status = ucp_ep_new(worker, remote_address->name, "remote-request",
+                                &ep);
             if (status != UCS_OK) {
                 return;
             }
-            ep->flags |= UCP_EP_FLAG_DEST_UUID_PEER;
+
+            /* add internal endpoint to hash */
+            ep->conn_sn = msg->conn_sn;
+            ucp_ep_match_insert_unexp(&worker->ep_match_ctx, remote_uuid, ep);
         } else {
-            ucs_assert(ep->flags & UCP_EP_FLAG_DEST_UUID_PEER);
+            ucp_ep_flush_state_reset(ep);
         }
-    } else {
-        /* Reply for a client-server connection (client side) */
-        ep = ucp_worker_ep_find(worker, msg->ep_uuid);
-        if (ep == NULL) {
-            ucs_trace("got connection request with invalid ep_uuid 0x%"PRIx64,
-                      msg->ep_uuid);
+
+        ucp_ep_update_dest_ep_ptr(ep, msg->src_ep_ptr);
+
+        /*
+         * If the current endpoint already sent a connection request, we have a
+         * "simultaneous connect" situation. In this case, only one of the endpoints
+         * (instead of both) should respect the connect request, otherwise they
+         * will end up being connected to "internal" endpoints on the remote side
+         * instead of each other. We use the uniqueness of worker uuid to decide
+         * which connect request should be ignored.
+         */
+        if ((ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED) && (remote_uuid > worker->uuid)) {
+            ucs_trace("ep %p: ignoring simultaneous connect request", ep);
+            ep->flags |= UCP_EP_FLAG_CONNECT_REQ_IGNORED;
             return;
         }
-
-        /* Reinsert to hash table with destination worker uuid */
-        ucs_assert(!(ep->flags & UCP_EP_FLAG_DEST_UUID_PEER));
-        ucp_ep_delete_from_hash(ep);
-        ep->dest_uuid = uuid;
-        ep->flags    |= UCP_EP_FLAG_DEST_UUID_PEER;
-        ucp_ep_add_to_hash(ep);
     }
 
     params.field_mask = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
     params.err_mode   = msg->err_mode;
 
+    if (ep->flags & UCP_EP_FLAG_LISTENER) {
+        /* If this is an ep on a listener (server) that received a partial
+         * worker address from the client, then the following lanes initialization
+         * will be done after an aux lane was already created on this ep.
+         * Therefore, remove the existing aux endpoint since will need to create
+         * new lanes now */
+        ucp_ep_cleanup_lanes(ep);
+    }
+
     /* Initialize lanes (possible destroy existing lanes) */
-    status = ucp_wireup_init_lanes(ep, &params, 0, address_count, address_list,
-                                   addr_indices);
+    status = ucp_wireup_init_lanes_by_request(worker, ep, &params, ep_init_flags,
+                                              remote_address->address_count,
+                                              remote_address->address_list,
+                                              addr_indices);
     if (status != UCS_OK) {
         return;
     }
 
+    /* Send a reply if remote side does not have ep_ptr (active-active flow) or
+     * there are p2p lanes (client-server flow)
+     */
+    send_reply = (msg->dest_ep_ptr == 0) || ucp_ep_config(ep)->p2p_lanes;
+
     /* Connect p2p addresses to remote endpoint */
     if (!(ep->flags & UCP_EP_FLAG_LOCAL_CONNECTED)) {
-        status = ucp_wireup_connect_local(ep, addr_indices, address_count,
-                                          address_list);
+        status = ucp_wireup_connect_local(ep, addr_indices,
+                                          remote_address->address_count,
+                                          remote_address->address_list);
         if (status != UCS_OK) {
             return;
         }
 
         ep->flags |= UCP_EP_FLAG_LOCAL_CONNECTED;
 
+        ucs_assert(send_reply);
+    }
+
+    /* mark the endpoint as connected to remote */
+    if (!ucp_ep_config(ep)->p2p_lanes) {
+        ucp_wireup_remote_connected(ep);
+    }
+
+    if (send_reply) {
+
+        listener_flag = ep->flags & UCP_EP_FLAG_LISTENER;
+        /* Remove this flag at this point if it's set
+         * (so that address packing would be correct) */
+        ep->flags &= ~UCP_EP_FLAG_LISTENER;
+
         /* Construct the list that tells the remote side with which address we
          * have connected to each of its lanes.
          */
@@ -266,35 +419,48 @@ static void ucp_wireup_process_request(ucp_worker_h worker, const ucp_wireup_msg
         }
 
         ucs_trace("ep %p: sending wireup reply", ep);
-        status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_REPLY, ep->dest_uuid,
-                                     tl_bitmap, rsc_tli);
+        status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_REPLY, tl_bitmap, rsc_tli);
         if (status != UCS_OK) {
             return;
         }
+
+        /* Restore saved flag value */
+        ep->flags |= listener_flag;
+    } else {
+        /* if in client-server flow, schedule invoking the user's callback
+         * (if server is connected) from the main thread */
+        if (ucs_test_all_flags(ep->flags,
+                               (UCP_EP_FLAG_LISTENER | UCP_EP_FLAG_LOCAL_CONNECTED))) {
+            ucp_listener_schedule_accept_cb(ep);
+        }
     }
 }
 
-static void ucp_wireup_process_reply(ucp_worker_h worker, ucp_wireup_msg_t *msg,
-                                     uint64_t uuid, unsigned address_count,
-                                     const ucp_address_entry_t *address_list)
+static UCS_F_NOINLINE void
+ucp_wireup_process_reply(ucp_worker_h worker, const ucp_wireup_msg_t *msg,
+                         const ucp_unpacked_address_t *remote_address)
 {
-    ucp_ep_h ep = ucp_worker_ep_find(worker, uuid);
     ucp_rsc_index_t rsc_tli[UCP_MAX_LANES];
     ucs_status_t status;
+    ucp_ep_h ep;
     int ack;
 
-    if (ep == NULL) {
-        ucs_debug("ignoring connection reply - not exists");
-        return;
-    }
+    ep = ucp_worker_get_ep_by_ptr(worker, msg->dest_ep_ptr);
 
-    ucs_trace("ep %p: got wireup reply", ep);
+    ucs_assert(msg->type == UCP_WIREUP_MSG_REPLY);
+    ucs_assert((!(ep->flags & UCP_EP_FLAG_LISTENER)));
+    ucs_trace("ep %p: got wireup reply src_ep 0x%lx dst_ep 0x%lx sn %d", ep,
+              msg->src_ep_ptr, msg->dest_ep_ptr, msg->conn_sn);
 
-    ucs_assert(ep->flags & UCP_EP_FLAG_DEST_UUID_PEER);
+    ucp_ep_match_remove_ep(&worker->ep_match_ctx, ep);
+    ucp_ep_update_dest_ep_ptr(ep, msg->src_ep_ptr);
+    ucp_ep_flush_state_reset(ep);
 
     /* Connect p2p addresses to remote endpoint */
     if (!(ep->flags & UCP_EP_FLAG_LOCAL_CONNECTED)) {
-        status = ucp_wireup_connect_local(ep, msg->tli, address_count, address_list);
+        status = ucp_wireup_connect_local(ep, msg->tli,
+                                          remote_address->address_count,
+                                          remote_address->address_list);
         if (status != UCS_OK) {
             return;
         }
@@ -305,40 +471,41 @@ static void ucp_wireup_process_reply(ucp_worker_h worker, ucp_wireup_msg_t *msg,
         ack = 0;
     }
 
-    if (!(ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED)) {
-        ucp_wireup_remote_connected(ep);
-        ep->flags |= UCP_EP_FLAG_REMOTE_CONNECTED;
-    }
+    ucp_wireup_remote_connected(ep);
 
     if (ack) {
         /* Send ACK without any address, we've already sent it as part of the request */
         ucs_trace("ep %p: sending wireup ack", ep);
         memset(rsc_tli, -1, sizeof(rsc_tli));
-        status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_ACK, ep->dest_uuid, 0,
-                                     rsc_tli);
+        status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_ACK, 0, rsc_tli);
         if (status != UCS_OK) {
             return;
         }
     }
 }
 
-static void ucp_wireup_process_ack(ucp_worker_h worker, uint64_t uuid)
+static UCS_F_NOINLINE
+void ucp_wireup_process_ack(ucp_worker_h worker, const ucp_wireup_msg_t *msg)
 {
-    ucp_ep_h ep = ucp_worker_ep_find(worker, uuid);
+    ucp_ep_h ep;
 
-    if (ep == NULL) {
-        ucs_debug("ignoring connection ack - ep not exists");
-        return;
-    }
+    ep = ucp_worker_get_ep_by_ptr(worker, msg->dest_ep_ptr);
 
+    ucs_assert(msg->type == UCP_WIREUP_MSG_ACK);
     ucs_trace("ep %p: got wireup ack", ep);
 
-    ucs_assert(ep->flags & UCP_EP_FLAG_DEST_UUID_PEER);
+    ucs_assert(ep->flags & UCP_EP_FLAG_DEST_EP);
     ucs_assert(ep->flags & UCP_EP_FLAG_CONNECT_REP_SENT);
     ucs_assert(ep->flags & UCP_EP_FLAG_LOCAL_CONNECTED);
 
-    ep->flags |= UCP_EP_FLAG_REMOTE_CONNECTED;
     ucp_wireup_remote_connected(ep);
+
+    /* if this ack is received as part of the client-server flow, when handling
+     * a large worker address from the client, invoke the cached user callback
+     * from the main thread */
+    if (ep->flags & UCP_EP_FLAG_LISTENER) {
+        ucp_listener_schedule_accept_cb(ep);
+    }
 }
 
 static ucs_status_t ucp_wireup_msg_handler(void *arg, void *data,
@@ -346,34 +513,31 @@ static ucs_status_t ucp_wireup_msg_handler(void *arg, void *data,
 {
     ucp_worker_h worker   = arg;
     ucp_wireup_msg_t *msg = data;
-    char peer_name[UCP_WORKER_NAME_MAX];
-    ucp_address_entry_t *address_list;
-    unsigned address_count;
+    ucp_unpacked_address_t remote_address;
     ucs_status_t status;
-    uint64_t uuid;
 
     UCS_ASYNC_BLOCK(&worker->async);
 
-    status = ucp_address_unpack(msg + 1, &uuid, peer_name, UCP_WORKER_NAME_MAX,
-                                &address_count, &address_list);
+    status = ucp_address_unpack(msg + 1, &remote_address);
     if (status != UCS_OK) {
         ucs_error("failed to unpack address: %s", ucs_status_string(status));
         goto out;
     }
 
     if (msg->type == UCP_WIREUP_MSG_ACK) {
-        ucs_assert(address_count == 0);
-        ucp_wireup_process_ack(worker, uuid);
+        ucs_assert(remote_address.address_count == 0);
+        ucp_wireup_process_ack(worker, msg);
+    } else if (msg->type == UCP_WIREUP_MSG_PRE_REQUEST) {
+        ucp_wireup_process_pre_request(worker, msg, &remote_address);
     } else if (msg->type == UCP_WIREUP_MSG_REQUEST) {
-        ucp_wireup_process_request(worker, msg, uuid, peer_name, address_count,
-                                   address_list);
+        ucp_wireup_process_request(worker, msg, &remote_address);
     } else if (msg->type == UCP_WIREUP_MSG_REPLY) {
-        ucp_wireup_process_reply(worker, msg, uuid, address_count, address_list);
+        ucp_wireup_process_reply(worker, msg, &remote_address);
     } else {
         ucs_bug("invalid wireup message");
     }
 
-    ucs_free(address_list);
+    ucs_free(remote_address.address_list);
 
 out:
     UCS_ASYNC_UNBLOCK(&worker->async);
@@ -590,7 +754,7 @@ ucs_status_t ucp_wireup_init_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
     status = ucp_wireup_select_lanes(ep, params, ep_init_flags, address_count,
                                      address_list, addr_indices, &key);
     if (status != UCS_OK) {
-        goto err;
+        return status;
     }
 
     key.reachable_md_map |= ucp_ep_config(ep)->key.reachable_md_map;
@@ -600,7 +764,7 @@ ucs_status_t ucp_wireup_init_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
         return UCS_OK; /* No change */
     }
 
-    if ((ep->cfg_index != 0) && !ucp_ep_is_stub(ep)) {
+    if ((ep->cfg_index != 0) && !ucp_ep_is_sockaddr_stub(ep)) {
         /*
          * TODO handle a case where we have to change lanes and reconfigure the ep:
          *
@@ -631,13 +795,13 @@ ucs_status_t ucp_wireup_init_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
         status = ucp_wireup_connect_lane(ep, params, lane, address_count,
                                          address_list, addr_indices[lane]);
         if (status != UCS_OK) {
-            goto err;
+            return status;
         }
     }
 
     status = ucp_wireup_resolve_proxy_lanes(ep);
     if (status != UCS_OK) {
-        goto err;
+        return status;
     }
 
     /* If we don't have a p2p transport, we're connected */
@@ -645,24 +809,10 @@ ucs_status_t ucp_wireup_init_lanes(ucp_ep_h ep, const ucp_ep_params_t *params,
         ep->flags |= UCP_EP_FLAG_LOCAL_CONNECTED;
     }
 
-    /* Cache tag offload state in the flags for fast-path */
-    if (ucp_ep_is_tag_offload_enabled(ucp_ep_config(ep))) {
-        ep->flags |= UCP_EP_FLAG_TAG_OFFLOAD_ENABLED;
-    }
-
     return UCS_OK;
-
-err:
-    for (lane = 0; lane < ucp_ep_num_lanes(ep); ++lane) {
-        if (ep->uct_eps[lane] != NULL) {
-            uct_ep_destroy(ep->uct_eps[lane]);
-            ep->uct_eps[lane] = NULL;
-        }
-    }
-    return status;
 }
 
-ucs_status_t ucp_wireup_send_request(ucp_ep_h ep, uint64_t ep_uuid)
+ucs_status_t ucp_wireup_send_request(ucp_ep_h ep)
 {
     ucp_worker_h worker = ep->worker;
     ucp_rsc_index_t rsc_tli[UCP_MAX_LANES];
@@ -671,12 +821,6 @@ ucs_status_t ucp_wireup_send_request(ucp_ep_h ep, uint64_t ep_uuid)
     ucp_lane_index_t lane;
     ucs_status_t status;
 
-    if (ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED) {
-        return UCS_OK;
-    }
-
-    ucs_assert_always(!ucp_ep_is_stub(ep));
-
     for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
         if (lane < ucp_ep_num_lanes(ep)) {
             rsc_index = ucp_ep_get_rsc_index(ep, lane);
@@ -696,9 +840,112 @@ ucs_status_t ucp_wireup_send_request(ucp_ep_h ep, uint64_t ep_uuid)
     }
 
     ucs_debug("ep %p: send wireup request (flags=0x%x)", ep, ep->flags);
-    status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_REQUEST, ep_uuid, tl_bitmap,
-                                 rsc_tli);
+    status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_REQUEST, tl_bitmap, rsc_tli);
+
     ep->flags |= UCP_EP_FLAG_CONNECT_REQ_QUEUED;
+
+    return status;
+}
+
+static void ucp_wireup_connect_remote_purge_cb(uct_pending_req_t *self, void *arg)
+{
+    ucp_request_t *req = ucs_container_of(self, ucp_request_t, send.uct);
+    ucs_queue_head_t *queue = arg;
+
+    ucs_trace_req("ep %p: extracted request %p from pending queue", req->send.ep,
+                  req);
+    ucs_queue_push(queue, (ucs_queue_elem_t*)&req->send.uct.priv);
+}
+
+ucs_status_t ucp_wireup_send_pre_request(ucp_ep_h ep)
+{
+    ucp_rsc_index_t rsc_tli[UCP_MAX_LANES];
+    uint64_t tl_bitmap = -1;  /* pack full worker address */
+    ucs_status_t status;
+
+    ucs_assert(ep->flags & UCP_EP_FLAG_LISTENER);
+    ucs_assert(!(ep->flags & UCP_EP_FLAG_CONNECT_PRE_REQ_QUEUED));
+    memset(rsc_tli, UCP_NULL_RESOURCE, sizeof(rsc_tli));
+
+    ucs_debug("ep %p: send wireup pre-request (flags=0x%x)", ep, ep->flags);
+    status = ucp_wireup_msg_send(ep, UCP_WIREUP_MSG_PRE_REQUEST, tl_bitmap, rsc_tli);
+
+    ep->flags |= UCP_EP_FLAG_CONNECT_PRE_REQ_QUEUED;
+    return status;
+}
+
+ucs_status_t ucp_wireup_connect_remote(ucp_ep_h ep, ucp_lane_index_t lane)
+{
+    ucs_queue_head_t tmp_q;
+    ucs_status_t status;
+    ucp_request_t *req;
+    uct_ep_h uct_ep;
+
+    ucs_trace("ep %p: connect lane %d to remote peer", ep, lane);
+
+    UCS_ASYNC_BLOCK(&ep->worker->async);
+
+    /* checking again, with lock held, if already connected or connection is
+     * in progress */
+    if ((ep->flags & UCP_EP_FLAG_DEST_EP) ||
+        ucp_wireup_ep_test(ep->uct_eps[lane])) {
+        status = UCS_OK;
+        goto out_unlock;
+    }
+
+    if (ucp_proxy_ep_test(ep->uct_eps[lane])) {
+        /* signaling ep is not needed now since we will send wireup request
+         * with signaling flag
+         */
+        uct_ep = ucp_proxy_ep_extract(ep->uct_eps[lane]);
+        uct_ep_destroy(ep->uct_eps[lane]);
+    } else {
+        uct_ep = ep->uct_eps[lane];
+    }
+
+    ucs_assert(!(ep->flags & UCP_EP_FLAG_REMOTE_CONNECTED));
+
+    ucs_trace("ep %p: connect lane %d to remote peer with wireup ep", ep, lane);
+
+    /* make ep->uct_eps[lane] a stub */
+    status = ucp_wireup_ep_create(ep, &ep->uct_eps[lane]);
+    if (status != UCS_OK) {
+        goto err;
+    }
+
+    /* Extract all pending requests from the transport endpoint, otherwise they
+     * will prevent the wireup message from being sent (because those requests
+     * could not be progressed any more after switching to wireup proxy).
+     */
+    ucs_queue_head_init(&tmp_q);
+    uct_ep_pending_purge(uct_ep, ucp_wireup_connect_remote_purge_cb, &tmp_q);
+
+    /* the wireup ep should use the existing [am_lane] as next_ep */
+    ucp_wireup_ep_set_next_ep(ep->uct_eps[lane], uct_ep);
+
+    if (!(ep->flags & UCP_EP_FLAG_CONNECT_REQ_QUEUED)) {
+        status = ucp_wireup_send_request(ep);
+        if (status != UCS_OK) {
+            goto err_destroy_wireup_ep;
+        }
+    }
+
+    ucs_queue_for_each_extract(req, &tmp_q, send.uct.priv, 1) {
+        ucs_trace_req("ep %p: requeue request %p after wireup request",
+                      req->send.ep, req);
+        status = uct_ep_pending_add(ep->uct_eps[lane], &req->send.uct, 0);
+        ucs_assert(status == UCS_OK); /* because it's a wireup proxy */
+    }
+
+    status = UCS_OK;
+    goto out_unlock;
+
+err_destroy_wireup_ep:
+    uct_ep_destroy(ep->uct_eps[lane]);
+err:
+    ep->uct_eps[lane] = uct_ep; /* restore am lane */
+out_unlock:
+    UCS_ASYNC_UNBLOCK(&ep->worker->async);
     return status;
 }
 
@@ -708,27 +955,37 @@ static void ucp_wireup_msg_dump(ucp_worker_h worker, uct_am_trace_type_t type,
 {
     ucp_context_h context       = worker->context;
     const ucp_wireup_msg_t *msg = data;
-    char peer_name[UCP_WORKER_NAME_MAX + 1];
-    ucp_address_entry_t *address_list, *ae;
+    ucp_unpacked_address_t unpacked_address;
+    const ucp_address_entry_t *ae;
     ucp_tl_resource_desc_t *rsc;
-    unsigned address_count;
     ucp_lane_index_t lane;
-    uint64_t uuid;
+    unsigned addr_index;
+    ucs_status_t status;
     char *p, *end;
 
-    ucp_address_unpack(msg + 1, &uuid, peer_name, sizeof(peer_name),
-                       &address_count, &address_list);
+    status = ucp_address_unpack(msg + 1, &unpacked_address);
+    if (status != UCS_OK) {
+        strncpy(unpacked_address.name, "<malformed address>", UCP_WORKER_NAME_MAX);
+        unpacked_address.uuid          = 0;
+        unpacked_address.address_count = 0;
+        unpacked_address.address_list  = NULL;
+    }
 
     p   = buffer;
     end = buffer + max;
-    snprintf(p, end - p, "WIREUP %s [%s uuid 0x%"PRIx64" ep_uuid 0x%"PRIx64"]",
-             (msg->type == UCP_WIREUP_MSG_REQUEST ) ? "REQ" :
-             (msg->type == UCP_WIREUP_MSG_REPLY   ) ? "REP" :
-             (msg->type == UCP_WIREUP_MSG_ACK     ) ? "ACK" : "",
-             peer_name, uuid, msg->ep_uuid);
 
+    snprintf(p, end - p,
+             "WIREUP %s [%s uuid 0x%"PRIx64" src_ep 0x%lx dst_ep 0x%lx conn_sn %d]",
+             (msg->type == UCP_WIREUP_MSG_PRE_REQUEST ) ? "PRE_REQ" :
+             (msg->type == UCP_WIREUP_MSG_REQUEST     ) ? "REQ" :
+             (msg->type == UCP_WIREUP_MSG_REPLY       ) ? "REP" :
+             (msg->type == UCP_WIREUP_MSG_ACK         ) ? "ACK" : "",
+             unpacked_address.name, unpacked_address.uuid, msg->src_ep_ptr,
+             msg->dest_ep_ptr, msg->conn_sn);
     p += strlen(p);
-    for (ae = address_list; ae < address_list + address_count; ++ae) {
+
+    for (addr_index = 0; addr_index < unpacked_address.address_count; ++addr_index) {
+        ae = &unpacked_address.address_list[addr_index];
         for (rsc = context->tl_rscs; rsc < context->tl_rscs + context->num_tls; ++rsc) {
             if (ae->tl_name_csum == rsc->tl_name_csum) {
                 snprintf(p, end - p, " "UCT_TL_RESOURCE_DESC_FMT,
@@ -741,14 +998,14 @@ static void ucp_wireup_msg_dump(ucp_worker_h worker, uct_am_trace_type_t type,
         p += strlen(p);
 
         for (lane = 0; lane < UCP_MAX_LANES; ++lane) {
-            if (msg->tli[lane] == (ae - address_list)) {
+            if (msg->tli[lane] == addr_index) {
                 snprintf(p, end - p, "/lane[%d]", lane);
                 p += strlen(p);
             }
         }
     }
 
-    ucs_free(address_list);
+    ucs_free(unpacked_address.address_list);
 }
 
 UCP_DEFINE_AM(-1, UCP_AM_ID_WIREUP, ucp_wireup_msg_handler, 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.h
index ae58ef10b..21bc6b490 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup.h
@@ -18,6 +18,7 @@
  * Wireup message types
  */
 enum {
+    UCP_WIREUP_MSG_PRE_REQUEST,
     UCP_WIREUP_MSG_REQUEST,
     UCP_WIREUP_MSG_REPLY,
     UCP_WIREUP_MSG_ACK,
@@ -51,6 +52,8 @@ typedef struct {
                               const ucp_address_iface_attr_t *remote_iface_attr);
     uint8_t     tl_rsc_flags; /* Flags that describe TL specifics */
 
+    ucp_tl_iface_atomic_flags_t local_atomic_flags;
+    ucp_tl_iface_atomic_flags_t remote_atomic_flags;
 } ucp_wireup_criteria_t;
 
 
@@ -60,7 +63,9 @@ typedef struct {
 typedef struct ucp_wireup_msg {
     uint8_t                 type;         /* Message type */
     ucp_err_handling_mode_t err_mode;     /* Peer error handling mode */
-    uint64_t                ep_uuid;      /* Peer endpoint dest_uuid */
+    ucp_ep_conn_sn_t        conn_sn;      /* Connection sequence number */
+    uintptr_t               src_ep_ptr;   /* Endpoint of source */
+    uintptr_t               dest_ep_ptr;  /* Endpoint of destination (0 - invalid) */
 
     /* REQUEST - which p2p lanes must be connected
      * REPLY - which p2p lanes have been connected
@@ -71,7 +76,11 @@ typedef struct ucp_wireup_msg {
 } UCS_S_PACKED ucp_wireup_msg_t;
 
 
-ucs_status_t ucp_wireup_send_request(ucp_ep_h ep, uint64_t ep_uuid);
+ucs_status_t ucp_wireup_send_request(ucp_ep_h ep);
+
+ucs_status_t ucp_wireup_send_pre_request(ucp_ep_h ep);
+
+ucs_status_t ucp_wireup_connect_remote(ucp_ep_h ep, ucp_lane_index_t lane);
 
 ucs_status_t ucp_wireup_select_aux_transport(ucp_ep_h ep,
                                              const ucp_ep_params_t *params,
@@ -113,5 +122,4 @@ static inline int ucp_worker_is_tl_p2p(ucp_worker_h worker, ucp_rsc_index_t rsc_
            !(flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE);
 }
 
-
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.c
index 9f2c3a075..3143edc43 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.c
@@ -56,7 +56,17 @@ static unsigned ucp_wireup_ep_progress(void *arg)
 
     /* If we still have pending wireup messages, send them out first */
     if (wireup_ep->pending_count != 0) {
-        goto out;
+        goto out_unblock;
+    }
+
+    /* If an error happened on the endpoint (but perhaps the deferred error handler,
+     * ucp_worker_iface_err_handle_progress(), was not called yet, avoid changing
+     * ep state, and let the error handler take care of cleanup.
+     */
+    if (ucp_ep->flags & UCP_EP_FLAG_FAILED) {
+        ucs_trace("ep %p: not switching wireup_ep %p to ready state because of error",
+                  ucp_ep, wireup_ep);
+        goto out_unblock;
     }
 
     ucs_trace("ep %p: switching wireup_ep %p to ready state", ucp_ep, wireup_ep);
@@ -73,15 +83,19 @@ static unsigned ucp_wireup_ep_progress(void *arg)
     ucp_proxy_ep_replace(&wireup_ep->super);
     wireup_ep = NULL;
 
+    UCS_ASYNC_UNBLOCK(&ucp_ep->worker->async);
+
     /* Replay pending requests */
     ucs_queue_for_each_extract(uct_req, &tmp_pending_queue, priv, 1) {
         req = ucs_container_of(uct_req, ucp_request_t, send.uct);
         ucs_assert(req->send.ep == ucp_ep);
         ucp_request_send(req);
-        --ucp_ep->worker->wireup_pend_count;
+        --ucp_ep->worker->flush_ops_count;
     }
 
-out:
+    return 0;
+
+out_unblock:
     UCS_ASYNC_UNBLOCK(&ucp_ep->worker->async);
     return 0;
 }
@@ -95,7 +109,7 @@ static uct_ep_h ucp_wireup_ep_get_msg_ep(ucp_wireup_ep_t *wireup_ep)
 {
     uct_ep_h wireup_msg_ep;
 
-    if (wireup_ep->flags & UCP_WIREUP_EP_FLAG_READY) {
+    if ((wireup_ep->flags & UCP_WIREUP_EP_FLAG_READY) || (wireup_ep->aux_ep == NULL)) {
         wireup_msg_ep = wireup_ep->super.uct_ep;
     } else {
         wireup_msg_ep = wireup_ep->aux_ep;
@@ -119,7 +133,7 @@ static ucs_status_t ucp_wireup_ep_progress_pending(uct_pending_req_t *self)
     status = req->func(req);
     if (status == UCS_OK) {
         ucs_atomic_add32(&wireup_ep->pending_count, -1);
-        ucp_request_put(proxy_req);
+        ucs_free(proxy_req);
     }
     return status;
 }
@@ -141,11 +155,12 @@ ucp_wireup_ep_pending_req_release(uct_pending_req_t *self, void *arg)
         ucs_free(req);
     }
 
-    ucp_request_put(proxy_req);
+    ucs_free(proxy_req);
 }
 
 static ucs_status_t ucp_wireup_ep_pending_add(uct_ep_h uct_ep,
-                                              uct_pending_req_t *req)
+                                              uct_pending_req_t *req,
+                                              unsigned flags)
 {
     ucp_wireup_ep_t *wireup_ep = ucs_derived_of(uct_ep, ucp_wireup_ep_t);
     ucp_ep_h ucp_ep = wireup_ep->super.ucp_ep;
@@ -156,7 +171,7 @@ static ucs_status_t ucp_wireup_ep_pending_add(uct_ep_h uct_ep,
 
     UCS_ASYNC_BLOCK(&worker->async);
     if (req->func == ucp_wireup_msg_progress) {
-        proxy_req = ucs_mpool_get(&worker->req_mp);
+        proxy_req = ucs_malloc(sizeof(*proxy_req), "ucp_wireup_proxy_req");
         if (proxy_req == NULL) {
             status = UCS_ERR_NO_MEMORY;
             goto out;
@@ -169,15 +184,15 @@ static ucs_status_t ucp_wireup_ep_pending_add(uct_ep_h uct_ep,
         proxy_req->send.proxy.wireup_ep     = wireup_ep;
         proxy_req->send.state.uct_comp.func = NULL;
 
-        status = uct_ep_pending_add(wireup_msg_ep, &proxy_req->send.uct);
+        status = uct_ep_pending_add(wireup_msg_ep, &proxy_req->send.uct, 0);
         if (status == UCS_OK) {
             ucs_atomic_add32(&wireup_ep->pending_count, +1);
         } else {
-            ucp_request_put(proxy_req);
+            ucs_free(proxy_req);
         }
     } else {
         ucs_queue_push(&wireup_ep->pending_q, ucp_wireup_ep_req_priv(req));
-        ++ucp_ep->worker->wireup_pend_count;
+        ++ucp_ep->worker->flush_ops_count;
         status = UCS_OK;
     }
 out:
@@ -189,19 +204,27 @@ static void
 ucp_wireup_ep_pending_purge(uct_ep_h uct_ep, uct_pending_purge_callback_t cb,
                             void *arg)
 {
-    ucp_wireup_ep_t *wireup_ep = ucs_derived_of(uct_ep, ucp_wireup_ep_t);
+    ucp_wireup_ep_t   *wireup_ep = ucs_derived_of(uct_ep, ucp_wireup_ep_t);
+    ucp_worker_h      worker;
     uct_pending_req_t *req;
-    ucp_request_t *ucp_req;
+    ucp_request_t     *ucp_req;
+
+    worker = wireup_ep->super.ucp_ep->worker;
 
     ucs_queue_for_each_extract(req, &wireup_ep->pending_q, priv, 1) {
         ucp_req = ucs_container_of(req, ucp_request_t, send.uct);
+        UCS_ASYNC_BLOCK(&worker->async);
+        --worker->flush_ops_count;
+        UCS_ASYNC_UNBLOCK(&worker->async);
         cb(&ucp_req->send.uct, arg);
     }
 
-    if (wireup_ep->aux_ep != NULL) {
-        uct_ep_pending_purge(wireup_ep->aux_ep, ucp_wireup_ep_pending_req_release,
-                             arg);
+    if (wireup_ep->pending_count > 0) {
+        uct_ep_pending_purge(ucp_wireup_ep_get_msg_ep(wireup_ep),
+                             ucp_wireup_ep_pending_req_release, arg);
     }
+
+    ucs_assert(wireup_ep->pending_count == 0);
 }
 
 static ssize_t ucp_wireup_ep_am_bcopy(uct_ep_h uct_ep, uint8_t id,
@@ -222,7 +245,7 @@ static ssize_t ucp_wireup_ep_am_bcopy(uct_ep_h uct_ep, uint8_t id,
 UCS_CLASS_DEFINE_NAMED_NEW_FUNC(ucp_wireup_ep_create, ucp_wireup_ep_t, uct_ep_t,
                                 ucp_ep_h);
 
-static ucs_status_t
+ucs_status_t
 ucp_wireup_ep_connect_aux(ucp_wireup_ep_t *wireup_ep,
                           const ucp_ep_params_t *params, unsigned address_count,
                           const ucp_address_entry_t *address_list)
@@ -290,6 +313,7 @@ UCS_CLASS_INIT_FUNC(ucp_wireup_ep_t, ucp_ep_h ucp_ep)
         .ep_put_short         = (void*)ucs_empty_function_return_no_resource,
         .ep_put_bcopy         = (void*)ucp_wireup_ep_bcopy_send_func,
         .ep_put_zcopy         = (void*)ucs_empty_function_return_no_resource,
+        .ep_get_short         = (void*)ucs_empty_function_return_no_resource,
         .ep_get_bcopy         = (void*)ucs_empty_function_return_no_resource,
         .ep_get_zcopy         = (void*)ucs_empty_function_return_no_resource,
         .ep_am_short          = (void*)ucs_empty_function_return_no_resource,
@@ -300,26 +324,29 @@ UCS_CLASS_INIT_FUNC(ucp_wireup_ep_t, ucp_ep_h ucp_ep)
         .ep_tag_eager_zcopy   = (void*)ucs_empty_function_return_no_resource,
         .ep_tag_rndv_zcopy    = (void*)ucs_empty_function_return_ptr_no_resource,
         .ep_tag_rndv_request  = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_add64      = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_fadd64     = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_swap64     = (void*)ucs_empty_function_return_no_resource,
+        .ep_atomic64_post     = (void*)ucs_empty_function_return_no_resource,
+        .ep_atomic64_fetch    = (void*)ucs_empty_function_return_no_resource,
         .ep_atomic_cswap64    = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_add32      = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_fadd32     = (void*)ucs_empty_function_return_no_resource,
-        .ep_atomic_swap32     = (void*)ucs_empty_function_return_no_resource,
+        .ep_atomic32_post     = (void*)ucs_empty_function_return_no_resource,
+        .ep_atomic32_fetch    = (void*)ucs_empty_function_return_no_resource,
         .ep_atomic_cswap32    = (void*)ucs_empty_function_return_no_resource
     };
 
     UCS_CLASS_CALL_SUPER_INIT(ucp_proxy_ep_t, &ops, ucp_ep, NULL, 0);
 
-    self->aux_ep        = NULL;
-    self->sockaddr_ep   = NULL;
-    self->aux_rsc_index = UCP_NULL_RESOURCE;
-    self->pending_count = 0;
-    self->flags         = 0;
-    self->progress_id   = UCS_CALLBACKQ_ID_NULL;
+    self->aux_ep             = NULL;
+    self->sockaddr_ep        = NULL;
+    self->aux_rsc_index      = UCP_NULL_RESOURCE;
+    self->sockaddr_rsc_index = UCP_NULL_RESOURCE;
+    self->pending_count      = 0;
+    self->flags              = 0;
+    self->progress_id        = UCS_CALLBACKQ_ID_NULL;
     ucs_queue_head_init(&self->pending_q);
 
+    UCS_ASYNC_BLOCK(&ucp_ep->worker->async);
+    ++ucp_ep->worker->flush_ops_count;
+    UCS_ASYNC_UNBLOCK(&ucp_ep->worker->async);
+
     ucs_trace("ep %p: created wireup ep %p to %s ", ucp_ep, self,
               ucp_ep_peer_name(ucp_ep));
     return UCS_OK;
@@ -343,6 +370,10 @@ static UCS_CLASS_CLEANUP_FUNC(ucp_wireup_ep_t)
     if (self->sockaddr_ep != NULL) {
         uct_ep_destroy(self->sockaddr_ep);
     }
+
+    UCS_ASYNC_BLOCK(&worker->async);
+    --worker->flush_ops_count;
+    UCS_ASYNC_UNBLOCK(&worker->async);
 }
 
 UCS_CLASS_DEFINE(ucp_wireup_ep_t, ucp_proxy_ep_t);
@@ -355,7 +386,10 @@ ucp_rsc_index_t ucp_wireup_ep_get_aux_rsc_index(uct_ep_h uct_ep)
         return UCP_NULL_RESOURCE;
     }
 
-    ucs_assert(wireup_ep->aux_ep != NULL);
+    if (wireup_ep->aux_ep == NULL) {
+        return UCP_NULL_RESOURCE;
+    }
+
     return wireup_ep->aux_rsc_index;
 }
 
@@ -401,17 +435,137 @@ err:
     return status;
 }
 
+static ucs_status_t ucp_wireup_ep_pack_sockaddr_aux_tls(ucp_worker_h worker,
+                                                        const char *dev_name,
+                                                        uint64_t *tl_bitmap_p,
+                                                        ucp_address_t **address_p,
+                                                        size_t *address_length_p)
+{
+    ucp_context_h context = worker->context;
+    int tl_id, found_supported_tl = 0;
+    ucs_status_t status;
+    uint64_t tl_bitmap = 0;
+
+    /* Find a transport which matches the given dev_name and the user's configuration.
+     * It also has to be a UCT_IFACE_FLAG_CONNECT_TO_IFACE transport and support
+     * active messaging for sending a wireup message */
+    ucs_for_each_bit(tl_id, context->config.sockaddr_aux_rscs_bitmap) {
+        if ((!strncmp(context->tl_rscs[tl_id].tl_rsc.dev_name, dev_name,
+                      UCT_DEVICE_NAME_MAX)) &&
+            (ucs_test_all_flags(worker->ifaces[tl_id].attr.cap.flags,
+                                UCT_IFACE_FLAG_CONNECT_TO_IFACE |
+                                UCT_IFACE_FLAG_AM_BCOPY))) {
+            found_supported_tl = 1;
+            tl_bitmap |= UCS_BIT(tl_id);
+        }
+    }
+
+    if (found_supported_tl) {
+        status = ucp_address_pack(worker, NULL, tl_bitmap, NULL,
+                                  address_length_p, (void**)address_p);
+    } else {
+        ucs_error("no supported sockaddr auxiliary transports found for %s", dev_name);
+        status = UCS_ERR_UNREACHABLE;
+    }
+
+    *tl_bitmap_p = tl_bitmap;
+    return status;
+}
+
+ssize_t ucp_wireup_ep_sockaddr_fill_private_data(void *arg, const char *dev_name,
+                                                void *priv_data)
+{
+    ucp_wireup_client_data_t *client_data = priv_data;
+    ucp_wireup_ep_t *wireup_ep            = arg;
+    ucp_ep_h ucp_ep                       = wireup_ep->super.ucp_ep;
+    ucp_rsc_index_t sockaddr_rsc          = wireup_ep->sockaddr_rsc_index;
+    ucp_worker_h worker                   = ucp_ep->worker;
+    ucp_context_h context                 = worker->context;
+    size_t address_length, conn_priv_len;
+    ucp_address_t *worker_address, *rsc_address;
+    ucp_worker_iface_t *wiface;
+    ucs_status_t status;
+    uint64_t tl_bitmap;
+    char aux_tls_str[64];
+
+    status = ucp_address_pack(worker, NULL, -1, NULL, &address_length,
+                              (void**)&worker_address);
+    if (status != UCS_OK) {
+        goto err;
+    }
+
+    conn_priv_len = sizeof(*client_data) + address_length;
+
+    /* pack client data */
+    client_data->err_mode = ucp_ep_config(ucp_ep)->key.err_mode;
+    client_data->ep_ptr   = (uintptr_t)ucp_ep;
+
+    wiface = &worker->ifaces[sockaddr_rsc];
+
+    /* check private data length limitation */
+    if (conn_priv_len > wiface->attr.max_conn_priv) {
+
+        /* since the full worker address is too large to fit into the trasnport's
+         * private data, try to pack sockaddr aux tls to pass in the address */
+        status = ucp_wireup_ep_pack_sockaddr_aux_tls(worker, dev_name, &tl_bitmap,
+                                                     &rsc_address, &address_length);
+        if (status != UCS_OK) {
+            goto err_free_address;
+        }
+
+        conn_priv_len = sizeof(*client_data) + address_length;
+
+        /* check the private data length limitation again, now with partial
+         * resources packed (and not the entire worker address) */
+        if (conn_priv_len > wiface->attr.max_conn_priv) {
+            ucs_error("sockaddr aux resources addresses (%s transports)"
+                      " information (%zu) exceeds max_priv on "
+                      UCT_TL_RESOURCE_DESC_FMT" (%zu)",
+                      ucp_tl_bitmap_str(context, tl_bitmap, aux_tls_str,
+                                        sizeof(aux_tls_str)),
+                      conn_priv_len,
+                      UCT_TL_RESOURCE_DESC_ARG(&context->tl_rscs[sockaddr_rsc].tl_rsc),
+                      wiface->attr.max_conn_priv);
+            status = UCS_ERR_UNREACHABLE;
+            ucs_free(rsc_address);
+            goto err_free_address;
+        }
+
+        client_data->is_full_addr = 0;
+        memcpy(client_data + 1, rsc_address, address_length);
+        ucp_ep->flags |= UCP_EP_FLAG_SOCKADDR_PARTIAL_ADDR;
+
+        ucs_free(rsc_address);
+
+        ucs_trace("sockaddr tl ("UCT_TL_RESOURCE_DESC_FMT") sending partial address: "
+                  "(%s transports) (len=%zu) to server. "
+                  "total client priv data len: %zu",
+                  context->tl_rscs[sockaddr_rsc].tl_rsc.tl_name, dev_name,
+                  ucp_tl_bitmap_str(context, tl_bitmap, aux_tls_str,
+                                    sizeof(aux_tls_str)),
+                  address_length, conn_priv_len);
+
+    } else {
+        client_data->is_full_addr = 1;
+        memcpy(client_data + 1, worker_address, address_length);
+    }
+
+    ucp_worker_release_address(worker, worker_address);
+    return conn_priv_len;
+
+err_free_address:
+    ucp_worker_release_address(worker, worker_address);
+err:
+    return status;
+}
+
 ucs_status_t ucp_wireup_ep_connect_to_sockaddr(uct_ep_h uct_ep,
                                                const ucp_ep_params_t *params)
 {
     ucp_wireup_ep_t *wireup_ep = ucs_derived_of(uct_ep, ucp_wireup_ep_t);
     ucp_ep_h ucp_ep            = wireup_ep->super.ucp_ep;
     ucp_worker_h worker        = ucp_ep->worker;
-    ucp_context_h context      = worker->context;
     char saddr_str[UCS_SOCKADDR_STRING_LEN];
-    ucp_wireup_sockaddr_priv_t *conn_priv;
-    size_t address_length, conn_priv_len;
-    ucp_address_t *worker_address;
     ucp_rsc_index_t sockaddr_rsc;
     ucp_worker_iface_t *wiface;
     ucs_status_t status;
@@ -419,56 +573,27 @@ ucs_status_t ucp_wireup_ep_connect_to_sockaddr(uct_ep_h uct_ep,
     ucs_assert(ucp_wireup_ep_test(uct_ep));
 
     status = ucp_wireup_select_sockaddr_transport(ucp_ep, params, &sockaddr_rsc);
-    if (status != UCS_OK) {
-        goto out;
-     }
-
-    status = ucp_worker_get_address(worker, &worker_address, &address_length);
     if (status != UCS_OK) {
         goto out;
     }
 
-    conn_priv_len = sizeof(*conn_priv) + address_length;
-
-    /* check private data limitation */
     wiface = &worker->ifaces[sockaddr_rsc];
-    if (conn_priv_len > wiface->attr.max_conn_priv) {
-        ucs_error("sockaddr connection priv data (%zu) exceeds max_priv on "
-                  UCT_TL_RESOURCE_DESC_FMT" (%zu)", conn_priv_len,
-                  UCT_TL_RESOURCE_DESC_ARG(&context->tl_rscs[sockaddr_rsc].tl_rsc),
-                  wiface->attr.max_conn_priv);
-        status = UCS_ERR_UNREACHABLE;
-        goto out_free_address;
-    }
 
-    conn_priv = ucs_malloc(conn_priv_len ,"ucp_sockaddr_conn_priv");
-    if (conn_priv == NULL) {
-        ucs_error("failed to allocate buffer for sockaddr conn priv");
-        status = UCS_ERR_NO_MEMORY;
-        goto out_free_address;
-    }
-
-    /* pack private data */
-    conn_priv->err_mode = UCP_PARAM_VALUE(EP, params, err_mode, ERR_HANDLING_MODE,
-                                          UCP_ERR_HANDLING_MODE_NONE);
-    conn_priv->ep_uuid  = ucp_ep->dest_uuid;
-    memcpy(conn_priv + 1, worker_address, address_length);
+    wireup_ep->sockaddr_rsc_index = sockaddr_rsc;
 
     /* send connection request using the transport */
-    status = uct_ep_create_sockaddr(wiface->iface, &params->sockaddr, conn_priv,
-                                    conn_priv_len, &wireup_ep->sockaddr_ep);
+    status = uct_ep_create_sockaddr(wiface->iface, &params->sockaddr,
+                                    ucp_wireup_ep_sockaddr_fill_private_data,
+                                    wireup_ep, UCT_CB_FLAG_ASYNC,
+                                    &wireup_ep->sockaddr_ep);
     if (status != UCS_OK) {
-        goto out_free_priv;
+        goto out;
     }
 
     ucs_debug("ep %p connecting to %s", ucp_ep,
               ucs_sockaddr_str(params->sockaddr.addr, saddr_str, sizeof(saddr_str)));
     status = UCS_OK;
 
-out_free_priv:
-    ucs_free(conn_priv);
-out_free_address:
-    ucp_worker_release_address(worker, worker_address);
 out:
     return status;
 }
@@ -526,8 +651,9 @@ int ucp_wireup_ep_is_owner(uct_ep_h uct_ep, uct_ep_h owned_ep)
     }
 
     wireup_ep = ucs_derived_of(uct_ep, ucp_wireup_ep_t);
-    return (wireup_ep->aux_ep == owned_ep) || (wireup_ep->sockaddr_ep == owned_ep);
-
+    return (wireup_ep->aux_ep == owned_ep) ||
+           (wireup_ep->sockaddr_ep == owned_ep) ||
+           (wireup_ep->super.uct_ep == owned_ep);
 }
 
 void ucp_wireup_ep_disown(uct_ep_h uct_ep, uct_ep_h owned_ep)
@@ -539,5 +665,7 @@ void ucp_wireup_ep_disown(uct_ep_h uct_ep, uct_ep_h owned_ep)
         wireup_ep->aux_ep = NULL;
     } else if (wireup_ep->sockaddr_ep == owned_ep) {
         wireup_ep->sockaddr_ep = NULL;
+    } else if (wireup_ep->super.uct_ep == owned_ep) {
+        ucp_proxy_ep_extract(uct_ep);
     }
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.h
index 2940505a3..e1a6a778e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucp/wireup/wireup_ep.h
@@ -35,16 +35,12 @@ struct ucp_wireup_ep {
     uct_ep_h                  aux_ep;        /**< Used to wireup the "real" endpoint */
     uct_ep_h                  sockaddr_ep;   /**< Used for client-server wireup */
     ucp_rsc_index_t           aux_rsc_index; /**< Index of auxiliary transport */
+    ucp_rsc_index_t           sockaddr_rsc_index; /**< Index of sockaddr transport */
     volatile uint32_t         pending_count; /**< Number of pending wireup operations */
     volatile uint32_t         flags;         /**< Connection state flags */
     uct_worker_cb_id_t        progress_id;   /**< ID of progress function */
 };
 
-typedef struct ucp_wireup_client_data {
-    ucp_err_handling_mode_t err_mode;
-    uint64_t                ep_uuid;
-    /* packed worker address follows */
-} UCS_S_PACKED ucp_wireup_sockaddr_priv_t;
 
 /**
  * Create a proxy endpoint for wireup.
@@ -77,6 +73,11 @@ ucs_status_t ucp_wireup_ep_connect(uct_ep_h uct_ep, const ucp_ep_params_t *param
 ucs_status_t ucp_wireup_ep_connect_to_sockaddr(uct_ep_h uct_ep,
                                                const ucp_ep_params_t *params);
 
+ucs_status_t ucp_wireup_ep_connect_aux(ucp_wireup_ep_t *wireup_ep,
+                                       const ucp_ep_params_t *params,
+                                       unsigned address_count,
+                                       const ucp_address_entry_t *address_list);
+
 void ucp_wireup_ep_set_next_ep(uct_ep_h uct_ep, uct_ep_h next_ep);
 
 uct_ep_h ucp_wireup_ep_extract_next_ep(uct_ep_h uct_ep);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/Makefile.am
index fc50d206b..0075285db 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/Makefile.am
@@ -25,19 +25,23 @@ nobase_dist_libucs_la_HEADERS = \
 	config/global_opts.h \
 	config/parser.h \
 	config/types.h \
-	datastruct/arbiter.h \
 	datastruct/callbackq.h \
 	datastruct/list_types.h \
 	datastruct/list.h \
 	datastruct/mpool.h \
 	datastruct/pgtable.h \
 	datastruct/queue_types.h \
+	datastruct/strided_alloc.h \
+	profile/profile_defs.h \
+	profile/profile_off.h \
+	profile/profile_on.h \
 	stats/stats_fwd.h \
 	stats/libstats.h \
 	sys/compiler_def.h\
 	sys/math.h \
 	sys/preprocessor.h \
 	sys/rcache.h \
+	sys/memtype_cache.h \
 	sys/string.h \
 	time/time_def.h \
 	type/class.h \
@@ -61,6 +65,7 @@ noinst_HEADERS = \
 	arch/atomic.h \
 	arch/bitops.h \
 	arch/cpu.h \
+	datastruct/arbiter.h \
 	datastruct/frag_list.h \
 	datastruct/mpmc.h \
 	datastruct/mpool.inl \
@@ -73,11 +78,12 @@ noinst_HEADERS = \
 	debug/debug.h \
 	debug/log.h \
 	debug/memtrack.h \
-	debug/profile.h \
+	profile/profile.h \
 	stats/stats.h \
 	sys/checker.h \
 	sys/compiler.h \
 	sys/numa.h \
+	sys/rcache_int.h \
 	sys/sys.h \
 	time/time.h \
 	time/timerq.h \
@@ -91,6 +97,7 @@ noinst_HEADERS = \
 libucs_la_SOURCES = \
 	algorithm/crc.c \
 	algorithm/qsort_r.c \
+	arch/aarch64/cpu.c \
 	arch/ppc64/timebase.c \
 	arch/x86_64/cpu.c \
 	async/async.c \
@@ -98,6 +105,7 @@ libucs_la_SOURCES = \
 	async/pipe.c \
 	async/thread.c \
 	config/global_opts.c \
+	config/ucm_opts.c \
 	config/parser.c \
 	datastruct/arbiter.c \
 	datastruct/callbackq.c \
@@ -106,16 +114,18 @@ libucs_la_SOURCES = \
 	datastruct/mpool.c \
 	datastruct/pgtable.c \
 	datastruct/ptr_array.c \
+	datastruct/strided_alloc.c \
 	debug/assert.c \
 	debug/debug.c \
 	debug/log.c \
 	debug/memtrack.c \
-	debug/profile.c \
+	profile/profile.c \
 	stats/stats.c \
 	sys/init.c \
 	sys/math.c \
 	sys/numa.c \
 	sys/rcache.c \
+	sys/memtype_cache.c \
 	sys/string.c \
 	sys/sys.c \
 	time/time.c \
@@ -123,6 +133,7 @@ libucs_la_SOURCES = \
 	time/timerq.c \
 	type/class.c \
 	type/component.c \
+	type/spinlock.c \
 	type/status.c
 
 if HAVE_STATS
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.c
new file mode 100644
index 000000000..4d8c83088
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.c
@@ -0,0 +1,67 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#if defined(__aarch64__)
+
+#include <ucs/arch/cpu.h>
+#include <stdio.h>
+
+
+static void ucs_aarch64_cpuid_from_proc(ucs_aarch64_cpuid_t *cpuid)
+{
+    char buf[256];
+    int value;
+    FILE* f;
+
+    cpuid->implementer  = -1;
+    cpuid->architecture = -1;
+    cpuid->variant      = -1;
+    cpuid->part         = -1;
+    cpuid->revision     = -1;
+
+    f = fopen("/proc/cpuinfo","r");
+    if (!f) {
+        return;
+    }
+
+    while (fgets(buf, sizeof(buf), f)) {
+        if (sscanf(buf, "CPU implementer : 0x%x", &value) == 1) {
+            cpuid->implementer  = value;
+        } else if (sscanf(buf, "CPU architecture : %d", &value) == 1) {
+            cpuid->architecture = value;
+        } else if (sscanf(buf, "CPU variant : 0x%x", &value) == 1) {
+            cpuid->variant      = value;
+        } else if (sscanf(buf, "CPU part : 0x%x", &value) == 1) {
+            cpuid->part         = value;
+        } else if (sscanf(buf, "CPU revision : %d", &value) == 1) {
+            cpuid->revision     = value;
+        }
+
+        if ((cpuid->implementer != -1) && (cpuid->architecture != -1) &&
+            (cpuid->variant != -1) && (cpuid->part != -1) && (cpuid->revision != -1)) {
+            break;
+        }
+    }
+
+    fclose(f);
+}
+
+void ucs_aarch64_cpuid(ucs_aarch64_cpuid_t *cpuid)
+{
+    static ucs_aarch64_cpuid_t cached_cpuid;
+    static int initialized = 0;
+
+    if (!initialized) {
+        ucs_aarch64_cpuid_from_proc(&cached_cpuid);
+        ucs_memory_cpu_store_fence();
+        initialized = 1;
+    }
+
+    ucs_memory_cpu_load_fence();
+    *cpuid = cached_cpuid;
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.h
index 431c02519..5f42b128d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/aarch64/cpu.h
@@ -13,6 +13,7 @@
 #include <sys/times.h>
 #include <ucs/sys/compiler_def.h>
 #include <ucs/arch/generic/cpu.h>
+#include <ucs/sys/math.h>
 #if __ARM_NEON
 #include <arm_neon.h>
 #endif
@@ -28,10 +29,30 @@ BEGIN_C_DECLS
 #define ucs_memory_bus_fence()        asm volatile ("dsb sy" ::: "memory");
 #define ucs_memory_bus_store_fence()  asm volatile ("dsb st" ::: "memory");
 #define ucs_memory_bus_load_fence()   asm volatile ("dsb ld" ::: "memory");
-
+#define ucs_memory_bus_wc_flush()
 #define ucs_memory_cpu_fence()        asm volatile ("dmb ish" ::: "memory");
 #define ucs_memory_cpu_store_fence()  asm volatile ("dmb ishst" ::: "memory");
 #define ucs_memory_cpu_load_fence()   asm volatile ("dmb ishld" ::: "memory");
+#define ucs_memory_cpu_wc_fence()     asm volatile ("dmb st" ::: "memory");
+
+
+/*
+ * ARM processor ID (ARM ISA - Main ID Register, EL1)
+ */
+typedef struct ucs_aarch64_cpuid {
+    int       implementer;
+    int       architecture;
+    int       variant;
+    int       part;
+    int       revision;
+} ucs_aarch64_cpuid_t;
+
+
+/**
+ * Get ARM CPU identifier and version
+ */
+void ucs_aarch64_cpuid(ucs_aarch64_cpuid_t *cpuid);
+
 
 #if HAVE_HW_TIMER
 static inline uint64_t ucs_arch_read_hres_clock(void)
@@ -69,13 +90,74 @@ static inline int ucs_arch_get_cpu_flag()
 static inline void ucs_arch_wait_mem(void *address)
 {
     unsigned long tmp;
-    __asm__ __volatile__ ("ldxr %0, [%1] \n"
-                          "wfe           \n"
-                          : "=&r"(tmp)
-                          : "r"(address));
+    asm volatile ("ldxrb %w0, %1 \n"
+                  "wfe           \n"
+                  : "=&r"(tmp)
+                  : "Q"(address));
 }
 
-END_C_DECLS
+#if !HAVE___CLEAR_CACHE
+static inline void ucs_arch_clear_cache(void *start, void *end)
+{
+#if HAVE___AARCH64_SYNC_CACHE_RANGE
+    /* do not allow global declaration of compiler intrinsic */
+    void __aarch64_sync_cache_range(void* beg, void* end);
 
+    __aarch64_sync_cache_range(start, end);
+#else
+    uintptr_t ptr;
+    unsigned icache;
+    unsigned dcache;
+    unsigned ctr_el0;
+
+    /* Get cache line size, using ctr_el0 register
+     *
+     * Bits    Name      Function
+     * *****************************
+     * [31]    -         Reserved, res1.
+     * [30:28] -         Reserved, res0.
+     * [27:24] CWG       Cache Write-Back granule. Log2 of the number of words of the
+     *                   maximum size of memory that can be overwritten as a result of
+     *                   the eviction of a cache entry that has had a memory location
+     *                   in it modified:
+     *                   0x4
+     *                   Cache Write-Back granule size is 16 words.
+     * [23:20] ERG       Exclusives Reservation Granule. Log2 of the number of words of
+     *                   the maximum size of the reservation granule that has been
+     *                   implemented for the Load-Exclusive and Store-Exclusive instructions:
+     *                   0x4
+     *                   Exclusive reservation granule size is 16 words.
+     * [19:16] DminLine  Log2 of the number of words in the smallest cache line of all the
+     *                   data and unified caches that the processor controls:
+     *                   0x4
+     *                   Smallest data cache line size is 16 words.
+     * [15:14] L1lp      L1 Instruction cache policy. Indicates the indexing and tagging
+     *                   policy for the L1 Instruction cache:
+     *                   0b10
+     *                   Virtually Indexed Physically Tagged (VIPT).
+     * [13:4]  -         Reserved, res0.
+     * [3:0]   IminLine  Log2 of the number of words in the smallest cache line of all
+     *                   the instruction caches that the processor controls.
+     *                   0x4
+     *                   Smallest instruction cache line size is 16 words.
+     */
+    asm volatile ("mrs\t%0, ctr_el0":"=r" (ctr_el0));
+    icache = sizeof(int) << (ctr_el0 & 0xf);
+    dcache = sizeof(int) << ((ctr_el0 >> 16) & 0xf);
+
+    for (ptr = ucs_align_down((uintptr_t)start, dcache); ptr < (uintptr_t)end; ptr += dcache) {
+        asm volatile ("dc cvau, %0" :: "r" (ptr) : "memory");
+    }
+    asm volatile ("dsb ish" ::: "memory");
+
+    for (ptr = ucs_align_down((uintptr_t)start, icache); ptr < (uintptr_t)end; ptr += icache) {
+        asm volatile ("ic ivau, %0" :: "r" (ptr) : "memory");
+    }
+    asm volatile ("dsb ish; isb" ::: "memory");
+#endif
+}
 #endif
 
+END_C_DECLS
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/atomic.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/atomic.h
index 7649971b4..0caea9b1f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/atomic.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/atomic.h
@@ -19,6 +19,42 @@
 #  error "Unsupported architecture"
 #endif
 
+#define UCS_DEFINE_ATOMIC_AND(_wordsize, _suffix) \
+    static inline void ucs_atomic_and##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                 uint##_wordsize##_t value) { \
+        __sync_and_and_fetch(ptr, value); \
+    }
+
+#define UCS_DEFINE_ATOMIC_FAND(_wordsize, _suffix) \
+    static inline uint##_wordsize##_t ucs_atomic_fand##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                                 uint##_wordsize##_t value) { \
+        return __sync_fetch_and_and(ptr, value); \
+    }
+
+#define UCS_DEFINE_ATOMIC_XOR(_wordsize, _suffix) \
+    static inline void ucs_atomic_xor##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                 uint##_wordsize##_t value) { \
+        __sync_xor_and_fetch(ptr, value); \
+    }
+
+#define UCS_DEFINE_ATOMIC_FXOR(_wordsize, _suffix) \
+    static inline uint##_wordsize##_t ucs_atomic_fxor##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                                 uint##_wordsize##_t value) { \
+        return __sync_fetch_and_xor(ptr, value); \
+    }
+
+#define UCS_DEFINE_ATOMIC_OR(_wordsize, _suffix) \
+    static inline void ucs_atomic_or##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                uint##_wordsize##_t value) { \
+        __sync_or_and_fetch(ptr, value); \
+    }
+
+#define UCS_DEFINE_ATOMIC_FOR(_wordsize, _suffix) \
+    static inline uint##_wordsize##_t ucs_atomic_for##_wordsize(volatile uint##_wordsize##_t *ptr, \
+                                                                uint##_wordsize##_t value) { \
+        return __sync_fetch_and_or(ptr, value); \
+    }
+
 /*
  * Define atomic functions
  */
@@ -32,6 +68,36 @@ UCS_DEFINE_ATOMIC_FADD(16, w);
 UCS_DEFINE_ATOMIC_FADD(32, l);
 UCS_DEFINE_ATOMIC_FADD(64, q);
 
+UCS_DEFINE_ATOMIC_AND(8,  b);
+UCS_DEFINE_ATOMIC_AND(16, w);
+UCS_DEFINE_ATOMIC_AND(32, l);
+UCS_DEFINE_ATOMIC_AND(64, q);
+
+UCS_DEFINE_ATOMIC_FAND(8,  b);
+UCS_DEFINE_ATOMIC_FAND(16, w);
+UCS_DEFINE_ATOMIC_FAND(32, l);
+UCS_DEFINE_ATOMIC_FAND(64, q);
+
+UCS_DEFINE_ATOMIC_OR(8,  b);
+UCS_DEFINE_ATOMIC_OR(16, w);
+UCS_DEFINE_ATOMIC_OR(32, l);
+UCS_DEFINE_ATOMIC_OR(64, q);
+
+UCS_DEFINE_ATOMIC_FOR(8,  b);
+UCS_DEFINE_ATOMIC_FOR(16, w);
+UCS_DEFINE_ATOMIC_FOR(32, l);
+UCS_DEFINE_ATOMIC_FOR(64, q);
+
+UCS_DEFINE_ATOMIC_XOR(8,  b);
+UCS_DEFINE_ATOMIC_XOR(16, w);
+UCS_DEFINE_ATOMIC_XOR(32, l);
+UCS_DEFINE_ATOMIC_XOR(64, q);
+
+UCS_DEFINE_ATOMIC_FXOR(8,  b);
+UCS_DEFINE_ATOMIC_FXOR(16, w);
+UCS_DEFINE_ATOMIC_FXOR(32, l);
+UCS_DEFINE_ATOMIC_FXOR(64, q);
+
 UCS_DEFINE_ATOMIC_SWAP(8,  b);
 UCS_DEFINE_ATOMIC_SWAP(16, w);
 UCS_DEFINE_ATOMIC_SWAP(32, l);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/bitops.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/bitops.h
index f4dd3ab45..2049b7c71 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/bitops.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/bitops.h
@@ -18,87 +18,89 @@
 #  error "Unsupported architecture"
 #endif
 
-#define ucs_ilog2(n)                   \
-(                                      \
-    __builtin_constant_p(n) ? (        \
-             (n) < 1 ? 0 :             \
-             (n) & (1ULL << 63) ? 63 : \
-             (n) & (1ULL << 62) ? 62 : \
-             (n) & (1ULL << 61) ? 61 : \
-             (n) & (1ULL << 60) ? 60 : \
-             (n) & (1ULL << 59) ? 59 : \
-             (n) & (1ULL << 58) ? 58 : \
-             (n) & (1ULL << 57) ? 57 : \
-             (n) & (1ULL << 56) ? 56 : \
-             (n) & (1ULL << 55) ? 55 : \
-             (n) & (1ULL << 54) ? 54 : \
-             (n) & (1ULL << 53) ? 53 : \
-             (n) & (1ULL << 52) ? 52 : \
-             (n) & (1ULL << 51) ? 51 : \
-             (n) & (1ULL << 50) ? 50 : \
-             (n) & (1ULL << 49) ? 49 : \
-             (n) & (1ULL << 48) ? 48 : \
-             (n) & (1ULL << 47) ? 47 : \
-             (n) & (1ULL << 46) ? 46 : \
-             (n) & (1ULL << 45) ? 45 : \
-             (n) & (1ULL << 44) ? 44 : \
-             (n) & (1ULL << 43) ? 43 : \
-             (n) & (1ULL << 42) ? 42 : \
-             (n) & (1ULL << 41) ? 41 : \
-             (n) & (1ULL << 40) ? 40 : \
-             (n) & (1ULL << 39) ? 39 : \
-             (n) & (1ULL << 38) ? 38 : \
-             (n) & (1ULL << 37) ? 37 : \
-             (n) & (1ULL << 36) ? 36 : \
-             (n) & (1ULL << 35) ? 35 : \
-             (n) & (1ULL << 34) ? 34 : \
-             (n) & (1ULL << 33) ? 33 : \
-             (n) & (1ULL << 32) ? 32 : \
-             (n) & (1ULL << 31) ? 31 : \
-             (n) & (1ULL << 30) ? 30 : \
-             (n) & (1ULL << 29) ? 29 : \
-             (n) & (1ULL << 28) ? 28 : \
-             (n) & (1ULL << 27) ? 27 : \
-             (n) & (1ULL << 26) ? 26 : \
-             (n) & (1ULL << 25) ? 25 : \
-             (n) & (1ULL << 24) ? 24 : \
-             (n) & (1ULL << 23) ? 23 : \
-             (n) & (1ULL << 22) ? 22 : \
-             (n) & (1ULL << 21) ? 21 : \
-             (n) & (1ULL << 20) ? 20 : \
-             (n) & (1ULL << 19) ? 19 : \
-             (n) & (1ULL << 18) ? 18 : \
-             (n) & (1ULL << 17) ? 17 : \
-             (n) & (1ULL << 16) ? 16 : \
-             (n) & (1ULL << 15) ? 15 : \
-             (n) & (1ULL << 14) ? 14 : \
-             (n) & (1ULL << 13) ? 13 : \
-             (n) & (1ULL << 12) ? 12 : \
-             (n) & (1ULL << 11) ? 11 : \
-             (n) & (1ULL << 10) ? 10 : \
-             (n) & (1ULL <<  9) ?  9 : \
-             (n) & (1ULL <<  8) ?  8 : \
-             (n) & (1ULL <<  7) ?  7 : \
-             (n) & (1ULL <<  6) ?  6 : \
-             (n) & (1ULL <<  5) ?  5 : \
-             (n) & (1ULL <<  4) ?  4 : \
-             (n) & (1ULL <<  3) ?  3 : \
-             (n) & (1ULL <<  2) ?  2 : \
-             (n) & (1ULL <<  1) ?  1 : \
-             (n) & (1ULL <<  0) ?  0 : \
-             0                         \
-                                ) :    \
-    (sizeof(n) <= 4) ?                 \
-    __ucs_ilog2_u32((uint32_t)(n)) :   \
-    __ucs_ilog2_u64((uint64_t)(n))     \
+#define ucs_ilog2(_n)                   \
+(                                       \
+    __builtin_constant_p(_n) ? (        \
+             (_n) < 1 ? 0 :             \
+             (_n) & (1ULL << 63) ? 63 : \
+             (_n) & (1ULL << 62) ? 62 : \
+             (_n) & (1ULL << 61) ? 61 : \
+             (_n) & (1ULL << 60) ? 60 : \
+             (_n) & (1ULL << 59) ? 59 : \
+             (_n) & (1ULL << 58) ? 58 : \
+             (_n) & (1ULL << 57) ? 57 : \
+             (_n) & (1ULL << 56) ? 56 : \
+             (_n) & (1ULL << 55) ? 55 : \
+             (_n) & (1ULL << 54) ? 54 : \
+             (_n) & (1ULL << 53) ? 53 : \
+             (_n) & (1ULL << 52) ? 52 : \
+             (_n) & (1ULL << 51) ? 51 : \
+             (_n) & (1ULL << 50) ? 50 : \
+             (_n) & (1ULL << 49) ? 49 : \
+             (_n) & (1ULL << 48) ? 48 : \
+             (_n) & (1ULL << 47) ? 47 : \
+             (_n) & (1ULL << 46) ? 46 : \
+             (_n) & (1ULL << 45) ? 45 : \
+             (_n) & (1ULL << 44) ? 44 : \
+             (_n) & (1ULL << 43) ? 43 : \
+             (_n) & (1ULL << 42) ? 42 : \
+             (_n) & (1ULL << 41) ? 41 : \
+             (_n) & (1ULL << 40) ? 40 : \
+             (_n) & (1ULL << 39) ? 39 : \
+             (_n) & (1ULL << 38) ? 38 : \
+             (_n) & (1ULL << 37) ? 37 : \
+             (_n) & (1ULL << 36) ? 36 : \
+             (_n) & (1ULL << 35) ? 35 : \
+             (_n) & (1ULL << 34) ? 34 : \
+             (_n) & (1ULL << 33) ? 33 : \
+             (_n) & (1ULL << 32) ? 32 : \
+             (_n) & (1ULL << 31) ? 31 : \
+             (_n) & (1ULL << 30) ? 30 : \
+             (_n) & (1ULL << 29) ? 29 : \
+             (_n) & (1ULL << 28) ? 28 : \
+             (_n) & (1ULL << 27) ? 27 : \
+             (_n) & (1ULL << 26) ? 26 : \
+             (_n) & (1ULL << 25) ? 25 : \
+             (_n) & (1ULL << 24) ? 24 : \
+             (_n) & (1ULL << 23) ? 23 : \
+             (_n) & (1ULL << 22) ? 22 : \
+             (_n) & (1ULL << 21) ? 21 : \
+             (_n) & (1ULL << 20) ? 20 : \
+             (_n) & (1ULL << 19) ? 19 : \
+             (_n) & (1ULL << 18) ? 18 : \
+             (_n) & (1ULL << 17) ? 17 : \
+             (_n) & (1ULL << 16) ? 16 : \
+             (_n) & (1ULL << 15) ? 15 : \
+             (_n) & (1ULL << 14) ? 14 : \
+             (_n) & (1ULL << 13) ? 13 : \
+             (_n) & (1ULL << 12) ? 12 : \
+             (_n) & (1ULL << 11) ? 11 : \
+             (_n) & (1ULL << 10) ? 10 : \
+             (_n) & (1ULL <<  9) ?  9 : \
+             (_n) & (1ULL <<  8) ?  8 : \
+             (_n) & (1ULL <<  7) ?  7 : \
+             (_n) & (1ULL <<  6) ?  6 : \
+             (_n) & (1ULL <<  5) ?  5 : \
+             (_n) & (1ULL <<  4) ?  4 : \
+             (_n) & (1ULL <<  3) ?  3 : \
+             (_n) & (1ULL <<  2) ?  2 : \
+             (_n) & (1ULL <<  1) ?  1 : \
+             (_n) & (1ULL <<  0) ?  0 : \
+             0                          \
+                                 ) :    \
+    (sizeof(_n) <= 4) ?                 \
+    __ucs_ilog2_u32((uint32_t)(_n)) :   \
+    __ucs_ilog2_u64((uint64_t)(_n))     \
 )
 
 /* Returns the number of 1-bits in x */
-#define ucs_count_one_bits(x)      __builtin_popcount(x)
+#define ucs_popcount(_n) \
+    ((sizeof(_n) <= 4) ? __builtin_popcount((uint32_t)(_n)) : __builtin_popcountl(_n))
 
 /* Returns the number of trailing 0-bits in x, starting at the least
  * significant bit position.  If x is 0, the result is undefined.
  */
-#define ucs_count_zero_bits(x)     __builtin_ctz(x)
+#define ucs_count_trailing_zero_bits(_n) \
+    ((sizeof(_n) <= 4) ? __builtin_ctz((uint32_t)(_n)) : __builtin_ctzl(_n))
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/cpu.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/cpu.h
index 1c362bc92..58a83825e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/cpu.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/cpu.h
@@ -22,6 +22,9 @@ typedef enum ucs_cpu_model {
     UCS_CPU_MODEL_INTEL_SANDYBRIDGE,
     UCS_CPU_MODEL_INTEL_NEHALEM,
     UCS_CPU_MODEL_INTEL_WESTMERE,
+    UCS_CPU_MODEL_INTEL_HASWELL,
+    UCS_CPU_MODEL_INTEL_BROADWELL,
+    UCS_CPU_MODEL_INTEL_SKYLAKE,
     UCS_CPU_MODEL_ARM_AARCH64,
     UCS_CPU_MODEL_LAST
 } ucs_cpu_model_t;
@@ -66,4 +69,22 @@ typedef enum ucs_cpu_flag {
 #define UCS_SYS_CACHE_LINE_SIZE    UCS_ARCH_CACHE_LINE_SIZE
 #endif
 
+/**
+ * Clear processor data and instruction caches, intended for
+ * self-modifying code.
+ *
+ * @start start of region to clear cache, including address
+ * @end   end of region to clear cache, excluding address
+ */
+static inline void ucs_clear_cache(void *start, void *end)
+{
+#if HAVE___CLEAR_CACHE
+    /* do not allow global declaration of compiler intrinsic */
+    void __clear_cache(void* beg, void* end);
+
+    __clear_cache(start, end);
+#else
+    ucs_arch_clear_cache(start, end);
+#endif
+}
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/ppc64/cpu.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/ppc64/cpu.h
index ae067c576..10cdc626c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/ppc64/cpu.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/ppc64/cpu.h
@@ -25,9 +25,14 @@ BEGIN_C_DECLS
 #define ucs_memory_bus_fence()        asm volatile ("sync"::: "memory")
 #define ucs_memory_bus_store_fence()  ucs_memory_bus_fence()
 #define ucs_memory_bus_load_fence()   ucs_memory_bus_fence()
+#define ucs_memory_bus_wc_flush()
 #define ucs_memory_cpu_fence()        ucs_memory_bus_fence()
-#define ucs_memory_cpu_store_fence()  ucs_memory_bus_fence()
-#define ucs_memory_cpu_load_fence()   ucs_memory_bus_fence()
+#define ucs_memory_cpu_store_fence()  asm volatile ("lwsync \n" \
+                                                    ::: "memory")
+#define ucs_memory_cpu_load_fence()   asm volatile ("lwsync \n" \
+                                                    "isync  \n" \
+                                                    ::: "memory")
+#define ucs_memory_cpu_wc_fence()     ucs_memory_bus_fence()
 
 
 static inline uint64_t ucs_arch_read_hres_clock()
@@ -55,6 +60,13 @@ double ucs_arch_get_clocks_per_sec();
 
 #define ucs_arch_wait_mem ucs_arch_generic_wait_mem
 
+#if !HAVE___CLEAR_CACHE
+static inline void ucs_arch_clear_cache(void *start, void *end)
+{
+    ucs_memory_cpu_fence();
+}
+#endif
+
 END_C_DECLS
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.c
index 7abe2423b..cec9c5c00 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.c
@@ -1,5 +1,5 @@
 /**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2013.  ALL RIGHTS RESERVED.
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
 *
 * See file LICENSE for terms.
 */
@@ -49,7 +49,9 @@ static void ucs_x86_check_invariant_tsc()
 
     return;
 warn:
-    ucs_warn("CPU does not support invariant TSC, time may be unstable");
+    if (ucs_global_opts.warn_inv_tsc) {
+        ucs_warn("CPU does not support invariant TSC, time may be unstable");
+    }
 }
 
 static double ucs_x86_tsc_freq_from_cpu_model()
@@ -156,6 +158,20 @@ ucs_cpu_model_t ucs_arch_get_cpu_model()
        case 0x2c:
        case 0x2f:
            return UCS_CPU_MODEL_INTEL_WESTMERE;
+       case 0x3c:
+       case 0x3f:
+       case 0x45:
+       case 0x46:
+           return UCS_CPU_MODEL_INTEL_HASWELL;
+       case 0x3d:
+       case 0x47:
+       case 0x4f:
+       case 0x56:
+           return UCS_CPU_MODEL_INTEL_BROADWELL;
+       case 0x5e:
+       case 0x4e:
+       case 0x55:
+           return UCS_CPU_MODEL_INTEL_SKYLAKE;
        }
     }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.h
index 583b06ad8..7b9b8f19d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/arch/x86_64/cpu.h
@@ -31,9 +31,11 @@ BEGIN_C_DECLS
 #define ucs_memory_bus_fence()        asm volatile ("mfence"::: "memory")
 #define ucs_memory_bus_store_fence()  asm volatile ("sfence" ::: "memory")
 #define ucs_memory_bus_load_fence()   asm volatile ("lfence" ::: "memory")
+#define ucs_memory_bus_wc_flush()
 #define ucs_memory_cpu_fence()        ucs_compiler_fence()
 #define ucs_memory_cpu_store_fence()  ucs_compiler_fence()
 #define ucs_memory_cpu_load_fence()   ucs_compiler_fence()
+#define ucs_memory_cpu_wc_fence()     asm volatile ("sfence" ::: "memory")
 
 
 static inline uint64_t ucs_arch_read_hres_clock()
@@ -50,6 +52,17 @@ ucs_cpu_flag_t ucs_arch_get_cpu_flag() UCS_F_NOOPTIMIZE;
 
 #define ucs_arch_wait_mem ucs_arch_generic_wait_mem
 
+#if !HAVE___CLEAR_CACHE
+static inline void ucs_arch_clear_cache(void *start, void *end)
+{
+    char *ptr;
+
+    for (ptr = (char*)start; ptr < (char*)end; ptr++) {
+        asm volatile("mfence; clflush %0; mfence" :: "m" (*ptr));
+    }
+}
+#endif
+
 END_C_DECLS
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.c
index 5c756cdb1..bb5aa5d08 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.c
@@ -25,13 +25,13 @@ KHASH_MAP_INIT_INT(ucs_async_handler, ucs_async_handler_t *);
 typedef struct ucs_async_global_context {
     khash_t(ucs_async_handler)     handlers;
     pthread_rwlock_t               handlers_lock;
-    volatile uint32_t              timer_id;
+    volatile uint32_t              handler_id;
 } ucs_async_global_context_t;
 
 
 static ucs_async_global_context_t ucs_async_global_context = {
     .handlers_lock   = PTHREAD_RWLOCK_INITIALIZER,
-    .timer_id        = UCS_ASYNC_TIMER_ID_MIN
+    .handler_id      = UCS_ASYNC_TIMER_ID_MIN
 };
 
 
@@ -65,6 +65,7 @@ static ucs_async_ops_t ucs_async_poll_ops = {
     .block              = ucs_empty_function,
     .unblock            = ucs_empty_function,
     .context_init       = ucs_async_poll_init,
+    .context_cleanup    = ucs_empty_function,
     .context_try_block  = ucs_async_poll_tryblock,
     .context_unblock    = ucs_empty_function,
     .add_event_fd       = ucs_empty_function_return_success,
@@ -147,26 +148,43 @@ static void ucs_async_handler_put(ucs_async_handler_t *handler)
 }
 
 /* add new handler to the table */
-static ucs_status_t ucs_async_handler_add(ucs_async_handler_t *handler)
+static ucs_status_t ucs_async_handler_add(int min_id, int max_id,
+                                          ucs_async_handler_t *handler)
 {
     int hash_extra_status;
     ucs_status_t status;
     khiter_t hash_it;
+    int i, id;
 
     pthread_rwlock_wrlock(&ucs_async_global_context.handlers_lock);
 
+    handler->id = -1;
     ucs_assert_always(handler->refcount == 1);
-    hash_it = kh_put(ucs_async_handler, &ucs_async_global_context.handlers,
-                     handler->id, &hash_extra_status);
-    if (hash_extra_status == -1) {
-        ucs_error("Failed to add async handler " UCS_ASYNC_HANDLER_FMT " to hash",
-                  UCS_ASYNC_HANDLER_ARG(handler));
-        status = UCS_ERR_NO_MEMORY;
-        goto out_unlock;
-    } else if (hash_extra_status == 0) {
-        ucs_error("Async handler " UCS_ASYNC_HANDLER_FMT " exists - cannot add %s()",
-                  UCS_ASYNC_HANDLER_ARG(kh_value(&ucs_async_global_context.handlers, hash_it)),
-                  ucs_debug_get_symbol_name(handler->cb));
+
+    /*
+     * Search for an empty key in the range [min_id, max_id)
+     * ucs_async_global_context.handler_id is used to generate "unique" keys.
+     */
+    for (i = min_id; i < max_id; ++i) {
+        id = min_id + (ucs_atomic_fadd32(&ucs_async_global_context.handler_id, 1) %
+                       (max_id - min_id));
+        hash_it = kh_put(ucs_async_handler, &ucs_async_global_context.handlers,
+                         id, &hash_extra_status);
+        if (hash_extra_status == -1) {
+            ucs_error("Failed to add async handler " UCS_ASYNC_HANDLER_FMT
+                      " to hash", UCS_ASYNC_HANDLER_ARG(handler));
+            status = UCS_ERR_NO_MEMORY;
+            goto out_unlock;
+        } else if (hash_extra_status != 0) {
+            handler->id = id;
+            ucs_assert(id != -1);
+            break;
+        }
+    }
+
+    if (handler->id == -1) {
+        ucs_error("Cannot add async handler %s() - id range [%d..%d) is full",
+                  ucs_debug_get_symbol_name(handler->cb), min_id, max_id);
         status = UCS_ERR_ALREADY_EXISTS;
         goto out_unlock;
     }
@@ -330,6 +348,8 @@ void ucs_async_context_cleanup(ucs_async_context_t *async)
         ucs_warn("releasing async context with %d handlers", async->num_handlers);
         pthread_rwlock_unlock(&ucs_async_global_context.handlers_lock);
     }
+
+    ucs_async_method_call(async->mode, context_cleanup, async);
     ucs_mpmc_queue_cleanup(&async->missed);
 }
 
@@ -340,17 +360,18 @@ void ucs_async_context_destroy(ucs_async_context_t *async)
 }
 
 static ucs_status_t
-ucs_async_alloc_handler(int id, ucs_async_mode_t mode, int events,
-                        ucs_async_event_cb_t cb, void *arg,
-                        ucs_async_context_t *async)
+ucs_async_alloc_handler(int min_id, int max_id, ucs_async_mode_t mode,
+                        int events, ucs_async_event_cb_t cb, void *arg,
+                        ucs_async_context_t *async, int *id_p)
 {
     ucs_async_handler_t *handler;
     ucs_status_t status;
 
     /* If async context is given, it should have same mode */
     if ((async != NULL) && (async->mode != mode)) {
-        ucs_error("Async mode mismatch for handler [id=%d], "
-                  "mode: %d async context mode: %d", id, mode, async->mode);
+        ucs_error("Async mode mismatch for handler %s(), "
+                  "mode: %d async context mode: %d",
+                  ucs_debug_get_symbol_name(cb), mode, async->mode);
         status = UCS_ERR_INVALID_PARAM;
         goto err;
     }
@@ -369,7 +390,6 @@ ucs_async_alloc_handler(int id, ucs_async_mode_t mode, int events,
         goto err_dec_num_handlers;
     }
 
-    handler->id       = id;
     handler->mode     = mode;
     handler->events   = events;
     handler->cb       = cb;
@@ -378,12 +398,14 @@ ucs_async_alloc_handler(int id, ucs_async_mode_t mode, int events,
     handler->missed   = 0;
     handler->refcount = 1;
     ucs_async_method_call(mode, block);
-    status = ucs_async_handler_add(handler);
+    status = ucs_async_handler_add(min_id, max_id, handler);
     ucs_async_method_call(mode, unblock);
     if (status != UCS_OK) {
         goto err_free;
     }
 
+    ucs_assert((handler->id >= min_id) && (handler->id < max_id));
+    *id_p = handler->id;
     return UCS_OK;
 
 err_free:
@@ -401,6 +423,7 @@ ucs_status_t ucs_async_set_event_handler(ucs_async_mode_t mode, int event_fd,
                                          void *arg, ucs_async_context_t *async)
 {
     ucs_status_t status;
+    int event_id;
 
     if (event_fd >= UCS_ASYNC_TIMER_ID_MIN) {
         /* File descriptor too large */
@@ -408,10 +431,12 @@ ucs_status_t ucs_async_set_event_handler(ucs_async_mode_t mode, int event_fd,
         goto err;
     }
 
-    status = ucs_async_alloc_handler(event_fd, mode, events, cb, arg, async);
+    status = ucs_async_alloc_handler(event_fd, event_fd + 1, mode, events, cb,
+                                     arg, async, &event_id);
     if (status != UCS_OK) {
         goto err;
     }
+    ucs_assert(event_id == event_fd);
 
     status = ucs_async_method_call(mode, add_event_fd, async, event_fd, events);
     if (status != UCS_OK) {
@@ -435,15 +460,8 @@ ucs_status_t ucs_async_add_timer(ucs_async_mode_t mode, ucs_time_t interval,
     ucs_status_t status;
     int timer_id;
 
-    /* Search for unused timer ID */
-    do {
-        timer_id = ucs_atomic_fadd32(&ucs_async_global_context.timer_id, 1);
-        if (timer_id >= UCS_ASYNC_TIMER_ID_MAX) {
-            timer_id = UCS_ASYNC_TIMER_ID_MIN;
-        }
-
-        status = ucs_async_alloc_handler(timer_id, mode, 1, cb, arg, async);
-    } while (status == UCS_ERR_ALREADY_EXISTS);
+    status = ucs_async_alloc_handler(UCS_ASYNC_TIMER_ID_MIN, UCS_ASYNC_TIMER_ID_MAX,
+                                     mode, 1, cb, arg, async, &timer_id);
     if (status != UCS_OK) {
         goto err;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.h
index 3866dbe2e..406b1f3f3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async.h
@@ -121,6 +121,25 @@ static inline int ucs_async_check_miss(ucs_async_context_t *async)
     } while (0)
 
 
+/**
+ * Check if asynchronous event delivery is blocked by the current thread.
+ *
+ * @param _async Event context to check status for.
+ */
+#define UCS_ASYNC_IS_RECURSIVELY_BLOCKED(_async) \
+    ({  \
+        int _ret; \
+        if ((_async)->mode == UCS_ASYNC_MODE_THREAD) { \
+            _ret = UCS_ASYNC_THREAD_IS_RECURSIVELY_BLOCKED(_async); \
+        } else if ((_async)->mode == UCS_ASYNC_MODE_SIGNAL) { \
+            _ret = UCS_ASYNC_SIGNAL_IS_RECURSIVELY_BLOCKED(_async); \
+        } else { \
+            _ret = (_async)->poll_block; \
+        } \
+        _ret; \
+    })
+
+
 END_C_DECLS
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async_int.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async_int.h
index e11f63eeb..88356421d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async_int.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/async_int.h
@@ -57,6 +57,7 @@ typedef struct ucs_async_ops {
     void         (*unblock)();
 
     ucs_status_t (*context_init)(ucs_async_context_t *async);
+    void         (*context_cleanup)(ucs_async_context_t *async);
     int          (*context_try_block)(ucs_async_context_t *async);
     void         (*context_unblock)(ucs_async_context_t *async);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.c
index c610be186..7784fd4fb 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.c
@@ -298,6 +298,14 @@ static ucs_status_t ucs_async_signal_init(ucs_async_context_t *async)
     return UCS_OK;
 }
 
+static void ucs_async_signal_cleanup(ucs_async_context_t *async)
+{
+    if (async->signal.block_count > 0) {
+        ucs_warn("destroying async signal context with block_count %d",
+                 async->signal.block_count);
+    }
+}
+
 static ucs_status_t ucs_async_signal_modify_event_fd(ucs_async_context_t *async,
                                                      int event_fd, int events)
 {
@@ -580,6 +588,7 @@ ucs_async_ops_t ucs_async_signal_ops = {
     .block              = ucs_async_signal_block_all,
     .unblock            = ucs_async_signal_unblock_all,
     .context_init       = ucs_async_signal_init,
+    .context_cleanup    = ucs_async_signal_cleanup,
     .context_try_block  = ucs_async_signal_try_block,
     .context_unblock    = ucs_async_signal_unblock,
     .add_event_fd       = ucs_async_signal_add_event_fd,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.h
index 51ed37cf3..72e78f3fe 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/signal.h
@@ -29,10 +29,17 @@ typedef struct ucs_async_signal_context {
         ucs_memory_cpu_fence(); \
     }
 
+
 #define UCS_ASYNC_SIGNAL_UNBLOCK(_async) \
     { \
         ucs_memory_cpu_fence(); \
         --(_async)->signal.block_count; \
     }
 
+
+#define UCS_ASYNC_SIGNAL_IS_RECURSIVELY_BLOCKED(_async) \
+    (((_async)->signal.block_count > 0) && \
+     ((_async)->signal.tid == ucs_get_tid()))
+
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.c
index 03f16f3ae..f44da04ac 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.c
@@ -257,7 +257,7 @@ static ucs_status_t ucs_async_thread_init(ucs_async_context_t *async)
         pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);
         ret = pthread_mutex_init(&async->thread.mutex, &attr);
         if (ret != 0) {
-            ucs_error("Failed to initialize lock: %s", strerror(ret));
+            ucs_error("failed to initialize async lock: %s", strerror(ret));
             return UCS_ERR_INVALID_PARAM;
         }
 
@@ -267,6 +267,21 @@ static ucs_status_t ucs_async_thread_init(ucs_async_context_t *async)
         return ucs_spinlock_init(&async->thread.spinlock);
 }
 
+static void ucs_async_thread_cleanup(ucs_async_context_t *async)
+{
+#if !(NVALGRIND)
+    int ret;
+
+    if (RUNNING_ON_VALGRIND) {
+        ret = pthread_mutex_destroy(&async->thread.mutex);
+        if (ret != 0) {
+            ucs_warn("failed to destroy async lock: %s", strerror(ret));
+        }
+    } else
+#endif
+        ucs_spinlock_destroy(&async->thread.spinlock);
+}
+
 static ucs_status_t ucs_async_thread_add_event_fd(ucs_async_context_t *async,
                                                   int event_fd, int events)
 {
@@ -407,6 +422,7 @@ ucs_async_ops_t ucs_async_thread_ops = {
     .block              = ucs_empty_function,
     .unblock            = ucs_empty_function,
     .context_init       = ucs_async_thread_init,
+    .context_cleanup    = ucs_async_thread_cleanup,
     .context_try_block  = ucs_async_thread_try_block,
     .context_unblock    = ucs_async_thread_unblock,
     .add_event_fd       = ucs_async_thread_add_event_fd,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.h
index 5a2726f90..074f5a92a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/async/thread.h
@@ -29,6 +29,8 @@ typedef struct ucs_async_thread_context {
 #define UCS_ASYNC_THREAD_UNBLOCK(_async) \
     ucs_spin_unlock(&(_async)->thread.spinlock)
 
+#define UCS_ASYNC_THREAD_IS_RECURSIVELY_BLOCKED(...)    0
+
 #else
 
 #define UCS_ASYNC_THREAD_BLOCK(_async) \
@@ -45,6 +47,14 @@ typedef struct ucs_async_thread_context {
             ucs_spin_unlock(&(_async)->thread.spinlock); \
     }
 
+#ifdef ENABLE_ASSERT
+
+#define UCS_ASYNC_THREAD_IS_RECURSIVELY_BLOCKED(_async) \
+    ((RUNNING_ON_VALGRIND) ? 0 : \
+     ucs_spin_is_owner(&(_async)->thread.spinlock, pthread_self()))
+
+#endif /* ENABLE_ASSERT */
+
 #endif /* NVALGRIND */
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.c
index 0fb6cd7b8..b1f37e2d5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.c
@@ -7,7 +7,7 @@
 #include "global_opts.h"
 
 #include <ucs/config/parser.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <ucs/debug/assert.h>
 #include <ucs/debug/log.h>
 #include <ucs/sys/compiler.h>
@@ -27,6 +27,8 @@ ucs_global_opts_t ucs_global_opts = {
     .error_mail_footer     = "",
     .gdb_command           = "gdb",
     .debug_signo           = SIGHUP,
+    .log_level_trigger     = UCS_LOG_LEVEL_FATAL,
+    .warn_unused_env_vars  = 1,
     .async_max_events      = 64,
     .async_signo           = SIGALRM,
     .stats_dest            = "",
@@ -37,6 +39,8 @@ ucs_global_opts_t ucs_global_opts = {
     .profile_file          = "",
     .stats_filter          = { NULL, 0 },
     .stats_format          = UCS_STATS_FULL,
+    .rcache_check_pfn      = 0,
+    .warn_inv_tsc          = 1
 };
 
 static const char *ucs_handle_error_modes[] = {
@@ -120,6 +124,11 @@ static ucs_config_field_t ucs_global_opts_table[] = {
   "Log level to trigger error handling.",
   ucs_offsetof(ucs_global_opts_t, log_level_trigger), UCS_CONFIG_TYPE_ENUM(ucs_log_level_names)},
 
+ {UCS_GLOBAL_OPTS_WARN_UNUSED_CONFIG, "yes",
+  "Issue warning about UCX_ environment variables which were not used by the\n"
+  "configuration parser.",
+  ucs_offsetof(ucs_global_opts_t, warn_unused_env_vars), UCS_CONFIG_TYPE_BOOL},
+
  {"ASYNC_MAX_EVENTS", "1024", /* TODO remove this; resize mpmc */
   "Maximal number of events which can be handled from one context",
   ucs_offsetof(ucs_global_opts_t, async_max_events), UCS_CONFIG_TYPE_UINT},
@@ -176,7 +185,6 @@ static ucs_config_field_t ucs_global_opts_table[] = {
   ucs_offsetof(ucs_global_opts_t, memtrack_dest), UCS_CONFIG_TYPE_STRING},
 #endif
 
-#if HAVE_PROFILING
   {"PROFILE_MODE", "",
    "Profile collection modes. If none is specified, profiling is disabled.\n"
    " - log   - Record all timestamps.\n"
@@ -192,10 +200,21 @@ static ucs_config_field_t ucs_global_opts_table[] = {
   {"PROFILE_LOG_SIZE", "4m",
    "Maximal size of profiling log. New records will replace old records.",
    ucs_offsetof(ucs_global_opts_t, profile_log_size), UCS_CONFIG_TYPE_MEMUNITS},
-#endif
+
+  {"RCACHE_CHECK_PFN", "n",
+   "Registration cache to check that the physical page frame number of a found\n"
+   "memory region was not changed since the time the region was registered.\n",
+   ucs_offsetof(ucs_global_opts_t, rcache_check_pfn), UCS_CONFIG_TYPE_BOOL},
+
+  {"WARN_INVARIANT_TSC", "y",
+   "Issue a warning in case of invariant TSC.\n",
+   ucs_offsetof(ucs_global_opts_t, warn_inv_tsc), UCS_CONFIG_TYPE_BOOL},
 
  {NULL}
 };
+UCS_CONFIG_REGISTER_TABLE(ucs_global_opts_table, "UCS global", NULL,
+                          ucs_global_opts_t)
+
 
 void ucs_global_opts_init()
 {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.h
index b45129612..35a3f3849 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/global_opts.h
@@ -17,6 +17,7 @@
 
 BEGIN_C_DECLS
 
+#define UCS_GLOBAL_OPTS_WARN_UNUSED_CONFIG    "WARN_UNUSED_ENV_VARS"
 
 /**
  * UCS global options.
@@ -63,6 +64,9 @@ typedef struct {
     /* Log level to trigger error handling */
     ucs_log_level_t          log_level_trigger;
 
+    /* Issue warning about UCX_ env vars which were not used by config parser */
+    int                      warn_unused_env_vars;
+
     /* Max. events per context, will be removed in the future */
     unsigned                 async_max_events;
 
@@ -94,7 +98,7 @@ typedef struct {
     char                     *profile_file;
 
     /* Limit for profiling log size */
-     size_t                   profile_log_size;
+    size_t                   profile_log_size;
 
     /* Counters to be included in statistics summary */
     ucs_config_names_array_t stats_filter;
@@ -102,6 +106,11 @@ typedef struct {
     /* statistics format options */
     ucs_stats_formats_t      stats_format;
 
+    /* registration cache checks if physical page is not moved */
+    int                      rcache_check_pfn;
+
+    /* Prompt/suppress invariant TSC warning (used in gtest) */
+    int                      warn_inv_tsc;
 } ucs_global_opts_t;
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.c
index 8e5384931..4bf4205db 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.c
@@ -9,8 +9,11 @@
 #endif
 #include "parser.h"
 
+#include <ucs/arch/atomic.h>
 #include <ucs/sys/sys.h>
 #include <ucs/sys/string.h>
+#include <ucs/datastruct/list.h>
+#include <ucs/datastruct/khash.h>
 #include <ucs/debug/assert.h>
 #include <ucs/debug/log.h>
 #include <ucs/debug/debug.h>
@@ -18,13 +21,36 @@
 #include <fnmatch.h>
 
 
+/* configuration value which specifies "infinity" for a numeric variable */
+#define UCS_CONFIG_PARSER_NUMERIC_INF_STR      "inf"
+
+/* width of titles in docstring */
+#define UCP_CONFIG_PARSER_DOCSTR_WIDTH         10
+
+
+/* list of prefixes for a configuration variable, used to dump all possible
+ * aliases.
+ */
+typedef struct ucs_config_parser_prefix_list {
+    const char                  *prefix;
+    ucs_list_link_t             list;
+} ucs_config_parser_prefix_t;
+
+
 typedef UCS_CONFIG_ARRAY_FIELD(void, data) ucs_config_array_field_t;
 
+KHASH_SET_INIT_STR(ucs_config_env_vars)
+
 
 /* Process environment variables */
 extern char **environ;
 
 
+UCS_LIST_HEAD(ucs_config_global_list);
+static khash_t(ucs_config_env_vars) ucs_config_parser_env_vars = {0};
+static pthread_mutex_t ucs_config_parser_env_vars_hash_lock    = PTHREAD_MUTEX_INITIALIZER;
+
+
 const char *ucs_async_mode_names[] = {
     [UCS_ASYNC_MODE_SIGNAL] = "signal",
     [UCS_ASYNC_MODE_THREAD] = "thread",
@@ -99,7 +125,12 @@ int ucs_config_sprintf_int(char *buf, size_t max, void *src, const void *arg)
 
 int ucs_config_sscanf_uint(const char *buf, void *dest, const void *arg)
 {
-    return sscanf(buf, "%u", (unsigned*)dest);
+    if (!strcasecmp(buf, UCS_CONFIG_PARSER_NUMERIC_INF_STR)) {
+        *(unsigned*)dest = UINT_MAX;
+        return 1;
+    } else {
+        return sscanf(buf, "%u", (unsigned*)dest);
+    }
 }
 
 ucs_status_t ucs_config_clone_uint(void *src, void *dest, const void *arg)
@@ -110,7 +141,13 @@ ucs_status_t ucs_config_clone_uint(void *src, void *dest, const void *arg)
 
 int ucs_config_sprintf_uint(char *buf, size_t max, void *src, const void *arg)
 {
-    return snprintf(buf, max, "%u", *(unsigned*)src);
+    unsigned value = *(unsigned*)src;
+    if (value == UINT_MAX) {
+        snprintf(buf, max, UCS_CONFIG_PARSER_NUMERIC_INF_STR);
+        return 1;
+    } else {
+        return snprintf(buf, max, "%u", value);
+    }
 }
 
 int ucs_config_sscanf_ulong(const char *buf, void *dest, const void *arg)
@@ -372,7 +409,7 @@ int ucs_config_sscanf_memunits(const char *buf, void *dest, const void *arg)
     size_t bytes;
 
     /* Special value: infinity */
-    if (!strcasecmp(buf, "inf")) {
+    if (!strcasecmp(buf, UCS_CONFIG_PARSER_NUMERIC_INF_STR)) {
         *(size_t*)dest = UCS_CONFIG_MEMUNITS_INF;
         return 1;
     }
@@ -414,7 +451,7 @@ int ucs_config_sprintf_memunits(char *buf, size_t max, void *src, const void *ar
     size_t sz = *(size_t*)src;
 
     if (sz == UCS_CONFIG_MEMUNITS_INF) {
-        snprintf(buf, max, "inf");
+        snprintf(buf, max, UCS_CONFIG_PARSER_NUMERIC_INF_STR);
     } else if (sz == UCS_CONFIG_MEMUNITS_AUTO) {
         snprintf(buf, max, "auto");
     } else {
@@ -843,6 +880,34 @@ ucs_config_parser_set_value_internal(void *opts, ucs_config_field_t *fields,
     return (count == 0) ? UCS_ERR_NO_ELEM : UCS_OK;
 }
 
+static void ucs_config_parser_mark_env_var_used(const char *name)
+{
+    khiter_t iter;
+    char *key;
+    int ret;
+
+    if (!ucs_global_opts.warn_unused_env_vars) {
+        return;
+    }
+
+    pthread_mutex_lock(&ucs_config_parser_env_vars_hash_lock);
+
+    iter = kh_get(ucs_config_env_vars, &ucs_config_parser_env_vars, name);
+    if (iter != kh_end(&ucs_config_parser_env_vars)) {
+        goto out; /* already exists */
+    }
+
+    key = ucs_strdup(name, "config_parser_env_var");
+    if (key == NULL) {
+        ucs_error("strdup(%s) failed", name);
+        goto out;
+    }
+
+    kh_put(ucs_config_env_vars, &ucs_config_parser_env_vars, key, &ret);
+out:
+    pthread_mutex_unlock(&ucs_config_parser_env_vars_hash_lock);
+}
+
 static ucs_status_t ucs_config_apply_env_vars(void *opts, ucs_config_field_t *fields,
                                              const char *prefix, const char *table_prefix,
                                              int recurse, int ignore_errors)
@@ -889,6 +954,7 @@ static ucs_status_t ucs_config_apply_env_vars(void *opts, ucs_config_field_t *fi
             env_value = getenv(buf);
             if (env_value != NULL) {
                 ucs_config_parser_release_field(field, var);
+                ucs_config_parser_mark_env_var_used(buf);
                 status = ucs_config_parser_parse_field(field, env_value, var);
                 if (status != UCS_OK) {
                     /* If set to ignore errors, restore the default value */
@@ -1075,14 +1141,18 @@ static void __print_stream_cb(int num, const char *line, void *arg)
 
 static void
 ucs_config_parser_print_field(FILE *stream, const void *opts, const char *env_prefix,
-                              const char *prefix, const char *name,
-                              const ucs_config_field_t *field,
-                              unsigned long flags, const char *docstr, ...)
+                              ucs_list_link_t *prefix_list, const char *name,
+                              const ucs_config_field_t *field, unsigned long flags,
+                              const char *docstr, ...)
 {
-    char value_buf[128] = {0};
+    ucs_config_parser_prefix_t *prefix, *head;
+    char value_buf[128]  = {0};
     char syntax_buf[256] = {0};
     va_list ap;
 
+    ucs_assert(!ucs_list_is_empty(prefix_list));
+    head = ucs_list_head(prefix_list, ucs_config_parser_prefix_t, list);
+
     field->parser.write(value_buf, sizeof(value_buf) - 1, (char*)opts + field->offset,
                         field->parser.arg);
     field->parser.help(syntax_buf, sizeof(syntax_buf) - 1, field->parser.arg);
@@ -1091,8 +1161,8 @@ ucs_config_parser_print_field(FILE *stream, const void *opts, const char *env_pr
         fprintf(stream, "#\n");
         ucs_config_print_doc_line_by_line(field, __print_stream_cb, stream);
         fprintf(stream, "#\n");
-        fprintf(stream, "# Syntax: %s\n", syntax_buf);
-        fprintf(stream, "#\n");
+        fprintf(stream, "# %-*s %s\n", UCP_CONFIG_PARSER_DOCSTR_WIDTH, "syntax:",
+                syntax_buf);
 
         /* Extra docstring */
         if (docstr != NULL) {
@@ -1102,9 +1172,27 @@ ucs_config_parser_print_field(FILE *stream, const void *opts, const char *env_pr
             va_end(ap);
             fprintf(stream, "\n");
         }
-     }
 
-    fprintf(stream, "%s%s%s=%s\n", env_prefix, prefix, name, value_buf);
+        /* Parents in configuration hierarchy */
+        if (prefix_list->next != prefix_list->prev) {
+            fprintf(stream, "# %-*s", UCP_CONFIG_PARSER_DOCSTR_WIDTH, "inherits:");
+            ucs_list_for_each(prefix, prefix_list, list) {
+                if (prefix == head) {
+                    continue;
+                }
+
+                fprintf(stream, " %s%s%s", env_prefix, prefix->prefix, name);
+                if (prefix != ucs_list_tail(prefix_list, ucs_config_parser_prefix_t, list)) {
+                    fprintf(stream, ",");
+                }
+            }
+            fprintf(stream, "\n");
+        }
+
+        fprintf(stream, "#\n");
+    }
+
+    fprintf(stream, "%s%s%s=%s\n", env_prefix, head->prefix, name, value_buf);
 
     if (flags & UCS_CONFIG_PRINT_DOC) {
         fprintf(stream, "\n");
@@ -1115,26 +1203,23 @@ static void
 ucs_config_parser_print_opts_recurs(FILE *stream, const void *opts,
                                     const ucs_config_field_t *fields,
                                     unsigned flags, const char *env_prefix,
-                                    const char *table_prefix)
+                                    ucs_list_link_t *prefix_list)
 {
     const ucs_config_field_t *field, *aliased_field;
+    ucs_config_parser_prefix_t inner_prefix;
     size_t alias_table_offset;
-    const char *prefix;
-
-    prefix = table_prefix == NULL ? "" : table_prefix;
 
     for (field = fields; field->name; ++field) {
         if (ucs_config_is_table_field(field)) {
-            /* Parse with sub-table prefix */
-            if (table_prefix == NULL) {
-                ucs_config_parser_print_opts_recurs(stream, opts + field->offset,
-                                                    field->parser.arg, flags,
-                                                    env_prefix, field->name);
-            } else {
-                ucs_config_parser_print_opts_recurs(stream, opts + field->offset,
-                                                    field->parser.arg, flags,
-                                                    env_prefix, table_prefix);
-            }
+            /* Parse with sub-table prefix.
+             * We start the leaf prefix and continue up the hierarchy.
+             */
+            inner_prefix.prefix = field->name;
+            ucs_list_add_tail(prefix_list, &inner_prefix.list);
+            ucs_config_parser_print_opts_recurs(stream, opts + field->offset,
+                                                field->parser.arg, flags,
+                                                env_prefix, prefix_list);
+            ucs_list_del(&inner_prefix.list);
         } else if (ucs_config_is_alias_field(field)) {
             if (flags & UCS_CONFIG_PRINT_HIDDEN) {
                 aliased_field = ucs_config_find_aliased_field(fields, field,
@@ -1144,14 +1229,15 @@ ucs_config_parser_print_opts_recurs(FILE *stream, const void *opts,
                 }
                 ucs_config_parser_print_field(stream,
                                               opts + alias_table_offset,
-                                              env_prefix, table_prefix,
+                                              env_prefix, prefix_list,
                                               field->name, aliased_field,
-                                              flags, "(alias of %s%s%s)",
-                                              env_prefix, table_prefix,
+                                              flags, "%-*s %s%s%s", "alias of:",
+                                              UCP_CONFIG_PARSER_DOCSTR_WIDTH,
+                                              env_prefix, prefix_list,
                                               aliased_field->name);
             }
         } else {
-            ucs_config_parser_print_field(stream, opts, env_prefix, prefix,
+            ucs_config_parser_print_field(stream, opts, env_prefix, prefix_list,
                                           field->name, field, flags, NULL);
         }
      }
@@ -1162,6 +1248,9 @@ void ucs_config_parser_print_opts(FILE *stream, const char *title, const void *o
                                   ucs_config_field_t *fields, const char *table_prefix,
                                   ucs_config_print_flags_t flags)
 {
+    ucs_config_parser_prefix_t table_prefix_elem;
+    UCS_LIST_HEAD(prefix_list);
+
     if (flags & UCS_CONFIG_PRINT_HEADER) {
         fprintf(stream, "\n");
         fprintf(stream, "#\n");
@@ -1171,8 +1260,10 @@ void ucs_config_parser_print_opts(FILE *stream, const char *title, const void *o
     }
 
     if (flags & UCS_CONFIG_PRINT_CONFIG) {
+        table_prefix_elem.prefix = table_prefix ? table_prefix : "";
+        ucs_list_add_tail(&prefix_list, &table_prefix_elem.list);
         ucs_config_parser_print_opts_recurs(stream, opts, fields, flags,
-                                            UCS_CONFIG_PREFIX, table_prefix);
+                                            UCS_CONFIG_PREFIX, &prefix_list);
     }
 
     if (flags & UCS_CONFIG_PRINT_HEADER) {
@@ -1180,6 +1271,107 @@ void ucs_config_parser_print_opts(FILE *stream, const char *title, const void *o
     }
 }
 
+void ucs_config_parser_print_all_opts(FILE *stream, ucs_config_print_flags_t flags)
+{
+    const ucs_config_global_list_entry_t *entry;
+    ucs_status_t status;
+    char title[64];
+    void *opts;
+
+    ucs_list_for_each(entry, &ucs_config_global_list, list) {
+        opts = ucs_malloc(entry->size, "tmp_opts");
+        if (opts == NULL) {
+            ucs_error("could not allocate configuration of size %zu", entry->size);
+            continue;
+        }
+
+        status = ucs_config_parser_fill_opts(opts, entry->fields, NULL,
+                                             entry->prefix, 0);
+        if (status != UCS_OK) {
+            ucs_free(opts);
+            continue;
+        }
+
+        snprintf(title, sizeof(title), "%s configuration", entry->name);
+        ucs_config_parser_print_opts(stream, title, opts, entry->fields,
+                                     entry->prefix, flags);
+
+        ucs_config_parser_release_opts(opts, entry->fields);
+        ucs_free(opts);
+    }
+}
+
+void ucs_config_parser_warn_unused_env_vars()
+{
+    static uint32_t warn_once = 1;
+    char unused_env_vars_names[40];
+    int num_unused_vars;
+    char **envp, *envstr;
+    size_t prefix_len;
+    char *var_name;
+    char *p, *endp;
+    khiter_t iter;
+    char *saveptr;
+    int truncated;
+    int ret;
+
+    if (!ucs_global_opts.warn_unused_env_vars) {
+        return;
+    }
+
+    if (!ucs_atomic_cswap32(&warn_once, 1, 0)) {
+        return;
+    }
+
+    pthread_mutex_lock(&ucs_config_parser_env_vars_hash_lock);
+
+    prefix_len      = strlen(UCS_CONFIG_PREFIX);
+    p               = unused_env_vars_names;
+    endp            = p + sizeof(unused_env_vars_names) - 1;
+    *endp           = '\0';
+    truncated       = 0;
+    num_unused_vars = 0;
+
+    for (envp = environ; !truncated && (*envp != NULL); ++envp) {
+        envstr = ucs_strdup(*envp, "env_str");
+        if (envstr == NULL) {
+            continue;
+        }
+
+        var_name = strtok_r(envstr, "=", &saveptr);
+        if (!var_name || strncmp(var_name, UCS_CONFIG_PREFIX, prefix_len)) {
+            ucs_free(envstr);
+            continue; /* Not UCX */
+        }
+
+        iter = kh_get(ucs_config_env_vars, &ucs_config_parser_env_vars, var_name);
+        if (iter == kh_end(&ucs_config_parser_env_vars)) {
+            ret = snprintf(p, endp - p, " %s,", var_name);
+            if (ret > endp - p) {
+                truncated = 1;
+                *p = '\0';
+            } else {
+                p += strlen(p);
+                ++num_unused_vars;
+            }
+        }
+
+        ucs_free(envstr);
+    }
+
+    if (num_unused_vars > 0) {
+        if (!truncated) {
+            p[-1] = '\0'; /* remove trailing comma */
+        }
+        ucs_warn("unused env variable%s:%s%s (set %s%s=n to suppress this warning)",
+                 num_unused_vars > 1 ? "s" : "", unused_env_vars_names,
+                 truncated ? "..." : "", UCS_CONFIG_PREFIX,
+                 UCS_GLOBAL_OPTS_WARN_UNUSED_CONFIG);
+    }
+
+    pthread_mutex_unlock(&ucs_config_parser_env_vars_hash_lock);
+}
+
 size_t ucs_config_memunits_get(size_t config_size, size_t auto_size,
                                size_t max_size)
 {
@@ -1204,3 +1396,11 @@ int ucs_config_names_search(ucs_config_names_array_t config_names,
     return -1;
 }
 
+UCS_STATIC_CLEANUP {
+    const char *key;
+
+    kh_foreach_key(&ucs_config_parser_env_vars, key, {
+        ucs_free((void*)key);
+    })
+    kh_destroy_inplace(ucs_config_env_vars, &ucs_config_parser_env_vars);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.h
index b85cf7c59..87c6142c9 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/parser.h
@@ -9,6 +9,8 @@
 
 #include "types.h"
 
+#include <ucs/datastruct/list_types.h>
+#include <ucs/type/component.h>
 #include <ucs/type/status.h>
 #include <ucs/sys/compiler_def.h>
 #include <stdio.h>
@@ -34,27 +36,27 @@ BEGIN_C_DECLS
  */
 
 typedef struct ucs_config_parser {
-    int              (*read) (const char *buf, void *dest, const void *arg);
-    int              (*write)(char *buf, size_t max, void *src, const void *arg);
-    ucs_status_t     (*clone)(void *src, void *dest, const void *arg);
-    void             (*release)(void *ptr, const void *arg);
-    void             (*help)(char *buf, size_t max, const void *arg);
-    const void       *arg;
+    int                      (*read) (const char *buf, void *dest, const void *arg);
+    int                      (*write)(char *buf, size_t max, void *src, const void *arg);
+    ucs_status_t             (*clone)(void *src, void *dest, const void *arg);
+    void                     (*release)(void *ptr, const void *arg);
+    void                     (*help)(char *buf, size_t max, const void *arg);
+    const void               *arg;
 } ucs_config_parser_t;
 
 
 typedef struct ucs_config_array {
-    size_t              elem_size;
-    ucs_config_parser_t parser;
+    size_t                   elem_size;
+    ucs_config_parser_t      parser;
 } ucs_config_array_t;
 
 
 typedef struct ucs_config_field {
-    const char           *name;
-    const char           *dfl_value;
-    const char           *doc;
-    size_t               offset;
-    ucs_config_parser_t  parser;
+    const char               *name;
+    const char               *dfl_value;
+    const char               *doc;
+    size_t                   offset;
+    ucs_config_parser_t      parser;
 } ucs_config_field_t;
 
 
@@ -65,11 +67,32 @@ typedef struct ucs_ib_port_spec {
 
 
 typedef struct ucs_range_spec {
-    unsigned    first;  /* the first value in the range */
-    unsigned    last;   /* the last value in the range */
+    unsigned                 first;  /* the first value in the range */
+    unsigned                 last;   /* the last value in the range */
 } ucs_range_spec_t;
 
 
+typedef struct ucs_config_global_list_entry {
+    ucs_list_link_t          list;
+    const char               *name;
+    const char               *prefix;
+    ucs_config_field_t       *fields;
+    size_t                   size;
+} ucs_config_global_list_entry_t;
+
+
+#define UCS_CONFIG_REGISTER_TABLE(_fields, _name, _prefix, _type) \
+    UCS_STATIC_INIT { \
+        extern ucs_list_link_t ucs_config_global_list; \
+        static ucs_config_global_list_entry_t entry; \
+        entry.fields = _fields; \
+        entry.name   = _name; \
+        entry.prefix = _prefix; \
+        entry.size   = sizeof(_type); \
+        ucs_list_add_tail(&ucs_config_global_list, &entry.list); \
+    }
+
+
 /*
  * Parsing and printing different data types
  */
@@ -293,6 +316,14 @@ void ucs_config_parser_print_opts(FILE *stream, const char *title, const void *o
                                   ucs_config_field_t *fields, const char *table_prefix,
                                   ucs_config_print_flags_t flags);
 
+/**
+ * Print all options defined in the library - names, values, documentation.
+ *
+ * @param stream         Output stream to print to.
+ * @param flags          Flags which control the output.
+ */
+void ucs_config_parser_print_all_opts(FILE *stream, ucs_config_print_flags_t flags);
+
 /**
  * Read a value from options structure.
  *
@@ -316,6 +347,13 @@ ucs_status_t ucs_config_parser_get_value(void *opts, ucs_config_field_t *fields,
 ucs_status_t ucs_config_parser_set_value(void *opts, ucs_config_field_t *fields,
                                          const char *name, const char *value);
 
+/**
+ * Check all UCX_ environment variables have been used so far by the
+ * configuration parser, issue a warning if not. Called just before program exit.
+ */
+void ucs_config_parser_warn_unused_env_vars();
+
+
 /**
  * Translate configuration value of "MEMUNITS" type to actual value.
  *
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/types.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/types.h
index 0578740e4..f1e168e4f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/types.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/types.h
@@ -95,13 +95,16 @@ typedef enum {
         unsigned pad; \
     }
 
+
 /* Specific structure for an array of strings */
 #define UCS_CONFIG_STRING_ARRAY_FIELD(_array_name) \
     UCS_CONFIG_ARRAY_FIELD(char*, _array_name)
 
+
 typedef UCS_CONFIG_STRING_ARRAY_FIELD(names) ucs_config_names_array_t;
 
 /**
+ * @ingroup UCS_RESOURCE
  * BSD socket address specification.
  */
 typedef struct ucs_sock_addr {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/ucm_opts.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/ucm_opts.c
new file mode 100644
index 000000000..97d48ace8
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/config/ucm_opts.c
@@ -0,0 +1,82 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "parser.h"
+
+#include <ucm/api/ucm.h>
+#include <ucm/util/log.h>
+#include <ucm/mmap/mmap.h>
+
+
+#define UCM_CONFIG_PREFIX   "MEM_"
+
+static const char *ucm_mmap_hook_modes[] = {
+    [UCM_MMAP_HOOK_NONE]   = "none",
+    [UCM_MMAP_HOOK_RELOC]  = UCM_MMAP_HOOK_RELOC_STR,
+#if UCM_BISTRO_HOOKS
+    [UCM_MMAP_HOOK_BISTRO] = UCM_MMAP_HOOK_BISTRO_STR,
+#endif
+    [UCM_MMAP_HOOK_LAST]   = NULL
+};
+
+static ucs_config_field_t ucm_global_config_table[] = {
+  {"LOG_LEVEL", "warn",
+   "Logging level for memory events", ucs_offsetof(ucm_global_config_t, log_level),
+   UCS_CONFIG_TYPE_ENUM(ucm_log_level_names)},
+
+  {"ALLOC_ALIGN", "16",
+   "Minimal alignment of allocated blocks",
+   ucs_offsetof(ucm_global_config_t, alloc_alignment), UCS_CONFIG_TYPE_MEMUNITS},
+
+  {"EVENTS", "yes",
+   "Enable memory events",
+   ucs_offsetof(ucm_global_config_t, enable_events), UCS_CONFIG_TYPE_BOOL},
+
+  {"MMAP_HOOK_MODE", UCM_DEFAULT_HOOK_MODE_STR,
+   "MMAP hook mode\n"
+   " none   - don't set mmap hooks.\n"
+   " reloc  - use ELF relocation table to set hooks.\n"
+#if UCM_BISTRO_HOOKS
+   " bistro - use binary instrumentation to set hooks.\n"
+#endif
+   ,ucs_offsetof(ucm_global_config_t, mmap_hook_mode), UCS_CONFIG_TYPE_ENUM(ucm_mmap_hook_modes)},
+
+  {"MALLOC_HOOKS", "yes",
+   "Enable using glibc malloc hooks",
+   ucs_offsetof(ucm_global_config_t, enable_malloc_hooks),
+   UCS_CONFIG_TYPE_BOOL},
+
+  {"MALLOC_RELOC", "yes",
+   "Enable installing malloc symbols in the relocation table.\n"
+   "This is unsafe and off by default, because sometimes glibc\n"
+   "calls malloc/free without going through the relocation table,\n"
+   "which would use the original implementation and not ours.",
+   ucs_offsetof(ucm_global_config_t, enable_malloc_reloc), UCS_CONFIG_TYPE_BOOL},
+
+  {"CUDA_RELOC", "yes",
+   "Enable installing CUDA symbols in the relocation table",
+   ucs_offsetof(ucm_global_config_t, enable_cuda_reloc),
+   UCS_CONFIG_TYPE_BOOL},
+
+  {"DYNAMIC_MMAP_THRESH", "yes",
+   "Enable dynamic mmap threshold: for every released block, the\n"
+   "mmap threshold is adjusted upward to the size of the size of\n"
+   "the block, and trim threshold is adjust to twice the size of\n"
+   "the dynamic mmap threshold.\n"
+   "Note: dynamic mmap threshold is disabled when running on valgrind.",
+   ucs_offsetof(ucm_global_config_t, enable_dynamic_mmap_thresh),
+   UCS_CONFIG_TYPE_BOOL},
+
+  {NULL}
+};
+
+UCS_CONFIG_REGISTER_TABLE(ucm_global_config_table, "UCM", UCM_CONFIG_PREFIX,
+                          ucm_global_config_t)
+
+UCS_STATIC_INIT {
+    (void)ucs_config_parser_fill_opts(&ucm_global_opts, ucm_global_config_table,
+                                      NULL, UCM_CONFIG_PREFIX, 0);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.c
index 092af593c..88651a321 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.c
@@ -15,6 +15,7 @@
 void ucs_arbiter_init(ucs_arbiter_t *arbiter)
 {
     arbiter->current = NULL;
+    UCS_ARBITER_GUARD_INIT(arbiter);
 }
 
 void ucs_arbiter_group_init(ucs_arbiter_group_t *group)
@@ -96,6 +97,8 @@ void ucs_arbiter_group_schedule_nonempty(ucs_arbiter_t *arbiter,
     ucs_arbiter_elem_t *tail = group->tail;
     ucs_arbiter_elem_t *current, *head;
 
+    UCS_ARBITER_GUARD_CHECK(arbiter);
+
     ucs_assert(tail != NULL);
     head = tail->next;
 
@@ -164,7 +167,9 @@ void ucs_arbiter_dispatch_nonempty(ucs_arbiter_t *arbiter, unsigned per_group,
 
             ucs_assert(elem->group == group);
             ucs_trace_poll("dispatching arbiter element %p", elem);
+            UCS_ARBITER_GUARD_ENTER(arbiter);
             result = cb(arbiter, elem, cb_arg);
+            UCS_ARBITER_GUARD_EXIT(arbiter);
             ucs_trace_poll("dispatch result %d", result);
             ++group_dispatch_count;
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.h
index bdcc8d5c6..1c9a23bd3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/arbiter.h
@@ -11,6 +11,7 @@
 #include <ucs/datastruct/list.h>
 #include <ucs/type/status.h>
 #include <stdio.h>
+#include <ucs/debug/assert.h>
 
 /*
  *  A mechanism to arbitrate among groups of queued work elements, which attempts
@@ -92,6 +93,22 @@ typedef enum {
                                            will start from the group that returned STOP */
 } ucs_arbiter_cb_result_t;
 
+#if ENABLE_ASSERT
+#define UCS_ARBITER_GUARD                   int guard
+#define UCS_ARBITER_GUARD_INIT(_arbiter)    (_arbiter)->guard = 0
+#define UCS_ARBITER_GUARD_ENTER(_arbiter)   (_arbiter)->guard++
+#define UCS_ARBITER_GUARD_EXIT(_arbiter)    (_arbiter)->guard--
+#define UCS_ARBITER_GUARD_CHECK(_arbiter) \
+    ucs_assertv((_arbiter)->guard == 0, \
+                "scheduling group from the arbiter callback")
+#else
+#define UCS_ARBITER_GUARD
+#define UCS_ARBITER_GUARD_INIT(_arbiter)
+#define UCS_ARBITER_GUARD_ENTER(_arbiter)
+#define UCS_ARBITER_GUARD_EXIT(_arbiter)
+#define UCS_ARBITER_GUARD_CHECK(_arbiter)
+#endif
+
 
 /**
  * Arbiter callback function.
@@ -112,6 +129,7 @@ typedef ucs_arbiter_cb_result_t (*ucs_arbiter_callback_t)(ucs_arbiter_t *arbiter
  */
 struct ucs_arbiter {
     ucs_arbiter_elem_t      *current;
+    UCS_ARBITER_GUARD;
 };
 
 
@@ -317,4 +335,13 @@ ucs_arbiter_elem_is_last(ucs_arbiter_group_t *group, ucs_arbiter_elem_t *elem)
     return group->tail == elem;
 }
 
+/**
+ * @return true if element is the only one in the group
+ */
+static inline int
+ucs_arbiter_elem_is_only(ucs_arbiter_group_t *group, ucs_arbiter_elem_t *elem)
+{
+    return ucs_arbiter_elem_is_last(group, elem) && (elem->next == elem);
+}
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/callbackq.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/callbackq.h
index 1085dc46f..472e4d2cc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/callbackq.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/callbackq.h
@@ -59,6 +59,7 @@ typedef int (*ucs_callbackq_predicate_t)(const ucs_callbackq_elem_t *elem,
 
 
 /**
+ * @ingroup UCS_RESOURCE
  * Callback flags
  */
 enum ucs_callbackq_flags {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/khash.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/khash.h
index dcc923770..a9d7dc6fc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/khash.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/khash.h
@@ -379,6 +379,12 @@ static const double __ac_HASH_UPPER = 0.77;
 #define KHASH_INIT(name, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal) \
 	KHASH_INIT2(name, static kh_inline klib_unused, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal)
 
+#define KHASH_TYPE(name, khkey_t, khval_t) \
+	__KHASH_TYPE(name, khkey_t, khval_t)
+
+#define KHASH_IMPL(name, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal) \
+	__KHASH_IMPL(name, static kh_inline klib_unused, khkey_t, khval_t, kh_is_map, __hash_func, __hash_equal)
+
 /* --- BEGIN OF HASH FUNCTIONS --- */
 
 /*! @function
@@ -591,6 +597,19 @@ static kh_inline khint_t __ac_Wang_hash(khint_t key)
 		code;												\
 	} }
 
+/*! @function
+  @abstract     Iterate over the keys in the hash table
+  @param  h     Pointer to the hash table [khash_t(name)*]
+  @param  kvar  Variable to which key will be assigned
+  @param  code  Block of code to execute
+ */
+#define kh_foreach_key(h, kvar, code) { khint_t __i;        \
+    for (__i = kh_begin(h); __i != kh_end(h); ++__i) {      \
+        if (!kh_exist(h,__i)) continue;                     \
+        (kvar) = kh_key(h,__i);                             \
+        code;                                               \
+    } }
+
 /*! @function
   @abstract     Iterate over the values in the hash table
   @param  h     Pointer to the hash table [khash_t(name)*]
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/mpool.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/mpool.c
index fcb06168b..21b72b99f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/mpool.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/mpool.c
@@ -172,8 +172,8 @@ void ucs_mpool_grow(ucs_mpool_t *mp, unsigned num_elems)
                  (num_elems * ucs_mpool_elem_total_size(data));
     status = data->ops->chunk_alloc(mp, &chunk_size, &ptr);
     if (status != UCS_OK) {
-        ucs_error("Failed to allocate memory pool chunk: %s",
-                  ucs_status_string(status));
+        ucs_error("Failed to allocate memory pool (name=%s) chunk: %s",
+                  ucs_mpool_name(mp), ucs_status_string(status));
         return;
     }
 
@@ -182,8 +182,8 @@ void ucs_mpool_grow(ucs_mpool_t *mp, unsigned num_elems)
     chunk_padding    = ucs_padding((uintptr_t)(chunk + 1) + data->align_offset,
                                    data->alignment);
     chunk->elems     = (void*)(chunk + 1) + chunk_padding;
-    chunk->num_elems = (chunk_size - chunk_padding - sizeof(*chunk)) /
-                       ucs_mpool_elem_total_size(data);
+    chunk->num_elems = ucs_min(data->quota, (chunk_size - chunk_padding - sizeof(*chunk)) /
+                       ucs_mpool_elem_total_size(data));
 
     ucs_debug("mpool %s: allocated chunk %p of %lu bytes with %u elements",
               ucs_mpool_name(mp), chunk, chunk_size, chunk->num_elems);
@@ -285,7 +285,7 @@ ucs_status_t ucs_mpool_hugetlb_malloc(ucs_mpool_t *mp, size_t *size_p, void **ch
     /* First, try hugetlb */
     real_size = *size_p;
     status = ucs_sysv_alloc(&real_size, real_size * 2, (void**)&ptr, SHM_HUGETLB,
-                            &shmid UCS_MEMTRACK_NAME(ucs_mpool_name(mp)));
+                            ucs_mpool_name(mp), &shmid);
     if (status == UCS_OK) {
         chunk = ptr;
         chunk->hugetlb = 1;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/pgtable.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/pgtable.c
index a71dbfdb2..1ee0d48c2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/pgtable.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/pgtable.c
@@ -287,7 +287,10 @@ ucs_pgtable_insert_page(ucs_pgtable_t *pgtable, ucs_pgt_addr_t address,
             ++pgd->count;
             break;
         } else {
-            ucs_assert(!ucs_pgt_entry_test(pte, UCS_PGT_ENTRY_FLAG_REGION));
+            if (ucs_pgt_entry_test(pte, UCS_PGT_ENTRY_FLAG_REGION)) {
+                goto err;
+            }
+
             ucs_assertv(shift >= UCS_PGT_ENTRY_SHIFT + order,
                         "shift=%u order=%u", shift, order);  /* sub PTE should be able to hold it */
 
@@ -549,6 +552,15 @@ void ucs_pgtable_search_range(const ucs_pgtable_t *pgtable,
     ucs_pgt_region_t *last;
     unsigned order = 0;
 
+    /* if the page table is covering only part of the address space, intersect
+     * the range with page table address span */
+    if (pgtable->shift < (sizeof(uint64_t) * 8)) {
+        address = ucs_max(address, pgtable->base);
+        end     = ucs_min(end,     pgtable->base + UCS_BIT(pgtable->shift));
+    } else {
+        ucs_assert(pgtable->base == 0);
+    }
+
     last = NULL;
     while ((address <= to) && (order != UCS_PGT_ADDR_ORDER)) {
         order = ucs_pgtable_get_next_page_order(address, end);
@@ -575,6 +587,7 @@ void ucs_pgtable_purge(ucs_pgtable_t *pgtable, ucs_pgt_search_callback_t cb,
     unsigned num_regions = pgtable->num_regions;
     ucs_pgt_region_t **all_regions, **next_region, *region;
     ucs_pgt_addr_t from, to;
+    ucs_status_t status;
     unsigned i;
 
     all_regions = ucs_calloc(num_regions, sizeof(*all_regions),
@@ -595,7 +608,11 @@ void ucs_pgtable_purge(ucs_pgtable_t *pgtable, ucs_pgt_search_callback_t cb,
 
     for (i = 0; i < num_regions; ++i) {
         region = all_regions[i];
-        ucs_pgtable_remove(pgtable, region);
+        status = ucs_pgtable_remove(pgtable, region);
+        if (status != UCS_OK) {
+            ucs_warn("failed to remove pgtable region" UCS_PGT_REGION_FMT,
+                     UCS_PGT_REGION_ARG(region));
+        }
         cb(pgtable, region, arg);
     }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/queue.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/queue.h
index 7a9402f36..5c1860d70 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/queue.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/queue.h
@@ -196,7 +196,7 @@ static inline void ucs_queue_splice(ucs_queue_head_t *queue,
 #define ucs_queue_for_each(elem, queue, member) \
     for (*(queue)->ptail = NULL, \
              elem = ucs_container_of((queue)->head, typeof(*elem), member); \
-         &elem->member != NULL; \
+         (elem) != ucs_container_of(NULL, typeof(*elem), member); \
          elem = ucs_container_of(elem->member.next, typeof(*elem), member))
 
 /**
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.c
new file mode 100644
index 000000000..b8cb08cba
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.c
@@ -0,0 +1,180 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#include "strided_alloc.h"
+#include "queue.h"
+
+#include <ucs/debug/log.h>
+#include <ucs/debug/memtrack.h>
+#include <ucs/sys/checker.h>
+#include <ucs/sys/sys.h>
+
+
+#define ucs_strided_alloc_chunk_to_mem(_chunk) \
+    UCS_PTR_BYTE_OFFSET(_chunk, + sizeof(ucs_strided_alloc_chunk_t)  \
+                                - UCS_STRIDED_ALLOC_STRIDE)
+
+#define ucs_strided_alloc_mem_to_chunk(_mem) \
+    UCS_PTR_BYTE_OFFSET(_mem,   - sizeof(ucs_strided_alloc_chunk_t)  \
+                                + UCS_STRIDED_ALLOC_STRIDE)
+
+typedef struct ucs_splitalloc_chunk {
+    ucs_queue_elem_t         queue;
+} ucs_strided_alloc_chunk_t;
+
+struct ucs_strided_alloc_elem {
+    ucs_strided_alloc_elem_t *next;
+};
+
+static ucs_strided_alloc_chunk_t *
+ucs_strided_alloc_chunk_alloc(ucs_strided_alloc_t *sa, size_t chunk_size
+                              UCS_MEMTRACK_ARG)
+{
+    ucs_status_t status;
+    size_t size;
+    void *ptr;
+
+    size   = chunk_size;
+    ptr    = NULL;
+    status = ucs_mmap_alloc(&size, &ptr, 0 UCS_MEMTRACK_VAL);
+    if (status != UCS_OK) {
+        ucs_error("failed to allocate a chunk of %zu bytes", chunk_size);
+        return NULL;
+    }
+
+    return ucs_strided_alloc_mem_to_chunk(ptr);
+}
+
+static void ucs_strided_alloc_chunk_free(ucs_strided_alloc_t *sa,
+                                         ucs_strided_alloc_chunk_t *chunk,
+                                         size_t chunk_size)
+{
+    /* coverity[offset_free] */
+    ucs_mmap_free(ucs_strided_alloc_chunk_to_mem(chunk), chunk_size);
+}
+
+static void ucs_strided_alloc_push_to_freelist(ucs_strided_alloc_t *sa,
+                                               ucs_strided_alloc_elem_t *elem)
+{
+    elem->next   = sa->freelist;
+    sa->freelist = elem;
+}
+
+static void ucs_strided_alloc_calc(ucs_strided_alloc_t *sa, size_t *chunk_size,
+                                   size_t *elems_per_chunk)
+{
+    *chunk_size      = ucs_align_up_pow2(UCS_STRIDED_ALLOC_STRIDE * sa->stride_count,
+                                         ucs_get_page_size());
+    *elems_per_chunk = (UCS_STRIDED_ALLOC_STRIDE -
+                        sizeof(ucs_strided_alloc_chunk_t)) / sa->elem_size;
+}
+
+static void ucs_strided_alloc_grow(ucs_strided_alloc_t *sa UCS_MEMTRACK_ARG)
+{
+    size_t chunk_size, elems_per_chunk;
+    ucs_strided_alloc_chunk_t *chunk;
+    ucs_strided_alloc_elem_t *elem;
+    void *chunk_mem;
+    ssize_t i;
+
+    ucs_strided_alloc_calc(sa, &chunk_size, &elems_per_chunk);
+
+    chunk = ucs_strided_alloc_chunk_alloc(sa, chunk_size UCS_MEMTRACK_VAL);
+    if (chunk == NULL) {
+        return;
+    }
+
+    chunk_mem = ucs_strided_alloc_chunk_to_mem(chunk);
+    for (i = elems_per_chunk - 1; i >= 0; --i) {
+        elem = chunk_mem + (i * sa->elem_size);
+        ucs_strided_alloc_push_to_freelist(sa, elem);
+    }
+
+    ucs_queue_push(&sa->chunks, &chunk->queue);
+
+    VALGRIND_MAKE_MEM_NOACCESS(chunk_mem, chunk_size);
+}
+
+void ucs_strided_alloc_init(ucs_strided_alloc_t *sa, size_t elem_size,
+                            unsigned stride_count)
+{
+    ucs_assert(elem_size >= sizeof(ucs_strided_alloc_elem_t));
+    ucs_assert(elem_size <= (UCS_STRIDED_ALLOC_STRIDE -
+                             sizeof(ucs_strided_alloc_chunk_t)));
+    ucs_assert(stride_count >= 1);
+
+    ucs_queue_head_init(&sa->chunks);
+
+    sa->freelist     = NULL;
+    sa->elem_size    = elem_size;
+    sa->stride_count = stride_count;
+    sa->inuse_count  = 0;
+    VALGRIND_CREATE_MEMPOOL(sa, 0, 0);
+}
+
+void ucs_strided_alloc_cleanup(ucs_strided_alloc_t *sa)
+{
+    size_t chunk_size, elems_per_chunk;
+    ucs_strided_alloc_chunk_t *chunk;
+
+    VALGRIND_DESTROY_MEMPOOL(sa);
+
+    ucs_strided_alloc_calc(sa, &chunk_size, &elems_per_chunk);
+
+    while (!ucs_queue_is_empty(&sa->chunks)) {
+        chunk = ucs_queue_head_elem_non_empty(&sa->chunks, ucs_strided_alloc_chunk_t,
+                                              queue);
+        VALGRIND_MAKE_MEM_DEFINED(chunk, sizeof(*chunk));
+        ucs_queue_pull_non_empty(&sa->chunks);
+        ucs_strided_alloc_chunk_free(sa, chunk, chunk_size);
+    }
+}
+
+void* ucs_strided_alloc_get(ucs_strided_alloc_t *sa, const char *alloc_name)
+{
+    ucs_strided_alloc_elem_t *elem;
+    unsigned i;
+
+    if (sa->freelist == NULL) {
+        ucs_strided_alloc_grow(sa UCS_MEMTRACK_VAL);
+    }
+
+    ucs_assert(sa->freelist != NULL);
+
+    elem         = sa->freelist;
+    VALGRIND_MAKE_MEM_DEFINED(elem, sizeof(*elem));
+    sa->freelist = elem->next;
+    VALGRIND_MAKE_MEM_NOACCESS(elem, sizeof(*elem));
+
+    for (i = 0; i < sa->stride_count; ++i) {
+        VALGRIND_MEMPOOL_ALLOC(sa, ucs_strided_elem_get(elem, 0, i),
+                               sa->elem_size);
+    }
+
+    ++sa->inuse_count;
+
+    return elem;
+}
+
+void ucs_strided_alloc_put(ucs_strided_alloc_t *sa, void *base)
+{
+    ucs_strided_alloc_elem_t *elem = base;
+    unsigned i;
+
+    ucs_assert(sa->inuse_count > 0);
+
+    ucs_strided_alloc_push_to_freelist(sa, elem);
+
+    for (i = 0; i < sa->stride_count; ++i) {
+        VALGRIND_MEMPOOL_FREE(sa, ucs_strided_elem_get(elem, 0, i));
+    }
+
+    --sa->inuse_count;
+}
+
+unsigned ucs_strided_alloc_inuse_count(ucs_strided_alloc_t *sa)
+{
+    return sa->inuse_count;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.h
new file mode 100644
index 000000000..2598ba86a
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/datastruct/strided_alloc.h
@@ -0,0 +1,134 @@
+/*
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCS_STRIDED_ALLOC_H_
+#define UCS_STRIDED_ALLOC_H_
+
+#include "queue_types.h"
+
+#include <ucs/type/status.h>
+#include <ucs/sys/compiler_def.h>
+#include <ucs/sys/math.h>
+#include <stddef.h>
+
+
+BEGIN_C_DECLS
+
+
+/* the distance between allocated elements */
+#define UCS_STRIDED_ALLOC_STRIDE (128 * UCS_KBYTE)
+
+
+/**
+ * Get a pointer to another element in the strided object
+ *
+ * Example with stride_count=3:
+ *
+ * chunk
+ * start -+
+ *        |
+ *        |
+ *        | <--    128 kB    --> . <--    128 kB    --> .
+ *        |                      .                      .
+ *        \/                     .                      .
+ *        +--------+--   ...   --+--------+--   ...   --+--------+
+ *        | stride |             | stride |             | stride |
+ * obj0:  | elem 0 |             | elem 1 |             | elem 2 |
+ *        | (base) |             |        |             |        |
+ *        +--------+--   ...   --+--------+--   ...   --+--------+
+ *                 +--------+--  ...    --+--------+--  ...    --+--------+
+ *                 | stride |             | stride |             | stride |
+ * obj1:           | elem 0 |             | elem 1 |             | elem 2 |
+ *                 | (base) |             |        |             |        |
+ *                 +--------+--  ...     -+--------+--  ...    --+--------+
+ *                          +--------+--  ...    --+--------+--   ...   --+--------+
+ *                          | stride |             | stride |             | stride |
+ * obj2:                    | elem 0 |             | elem 1 |             | elem 2 |
+ *                          | (base) |             |        |             |        |
+ *                          +--------+--  ...     -+--------+--   ...   --+--------+
+ *
+ * ...
+ *
+ * @param _elem        Pointer to the current element
+ * @param _stride_idx  Stride index of the current element
+ * @param _wanted_idx  Stride index of the desired element
+ *
+ * @return Pointer to the desired element
+ */
+#define ucs_strided_elem_get(_elem, _stride_idx, _wanted_idx) \
+    UCS_PTR_BYTE_OFFSET(_elem, UCS_STRIDED_ALLOC_STRIDE * \
+                        ((ptrdiff_t)(_wanted_idx) - (ptrdiff_t)(_stride_idx)))
+
+
+/* Forward declaration, used internally */
+typedef struct ucs_strided_alloc_elem ucs_strided_alloc_elem_t;
+
+
+/**
+ * Strided allocator - allows allocating objects which are split to several
+ * memory areas with a constant stride (gap) in-between.
+ * This improves the cache locality when the first memory area is used mostly.
+ */
+typedef struct ucs_strided_alloc {
+    ucs_strided_alloc_elem_t  *freelist;    /* LIFO of free elements */
+    ucs_queue_head_t          chunks;       /* Queue of allocated chunks */
+    size_t                    elem_size;    /* Size of a single memory area */
+    unsigned                  stride_count; /* Number of strides */
+    unsigned                  inuse_count;  /* Number of allocated elements */
+} ucs_strided_alloc_t;
+
+
+/**
+ * Initialize the split allocator context
+ *
+ * @param [in] sa            Strided allocator structure to initialize
+ * @param [in] elem_size     Size of a single stride element
+ * @param [in] stride_count  How many memory strides per object
+ */
+void ucs_strided_alloc_init(ucs_strided_alloc_t *sa, size_t elem_size,
+                            unsigned stride_count);
+
+
+/**
+ * Cleanup the split allocator context
+ *
+ * @param [in] sa           Strided allocator structure to cleanup
+ */
+void ucs_strided_alloc_cleanup(ucs_strided_alloc_t *sa);
+
+
+/**
+ * Allocate an object
+ *
+ * @param [in] sa            Strided allocator to allocate on
+ * @param [in] alloc_name    Debug name of the allocation
+ *
+ * @return Pointer to the first stride of the allocated object.
+ */
+void* ucs_strided_alloc_get(ucs_strided_alloc_t *sa, const char *alloc_name);
+
+
+/**
+ * Release an object
+ *
+ * @param [in] sa            Strided allocator to release the object to
+ * @param [in] base          Pointer to the first stride of the object to release
+ */
+void ucs_strided_alloc_put(ucs_strided_alloc_t *sa, void *base);
+
+
+/**
+ * Get the number of currently allocated objects
+ *
+ * @param [in] sa            Strided allocator to get the information for
+*
+ * @return Number of currently allocated objects
+ */
+unsigned ucs_strided_alloc_inuse_count(ucs_strided_alloc_t *sa);
+
+
+END_C_DECLS
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/assert.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/assert.c
index 43a1f335a..fbc0f8289 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/assert.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/assert.c
@@ -28,12 +28,9 @@ void ucs_fatal_error(const char *error_type, const char *file, unsigned line,
     vsnprintf(buffer, buffer_size, format, ap);
     va_end(ap);
 
-    ucs_debug_cleanup();
-    ucs_log_flush();
-
     short_file = strrchr(file, '/');
     short_file = (short_file == NULL) ? file : short_file + 1;
-    ucs_handle_error(error_type, "%13s:%-4u %s", short_file, line, buffer);
+    ucs_handle_error(error_type, "%13s:%-4u UCX %s", short_file, line, buffer);
 
     abort();
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/debug.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/debug.c
index 421672d56..e45811306 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/debug.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/debug.c
@@ -10,11 +10,12 @@
 
 #include "debug.h"
 #include "log.h"
-#include "profile.h"
 
 #include <ucs/datastruct/khash.h>
+#include <ucs/profile/profile.h>
 #include <ucs/sys/checker.h>
 #include <ucs/sys/string.h>
+#include <ucs/sys/math.h>
 #include <ucs/sys/sys.h>
 #include <sys/wait.h>
 #include <execinfo.h>
@@ -891,6 +892,7 @@ static const char *ucs_signal_cause_segv(int si_code)
 static const char *ucs_signal_cause_bus(int si_code)
 {
     switch (si_code) {
+    case BUS_ADRALN   : return "invalid address alignment";
     case BUS_ADRERR   : return "nonexistent physical address";
     case BUS_OBJERR   : return "object-specific hardware error";
     default           : return ucs_signal_cause_common(si_code);
@@ -975,6 +977,9 @@ void ucs_handle_error(const char *error_type, const char *message, ...)
     char *buffer;
     va_list ap;
 
+    ucs_debug_cleanup();
+    ucs_log_flush();
+
     buffer = ucs_alloca(buffer_size + 1);
     va_start(ap, message);
     vsnprintf(buffer, buffer_size, message, ap);
@@ -1134,6 +1139,7 @@ static int ucs_debug_backtrace_is_excluded(void *address, const char *symbol)
            !strcmp(symbol, "ucs_debug_show_innermost_source_file") ||
            !strcmp(symbol, "ucs_log_default_handler") ||
            !strcmp(symbol, "__ucs_abort") ||
+           !strcmp(symbol, "ucs_log_dispatch") ||
            !strcmp(symbol, "__ucs_log") ||
            !strcmp(symbol, "ucs_debug_send_mail") ||
            (strstr(symbol, "_L_unlock_") == symbol) ||
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.c
index d4a39f66c..2bb00ea0f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.c
@@ -32,7 +32,7 @@ const char *ucs_log_level_names[] = {
     [UCS_LOG_LEVEL_PRINT]        = "PRINT"
 };
 
-static unsigned ucs_log_num_handlers   = 0;
+static unsigned ucs_log_handlers_count = 0;
 static ucs_log_func_t ucs_log_handlers[UCS_MAX_LOG_HANDLERS];
 static int ucs_log_initialized         = 0;
 static char ucs_log_hostname[256]      = {0};
@@ -100,7 +100,7 @@ ucs_log_default_handler(const char *file, unsigned line, const char *function,
         return UCS_LOG_FUNC_RC_CONTINUE;
     }
 
-   buf = ucs_alloca(buffer_size + 1);
+    buf = ucs_alloca(buffer_size + 1);
     buf[buffer_size] = 0;
 
     vsnprintf(buf, buffer_size, format, ap);
@@ -141,18 +141,23 @@ ucs_log_default_handler(const char *file, unsigned line, const char *function,
 
 void ucs_log_push_handler(ucs_log_func_t handler)
 {
-    if (ucs_log_num_handlers < UCS_MAX_LOG_HANDLERS) {
-        ucs_log_handlers[ucs_log_num_handlers++] = handler;
+    if (ucs_log_handlers_count < UCS_MAX_LOG_HANDLERS) {
+        ucs_log_handlers[ucs_log_handlers_count++] = handler;
     }
 }
 
 void ucs_log_pop_handler()
 {
-    if (ucs_log_num_handlers > 0) {
-        --ucs_log_num_handlers;
+    if (ucs_log_handlers_count > 0) {
+        --ucs_log_handlers_count;
     }
 }
 
+unsigned ucs_log_num_handlers()
+{
+    return ucs_log_handlers_count;
+}
+
 void ucs_log_dispatch(const char *file, unsigned line, const char *function,
                       ucs_log_level_t level, const char *format, ...)
 {
@@ -162,7 +167,7 @@ void ucs_log_dispatch(const char *file, unsigned line, const char *function,
 
     /* Call handlers in reverse order */
     rc    = UCS_LOG_FUNC_RC_CONTINUE;
-    index = ucs_log_num_handlers;
+    index = ucs_log_handlers_count;
     while ((index > 0) && (rc == UCS_LOG_FUNC_RC_CONTINUE)) {
         --index;
         va_start(ap, format);
@@ -331,7 +336,7 @@ void ucs_log_cleanup()
     if (ucs_log_file_close) {
         fclose(ucs_log_file);
     }
-    ucs_log_file = NULL;
-    ucs_log_initialized  = 0;
-    ucs_log_num_handlers = 0;
+    ucs_log_file           = NULL;
+    ucs_log_initialized    = 0;
+    ucs_log_handlers_count = 0;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.h
index 4c6e724ff..ba5be3ac5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/log.h
@@ -148,6 +148,7 @@ const char *ucs_log_dump_hex(const void* data, size_t length, char *buf,
  */
 void ucs_log_push_handler(ucs_log_func_t handler);
 void ucs_log_pop_handler();
+unsigned ucs_log_num_handlers();
 
 END_C_DECLS
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.c
index c947a94dc..0fb469930 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.c
@@ -6,42 +6,34 @@
 
 #include "memtrack.h"
 
-#include <stdio.h>
-#include <string.h>
-#include <malloc.h>
-
+#include <ucs/datastruct/khash.h>
 #include <ucs/debug/log.h>
 #include <ucs/stats/stats.h>
-#include <ucs/datastruct/list.h>
-#include <ucs/datastruct/mpool.h>
-#include <ucs/datastruct/sglib_wrapper.h>
-#include <ucs/sys/compiler.h>
-#include <ucs/sys/math.h>
-#include <ucs/sys/checker.h>
-#include <ucs/sys/string.h>
 #include <ucs/sys/sys.h>
+#include <ucs/sys/math.h>
+#include <malloc.h>
+#include <stdio.h>
 
 
 #if ENABLE_MEMTRACK
 
-#define UCS_MEMTRACK_MAGIC            0x1ee7beefa880feedULL
-#define UCS_MEMTRACK_FORMAT_STRING    ("%22s: size: %9lu / %9lu\tcount: %9lu / %9lu\n")
-#define UCS_MEMTRACK_ENTRY_HASH_SIZE  127
+#define UCS_MEMTRACK_FORMAT_STRING    ("%22s: size: %9lu / %9lu\tcount: %9u / %9u\n")
 
 
-typedef struct ucs_memtrack_buffer {
-    uint64_t              magic;  /* Make sure this buffer is "memtracked" */
-    size_t                size; /* length of user-requested buffer */
-    off_t                 offset; /* Offset between result of memory allocation and the
-                                     location of this buffer struct (mainly for ucs_memalign) */
-    ucs_memtrack_entry_t  *entry; /* Entry which tracks this allocation */
-} ucs_memtrack_buffer_t;
+typedef struct ucs_memtrack_ptr {
+    size_t                  size;   /* Length of allocated buffer */
+    ucs_memtrack_entry_t    *entry; /* Entry which tracks this allocation */
+} ucs_memtrack_ptr_t;
 
+KHASH_MAP_INIT_INT64(ucs_memtrack_ptr_hash, ucs_memtrack_ptr_t)
+KHASH_MAP_INIT_STR(ucs_memtrack_entry_hash, ucs_memtrack_entry_t*);
 
 typedef struct ucs_memtrack_context {
-    int                     enabled;
-    pthread_mutex_t         lock;
-    ucs_memtrack_entry_t    *entries[UCS_MEMTRACK_ENTRY_HASH_SIZE];
+    int                              enabled;
+    pthread_mutex_t                  lock;
+    ucs_memtrack_entry_t             total;
+    khash_t(ucs_memtrack_ptr_hash)   ptrs;
+    khash_t(ucs_memtrack_entry_hash) entries;
     UCS_STATS_NODE_DECLARE(stats);
 } ucs_memtrack_context_t;
 
@@ -49,14 +41,10 @@ typedef struct ucs_memtrack_context {
 /* Global context for tracking allocated memory */
 static ucs_memtrack_context_t ucs_memtrack_context = {
     .enabled = 0,
-    .lock    = PTHREAD_MUTEX_INITIALIZER
+    .lock    = PTHREAD_MUTEX_INITIALIZER,
+    .total   = {0}
 };
 
-SGLIB_DEFINE_LIST_PROTOTYPES(ucs_memtrack_entry_t, ucs_memtrack_entry_compare, next)
-SGLIB_DEFINE_HASHED_CONTAINER_PROTOTYPES(ucs_memtrack_entry_t,
-                                         UCS_MEMTRACK_ENTRY_HASH_SIZE,
-                                         ucs_memtrack_entry_hash)
-
 #if ENABLE_STATS
 static ucs_stats_class_t ucs_memtrack_stats_class = {
     .name = "memtrack",
@@ -68,299 +56,177 @@ static ucs_stats_class_t ucs_memtrack_stats_class = {
 };
 #endif
 
+static void ucs_memtrack_entry_reset(ucs_memtrack_entry_t *entry)
+{
+    entry->size       = 0;
+    entry->peak_size  = 0;
+    entry->count      = 0;
+    entry->peak_count = 0;
+}
 
-static inline ucs_memtrack_entry_t* ucs_memtrack_entry_new(const char* name)
+static ucs_memtrack_entry_t* ucs_memtrack_entry_get(const char* name)
 {
     ucs_memtrack_entry_t *entry;
+    khiter_t iter;
+    int ret;
 
-    entry = malloc(sizeof(*entry));
+    iter = kh_get(ucs_memtrack_entry_hash, &ucs_memtrack_context.entries, name);
+    if (iter != kh_end(&ucs_memtrack_context.entries)) {
+        return kh_val(&ucs_memtrack_context.entries, iter);
+    }
+
+    entry = malloc(sizeof(*entry) + strlen(name) + 1);
     if (entry == NULL) {
         return NULL;
     }
 
-    entry->size       = 0;
-    entry->peak_size  = 0;
-    entry->count      = 0;
-    entry->peak_count = 0;
-    ucs_snprintf_zero(entry->name, UCS_MEMTRACK_NAME_MAX, "%s", name);
-    sglib_hashed_ucs_memtrack_entry_t_add(ucs_memtrack_context.entries, entry);
+    ucs_memtrack_entry_reset(entry);
+    strcpy(entry->name, name);
+
+    iter = kh_put(ucs_memtrack_entry_hash, &ucs_memtrack_context.entries,
+                  entry->name, &ret);
+    ucs_assertv(ret == 1 || ret == 2, "ret=%d", ret);
+    kh_val(&ucs_memtrack_context.entries, iter) = entry;
+
     return entry;
 }
 
-static void ucs_memtrack_record_alloc(ucs_memtrack_buffer_t* buffer, size_t size,
-                                      off_t offset, const char *name)
+static void ucs_memtrack_entry_update(ucs_memtrack_entry_t *entry, ssize_t size)
 {
-    ucs_memtrack_entry_t *entry, search;
-    if (!ucs_memtrack_is_enabled()) {
-        goto out;
-    }
+    int count = (size < 0) ? -1 : 1;
+
+    ucs_assert((int)entry->count    >= -count);
+    ucs_assert((ssize_t)entry->size >= -size);
+    entry->count      += count;
+    entry->size       += size;
+    entry->peak_count  = ucs_max(entry->peak_count, entry->count);
+    entry->peak_size   = ucs_max(entry->peak_size,  entry->size);
+}
 
-    if (strlen(name) >= UCS_MEMTRACK_NAME_MAX - 1) {
-        ucs_fatal("memory allocation name too long: '%s' (len: %ld, max: %d)",
-                  name, strlen(name), UCS_MEMTRACK_NAME_MAX - 1);
+void ucs_memtrack_allocated(void *ptr, size_t size, const char *name)
+{
+    ucs_memtrack_entry_t *entry;
+    khiter_t iter;
+    int ret;
+
+    if ((ptr == NULL) || !ucs_memtrack_is_enabled()) {
+        return;
     }
 
-    ucs_assert(buffer != NULL);
-    ucs_assert(ucs_memtrack_context.entries != NULL); // context initialized
     pthread_mutex_lock(&ucs_memtrack_context.lock);
 
-    ucs_snprintf_zero(search.name, UCS_MEMTRACK_NAME_MAX, "%s", name);
-    entry = sglib_hashed_ucs_memtrack_entry_t_find_member(ucs_memtrack_context.entries,
-                                                          &search);
+    entry = ucs_memtrack_entry_get(name);
     if (entry == NULL) {
-        entry = ucs_memtrack_entry_new(name);
-        if (entry == NULL) {
-            goto out_unlock;
-        }
+        goto out_unlock;
     }
 
-    ucs_assert(!strcmp(name, entry->name));
-    buffer->magic   = UCS_MEMTRACK_MAGIC;
-    buffer->size    = size;
-    buffer->offset  = offset;
-    buffer->entry   = entry;
-    VALGRIND_MAKE_MEM_NOACCESS(buffer, sizeof(*buffer));
+    /* Add pointer to hash */
+    iter = kh_put(ucs_memtrack_ptr_hash, &ucs_memtrack_context.ptrs,
+                  (uintptr_t)ptr, &ret);
+    ucs_assertv(ret == 1 || ret == 2, "ret=%d", ret);
+    kh_value(&ucs_memtrack_context.ptrs, iter).entry = entry;
+    kh_value(&ucs_memtrack_context.ptrs, iter).size  = size;
 
-    /* Update total count */
-    entry->count++;
-    UCS_STATS_UPDATE_COUNTER(ucs_memtrack_context.stats, UCS_MEMTRACK_STAT_ALLOCATION_COUNT, 1);
-    entry->peak_count = ucs_max(entry->peak_count, entry->count);
+    /* update specific and global entries */
+    ucs_memtrack_entry_update(entry, size);
+    ucs_memtrack_entry_update(&ucs_memtrack_context.total, size);
 
-    /* Update total size */
-    entry->size += size;
+    UCS_STATS_UPDATE_COUNTER(ucs_memtrack_context.stats, UCS_MEMTRACK_STAT_ALLOCATION_COUNT, 1);
     UCS_STATS_UPDATE_COUNTER(ucs_memtrack_context.stats, UCS_MEMTRACK_STAT_ALLOCATION_SIZE, size);
-    entry->peak_size = ucs_max(entry->peak_size, entry->size);
 
 out_unlock:
     pthread_mutex_unlock(&ucs_memtrack_context.lock);
-out:
-    UCS_EMPTY_STATEMENT;
 }
 
-static ucs_memtrack_entry_t*
-ucs_memtrack_record_release(ucs_memtrack_buffer_t *buffer, size_t size)
+void ucs_memtrack_releasing(void* ptr)
 {
     ucs_memtrack_entry_t *entry;
+    khiter_t iter;
+    size_t size;
 
-    if (!ucs_memtrack_is_enabled()) {
-        return NULL;
+    if ((ptr == NULL) || !ucs_memtrack_is_enabled()) {
+        return;
     }
 
     pthread_mutex_lock(&ucs_memtrack_context.lock);
-    VALGRIND_MAKE_MEM_DEFINED(buffer, sizeof(*buffer));
 
-    ucs_assert_always(buffer->magic == UCS_MEMTRACK_MAGIC);
-    buffer->magic = UCS_MEMTRACK_MAGIC + 1; /* protect from double free */
-    if (size != 0) {
-        ucs_assert(buffer->size == size);
+    iter = kh_get(ucs_memtrack_ptr_hash, &ucs_memtrack_context.ptrs, (uintptr_t)ptr);
+    if (iter == kh_end(&ucs_memtrack_context.ptrs)) {
+        ucs_debug("address %p not found in memtrack ptr hash", ptr);
+        goto out_unlock;
     }
 
-    entry = buffer->entry;
-
-    /* Update total count */
-    ucs_assert(entry->count >= 1);
-    --entry->count;
+    /* remote pointer from hash */
+    entry = kh_val(&ucs_memtrack_context.ptrs, iter).entry;
+    size  = kh_val(&ucs_memtrack_context.ptrs, iter).size;
+    kh_del(ucs_memtrack_ptr_hash, &ucs_memtrack_context.ptrs, iter);
 
-    /* Update total size */
-    ucs_assert(entry->size >= buffer->size);
-    entry->size -= buffer->size;
+    /* update counts */
+    ucs_memtrack_entry_update(entry, -size);
+    ucs_memtrack_entry_update(&ucs_memtrack_context.total, -size);
 
+out_unlock:
     pthread_mutex_unlock(&ucs_memtrack_context.lock);
-    return entry;
 }
 
 void *ucs_malloc(size_t size, const char *name)
 {
-    ucs_memtrack_buffer_t *buffer;
-
-    buffer = malloc(size + (ucs_memtrack_is_enabled() ? sizeof(*buffer) : 0));
-    if ((buffer == NULL) || (!ucs_memtrack_is_enabled())) {
-        return buffer;
-    }
-
-    ucs_memtrack_record_alloc(buffer, size, 0, name);
-    return buffer + 1;
+    void *ptr = malloc(size);
+    ucs_memtrack_allocated(ptr, size, name);
+    return ptr;
 }
 
 void *ucs_calloc(size_t nmemb, size_t size, const char *name)
 {
-    ucs_memtrack_buffer_t *buffer;
-
-    buffer = calloc(1, nmemb * size + (ucs_memtrack_is_enabled() ? sizeof(*buffer) : 0));
-    if ((buffer == NULL) || (!ucs_memtrack_is_enabled())) {
-        return buffer;
-    }
-
-    ucs_memtrack_record_alloc(buffer, nmemb * size, 0, name);
-    return buffer + 1;
+    void *ptr = calloc(nmemb, size);
+    ucs_memtrack_allocated(ptr, nmemb * size, name);
+    return ptr;
 }
 
 void *ucs_realloc(void *ptr, size_t size, const char *name)
 {
-    ucs_memtrack_buffer_t *buffer = (ucs_memtrack_buffer_t*)ptr - 1;
-    ucs_memtrack_entry_t *entry;
-
-    if (!ucs_memtrack_is_enabled()) {
-        return realloc(ptr, size);
-    }
-
-    if (ptr == NULL) {
-        return ucs_malloc(size, name);
-    }
-
-    entry = ucs_memtrack_record_release(buffer, 0);
-
-    buffer = realloc((void*)buffer - buffer->offset, size + sizeof(*buffer));
-    if (buffer == NULL) {
-        return NULL;
-    }
-
-    ucs_memtrack_record_alloc(buffer, size, 0, entry->name);
-    return buffer + 1;
+    ucs_memtrack_releasing(ptr);
+    ptr = realloc(ptr, size);
+    ucs_memtrack_allocated(ptr, size, name);
+    return ptr;
 }
 
 void *ucs_memalign(size_t boundary, size_t size, const char *name)
 {
-    ucs_memtrack_buffer_t *buffer;
-    off_t offset;
-
-    if (!ucs_memtrack_is_enabled()) {
-        return memalign(boundary, size);
-    }
-
-    if (boundary > sizeof(*buffer)) {
-        buffer = memalign(boundary, size + boundary);
-        offset = boundary - sizeof(*buffer);
-    } else {
-        if (sizeof(*buffer) % boundary != 0) {
-            offset = boundary - (sizeof(*buffer) % boundary);
-        } else {
-            offset = 0;
-        }
-        buffer = memalign(boundary, size + sizeof(*buffer) + offset);
-    }
-    if ((buffer == NULL) || (!ucs_memtrack_is_enabled())) {
-        return buffer;
-    }
-
-    buffer = (void*)buffer + offset;
-    ucs_memtrack_record_alloc(buffer, size, offset, name);
-    return buffer + 1;
+    void *ptr = memalign(boundary, size);
+    ucs_memtrack_allocated(ptr, size, name);
+    return ptr;
 }
 
 void ucs_free(void *ptr)
 {
-    ucs_memtrack_buffer_t *buffer;
-
-    if ((ptr == NULL) || !ucs_memtrack_is_enabled()) {
-        free(ptr);
-        return;
-    }
-
-    buffer = (ucs_memtrack_buffer_t*)ptr - 1;
-    ucs_memtrack_record_release(buffer, 0);
-    free((void*)buffer - buffer->offset);
+    ucs_memtrack_releasing(ptr);
+    free(ptr);
 }
 
 void *ucs_mmap(void *addr, size_t length, int prot, int flags, int fd,
                off_t offset, const char *name)
 {
-    ucs_memtrack_buffer_t *buffer;
-
-    if (ucs_memtrack_is_enabled() &&
-        ((flags & MAP_FIXED) || !(prot & PROT_WRITE))) {
-        return MAP_FAILED;
+    void *ptr = mmap(addr, length, prot, flags, fd, offset);
+    if (ptr != MAP_FAILED) {
+        ucs_memtrack_allocated(ptr, length, name);
     }
-
-    buffer = mmap(addr, length + (ucs_memtrack_is_enabled() ? sizeof(*buffer) : 0),
-               prot, flags, fd, offset);
-    if ((buffer == MAP_FAILED) || (!ucs_memtrack_is_enabled())) {
-        return buffer;
-    }
-
-    if (fd > 0) {
-        memmove(buffer + 1, buffer, length);
-    }
-
-    ucs_memtrack_record_alloc(buffer, length, 0, name);
-    return buffer + 1;
-}
-
-#ifdef __USE_LARGEFILE64
-void *ucs_mmap64(void *addr, size_t size, int prot, int flags, int fd,
-                 off64_t offset, const char *name)
-{
-    ucs_memtrack_buffer_t *buffer;
-
-    if ((flags & MAP_FIXED) || !(prot & PROT_WRITE)) {
-        return NULL;
-    }
-
-    buffer = mmap64(addr, size + (ucs_memtrack_is_enabled() ? sizeof(*buffer) : 0),
-                    prot, flags, fd, offset);
-    if ((buffer == MAP_FAILED) || (!ucs_memtrack_is_enabled())) {
-        return buffer;
-    }
-
-    if (fd > 0) {
-        memmove(buffer + 1, buffer, size);
-    }
-
-    ucs_memtrack_record_alloc(buffer, size, 0, name);
-    return buffer + 1;
+    return ptr;
 }
-#endif
 
 int ucs_munmap(void *addr, size_t length)
 {
-    ucs_memtrack_buffer_t *buffer;
-
-    if (!ucs_memtrack_is_enabled()) {
-        return munmap(addr, length);
-    }
-
-    buffer = (ucs_memtrack_buffer_t*)addr - 1;
-    ucs_memtrack_record_release(buffer, length);
-    return munmap((void*)buffer - buffer->offset,
-                  length + sizeof(*buffer) + buffer->offset);
+    ucs_memtrack_releasing(addr);
+    return munmap(addr, length);
 }
 
 char *ucs_strdup(const char *src, const char *name)
 {
-    char *str;
-    size_t len = strlen(src);
-
-    str = ucs_malloc(len + 1, name);
-    if (str) {
-        memcpy(str, src, len + 1);
-    }
-
+    char *str = strdup(src);
+    ucs_memtrack_allocated(str, strlen(str) + 1, name);
     return str;
 }
 
-static unsigned ucs_memtrack_total_internal(ucs_memtrack_entry_t* total)
-{
-    struct sglib_hashed_ucs_memtrack_entry_t_iterator entry_it;
-    ucs_memtrack_entry_t *entry;
-    unsigned num_entries;
-
-    ucs_memtrack_total_reset(total);
-
-    num_entries          = 0;
-    for (entry = sglib_hashed_ucs_memtrack_entry_t_it_init(&entry_it,
-                                                           ucs_memtrack_context.entries);
-         entry != NULL;
-         entry = sglib_hashed_ucs_memtrack_entry_t_it_next(&entry_it))
-    {
-        total->size          += entry->size;
-        total->peak_size     += entry->peak_size;
-        total->count         += entry->count;
-        total->peak_count    += entry->peak_count;
-        ++num_entries;
-    }
-    return num_entries;
-}
-
 void ucs_memtrack_total(ucs_memtrack_entry_t* total)
 {
     if (!ucs_memtrack_is_enabled()) {
@@ -368,58 +234,51 @@ void ucs_memtrack_total(ucs_memtrack_entry_t* total)
     }
 
     pthread_mutex_lock(&ucs_memtrack_context.lock);
-    ucs_memtrack_total_internal(total);
+    *total = ucs_memtrack_context.total;
     pthread_mutex_unlock(&ucs_memtrack_context.lock);
 }
 
 static int ucs_memtrack_cmp_entries(const void *ptr1, const void *ptr2)
 {
-    const ucs_memtrack_entry_t *e1 = ptr1;
-    const ucs_memtrack_entry_t *e2 = ptr2;
+    ucs_memtrack_entry_t * const *e1 = ptr1;
+    ucs_memtrack_entry_t * const *e2 = ptr2;
 
-    return (int)((ssize_t)e2->peak_size - (ssize_t)e1->peak_size);
+    return (int)((ssize_t)(*e2)->peak_size - (ssize_t)(*e1)->peak_size);
 }
 
 static void ucs_memtrack_dump_internal(FILE* output_stream)
 {
-    struct sglib_hashed_ucs_memtrack_entry_t_iterator entry_it;
-    ucs_memtrack_entry_t *entry, *all_entries;
-    ucs_memtrack_entry_t total = {"", 0};
+    ucs_memtrack_entry_t *entry, **all_entries;
     unsigned num_entries, i;
 
     if (!ucs_memtrack_is_enabled()) {
         return;
     }
 
-    num_entries = ucs_memtrack_total_internal(&total);
+    /* collect all entries to one array */
+    all_entries = ucs_alloca(sizeof(*all_entries) *
+                             kh_size(&ucs_memtrack_context.entries));
+    num_entries = 0;
+    kh_foreach_value(&ucs_memtrack_context.entries, entry, {
+        all_entries[num_entries++] = entry;
+    });
+    ucs_assert(num_entries <= kh_size(&ucs_memtrack_context.entries));
+
+    /* sort entries according to peak size */
+    qsort(all_entries, num_entries, sizeof(*all_entries), ucs_memtrack_cmp_entries);
 
+    /* print title */
     fprintf(output_stream, "%31s current / peak  %16s current / peak\n", "", "");
     fprintf(output_stream, UCS_MEMTRACK_FORMAT_STRING, "TOTAL",
-            total.size, total.peak_size,
-            total.count, total.peak_count);
-
-    all_entries = malloc(sizeof(ucs_memtrack_entry_t) * num_entries);
-
-    /* Copy all entries to one array */
-    i = 0;
-    for (entry = sglib_hashed_ucs_memtrack_entry_t_it_init(&entry_it,
-                                                           ucs_memtrack_context.entries);
-         entry != NULL;
-         entry = sglib_hashed_ucs_memtrack_entry_t_it_next(&entry_it))
-    {
-        all_entries[i++] = *entry;
-    }
-    ucs_assert(i == num_entries);
+            ucs_memtrack_context.total.size, ucs_memtrack_context.total.peak_size,
+            ucs_memtrack_context.total.count, ucs_memtrack_context.total.peak_count);
 
-    /* Sort the entries from large to small */
-    qsort(all_entries, num_entries, sizeof(ucs_memtrack_entry_t), ucs_memtrack_cmp_entries);
+    /* print sorted entries */
     for (i = 0; i < num_entries; ++i) {
-        entry = &all_entries[i];
+        entry = all_entries[i];
         fprintf(output_stream, UCS_MEMTRACK_FORMAT_STRING, entry->name,
                 entry->size, entry->peak_size, entry->count, entry->peak_count);
     }
-
-    free(all_entries);
 }
 
 void ucs_memtrack_dump(FILE* output_stream)
@@ -461,7 +320,11 @@ void ucs_memtrack_init()
         return;
     }
 
-    sglib_hashed_ucs_memtrack_entry_t_init(ucs_memtrack_context.entries);
+    // TODO use ucs_memtrack_entry_reset
+    ucs_memtrack_entry_reset(&ucs_memtrack_context.total);
+    kh_init_inplace(ucs_memtrack_ptr_hash, &ucs_memtrack_context.ptrs);
+    kh_init_inplace(ucs_memtrack_entry_hash, &ucs_memtrack_context.entries);
+
     status = UCS_STATS_NODE_ALLOC(&ucs_memtrack_context.stats,
                                   &ucs_memtrack_stats_class,
                                   ucs_stats_get_root());
@@ -475,7 +338,6 @@ void ucs_memtrack_init()
 
 void ucs_memtrack_cleanup()
 {
-    struct sglib_hashed_ucs_memtrack_entry_t_iterator entry_it;
     ucs_memtrack_entry_t *entry;
 
     if (!ucs_memtrack_context.enabled) {
@@ -489,82 +351,22 @@ void ucs_memtrack_cleanup()
     /* disable before releasing the stats node */
     ucs_memtrack_context.enabled = 0;
     UCS_STATS_NODE_FREE(ucs_memtrack_context.stats);
-    for (entry = sglib_hashed_ucs_memtrack_entry_t_it_init(&entry_it,
-                                                           ucs_memtrack_context.entries);
-         entry != NULL;
-         entry = sglib_hashed_ucs_memtrack_entry_t_it_next(&entry_it))
-    {
-        sglib_hashed_ucs_memtrack_entry_t_delete(ucs_memtrack_context.entries, entry);
-        free(entry);
-    }
-    pthread_mutex_unlock(&ucs_memtrack_context.lock);
-}
 
-int ucs_memtrack_is_enabled()
-{
-    return ucs_memtrack_context.enabled;
-}
-
-size_t ucs_memtrack_adjust_alloc_size(size_t size)
-{
-    return size + sizeof(ucs_memtrack_buffer_t);
-}
-
-void ucs_memtrack_allocated(void **ptr_p, size_t *size_p, const char *name)
-{
-    ucs_memtrack_buffer_t *buffer;
-
-    if (!ucs_memtrack_is_enabled()) {
-        return;
-    }
-
-    buffer   = *ptr_p;
-    *ptr_p   = buffer + 1;
-    *size_p -= sizeof(*buffer);
-    ucs_memtrack_record_alloc(buffer, *size_p, 0, name);
-}
-
-void ucs_memtrack_releasing(void **ptr_p)
-{
-    ucs_memtrack_buffer_t *buffer;
+    /* cleanup entries */
+    kh_foreach_value(&ucs_memtrack_context.entries, entry, {
+         free(entry);
+    });
 
-    if (!ucs_memtrack_is_enabled()) {
-        return;
-    }
-
-    buffer = *ptr_p -= sizeof(*buffer);
-    ucs_memtrack_record_release(buffer, 0);
-}
+    /* destroy hash tables */
+    kh_destroy_inplace(ucs_memtrack_entry_hash, &ucs_memtrack_context.entries);
+    kh_destroy_inplace(ucs_memtrack_ptr_hash, &ucs_memtrack_context.ptrs);
 
-void ucs_memtrack_releasing_adjusted(void *ptr)
-{
-    ucs_memtrack_record_release(ptr, 0);
-}
-
-static uint64_t ucs_memtrack_entry_hash(ucs_memtrack_entry_t *entry)
-{
-    return ucs_string_to_id(entry->name);
+    pthread_mutex_unlock(&ucs_memtrack_context.lock);
 }
 
-static int ucs_memtrack_entry_compare(ucs_memtrack_entry_t *entry1,
-                                      ucs_memtrack_entry_t *entry2)
+int ucs_memtrack_is_enabled()
 {
-    return strcmp(entry1->name, entry2->name);
+    return ucs_memtrack_context.enabled;
 }
 
-SGLIB_DEFINE_LIST_FUNCTIONS(ucs_memtrack_entry_t, ucs_memtrack_entry_compare, next)
-SGLIB_DEFINE_HASHED_CONTAINER_FUNCTIONS(ucs_memtrack_entry_t,
-                                        UCS_MEMTRACK_ENTRY_HASH_SIZE,
-                                        ucs_memtrack_entry_hash)
-
 #endif
-
-
-void ucs_memtrack_total_reset(ucs_memtrack_entry_t* total)
-{
-    ucs_snprintf_zero(total->name, UCS_MEMTRACK_NAME_MAX, "total");
-    total->size          = 0;
-    total->peak_size     = 0;
-    total->count         = 0;
-    total->peak_count    = 0;
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.h
index 695620ecb..b02cd10c5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/memtrack.h
@@ -12,12 +12,11 @@
 #endif
 
 #include <ucs/sys/compiler_def.h>
-
-#include <sys/types.h>
-#include <malloc.h>
 #include <stdlib.h>
+#include <malloc.h>
 #include <stdio.h>
 
+
 BEGIN_C_DECLS
 
 enum {
@@ -26,27 +25,19 @@ enum {
     UCS_MEMTRACK_STAT_LAST
 };
 
-#define UCS_MEMTRACK_NAME_MAX  31
 
 /**
- * Allocation site entry.
+ * Allocation site entry
  */
-typedef struct ucs_memtrack_entry ucs_memtrack_entry_t;
-struct ucs_memtrack_entry {
-    char                  name[UCS_MEMTRACK_NAME_MAX];
-    size_t                size;
-    size_t                peak_size;
-    size_t                count;
-    size_t                peak_count;
-    ucs_memtrack_entry_t  *next;
-};
+typedef struct ucs_memtrack_entry {
+    size_t                  size;       /* currently allocated total size */
+    size_t                  peak_size;  /* peak allocated total size */
+    unsigned                count;      /* number of currently allocated blocks */
+    unsigned                peak_count; /* peak number of allocated blocks */
+    char                    name[0];    /* allocation name */
+} ucs_memtrack_entry_t;
 
 
-/**
- * Initialize the total allocations structure.
- */
-void ucs_memtrack_total_reset(ucs_memtrack_entry_t* total);
-
 
 #if ENABLE_MEMTRACK
 
@@ -61,16 +52,19 @@ void ucs_memtrack_total_reset(ucs_memtrack_entry_t* total);
  */
 void ucs_memtrack_init();
 
+
 /**
  * Stop trakcing memory (or decrement reference count).
  */
 void ucs_memtrack_cleanup();
 
+
 /*
  * Check if memtrack is enabled at the moment.
  */
 int ucs_memtrack_is_enabled();
 
+
 /**
  * Print a summary of memory tracked so far.
  *
@@ -78,6 +72,7 @@ int ucs_memtrack_is_enabled();
  */
 void ucs_memtrack_dump(FILE* output);
 
+
 /**
  * Calculates the total of buffers currently tracked.
  *
@@ -85,32 +80,18 @@ void ucs_memtrack_dump(FILE* output);
  */
 void ucs_memtrack_total(ucs_memtrack_entry_t* total);
 
-/**
- * Adjust size before doing custom allocation. Need to be called in order to
- * obtain the size of a custom allocation to have room for memtrack descriptor.
- */
-size_t ucs_memtrack_adjust_alloc_size(size_t size);
-
-/**
- * Track custom allocation. Need to be called after custom allocation returns,
- * it will adjust the pointer and size to user buffer instead of the memtrack
- * descriptor.
- */
-void ucs_memtrack_allocated(void **ptr_p, size_t *size_p, const char *name);
 
 /**
- * Track release of custom allocation. Need to be called before actually
- * releasing the memory.
+ * Track custom allocation. Need to be called after custom allocation returns.
  */
-void ucs_memtrack_releasing(void **ptr_p);
+void ucs_memtrack_allocated(void *ptr, size_t size, const char *name);
 
 
 /**
  * Track release of custom allocation. Need to be called before actually
- * releasing the memory. Unlike @ref ucs_memtrack_releasing(), the pointer passed
- * to this function is the actual memory block including memtrack header.
+ * releasing the memory.
  */
-void ucs_memtrack_releasing_adjusted(void *ptr);
+void ucs_memtrack_releasing(void *ptr);
 
 
 /*
@@ -124,10 +105,6 @@ void *ucs_memalign(size_t boundary, size_t size, const char *name);
 void ucs_free(void *ptr);
 void *ucs_mmap(void *addr, size_t length, int prot, int flags, int fd,
                off_t offset, const char *name);
-#ifdef __USE_LARGEFILE64
-void *ucs_mmap64(void *addr, size_t size, int prot, int flags, int fd,
-                 off64_t offset, const char *name);
-#endif
 int ucs_munmap(void *addr, size_t length);
 char *ucs_strdup(const char *src, const char *name);
 
@@ -144,10 +121,8 @@ char *ucs_strdup(const char *src, const char *name);
 #define ucs_memtrack_dump(_output)                 UCS_EMPTY_STATEMENT
 #define ucs_memtrack_total(_total)                 ucs_memtrack_total_init(_total)
 
-#define ucs_memtrack_adjust_alloc_size(_size)      (_size)
-#define ucs_memtrack_allocated(_ptr_p, _sz_p, ...) UCS_EMPTY_STATEMENT
+#define ucs_memtrack_allocated(_ptr, _sz, ...)     UCS_EMPTY_STATEMENT
 #define ucs_memtrack_releasing(_ptr)               UCS_EMPTY_STATEMENT
-#define ucs_memtrack_releasing_adjusted(_ptr)      UCS_EMPTY_STATEMENT
 
 #define ucs_malloc(_s, ...)                        malloc(_s)
 #define ucs_calloc(_n, _s, ...)                    calloc(_n, _s)
@@ -155,7 +130,6 @@ char *ucs_strdup(const char *src, const char *name);
 #define ucs_memalign(_b, _s, ...)                  memalign(_b, _s)
 #define ucs_free(_p)                               free(_p)
 #define ucs_mmap(_a, _l, _p, _fl, _fd, _o, ...)    mmap(_a, _l, _p, _fl, _fd, _o)
-#define ucs_mmap64(_a, _l, _p, _fl, _fd, _o, ...)  mmap64(_a, _l, _p, _fl, _fd, _o)
 #define ucs_munmap(_a, _l)                         munmap(_a, _l)
 #define ucs_strdup(_src, ...)                      strdup(_src)
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/profile.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/profile.h
deleted file mode 100644
index 51d2b7e2f..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/profile.h
+++ /dev/null
@@ -1,473 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#ifndef UCS_PROFILE_H_
-#define UCS_PROFILE_H_
-
-#ifdef HAVE_CONFIG_H
-#  include "config.h"
-#endif
-
-#include <ucs/config/global_opts.h>
-#include <ucs/debug/assert.h>
-#include <ucs/sys/compiler.h>
-#include <ucs/sys/preprocessor.h>
-#include <ucs/time/time.h>
-
-
-#define UCS_PROFILE_STACK_MAX 64
-
-
-/**
- * Profiling modes
- */
-enum {
-    UCS_PROFILE_MODE_ACCUM, /**< Accumulate elapsed time per location */
-    UCS_PROFILE_MODE_LOG,   /**< Record all events */
-    UCS_PROFILE_MODE_LAST
-};
-
-
-/**
- * Profiling location type
- */
-typedef enum {
-    UCS_PROFILE_TYPE_SAMPLE,        /**< Sample only */
-    UCS_PROFILE_TYPE_SCOPE_BEGIN,   /**< Begin a scope */
-    UCS_PROFILE_TYPE_SCOPE_END,     /**< End a scope */
-    UCS_PROFILE_TYPE_REQUEST_NEW,   /**< New asynchronous request */
-    UCS_PROFILE_TYPE_REQUEST_EVENT, /**< Some progress is made on a request */
-    UCS_PROFILE_TYPE_REQUEST_FREE,  /**< Asynchronous request released */
-    UCS_PROFILE_TYPE_LAST
-} ucs_profile_type_t;
-
-
-/**
- * Profile output file header
- */
-typedef struct ucs_profile_header {
-    char                     cmdline[1024]; /**< Command line */
-    char                     hostname[40];  /**< Host name */
-    uint32_t                 pid;           /**< Process ID */
-    uint32_t                 mode;          /**< Profiling mode */
-    uint32_t                 num_locations; /**< Number of locations in the file */
-    uint64_t                 num_records;   /**< Number of records in the file */
-    uint64_t                 one_second;    /**< How much time is one second on the sampled machine */
-} UCS_S_PACKED ucs_profile_header_t;
-
-
-/**
- * Profile output file sample record
- */
-typedef struct ucs_profile_record {
-    uint64_t                 timestamp;     /**< Record timestamp */
-    uint64_t                 param64;       /**< Custom 64-bit parameter */
-    uint32_t                 param32;       /**< Custom 32-bit parameter */
-    uint32_t                 location;      /**< Location identifier */
-} UCS_S_PACKED ucs_profile_record_t;
-
-
-/**
- * Profile location record
- */
-typedef struct ucs_profile_location {
-    char                     file[64];      /**< Source file name */
-    char                     function[64];  /**< Function name */
-    char                     name[32];      /**< User-provided name */
-    int                      *loc_id_p;     /**< Back-pointer for location ID */
-    int                      line;          /**< Source line number */
-    uint8_t                  type;          /**< From ucs_profile_type_t */
-    uint64_t                 total_time;    /**< Total interval from previous location */
-    size_t                   count;         /**< Number of times we've hit this location */
-} UCS_S_PACKED ucs_profile_location_t;
-
-
-/**
- * Profiling global context
- */
-typedef struct ucs_profile_global_context {
-
-    ucs_profile_location_t   *locations;    /**< Array of all locations */
-    unsigned                 num_locations; /**< Number of valid locations */
-    unsigned                 max_locations; /**< Size of locations array */
-
-    struct {
-        ucs_profile_record_t *start, *end;  /**< Circular log buffer */
-        ucs_profile_record_t *current;      /**< Current log pointer */
-        int                  wraparound;    /**< Whether log was rotated */
-    } log;
-
-    struct {
-        int                  stack_top;     /**< Index of stack top */
-        ucs_time_t           stack[UCS_PROFILE_STACK_MAX]; /**< Timestamps for each nested scope */
-    } accum;
-
-} ucs_profile_global_context_t;
-
-
-/**
- * Initialize profiling system.
- */
-void ucs_profile_global_init();
-
-
-/**
- * Save and cleanup profiling.
- */
-void ucs_profile_global_cleanup();
-
-
-/**
- * Save and reset profiling.
- */
-void ucs_profile_dump();
-
-
-#if HAVE_PROFILING
-
-extern const char *ucs_profile_mode_names[];
-
-/*
- * Register a profiling location - should be called once per location in the
- * code, before the first record of each such location is made.
- * Should not be used directly - use UCS_PROFILE macros instead.
- *
- * @param [in]  type      Location type.
- * @param [in]  file      Source file name.
- * @param [in]  line      Source line number.
- * @param [in]  function  Calling function name.
- * @param [in]  name      Location name.
- * @param [out] loc_id_p  Filled with location ID:
- *                          0   - profiling is disabled
- *                          >0  - location index + 1
- */
-void ucs_profile_get_location(ucs_profile_type_t type, const char *name,
-                              const char *file, int line, const char *function,
-                              int *loc_id_p);
-
-
-/*
- * Store a new record with the given data.
- * Should not be used directly - use UCS_PROFILE macros instead.
- *
- * @param [in]     type        Location type.
- * @param [in]     name        Location name.
- * @param [in]     param32     custom 32-bit parameter.
- * @param [in]     param64     custom 64-bit parameter.
- * @param [in]     file        Source file name.
- * @param [in]     line        Source line number.
- * @param [in]     function    Calling function name.
- * @param [in,out] loc_id_p    Variable used to maintain the location ID.
- */
-static inline void ucs_profile_record(ucs_profile_type_t type, const char *name,
-                                      uint32_t param32, uint64_t param64,
-                                      const char *file, int line,
-                                      const char *function, int *loc_id_p)
-{
-    extern ucs_profile_global_context_t ucs_profile_ctx;
-    ucs_profile_global_context_t *ctx = &ucs_profile_ctx;
-    ucs_profile_record_t   *rec;
-    ucs_profile_location_t *loc;
-    ucs_time_t current_time;
-    int loc_id;
-
-retry:
-    loc_id = *loc_id_p;
-    if (ucs_likely(loc_id == 0)) {
-        return;
-    }
-
-    if (ucs_unlikely(loc_id == -1)) {
-        ucs_profile_get_location(type, name, file, line, function, loc_id_p);
-        goto retry;
-    }
-
-    current_time = ucs_get_time();
-    if (ucs_global_opts.profile_mode & UCS_BIT(UCS_PROFILE_MODE_ACCUM)) {
-        loc              = &ctx->locations[loc_id - 1];
-        switch (type) {
-        case UCS_PROFILE_TYPE_SCOPE_BEGIN:
-            ctx->accum.stack[++ctx->accum.stack_top] = current_time;
-            ucs_assert(ctx->accum.stack_top < UCS_PROFILE_STACK_MAX);
-            break;
-        case UCS_PROFILE_TYPE_SCOPE_END:
-            ucs_assert(ctx->accum.stack_top >= 0);
-            loc->total_time += current_time - ctx->accum.stack[ctx->accum.stack_top];
-            --ctx->accum.stack_top;
-            break;
-        default:
-            break;
-        }
-        ++loc->count;
-    }
-    if (ucs_global_opts.profile_mode & UCS_BIT(UCS_PROFILE_MODE_LOG)) {
-        rec              = ctx->log.current;
-        rec->timestamp   = current_time;
-        rec->param64     = param64;
-        rec->param32     = param32;
-        rec->location    = loc_id - 1;
-        if (++ctx->log.current >= ctx->log.end) {
-            ctx->log.current    = ctx->log.start;
-            ctx->log.wraparound = 1;
-        }
-    }
-}
-
-/* Helper macro */
-#define _UCS_PROFILE_RECORD(_type, _name, _param64, _param32, _loc_id_p) \
-    ucs_profile_record((_type), (_name), (_param64), (_param32),  __FILE__, \
-                       __LINE__, __FUNCTION__, (_loc_id_p))
-
-
-/* Helper macro */
-#define __UCS_PROFILE_CODE(_name, _loop_var) \
-    int _loop_var ; \
-    for (({ UCS_PROFILE_SCOPE_BEGIN(); _loop_var = 1;}); \
-         _loop_var; \
-         ({ UCS_PROFILE_SCOPE_END(_name); _loop_var = 0;}))
-
-
-/* Helper macro */
-#define _UCS_PROFILE_CODE(_name, _var_suffix) \
-    __UCS_PROFILE_CODE(_name, UCS_PP_TOKENPASTE(loop, _var_suffix))
-
-
-/**
- * Record a profiling event.
- *
- * @param _type     Event type.
- * @param _name     Event name.
- * @param _param32  Custom 32-bit parameter.
- * @param _param64  Custom 64-bit parameter.
- */
-#define UCS_PROFILE(_type, _name, _param32, _param64) \
-    { \
-        static int loc_id = -1; \
-        _UCS_PROFILE_RECORD((_type), (_name), (_param32), (_param64), &loc_id); \
-    }
-
-
-/**
- * Record a profiling sample event.
- *
- * @param _name   Event name.
- */
-#define UCS_PROFILE_SAMPLE(_name) \
-    UCS_PROFILE(UCS_PROFILE_TYPE_SAMPLE, (_name), 0, 0)
-
-
-/**
- * Record a scope-begin profiling event.
- */
-#define UCS_PROFILE_SCOPE_BEGIN() \
-    { \
-        UCS_PROFILE(UCS_PROFILE_TYPE_SCOPE_BEGIN, "", 0, 0); \
-        ucs_compiler_fence(); \
-    }
-
-
-/**
- * Record a scope-end profiling event.
- *
- * @param _name   Scope name.
- */
-#define UCS_PROFILE_SCOPE_END(_name) \
-    { \
-        ucs_compiler_fence(); \
-        UCS_PROFILE(UCS_PROFILE_TYPE_SCOPE_END, _name, 0, 0); \
-    }
-
-
-/**
- * Declare a profiled scope of code.
- *
- * Usage:
- *  UCS_PROFILE_CODE(<name>) {
- *     <code>
- *  }
- *
- * @param _name   Scope name.
- */
-#define UCS_PROFILE_CODE(_name) \
-    _UCS_PROFILE_CODE(_name, UCS_PP_UNIQUE_ID)
-
-
-/**
- * Create a profiled function.
- *
- * Usage:
- *  UCS_PROFILE_FUNC(<retval>, <name>, (a, b), int a, char b)
- *
- * @param _ret_type   Function return type.
- * @param _name       Function name.
- * @param _arglist    List of argument *names* only.
- * @param ...         Argument declarations (with types).
- */
-#define UCS_PROFILE_FUNC(_ret_type, _name, _arglist, ...) \
-    static UCS_F_ALWAYS_INLINE _ret_type _name##_inner(__VA_ARGS__); \
-    \
-    _ret_type _name(__VA_ARGS__) { \
-        UCS_PROFILE_SCOPE_BEGIN(); \
-        _ret_type _ret = _name##_inner _arglist; \
-        UCS_PROFILE_SCOPE_END(#_name); \
-        return _ret; \
-    } \
-    static UCS_F_ALWAYS_INLINE _ret_type _name##_inner(__VA_ARGS__)
-
-
-/**
- * Create a profiled function whose return type is void.
- *
- * Usage:
- *  UCS_PROFILE_FUNC_VOID(<name>, (a, b), int a, char b)
- *
- * @param _name       Function name.
- * @param _arglist    List of argument *names* only.
- * @param ...         Argument declarations (with types).
- */
-#define UCS_PROFILE_FUNC_VOID(_name, _arglist, ...) \
-    static UCS_F_ALWAYS_INLINE void _name##_inner(__VA_ARGS__); \
-    \
-    void _name(__VA_ARGS__) { \
-        UCS_PROFILE_SCOPE_BEGIN(); \
-        _name##_inner _arglist; \
-        UCS_PROFILE_SCOPE_END(#_name); \
-    } \
-    static UCS_F_ALWAYS_INLINE void _name##_inner(__VA_ARGS__)
-
-
-/*
- * Profile a function call, and specify explicit name string for the profile.
- * Useful when calling a function by a pointer.
- *
- * Usage:
- *  UCS_PROFILE_NAMED_CALL("name", function, arg1, arg2)
- *
- * @param _name   Name string for the profile.
- * @param _func   Function name.
- * @param ...     Function call arguments.
- */
-#define UCS_PROFILE_NAMED_CALL(_name, _func, ...) \
-    ({ \
-        typeof(_func(__VA_ARGS__)) retval; \
-        UCS_PROFILE_SCOPE_BEGIN(); \
-        retval = _func(__VA_ARGS__); \
-        UCS_PROFILE_SCOPE_END(_name); \
-        retval; \
-    })
-
-
-/*
- * Profile a function call.
- *
- * Usage:
- *  UCS_PROFILE_CALL(function, arg1, arg2)
- *
- * @param _func   Function name.
- * @param ...     Function call arguments.
- */
-#define UCS_PROFILE_CALL(_func, ...) \
-    UCS_PROFILE_NAMED_CALL(#_func, _func, ## __VA_ARGS__)
-
-
-/*
- * Profile a function call which does not return a value, and specify explicit
- * name string for the profile. Useful when calling a function by a pointer.
- *
- * Usage:
- *  UCS_PROFILE_NAMED_CALL_VOID("name", function, arg1, arg2)
- *
- * @param _name   Name string for the profile.
- * @param _func   Function name.
- * @param ...     Function call arguments.
- */
-#define UCS_PROFILE_NAMED_CALL_VOID(_name, _func, ...) \
-    { \
-        UCS_PROFILE_SCOPE_BEGIN(); \
-        _func(__VA_ARGS__); \
-        UCS_PROFILE_SCOPE_END(_name); \
-    }
-
-
-/*
- * Profile a function call which does not return a value.
- *
- * Usage:
- *  UCS_PROFILE_CALL_VOID(function, arg1, arg2)
- *
- * @param _func   Function name.
- * @param ...     Function call arguments.
- */
-#define UCS_PROFILE_CALL_VOID(_func, ...) \
-    UCS_PROFILE_NAMED_CALL_VOID(#_func, _func, ## __VA_ARGS__)
-
-
-/*
- * Profile a new request allocation.
- *
- * @param _req      Request pointer.
- * @param _name     Allocation site name.
- * @param _param32  Custom 32-bit parameter.
- */
-#define UCS_PROFILE_REQUEST_NEW(_req, _name, _param32) \
-    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_NEW, (_name), (_param32), (uintptr_t)(_req));
-
-
-/*
- * Profile a request progress event.
- *
- * @param _req      Request pointer.
- * @param _name     Event name.
- * @param _param32  Custom 32-bit parameter.
- */
-#define UCS_PROFILE_REQUEST_EVENT(_req, _name, _param32) \
-    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_EVENT, (_name), (_param32), (uintptr_t)(_req));
-
-
-/*
- * Profile a request progress event with status check.
- *
- * @param _req      Request pointer.
- * @param _name     Event name.
- * @param _param32  Custom 32-bit parameter.
- * @param _status   Status of the last progress event.
- */
-#define UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(_req, _name, _param32, _status) \
-    if (!UCS_STATUS_IS_ERR(_status)) { \
-        UCS_PROFILE_REQUEST_EVENT((_req), (_name), (_param32)); \
-    }
-
-
-/*
- * Profile a request release.
- *
- * @param _req      Request pointer.
- */
-#define UCS_PROFILE_REQUEST_FREE(_req) \
-    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_FREE, "", 0, (uintptr_t)(_req));
-
-
-#else
-
-#define UCS_PROFILE(...)                                    UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_SAMPLE(_name)                           UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_SCOPE_BEGIN()                           UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_SCOPE_END(_name)                        UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_CODE(_name)
-#define UCS_PROFILE_FUNC(_ret_type, _name, _arglist, ...)   _ret_type _name(__VA_ARGS__)
-#define UCS_PROFILE_FUNC_VOID(_name, _arglist, ...)         void _name(__VA_ARGS__)
-#define UCS_PROFILE_NAMED_CALL(_name, _func, ...)           _func(__VA_ARGS__)
-#define UCS_PROFILE_CALL(_func, ...)                        _func(__VA_ARGS__)
-#define UCS_PROFILE_NAMED_CALL_VOID(_name, _func, ...)      _func(__VA_ARGS__)
-#define UCS_PROFILE_CALL_VOID(_func, ...)                   _func(__VA_ARGS__)
-#define UCS_PROFILE_REQUEST_NEW(...)                        UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_REQUEST_EVENT(...)                      UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(...)         UCS_EMPTY_STATEMENT
-#define UCS_PROFILE_REQUEST_FREE(...)                       UCS_EMPTY_STATEMENT
-
-#endif
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/profile.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile.c
similarity index 60%
rename from src/ucs/debug/profile.c
rename to src/ucs/profile/profile.c
index 1bb9eb91f..8f7c1c010 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/debug/profile.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile.c
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
@@ -9,8 +9,33 @@
 #include <ucs/debug/log.h>
 #include <ucs/sys/string.h>
 #include <ucs/sys/sys.h>
+#include <ucs/time/time.h>
+#include <pthread.h>
+
+
+/**
+ * Profiling global context
+ */
+typedef struct ucs_profile_global_context {
+
+    ucs_profile_location_t   *locations;    /**< Array of all locations */
+    unsigned                 num_locations; /**< Number of valid locations */
+    unsigned                 max_locations; /**< Size of locations array */
+    pthread_mutex_t          mutex;         /**< Protects updating the locations array */
+
+    struct {
+        ucs_profile_record_t *start, *end;  /**< Circular log buffer */
+        ucs_profile_record_t *current;      /**< Current log pointer */
+        int                  wraparound;    /**< Whether log was rotated */
+    } log;
+
+    struct {
+        int                  stack_top;     /**< Index of stack top */
+        ucs_time_t           stack[UCS_PROFILE_STACK_MAX]; /**< Timestamps for each nested scope */
+    } accum;
+
+} ucs_profile_global_context_t;
 
-#if HAVE_PROFILING
 
 const char *ucs_profile_mode_names[] = {
     [UCS_PROFILE_MODE_ACCUM] = "accum",
@@ -27,6 +52,7 @@ ucs_profile_global_context_t ucs_profile_ctx = {
     .accum.stack_top = -1,
     .num_locations   = 0,
     .max_locations   = 0,
+    .mutex           = PTHREAD_MUTEX_INITIALIZER
 };
 
 static void ucs_profile_file_write_data(int fd, void *data, size_t size)
@@ -96,22 +122,58 @@ static void ucs_profile_write()
     close(fd);
 }
 
-void ucs_profile_get_location(ucs_profile_type_t type, const char *name,
-                              const char *file, int line, const char *function,
-                              int *loc_id_p)
+/*
+ * Register a profiling location - should be called once per location in the
+ * code, before the first record of each such location is made.
+ * SHOULD NOT be used directly - use UCS_PROFILE macros instead.
+ *
+ * @param [in]  type      Location type.
+ * @param [in]  file      Source file name.
+ * @param [in]  line      Source line number.
+ * @param [in]  function  Calling function name.
+ * @param [in]  name      Location name.
+ * @param [out] loc_id_p  Filled with location ID:
+ *                          0   - profiling is disabled
+ *                          >0  - location index + 1
+ */
+static void ucs_profile_get_location(ucs_profile_type_t type, const char *name,
+                                     const char *file, int line,
+                                     const char *function, volatile int *loc_id_p)
 {
+    ucs_profile_global_context_t *ctx = &ucs_profile_ctx;
     ucs_profile_location_t *loc;
     int location;
+    int i;
+
+    pthread_mutex_lock(&ucs_profile_ctx.mutex);
+
+    if (*loc_id_p == 0) {
+        goto out_unlock;
+    }
 
     /* Check if profiling is disabled */
     if (!ucs_global_opts.profile_mode) {
         *loc_id_p = 0;
-        return;
+        goto out_unlock;
     }
 
     /* Location ID must be uninitialized */
     ucs_assert(*loc_id_p == -1);
 
+    for (i = 0; i < ctx->num_locations; ++i) {
+        loc = &ctx->locations[i];
+
+        if ((type == loc->type) &&
+            (line == loc->line) &&
+            !strcmp(loc->name, name) &&
+            !strcmp(loc->file, basename(file)) &&
+            !strcmp(loc->function, function)) {
+
+            *loc_id_p = i + 1;
+            goto out_unlock;
+        }
+    }
+
     location = ucs_profile_ctx.num_locations++;
 
     /* Reallocate array if needed */
@@ -124,7 +186,7 @@ void ucs_profile_get_location(ucs_profile_type_t type, const char *name,
         if (ucs_profile_ctx.locations == NULL) {
             ucs_warn("failed to expand locations array");
             *loc_id_p = 0;
-            return;
+            goto out_unlock;
         }
     }
 
@@ -138,9 +200,68 @@ void ucs_profile_get_location(ucs_profile_type_t type, const char *name,
     loc->total_time = 0;
     loc->count      = 0;
     loc->loc_id_p   = loc_id_p;
+
+    ucs_memory_cpu_store_fence();
     *loc_id_p       = location + 1;
+
+out_unlock:
+    pthread_mutex_unlock(&ucs_profile_ctx.mutex);
+}
+
+void ucs_profile_record(ucs_profile_type_t type, const char *name,
+                        uint32_t param32, uint64_t param64, const char *file,
+                        int line, const char *function, volatile int *loc_id_p)
+{
+    extern ucs_profile_global_context_t ucs_profile_ctx;
+    ucs_profile_global_context_t *ctx = &ucs_profile_ctx;
+    ucs_profile_record_t   *rec;
+    ucs_profile_location_t *loc;
+    ucs_time_t current_time;
+    int loc_id;
+
+    /* If the location id is -1 or 0, need to re-read it with lock held */
+    if (ucs_unlikely((loc_id = *loc_id_p) <= 0)) {
+        ucs_profile_get_location(type, name, file, line, function, loc_id_p);
+        if ((loc_id = *loc_id_p) == 0) {
+            return;
+        }
+    }
+
+    ucs_memory_cpu_load_fence();
+    ucs_assert(*loc_id_p                    != 0);
+    ucs_assert(ucs_global_opts.profile_mode != 0);
+
+    current_time = ucs_get_time();
+    if (ucs_global_opts.profile_mode & UCS_BIT(UCS_PROFILE_MODE_ACCUM)) {
+        loc = &ctx->locations[loc_id - 1];
+        switch (type) {
+        case UCS_PROFILE_TYPE_SCOPE_BEGIN:
+            ctx->accum.stack[++ctx->accum.stack_top] = current_time;
+            break;
+        case UCS_PROFILE_TYPE_SCOPE_END:
+            loc->total_time += current_time - ctx->accum.stack[ctx->accum.stack_top];
+            --ctx->accum.stack_top;
+            break;
+        default:
+            break;
+        }
+        ++loc->count;
+    }
+
+    if (ucs_global_opts.profile_mode & UCS_BIT(UCS_PROFILE_MODE_LOG)) {
+        rec              = ctx->log.current;
+        rec->timestamp   = current_time;
+        rec->param64     = param64;
+        rec->param32     = param32;
+        rec->location    = loc_id - 1;
+        if (++ctx->log.current >= ctx->log.end) {
+            ctx->log.current    = ctx->log.start;
+            ctx->log.wraparound = 1;
+        }
+    }
 }
 
+
 void ucs_profile_global_init()
 {
     size_t num_records;
@@ -185,6 +306,7 @@ static void ucs_profile_reset_locations()
 {
     ucs_profile_location_t *loc;
 
+    pthread_mutex_lock(&ucs_profile_ctx.mutex);
     for (loc = ucs_profile_ctx.locations;
          loc < ucs_profile_ctx.locations + ucs_profile_ctx.num_locations;
          ++loc)
@@ -196,6 +318,7 @@ static void ucs_profile_reset_locations()
     ucs_profile_ctx.max_locations = 0;
     ucs_free(ucs_profile_ctx.locations);
     ucs_profile_ctx.locations = NULL;
+    pthread_mutex_unlock(&ucs_profile_ctx.mutex);
 }
 
 void ucs_profile_global_cleanup()
@@ -228,19 +351,3 @@ void ucs_profile_dump()
         ucs_profile_ctx.log.current    = ucs_profile_ctx.log.start;
     }
 }
-
-#else
-
-void ucs_profile_global_init()
-{
-}
-
-void ucs_profile_global_cleanup()
-{
-}
-
-void ucs_profile_dump()
-{
-}
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile.h
new file mode 100644
index 000000000..e8e7fe857
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile.h
@@ -0,0 +1,20 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#ifndef UCS_PROFILE_H_
+#define UCS_PROFILE_H_
+
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+#if HAVE_PROFILING
+#  include "profile_on.h"
+#else
+#  include "profile_off.h"
+#endif
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_defs.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_defs.h
new file mode 100644
index 000000000..930ed6bbf
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_defs.h
@@ -0,0 +1,106 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#ifndef UCS_PROFILE_DEFS_H_
+#define UCS_PROFILE_DEFS_H_
+
+#include <ucs/config/global_opts.h>
+#include <ucs/sys/compiler_def.h>
+#include <ucs/time/time_def.h>
+
+
+BEGIN_C_DECLS
+
+#define UCS_PROFILE_STACK_MAX 64
+
+
+/**
+ * Profiling modes
+ */
+enum {
+    UCS_PROFILE_MODE_ACCUM, /**< Accumulate elapsed time per location */
+    UCS_PROFILE_MODE_LOG,   /**< Record all events */
+    UCS_PROFILE_MODE_LAST
+};
+
+
+/**
+ * Profiling location type
+ */
+typedef enum {
+    UCS_PROFILE_TYPE_SAMPLE,        /**< Sample only */
+    UCS_PROFILE_TYPE_SCOPE_BEGIN,   /**< Begin a scope */
+    UCS_PROFILE_TYPE_SCOPE_END,     /**< End a scope */
+    UCS_PROFILE_TYPE_REQUEST_NEW,   /**< New asynchronous request */
+    UCS_PROFILE_TYPE_REQUEST_EVENT, /**< Some progress is made on a request */
+    UCS_PROFILE_TYPE_REQUEST_FREE,  /**< Asynchronous request released */
+    UCS_PROFILE_TYPE_LAST
+} ucs_profile_type_t;
+
+
+/**
+ * Profile output file header
+ */
+typedef struct ucs_profile_header {
+    char                     cmdline[1024]; /**< Command line */
+    char                     hostname[40];  /**< Host name */
+    uint32_t                 pid;           /**< Process ID */
+    uint32_t                 mode;          /**< Profiling mode */
+    uint32_t                 num_locations; /**< Number of locations in the file */
+    uint64_t                 num_records;   /**< Number of records in the file */
+    uint64_t                 one_second;    /**< How much time is one second on the sampled machine */
+} UCS_S_PACKED ucs_profile_header_t;
+
+
+/**
+ * Profile output file sample record
+ */
+typedef struct ucs_profile_record {
+    uint64_t                 timestamp;     /**< Record timestamp */
+    uint64_t                 param64;       /**< Custom 64-bit parameter */
+    uint32_t                 param32;       /**< Custom 32-bit parameter */
+    uint32_t                 location;      /**< Location identifier */
+} UCS_S_PACKED ucs_profile_record_t;
+
+
+/**
+ * Profile location record
+ */
+typedef struct ucs_profile_location {
+    char                     file[64];      /**< Source file name */
+    char                     function[64];  /**< Function name */
+    char                     name[32];      /**< User-provided name */
+    volatile int             *loc_id_p;     /**< Back-pointer for location ID */
+    int                      line;          /**< Source line number */
+    uint8_t                  type;          /**< From ucs_profile_type_t */
+    uint64_t                 total_time;    /**< Total interval from previous location */
+    size_t                   count;         /**< Number of times we've hit this location */
+} UCS_S_PACKED ucs_profile_location_t;
+
+
+extern const char *ucs_profile_mode_names[];
+
+
+/**
+ * Initialize profiling system.
+ */
+void ucs_profile_global_init();
+
+
+/**
+ * Save and cleanup profiling.
+ */
+void ucs_profile_global_cleanup();
+
+
+/**
+ * Save and reset profiling.
+ */
+void ucs_profile_dump();
+
+END_C_DECLS
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_off.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_off.h
new file mode 100644
index 000000000..06df5a137
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_off.h
@@ -0,0 +1,31 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCS_PROFILE_OFF_H_
+#define UCS_PROFILE_OFF_H_
+
+#include "profile_defs.h"
+
+#include <ucs/sys/compiler_def.h>
+
+
+#define UCS_PROFILE(...)                                    UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_SAMPLE(_name)                           UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_SCOPE_BEGIN()                           UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_SCOPE_END(_name)                        UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_CODE(_name)
+#define UCS_PROFILE_FUNC(_ret_type, _name, _arglist, ...)   _ret_type _name(__VA_ARGS__)
+#define UCS_PROFILE_FUNC_VOID(_name, _arglist, ...)         void _name(__VA_ARGS__)
+#define UCS_PROFILE_NAMED_CALL(_name, _func, ...)           _func(__VA_ARGS__)
+#define UCS_PROFILE_CALL(_func, ...)                        _func(__VA_ARGS__)
+#define UCS_PROFILE_NAMED_CALL_VOID(_name, _func, ...)      _func(__VA_ARGS__)
+#define UCS_PROFILE_CALL_VOID(_func, ...)                   _func(__VA_ARGS__)
+#define UCS_PROFILE_REQUEST_NEW(...)                        UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_REQUEST_EVENT(...)                      UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(...)         UCS_EMPTY_STATEMENT
+#define UCS_PROFILE_REQUEST_FREE(...)                       UCS_EMPTY_STATEMENT
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_on.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_on.h
new file mode 100644
index 000000000..49eaa153a
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/profile/profile_on.h
@@ -0,0 +1,276 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#ifndef UCS_PROFILE_ON_H_
+#define UCS_PROFILE_ON_H_
+
+#include "profile_defs.h"
+
+#include <ucs/sys/compiler_def.h>
+#include <ucs/sys/preprocessor.h>
+#include <ucs/config/global_opts.h>
+
+
+BEGIN_C_DECLS
+
+/* Helper macro */
+#define _UCS_PROFILE_RECORD(_type, _name, _param64, _param32, _loc_id_p) \
+    { \
+        if (*(_loc_id_p) != 0) { \
+            ucs_profile_record((_type), (_name), (_param64), (_param32),  \
+                               __FILE__, __LINE__, __FUNCTION__, (_loc_id_p)); \
+        } \
+    }
+
+
+/* Helper macro */
+#define __UCS_PROFILE_CODE(_name, _loop_var) \
+    int _loop_var ; \
+    for (({ UCS_PROFILE_SCOPE_BEGIN(); _loop_var = 1;}); \
+         _loop_var; \
+         ({ UCS_PROFILE_SCOPE_END(_name); _loop_var = 0;}))
+
+
+/* Helper macro */
+#define _UCS_PROFILE_CODE(_name, _var_suffix) \
+    __UCS_PROFILE_CODE(_name, UCS_PP_TOKENPASTE(loop, _var_suffix))
+
+
+/**
+ * Record a profiling event.
+ *
+ * @param _type     Event type.
+ * @param _name     Event name.
+ * @param _param32  Custom 32-bit parameter.
+ * @param _param64  Custom 64-bit parameter.
+ */
+#define UCS_PROFILE(_type, _name, _param32, _param64) \
+    { \
+        static int loc_id = -1; \
+        _UCS_PROFILE_RECORD((_type), (_name), (_param32), (_param64), &loc_id); \
+    }
+
+
+/**
+ * Record a profiling sample event.
+ *
+ * @param _name   Event name.
+ */
+#define UCS_PROFILE_SAMPLE(_name) \
+    UCS_PROFILE(UCS_PROFILE_TYPE_SAMPLE, (_name), 0, 0)
+
+
+/**
+ * Record a scope-begin profiling event.
+ */
+#define UCS_PROFILE_SCOPE_BEGIN() \
+    { \
+        UCS_PROFILE(UCS_PROFILE_TYPE_SCOPE_BEGIN, "", 0, 0); \
+        ucs_compiler_fence(); \
+    }
+
+
+/**
+ * Record a scope-end profiling event.
+ *
+ * @param _name   Scope name.
+ */
+#define UCS_PROFILE_SCOPE_END(_name) \
+    { \
+        ucs_compiler_fence(); \
+        UCS_PROFILE(UCS_PROFILE_TYPE_SCOPE_END, _name, 0, 0); \
+    }
+
+
+/**
+ * Declare a profiled scope of code.
+ *
+ * Usage:
+ *  UCS_PROFILE_CODE(<name>) {
+ *     <code>
+ *  }
+ *
+ * @param _name   Scope name.
+ */
+#define UCS_PROFILE_CODE(_name) \
+    _UCS_PROFILE_CODE(_name, UCS_PP_UNIQUE_ID)
+
+
+/**
+ * Create a profiled function.
+ *
+ * Usage:
+ *  UCS_PROFILE_FUNC(<retval>, <name>, (a, b), int a, char b)
+ *
+ * @param _ret_type   Function return type.
+ * @param _name       Function name.
+ * @param _arglist    List of argument *names* only.
+ * @param ...         Argument declarations (with types).
+ */
+#define UCS_PROFILE_FUNC(_ret_type, _name, _arglist, ...) \
+    static UCS_F_ALWAYS_INLINE _ret_type _name##_inner(__VA_ARGS__); \
+    \
+    _ret_type _name(__VA_ARGS__) { \
+        UCS_PROFILE_SCOPE_BEGIN(); \
+        _ret_type _ret = _name##_inner _arglist; \
+        UCS_PROFILE_SCOPE_END(#_name); \
+        return _ret; \
+    } \
+    static UCS_F_ALWAYS_INLINE _ret_type _name##_inner(__VA_ARGS__)
+
+
+/**
+ * Create a profiled function whose return type is void.
+ *
+ * Usage:
+ *  UCS_PROFILE_FUNC_VOID(<name>, (a, b), int a, char b)
+ *
+ * @param _name       Function name.
+ * @param _arglist    List of argument *names* only.
+ * @param ...         Argument declarations (with types).
+ */
+#define UCS_PROFILE_FUNC_VOID(_name, _arglist, ...) \
+    static UCS_F_ALWAYS_INLINE void _name##_inner(__VA_ARGS__); \
+    \
+    void _name(__VA_ARGS__) { \
+        UCS_PROFILE_SCOPE_BEGIN(); \
+        _name##_inner _arglist; \
+        UCS_PROFILE_SCOPE_END(#_name); \
+    } \
+    static UCS_F_ALWAYS_INLINE void _name##_inner(__VA_ARGS__)
+
+
+/*
+ * Profile a function call, and specify explicit name string for the profile.
+ * Useful when calling a function by a pointer.
+ *
+ * Usage:
+ *  UCS_PROFILE_NAMED_CALL("name", function, arg1, arg2)
+ *
+ * @param _name   Name string for the profile.
+ * @param _func   Function name.
+ * @param ...     Function call arguments.
+ */
+#define UCS_PROFILE_NAMED_CALL(_name, _func, ...) \
+    ({ \
+        typeof(_func(__VA_ARGS__)) retval; \
+        UCS_PROFILE_SCOPE_BEGIN(); \
+        retval = _func(__VA_ARGS__); \
+        UCS_PROFILE_SCOPE_END(_name); \
+        retval; \
+    })
+
+
+/*
+ * Profile a function call.
+ *
+ * Usage:
+ *  UCS_PROFILE_CALL(function, arg1, arg2)
+ *
+ * @param _func   Function name.
+ * @param ...     Function call arguments.
+ */
+#define UCS_PROFILE_CALL(_func, ...) \
+    UCS_PROFILE_NAMED_CALL(#_func, _func, ## __VA_ARGS__)
+
+
+/*
+ * Profile a function call which does not return a value, and specify explicit
+ * name string for the profile. Useful when calling a function by a pointer.
+ *
+ * Usage:
+ *  UCS_PROFILE_NAMED_CALL_VOID("name", function, arg1, arg2)
+ *
+ * @param _name   Name string for the profile.
+ * @param _func   Function name.
+ * @param ...     Function call arguments.
+ */
+#define UCS_PROFILE_NAMED_CALL_VOID(_name, _func, ...) \
+    { \
+        UCS_PROFILE_SCOPE_BEGIN(); \
+        _func(__VA_ARGS__); \
+        UCS_PROFILE_SCOPE_END(_name); \
+    }
+
+
+/*
+ * Profile a function call which does not return a value.
+ *
+ * Usage:
+ *  UCS_PROFILE_CALL_VOID(function, arg1, arg2)
+ *
+ * @param _func   Function name.
+ * @param ...     Function call arguments.
+ */
+#define UCS_PROFILE_CALL_VOID(_func, ...) \
+    UCS_PROFILE_NAMED_CALL_VOID(#_func, _func, ## __VA_ARGS__)
+
+
+/*
+ * Profile a new request allocation.
+ *
+ * @param _req      Request pointer.
+ * @param _name     Allocation site name.
+ * @param _param32  Custom 32-bit parameter.
+ */
+#define UCS_PROFILE_REQUEST_NEW(_req, _name, _param32) \
+    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_NEW, (_name), (_param32), (uintptr_t)(_req));
+
+
+/*
+ * Profile a request progress event.
+ *
+ * @param _req      Request pointer.
+ * @param _name     Event name.
+ * @param _param32  Custom 32-bit parameter.
+ */
+#define UCS_PROFILE_REQUEST_EVENT(_req, _name, _param32) \
+    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_EVENT, (_name), (_param32), (uintptr_t)(_req));
+
+
+/*
+ * Profile a request progress event with status check.
+ *
+ * @param _req      Request pointer.
+ * @param _name     Event name.
+ * @param _param32  Custom 32-bit parameter.
+ * @param _status   Status of the last progress event.
+ */
+#define UCS_PROFILE_REQUEST_EVENT_CHECK_STATUS(_req, _name, _param32, _status) \
+    if (!UCS_STATUS_IS_ERR(_status)) { \
+        UCS_PROFILE_REQUEST_EVENT((_req), (_name), (_param32)); \
+    }
+
+
+/*
+ * Profile a request release.
+ *
+ * @param _req      Request pointer.
+ */
+#define UCS_PROFILE_REQUEST_FREE(_req) \
+    UCS_PROFILE(UCS_PROFILE_TYPE_REQUEST_FREE, "", 0, (uintptr_t)(_req));
+
+
+/*
+ * Store a new record with the given data.
+ * SHOULD NOT be used directly - use UCS_PROFILE macros instead.
+ *
+ * @param [in]     type        Location type.
+ * @param [in]     name        Location name.
+ * @param [in]     param32     custom 32-bit parameter.
+ * @param [in]     param64     custom 64-bit parameter.
+ * @param [in]     file        Source file name.
+ * @param [in]     line        Source line number.
+ * @param [in]     function    Calling function name.
+ * @param [in,out] loc_id_p    Variable used to maintain the location ID.
+ */
+void ucs_profile_record(ucs_profile_type_t type, const char *name,
+                        uint32_t param32, uint64_t param64, const char *file,
+                        int line, const char *function, volatile int *loc_id_p);
+
+END_C_DECLS
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/libstats.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/libstats.h
index 425aab6a4..64abd50fc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/libstats.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/libstats.h
@@ -28,7 +28,7 @@ enum {
 #define UCS_STATS_DEFAULT_UDP_PORT 37873
 
 
-#define UCS_STAT_NAME_MAX          31
+#define UCS_STAT_NAME_MAX          39
 
 #define UCS_STATS_NODE_FMT \
     "%s%s"
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/stats_fwd.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/stats_fwd.h
index 69a0d7220..7fc0db647 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/stats_fwd.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/stats/stats_fwd.h
@@ -26,7 +26,7 @@ typedef enum {
 } ucs_stats_formats_t;
 
 extern const char *ucs_stats_formats_names[];
-ucs_stats_node_t * ucs_stats_get_root();
+ucs_stats_node_t * ucs_stats_get_root(void);
 
 END_C_DECLS
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler.h
index 8f56c470b..24e838dee 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler.h
@@ -36,23 +36,21 @@
 #endif
 
 
-/* Helper macro for address arithmetic in bytes */
-#define UCS_PTR_BYTE_OFFSET(_ptr, _offset) \
-    ((void *)((uintptr_t)(_ptr) + (_offset)))
-
 /**
  * Copy words from _src to _dst.
  *
+ * @param _dst_type    Type to use for destination buffer.
  * @param _dst         Destination buffer.
+ * @param _src_type    Type to use for source buffer.
  * @param _src         Source buffer.
- * @param _word_type   Type to use for copying.
  * @param _size        Number of bytes to copy.
  */
-#define UCS_WORD_COPY(_dst, _src, _word_type, _size) \
+#define UCS_WORD_COPY(_dst_type, _dst, _src_type, _src, _size) \
     { \
         unsigned i; \
-        for (i = 0; i < (_size) / sizeof(_word_type); ++i) { \
-            *((_word_type*)(_dst) + i) = *((_word_type*)(_src) + i); \
+        UCS_STATIC_ASSERT(sizeof(_src_type) == sizeof(_dst_type)); \
+        for (i = 0; i < (_size) / sizeof(_src_type); ++i) { \
+            *((_dst_type*)(_dst) + i) = *((_src_type*)(_src) + i); \
         } \
     }
 
@@ -67,6 +65,11 @@
         alloca(_size); \
     })
 
+/**
+ * suppress unaligned pointer warning (actual on armclang5 platform)
+ */
+#define ucs_unaligned_ptr(_ptr) ((void*)(_ptr))
+
 
 /**
  * Define cache-line padding variable inside a structure
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler_def.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler_def.h
index c337e9d60..011cb8195 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler_def.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/compiler_def.h
@@ -87,6 +87,10 @@
 /* Used for labels */
 #define UCS_EMPTY_STATEMENT {}
 
+/* Helper macro for address arithmetic in bytes */
+#define UCS_PTR_BYTE_OFFSET(_ptr, _offset) \
+    ((void *)((uintptr_t)(_ptr) + (_offset)))
+
 /**
  * Size of statically-declared array
  */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/init.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/init.c
index f3bdecd5d..2c2a51cad 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/init.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/init.c
@@ -6,10 +6,11 @@
 
 #include <ucs/sys/compiler.h>
 #include <ucs/arch/cpu.h>
+#include <ucs/config/parser.h>
 #include <ucs/debug/debug.h>
 #include <ucs/debug/log.h>
 #include <ucs/debug/memtrack.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <ucs/stats/stats.h>
 #include <ucs/async/async.h>
 #include <ucs/sys/sys.h>
@@ -86,6 +87,7 @@ static void UCS_F_CTOR ucs_init()
     ucs_async_global_init();
     ucs_debug("%s loaded at 0x%lx", ucs_debug_get_lib_path(),
               ucs_debug_get_lib_base_addr());
+    ucs_debug("cmd line: %s", ucs_get_process_cmdline());
 }
 
 static void UCS_F_DTOR ucs_cleanup(void)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.c
new file mode 100644
index 000000000..de783811f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.c
@@ -0,0 +1,214 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "memtype_cache.h"
+
+#include <ucs/arch/atomic.h>
+#include <ucs/type/class.h>
+#include <ucs/datastruct/queue.h>
+#include <ucs/debug/log.h>
+#include <ucs/profile/profile.h>
+#include <ucs/debug/memtrack.h>
+#include <ucs/stats/stats.h>
+#include <ucs/sys/sys.h>
+#include <ucs/sys/math.h>
+#include <ucm/api/ucm.h>
+
+
+static ucs_pgt_dir_t *ucs_memtype_cache_pgt_dir_alloc(const ucs_pgtable_t *pgtable)
+{
+    return ucs_memalign(UCS_PGT_ENTRY_MIN_ALIGN, sizeof(ucs_pgt_dir_t),
+                        "memtype_cache_pgdir");
+}
+
+static void ucs_memtype_cache_pgt_dir_release(const ucs_pgtable_t *pgtable,
+                                              ucs_pgt_dir_t *dir)
+{
+    ucs_free(dir);
+}
+
+static UCS_F_ALWAYS_INLINE void
+ucs_memtype_cache_insert(ucs_memtype_cache_t *memtype_cache, void *address,
+                         size_t size, ucm_mem_type_t mem_type)
+{
+    ucs_memtype_cache_region_t *region;
+    ucs_pgt_addr_t start, end;
+    ucs_status_t status;
+
+    ucs_trace("memtype_cache:insert address:%p length:%zu mem_type:%d",
+              address, size, mem_type);
+
+    pthread_rwlock_wrlock(&memtype_cache->lock);
+
+    /* Align to page size */
+    start  = ucs_align_down_pow2((uintptr_t)address, UCS_PGT_ADDR_ALIGN);
+    end    = ucs_align_up_pow2  ((uintptr_t)address + size, UCS_PGT_ADDR_ALIGN);
+    region = NULL;
+
+    /* Allocate structure for new region */
+    region = ucs_memalign(UCS_PGT_ENTRY_MIN_ALIGN, sizeof(ucs_memtype_cache_region_t),
+                          "memtype_cache_region");
+    if (region == NULL) {
+        ucs_warn("failed to allocate memtype_cache region");
+        goto out_unlock;
+    }
+
+    region->super.start = start;
+    region->super.end   = end;
+    region->mem_type    = mem_type;
+    status = UCS_PROFILE_CALL(ucs_pgtable_insert, &memtype_cache->pgtable,
+                              &region->super);
+    if (status != UCS_OK) {
+        ucs_error("failed to insert region " UCS_PGT_REGION_FMT ": %s",
+                  UCS_PGT_REGION_ARG(&region->super), ucs_status_string(status));
+        ucs_free(region);
+        goto out_unlock;
+    }
+
+out_unlock:
+    pthread_rwlock_unlock(&memtype_cache->lock);
+}
+
+static UCS_F_ALWAYS_INLINE void
+ucs_memtype_cache_delete(ucs_memtype_cache_t *memtype_cache, void *address,
+                         size_t size, ucm_mem_type_t mem_type)
+{
+    ucs_pgt_addr_t start = (uintptr_t)address;
+    ucs_pgt_region_t *pgt_region;
+    ucs_memtype_cache_region_t *region;
+    ucs_status_t status;
+
+    ucs_trace("memtype_cache:delete address:%p length:%zu mem_type:%d",
+              address, size, mem_type);
+
+    pthread_rwlock_rdlock(&memtype_cache->lock);
+
+    pgt_region = UCS_PROFILE_CALL(ucs_pgtable_lookup, &memtype_cache->pgtable, start);
+    assert(pgt_region != NULL);
+
+    region = ucs_derived_of(pgt_region, ucs_memtype_cache_region_t);
+
+    status = ucs_pgtable_remove(&memtype_cache->pgtable, &region->super);
+    if (status != UCS_OK) {
+        ucs_warn("failed to remove address:%p from memtype_cache", address);
+    }
+    ucs_free(region);
+    pthread_rwlock_unlock(&memtype_cache->lock);
+}
+
+static void ucs_memtype_cache_event_callback(ucm_event_type_t event_type,
+                                              ucm_event_t *event, void *arg)
+{
+    ucs_memtype_cache_t *memtype_cache = arg;
+
+    if (event_type & UCM_EVENT_MEM_TYPE_ALLOC) {
+        ucs_memtype_cache_insert(memtype_cache, event->mem_type.address,
+                                 event->mem_type.size, event->mem_type.mem_type);
+    } else if (event_type & UCM_EVENT_MEM_TYPE_FREE) {
+        ucs_memtype_cache_delete(memtype_cache, event->mem_type.address,
+                                 event->mem_type.size, event->mem_type.mem_type);
+    }
+}
+
+static void ucs_memtype_cache_region_collect_callback(const ucs_pgtable_t *pgtable,
+                                                      ucs_pgt_region_t *pgt_region,
+                                                      void *arg)
+{
+    ucs_memtype_cache_region_t *region = ucs_derived_of(pgt_region,
+                                                        ucs_memtype_cache_region_t);
+    ucs_list_link_t *list = arg;
+    ucs_list_add_tail(list, &region->list);
+}
+
+static void ucs_memtype_cache_purge(ucs_memtype_cache_t *memtype_cache)
+{
+    ucs_memtype_cache_region_t *region, *tmp;
+    ucs_list_link_t region_list;
+
+    ucs_trace_func("memtype_cache purge");
+
+    ucs_list_head_init(&region_list);
+    ucs_pgtable_purge(&memtype_cache->pgtable, ucs_memtype_cache_region_collect_callback,
+                      &region_list);
+    ucs_list_for_each_safe(region, tmp, &region_list, list) {
+        ucs_warn("destroying inuse address:%p ", (void *)region->super.start);
+        ucs_free(region);
+    }
+}
+
+UCS_PROFILE_FUNC(ucs_status_t, ucs_memtype_cache_lookup,
+                 (memtype_cache, address, length, ucm_mem_type),
+                 ucs_memtype_cache_t *memtype_cache, void *address,
+                 size_t length, ucm_mem_type_t *ucm_mem_type)
+{
+    ucs_pgt_addr_t start = (uintptr_t)address;
+    ucs_pgt_region_t *pgt_region;
+    ucs_memtype_cache_region_t *region;
+    ucs_status_t status;
+
+    pthread_rwlock_rdlock(&memtype_cache->lock);
+
+    pgt_region = UCS_PROFILE_CALL(ucs_pgtable_lookup, &memtype_cache->pgtable, start);
+    if (pgt_region && pgt_region->end >= (start + length)) {
+        region = ucs_derived_of(pgt_region, ucs_memtype_cache_region_t);
+        *ucm_mem_type = region->mem_type;
+        status = UCS_OK;
+        goto out_unlock;
+    }
+    status = UCS_ERR_NO_ELEM;
+out_unlock:
+    pthread_rwlock_unlock(&memtype_cache->lock);
+    return status;
+}
+
+static UCS_CLASS_INIT_FUNC(ucs_memtype_cache_t)
+{
+    ucs_status_t status;
+    int ret;
+
+    ret = pthread_rwlock_init(&self->lock, NULL);
+    if (ret) {
+        ucs_error("pthread_rwlock_init() failed: %m");
+        status = UCS_ERR_INVALID_PARAM;
+        goto err;
+    }
+
+    status = ucs_pgtable_init(&self->pgtable, ucs_memtype_cache_pgt_dir_alloc,
+                              ucs_memtype_cache_pgt_dir_release);
+    if (status != UCS_OK) {
+        goto err_destroy_rwlock;
+    }
+
+    status = ucm_set_event_handler((UCM_EVENT_MEM_TYPE_ALLOC | UCM_EVENT_MEM_TYPE_FREE),
+                                   1000, ucs_memtype_cache_event_callback, self);
+    if (status != UCS_OK) {
+        goto err_cleanup_pgtable;
+    }
+
+    return UCS_OK;
+
+err_cleanup_pgtable:
+    ucs_pgtable_cleanup(&self->pgtable);
+err_destroy_rwlock:
+    pthread_rwlock_destroy(&self->lock);
+err:
+    return status;
+}
+
+static UCS_CLASS_CLEANUP_FUNC(ucs_memtype_cache_t)
+{
+    ucm_unset_event_handler((UCM_EVENT_MEM_TYPE_ALLOC | UCM_EVENT_MEM_TYPE_FREE),
+                            ucs_memtype_cache_event_callback, self);
+    ucs_memtype_cache_purge(self);
+    ucs_pgtable_cleanup(&self->pgtable);
+    pthread_rwlock_destroy(&self->lock);
+}
+
+UCS_CLASS_DEFINE(ucs_memtype_cache_t, void);
+UCS_CLASS_DEFINE_NAMED_NEW_FUNC(ucs_memtype_cache_create, ucs_memtype_cache_t,
+                                ucs_memtype_cache_t)
+UCS_CLASS_DEFINE_NAMED_DELETE_FUNC(ucs_memtype_cache_destroy, ucs_memtype_cache_t,
+                                   ucs_memtype_cache_t)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.h
new file mode 100644
index 000000000..209be0500
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/memtype_cache.h
@@ -0,0 +1,61 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCS_MEMTYPE_CACHE_H_
+#define UCS_MEMTYPE_CACHE_H_
+
+#include <ucs/datastruct/pgtable.h>
+#include <ucs/datastruct/list.h>
+#include <ucs/stats/stats_fwd.h>
+#include <ucm/api/ucm.h>
+
+typedef struct ucs_memtype_cache         ucs_memtype_cache_t;
+typedef struct ucs_memtype_cache_region  ucs_memtype_cache_region_t;
+
+
+struct ucs_memtype_cache_region {
+    ucs_pgt_region_t    super;    /**< Base class - page table region */
+    ucs_list_link_t     list;     /**< List element */
+    ucm_mem_type_t      mem_type; /**< Memory type the address belongs to */
+};
+
+
+struct ucs_memtype_cache {
+    pthread_rwlock_t      lock;       /**< protests the page table */
+    ucs_pgtable_t         pgtable;    /**< Page table to hold the regions */
+};
+
+
+/**
+ * Create a memtype cache.
+ *
+ * @param [out] memtype_cache_p Filled with a pointer to the memtype cache.
+ */
+ucs_status_t ucs_memtype_cache_create(ucs_memtype_cache_t **memtype_cache_p);
+
+
+/**
+ * Destroy a memtype cache.
+ *
+ * @param [in]  memtype_cache       Memtype cache to destroy.
+ */
+void ucs_memtype_cache_destroy(ucs_memtype_cache_t *memtype_cache);
+
+
+/** Find if address range is in memtype cache.
+ *
+ * @param [in]  memtype_cache   Memtype cache to search
+ * @param [in]  address         Address to lookup
+ * @param [in]  length          Length of the memory
+ * @param [out] ucm_mem_type    Memory type of the address
+ *
+ * @return Error code.
+ */
+ucs_status_t ucs_memtype_cache_lookup(ucs_memtype_cache_t *memtype_cache, void *address,
+                                      size_t length, ucm_mem_type_t *ucm_mem_type);
+
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.c
index 39ac9d21b..0594be3b2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.c
@@ -1,21 +1,23 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
 
-#include "rcache.h"
 
 #include <ucs/arch/atomic.h>
 #include <ucs/type/class.h>
 #include <ucs/datastruct/queue.h>
 #include <ucs/debug/log.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
 #include <ucs/debug/memtrack.h>
 #include <ucs/stats/stats.h>
+#include <ucs/sys/math.h>
 #include <ucs/sys/sys.h>
 #include <ucm/api/ucm.h>
 
+#include "rcache.h"
+#include "rcache_int.h"
 
 #define ucs_rcache_region_log(_level, _message, ...) \
     do { \
@@ -34,6 +36,9 @@
 #define ucs_rcache_region_trace(_message, ...) \
     ucs_rcache_region_log(UCS_LOG_LEVEL_TRACE, _message, ## __VA_ARGS__)
 
+#define ucs_rcache_region_pfn(_region) \
+    ((_region)->priv)
+
 
 typedef struct ucs_rcache_inv_entry {
     ucs_queue_elem_t         queue;
@@ -42,6 +47,26 @@ typedef struct ucs_rcache_inv_entry {
 } ucs_rcache_inv_entry_t;
 
 
+#if ENABLE_STATS
+static ucs_stats_class_t ucs_rcache_stats_class = {
+    .name = "rcache",
+    .num_counters = UCS_RCACHE_STAT_LAST,
+    .counter_names = {
+        [UCS_RCACHE_GETS]               = "gets",
+        [UCS_RCACHE_HITS_FAST]          = "hits_fast",
+        [UCS_RCACHE_HITS_SLOW]          = "hits_slow",
+        [UCS_RCACHE_MISSES]             = "misses",
+        [UCS_RCACHE_MERGES]             = "regions_merged",
+        [UCS_RCACHE_UNMAPS]             = "unmap_events",
+        [UCS_RCACHE_UNMAP_INVALIDATES]  = "regions_inv_unmap",
+        [UCS_RCACHE_PUTS]               = "puts",
+        [UCS_RCACHE_REGS]               = "mem_regs",
+        [UCS_RCACHE_DEREGS]             = "mem_deregs",
+    }
+};
+#endif
+
+
 static void __ucs_rcache_region_log(const char *file, int line, const char *function,
                                     ucs_log_level_t level, ucs_rcache_t *rcache,
                                     ucs_rcache_region_t *region, const char *fmt,
@@ -127,6 +152,26 @@ static ucs_mpool_ops_t ucs_rcache_mp_ops = {
     .obj_cleanup   = NULL
 };
 
+/* Lock must be held for read */
+static void ucs_rcache_region_validate_pfn(ucs_rcache_t *rcache,
+                                           ucs_rcache_region_t *region)
+{
+    unsigned long region_pfn, actual_pfn;
+
+    if (!ucs_unlikely(ucs_global_opts.rcache_check_pfn)) {
+        return;
+    }
+
+    region_pfn = ucs_rcache_region_pfn(region);
+    actual_pfn = ucs_sys_get_pfn(region->super.start);
+    if (region_pfn != actual_pfn) {
+        ucs_rcache_region_error(rcache, region, "pfn check failed");
+        ucs_fatal("%s: page at virtual address 0x%lx moved from pfn 0x%lx to pfn 0x%lx",
+                  rcache->name, region->super.start, region_pfn, actual_pfn);
+    } else {
+        ucs_rcache_region_trace(rcache, region, "pfn ok");
+    }
+}
 
 /* Lock must be held */
 static void ucs_rcache_region_collect_callback(const ucs_pgtable_t *pgtable,
@@ -156,6 +201,7 @@ static void ucs_mem_region_destroy_internal(ucs_rcache_t *rcache,
     ucs_assert(!(region->flags & UCS_RCACHE_REGION_FLAG_PGTABLE));
 
     if (region->flags & UCS_RCACHE_REGION_FLAG_REGISTERED) {
+        UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_DEREGS, 1);
         UCS_PROFILE_CODE("mem_dereg") {
             rcache->params.ops->mem_dereg(rcache->params.context, rcache, region);
         }
@@ -223,6 +269,7 @@ static void ucs_rcache_invalidate_range(ucs_rcache_t *rcache, ucs_pgt_addr_t sta
     ucs_list_for_each_safe(region, tmp, &region_list, list) {
         /* all regions on the list are in the page table */
         ucs_rcache_region_invalidate(rcache, region, 1, 0);
+        UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_UNMAP_INVALIDATES, 1);
     }
 }
 
@@ -260,10 +307,20 @@ static void ucs_rcache_unmapped_callback(ucm_event_type_t event_type,
     ucs_rcache_inv_entry_t *entry;
     ucs_pgt_addr_t start, end;
 
-    ucs_assert(event_type == UCM_EVENT_VM_UNMAPPED);
+    ucs_assert(event_type == UCM_EVENT_VM_UNMAPPED ||
+               event_type == UCM_EVENT_MEM_TYPE_FREE);
+
+    if (event_type == UCM_EVENT_VM_UNMAPPED) {
+        start = (uintptr_t)event->vm_unmapped.address;
+        end   = (uintptr_t)event->vm_unmapped.address + event->vm_unmapped.size;
+    } else if(event_type == UCM_EVENT_MEM_TYPE_FREE) {
+        start = (uintptr_t)event->mem_type.address;
+        end   = (uintptr_t)event->mem_type.address + event->mem_type.size;
+    } else {
+        ucs_warn("%s: unknown event type: %x", rcache->name, event_type);
+        return;
+    }
 
-    start = (uintptr_t)event->vm_unmapped.address;
-    end   = (uintptr_t)event->vm_unmapped.address + event->vm_unmapped.size;
     ucs_trace_func("%s: event vm_unmapped 0x%lx..0x%lx", rcache->name, start, end);
 
     pthread_spin_lock(&rcache->inv_lock);
@@ -273,6 +330,7 @@ static void ucs_rcache_unmapped_callback(ucm_event_type_t event_type,
         entry->start = start;
         entry->end   = end;
         ucs_queue_push(&rcache->inv_q, &entry->queue);
+        UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_UNMAPS, 1);
     } else {
         ucs_error("Failed to allocate invalidation entry for 0x%lx..0x%lx, "
                   "data corruption may occur", start, end);
@@ -341,6 +399,7 @@ ucs_rcache_check_overlap(ucs_rcache_t *rcache, ucs_pgt_addr_t *start,
             return UCS_ERR_ALREADY_EXISTS;
         }
 
+        UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_MERGES, 1);
         /*
          * If we don't provide some of the permissions the other region had,
          * we might want to expand our permissions to support them. We can
@@ -426,7 +485,9 @@ retry:
         /* Found a matching region (it could have been added after we released
          * the lock)
          */
+        ucs_rcache_region_validate_pfn(rcache, region);
         status = region->status;
+        UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_HITS_SLOW, 1);
         goto out_set_region;
     } else if (status != UCS_OK) {
         /* Could not create a region because there are overlapping regions which
@@ -458,12 +519,15 @@ retry:
     /* If memory registration failed, keep the region and mark it as invalid,
      * to avoid numerous retries of registering the region.
      */
+    UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_REGS, 1);
+
     region->prot     = prot;
     region->flags    = UCS_RCACHE_REGION_FLAG_PGTABLE;
     region->refcount = 1;
     region->status = status =
         UCS_PROFILE_NAMED_CALL("mem_reg", rcache->params.ops->mem_reg,
-                               rcache->params.context, rcache, arg, region);
+                               rcache->params.context, rcache, arg, region,
+                               merged ? UCS_RCACHE_MEM_REG_HIDE_ERRORS : 0);
     if (status != UCS_OK) {
         if (merged) {
             /* failure may be due to merge, because memory of the merged
@@ -477,8 +541,8 @@ retry:
             ucs_rcache_region_invalidate(rcache, region, 1, 1);
             goto retry;
         } else {
-            ucs_warn("failed to register region " UCS_PGT_REGION_FMT ": %s",
-                     UCS_PGT_REGION_ARG(&region->super), ucs_status_string(status));
+            ucs_debug("failed to register region " UCS_PGT_REGION_FMT ": %s",
+                      UCS_PGT_REGION_ARG(&region->super), ucs_status_string(status));
             goto out_unlock;
         }
     }
@@ -486,6 +550,14 @@ retry:
     region->flags   |= UCS_RCACHE_REGION_FLAG_REGISTERED;
     region->refcount = 2; /* Page-table + user */
 
+    if (ucs_global_opts.rcache_check_pfn) {
+        ucs_rcache_region_pfn(region) = ucs_sys_get_pfn(region->super.start);
+    } else {
+        ucs_rcache_region_pfn(region) = 0;
+    }
+
+    UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_MISSES, 1);
+
     ucs_rcache_region_trace(rcache, region, "created");
 
 out_set_region:
@@ -512,6 +584,7 @@ ucs_status_t ucs_rcache_get(ucs_rcache_t *rcache, void *address, size_t length,
                    length);
 
     pthread_rwlock_rdlock(&rcache->lock);
+    UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_GETS, 1);
     if (ucs_queue_is_empty(&rcache->inv_q)) {
         pgt_region = UCS_PROFILE_CALL(ucs_pgtable_lookup, &rcache->pgtable,
                                       start);
@@ -521,7 +594,9 @@ ucs_status_t ucs_rcache_get(ucs_rcache_t *rcache, void *address, size_t length,
                 ucs_rcache_region_test(region, prot))
             {
                 ucs_rcache_region_hold(rcache, region);
+                ucs_rcache_region_validate_pfn(rcache, region);
                 *region_p = region;
+                UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_HITS_FAST, 1);
                 pthread_rwlock_unlock(&rcache->lock);
                 return UCS_OK;
             }
@@ -541,6 +616,7 @@ ucs_status_t ucs_rcache_get(ucs_rcache_t *rcache, void *address, size_t length,
 void ucs_rcache_region_put(ucs_rcache_t *rcache, ucs_rcache_region_t *region)
 {
     ucs_rcache_region_put_internal(rcache, region, 1, 0);
+    UCS_STATS_UPDATE_COUNTER(rcache->stats, UCS_RCACHE_PUTS, 1);
 }
 
 static UCS_CLASS_INIT_FUNC(ucs_rcache_t, const ucs_rcache_params_t *params,
@@ -565,12 +641,18 @@ static UCS_CLASS_INIT_FUNC(ucs_rcache_t, const ucs_rcache_params_t *params,
         goto err;
     }
 
+    status = UCS_STATS_NODE_ALLOC(&self->stats, &ucs_rcache_stats_class,
+                                  stats_parent);
+    if (status != UCS_OK) {
+        goto err;
+    }
+
     self->params = *params;
 
     self->name = strdup(name);
     if (self->name == NULL) {
         status = UCS_ERR_NO_MEMORY;
-        goto err;
+        goto err_destroy_stats;
     }
 
     ret = pthread_rwlock_init(&self->lock, NULL);
@@ -599,13 +681,14 @@ static UCS_CLASS_INIT_FUNC(ucs_rcache_t, const ucs_rcache_params_t *params,
         goto err_cleanup_pgtable;
     }
 
-    status = ucm_set_event_handler(UCM_EVENT_VM_UNMAPPED, params->ucm_event_priority,
+    ucs_queue_head_init(&self->inv_q);
+
+    status = ucm_set_event_handler(params->ucm_events, params->ucm_event_priority,
                                    ucs_rcache_unmapped_callback, self);
     if (status != UCS_OK) {
         goto err_destroy_mp;
     }
 
-    ucs_queue_head_init(&self->inv_q);
     return UCS_OK;
 
 err_destroy_mp:
@@ -618,13 +701,15 @@ err_destroy_rwlock:
     pthread_rwlock_destroy(&self->lock);
 err_free_name:
     free(self->name);
+err_destroy_stats:
+    UCS_STATS_NODE_FREE(self->stats);
 err:
     return status;
 }
 
 static UCS_CLASS_CLEANUP_FUNC(ucs_rcache_t)
 {
-    ucm_unset_event_handler(UCM_EVENT_VM_UNMAPPED, ucs_rcache_unmapped_callback,
+    ucm_unset_event_handler(self->params.ucm_events, ucs_rcache_unmapped_callback,
                             self);
     ucs_rcache_check_inv_queue(self);
     ucs_rcache_purge(self);
@@ -633,6 +718,7 @@ static UCS_CLASS_CLEANUP_FUNC(ucs_rcache_t)
     ucs_pgtable_cleanup(&self->pgtable);
     pthread_spin_destroy(&self->inv_lock);
     pthread_rwlock_destroy(&self->lock);
+    UCS_STATS_NODE_FREE(self->stats);
     free(self->name);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.h
index 47c8c428e..d9db909b2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache.h
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
@@ -39,6 +39,12 @@ enum {
     UCS_RCACHE_REGION_FLAG_PGTABLE    = UCS_BIT(1)  /**< In the page table */
 };
 
+/*
+ * Memory registration flags.
+ */
+enum {
+    UCS_RCACHE_MEM_REG_HIDE_ERRORS = UCS_BIT(0) /**< Hide errors on memory registration */
+};
 
 /*
  * Registration cache operations.
@@ -55,6 +61,7 @@ struct ucs_rcache_ops {
      *                          `region_struct_size' in @ref ucs_rcache_params.
      *                         This function may store relevant information (such
      *                          as memory keys) inside the larger structure.
+     * @param [in]  flags      Memory registration flags.
      *
      * @return UCS_OK if registration is successful, error otherwise.
      *
@@ -63,7 +70,8 @@ struct ucs_rcache_ops {
      *       such as error messages or fatal failure.
      */
     ucs_status_t           (*mem_reg)(void *context, ucs_rcache_t *rcache,
-                                      void *arg, ucs_rcache_region_t *region);
+                                      void *arg, ucs_rcache_region_t *region,
+                                      uint16_t flags);
    /**
     * Deregister a memory region.
     *
@@ -98,6 +106,9 @@ struct ucs_rcache_params {
                                                      Must be smaller or equal to
                                                      system page size. */
     size_t                 max_alignment;       /**< Maximum alignment */
+    int                    ucm_events;          /**< UCM events to register. Currently
+                                                     UCM_EVENT_VM_UNMAPPED and
+                                                     UCM_EVENT_MEM_TYPE_FREE are supported */
     int                    ucm_event_priority;  /**< Priority of memory events */
     const ucs_rcache_ops_t *ops;                /**< Memory operations functions */
     void                   *context;            /**< User-defined context that will
@@ -113,26 +124,7 @@ struct ucs_rcache_region {
     ucs_status_t           status;   /**< Current status code */
     uint8_t                prot;     /**< Protection bits */
     uint16_t               flags;    /**< Status flags. Protected by page table lock. */
-};
-
-
-struct ucs_rcache {
-    ucs_rcache_params_t    params;   /**< rcache parameters (immutable) */
-    pthread_rwlock_t       lock;     /**< Protects the page table and all regions
-                                          whose refcount is 0 */
-    ucs_pgtable_t          pgtable;  /**< page table to hold the regions */
-
-    pthread_spinlock_t     inv_lock; /**< Lock for inv_q and inv_mp. This is a
-                                          separate lock because we may want to put
-                                          regions on inv_q while the page table
-                                          lock is held by the calling context */
-    ucs_queue_head_t       inv_q;    /**< Regions which were invalidated during
-                                          memory events */
-    ucs_mpool_t            inv_mp;   /**< Memory pool to allocate entries for inv_q,
-                                          since we cannot use regulat malloc().
-                                          The backing storage is original mmap()
-                                          which does not generate memory events */
-    char                   *name;
+    uint64_t               priv;     /**< Used internally */
 };
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache_int.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache_int.h
new file mode 100644
index 000000000..2d8d53294
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/rcache_int.h
@@ -0,0 +1,47 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCS_REG_CACHE_INT_H_
+#define UCS_REG_CACHE_INT_H_
+
+/* Names of rcache stats counters */
+enum {
+    UCS_RCACHE_GETS,                /* number of get operations */
+    UCS_RCACHE_HITS_FAST,           /* number of fast path hits */
+    UCS_RCACHE_HITS_SLOW,           /* number of slow path hits */
+    UCS_RCACHE_MISSES,              /* number of misses */
+    UCS_RCACHE_MERGES,              /* number of region merges */
+    UCS_RCACHE_UNMAPS,              /* number of memory unmap events */
+    UCS_RCACHE_UNMAP_INVALIDATES,   /* number of regions invalidated because
+                                       of unmap events */
+    UCS_RCACHE_PUTS,                /* number of put operations */
+    UCS_RCACHE_REGS,                /* number of memory registrations */
+    UCS_RCACHE_DEREGS,              /* number of memory deregistrations */
+    UCS_RCACHE_STAT_LAST
+};
+
+
+struct ucs_rcache {
+    ucs_rcache_params_t    params;   /**< rcache parameters (immutable) */
+    pthread_rwlock_t       lock;     /**< Protects the page table and all regions
+                                          whose refcount is 0 */
+    ucs_pgtable_t          pgtable;  /**< page table to hold the regions */
+
+    pthread_spinlock_t     inv_lock; /**< Lock for inv_q and inv_mp. This is a
+                                          separate lock because we may want to put
+                                          regions on inv_q while the page table
+                                          lock is held by the calling context */
+    ucs_queue_head_t       inv_q;    /**< Regions which were invalidated during
+                                          memory events */
+    ucs_mpool_t            inv_mp;   /**< Memory pool to allocate entries for inv_q,
+                                          since we cannot use regulat malloc().
+                                          The backing storage is original mmap()
+                                          which does not generate memory events */
+    char                   *name;
+    UCS_STATS_NODE_DECLARE(stats);
+};
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/string.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/string.c
index d2407e037..5ce1734dc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/string.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/string.c
@@ -162,3 +162,5 @@ char* ucs_strncpy_safe(char *dst, const char *src, size_t len)
     dst[length - 1] = '\0';
     return dst;
 }
+
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.c
index 498021e03..fd72d4962 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.c
@@ -27,9 +27,11 @@
 #include <sched.h>
 #include <ctype.h>
 
+#if HAVE_SYS_CAPABILITY_H
+#  include <sys/capability.h>
+#endif
 
 /* Default huge page size is 2 MBytes */
-#define UCS_DEFAULT_HUGEPAGE_SIZE  (2 * UCS_MBYTE)
 #define UCS_DEFAULT_MEM_FREE       640000
 #define UCS_PROCESS_MAPS_FILE      "/proc/self/maps"
 
@@ -403,12 +405,12 @@ size_t ucs_get_page_size()
     return page_size;
 }
 
-size_t ucs_get_meminfo_entry(const char* pattern)
+static ssize_t ucs_get_meminfo_entry(const char* pattern)
 {
     char buf[256];
     char final_pattern[80];
     int val = 0;
-    size_t val_b = 0;
+    ssize_t val_b = -1;
     FILE *f;
 
     f = fopen("/proc/meminfo", "r");
@@ -432,7 +434,7 @@ size_t ucs_get_memfree_size()
     size_t mem_free;
 
     mem_free = ucs_get_meminfo_entry("MemFree");
-    if (mem_free == 0) {
+    if (mem_free == -1) {
         mem_free = UCS_DEFAULT_MEM_FREE;
         ucs_info("cannot determine free mem size, using default: %zu",
                   mem_free);
@@ -441,17 +443,15 @@ size_t ucs_get_memfree_size()
     return mem_free;
 }
 
-size_t ucs_get_huge_page_size()
+ssize_t ucs_get_huge_page_size()
 {
-    static size_t huge_page_size = 0;
+    static ssize_t huge_page_size = 0;
 
     /* Cache the huge page size value */
     if (huge_page_size == 0) {
         huge_page_size = ucs_get_meminfo_entry("Hugepagesize");
-        if (huge_page_size == 0) {
-            huge_page_size = UCS_DEFAULT_HUGEPAGE_SIZE;
-            ucs_warn("cannot determine huge page size, using default: %zu",
-                      huge_page_size);
+        if (huge_page_size == -1) {
+            ucs_debug("huge pages are not supported on the system");
         } else {
             ucs_trace("detected huge page size: %zu", huge_page_size);
         }
@@ -509,20 +509,137 @@ size_t ucs_get_shmmax()
     return size;
 }
 
+static void ucs_sysv_shmget_error_check_ENOSPC(size_t alloc_size,
+                                               const struct shminfo *ipc_info,
+                                               char *buf, size_t max)
+{
+    unsigned long new_used_ids;
+    unsigned long new_shm_tot;
+    struct shm_info shm_info;
+    char *p, *endp;
+    int ret;
+
+    p    = buf;
+    endp = p + max;
+
+    ret = shmctl(0, SHM_INFO, (struct shmid_ds *)&shm_info);
+    if (ret >= 0) {
+        return;
+    }
+
+    new_used_ids = shm_info.used_ids;
+    if (new_used_ids > ipc_info->shmmni) {
+        snprintf(p, endp - p,
+                 ", total number of segments in the system (%lu) would exceed the"
+                 " limit in /proc/sys/kernel/shmmni (=%lu)",
+                 new_used_ids, ipc_info->shmmni);
+        p += strlen(p);
+    }
+
+    new_shm_tot = shm_info.shm_tot +
+                  (alloc_size + ucs_get_page_size() - 1) / ucs_get_page_size();
+    if (new_shm_tot > ipc_info->shmall) {
+        snprintf(p, endp - p,
+                 ", total shared memory pages in the system (%lu) would exceed the"
+                 " limit in /proc/sys/kernel/shmall (=%lu)",
+                 new_shm_tot, ipc_info->shmall);
+        p += strlen(p);
+    }
+}
+
+static void ucs_sysv_shmget_error_check_EPERM(int flags, char *buf, size_t max)
+{
+#if HAVE_SYS_CAPABILITY_H
+    cap_user_header_t hdr = ucs_alloca(sizeof(*hdr));
+    cap_user_data_t data  = ucs_alloca(sizeof(*data) * _LINUX_CAPABILITY_U32S_3);
+    int ret;
+
+    hdr->pid     = 0; /* current thread */
+    hdr->version = _LINUX_CAPABILITY_VERSION_3;
+    ret = capget(hdr, data);
+    if (ret == 0) {
+        UCS_STATIC_ASSERT(CAP_IPC_LOCK < 32); /* we check this bit in data[0] */
+        if (!(data->effective & UCS_BIT(CAP_IPC_LOCK))) {
+            /* detected missing CAP_IPC_LOCK */
+            snprintf(buf, max, ", CAP_IPC_LOCK privilege is needed for SHM_HUGETLB");
+        }
+        return;
+    }
+
+    /* log error and fallback to speculative error message */
+    ucs_debug("capget(pid=%d version=0x%x) failed: %m", hdr->pid, hdr->version);
+#endif
+
+    snprintf(buf, max,
+             ", please check for CAP_IPC_LOCK privilege for using SHM_HUGETLB");
+}
+
+static void ucs_sysv_shmget_format_error(size_t alloc_size, int flags,
+                                         const char *alloc_name, int sys_errno,
+                                         char *buf, size_t max)
+{
+    struct shminfo ipc_info;
+    char *p, *endp, *errp;
+    int ret;
+
+    buf[0] = '\0';
+    p      = buf;
+    endp   = p + max;
+
+    snprintf(p, endp - p, "shmget(size=%zu flags=0x%x) for %s failed: %s",
+             alloc_size, flags, alloc_name, strerror(sys_errno));
+    p   += strlen(p);
+    errp = p; /* save current string pointer to detect if anything was added */
+
+    ret = shmctl(0, IPC_INFO, (struct shmid_ds *)&ipc_info);
+    if (ret >= 0) {
+        if ((sys_errno == EINVAL) && (alloc_size > ipc_info.shmmax)) {
+            snprintf(p, endp - p,
+                     ", allocation size exceeds /proc/sys/kernel/shmmax (=%zu)",
+                     ipc_info.shmmax);
+            p += strlen(p);
+        }
+
+        if (sys_errno == ENOSPC) {
+            ucs_sysv_shmget_error_check_ENOSPC(alloc_size, &ipc_info, p, endp - p);
+            p += strlen(p);
+        }
+    }
+
+    if (sys_errno == EPERM) {
+        ucs_sysv_shmget_error_check_EPERM(flags, p, endp - p);
+        p += strlen(p);
+    }
+
+    /* default error message if no useful information was added to the string */
+    if (p == errp) {
+        snprintf(p, endp - p, ", please check shared memory limits by 'ipcs -l'");
+        p += strlen(p);
+    }
+}
+
 ucs_status_t ucs_sysv_alloc(size_t *size, size_t max_size, void **address_p,
-                            int flags, int *shmid UCS_MEMTRACK_ARG)
+                            int flags, const char *alloc_name, int *shmid)
 {
-    struct shminfo shminfo, *shminfo_ptr;
+    char error_string[256];
+    ssize_t huge_page_size;
     size_t alloc_size;
+    int sys_errno;
     void *ptr;
-    int ret, err;
+    int ret;
 
-    alloc_size = ucs_memtrack_adjust_alloc_size(*size);
+    if (flags & SHM_HUGETLB) {
+        huge_page_size = ucs_get_huge_page_size();
+        if (huge_page_size <= 0) {
+            ucs_debug("huge pages are not supported on the system");
+            return UCS_ERR_NO_MEMORY; /* Huge pages not supported */
+        }
+    }
 
-    if (flags & SHM_HUGETLB){
-        alloc_size = ucs_align_up(alloc_size, ucs_get_huge_page_size());
+    if (flags & SHM_HUGETLB) {
+        alloc_size = ucs_align_up(*size, huge_page_size);
     } else {
-        alloc_size = ucs_align_up(alloc_size, ucs_get_page_size());
+        alloc_size = ucs_align_up(*size, ucs_get_page_size());
     }
 
     if (alloc_size >= max_size) {
@@ -532,32 +649,22 @@ ucs_status_t ucs_sysv_alloc(size_t *size, size_t max_size, void **address_p,
     flags |= IPC_CREAT | SHM_R | SHM_W;
     *shmid = shmget(IPC_PRIVATE, alloc_size, flags);
     if (*shmid < 0) {
-        switch (errno) {
-        case ENFILE:
+        sys_errno = errno;
+        ucs_sysv_shmget_format_error(alloc_size, flags, alloc_name, sys_errno,
+                                     error_string, sizeof(error_string));
+        switch (sys_errno) {
         case ENOMEM:
-        case ENOSPC:
         case EPERM:
             if (!(flags & SHM_HUGETLB)) {
-                err = errno;
-                shminfo_ptr = &shminfo;
-                if ((shmctl(0, IPC_INFO, (struct shmid_ds *) shminfo_ptr)) > -1) {
-                    ucs_error("shmget failed: %s. (size=%zu). The max number of shared memory segments in the system is = %ld. "
-                              "Please try to increase this value through /proc/sys/kernel/shmmni",
-                              strerror(err), alloc_size, shminfo.shmmni);
-                }
+                ucs_error("%s", error_string);
             }
-
             return UCS_ERR_NO_MEMORY;
+        case ENOSPC:
         case EINVAL:
-            ucs_error("A new segment was to be created and size < SHMMIN or size > SHMMAX, "
-                      "or no new segment was to be created. A segment with given key existed, "
-                      "but size is greater than the size of that segment. "
-                      "Please check shared memory limits by 'ipcs -l'.");
+            ucs_error("%s", error_string);
             return UCS_ERR_NO_MEMORY;
         default:
-            ucs_error("shmget(size=%zu, flags=0x%x) returned unexpected error: %m. "
-                      "Please check shared memory limits by 'ipcs -l'.",
-                      alloc_size, flags);
+            ucs_error("%s", error_string);
             return UCS_ERR_SHMEM_SEGMENT;
         }
     }
@@ -589,10 +696,9 @@ ucs_status_t ucs_sysv_alloc(size_t *size, size_t max_size, void **address_p,
         }
     }
 
+    ucs_memtrack_allocated(ptr, alloc_size UCS_MEMTRACK_VAL);
     *address_p = ptr;
     *size      = alloc_size;
-
-    ucs_memtrack_allocated(address_p, size UCS_MEMTRACK_VAL);
     return UCS_OK;
 }
 
@@ -600,7 +706,7 @@ ucs_status_t ucs_sysv_free(void *address)
 {
     int ret;
 
-    ucs_memtrack_releasing(&address);
+    ucs_memtrack_releasing(address);
     ret = shmdt(address);
     if (ret) {
         ucs_warn("Unable to detach shared memory segment at %p: %m", address);
@@ -767,6 +873,43 @@ const char* ucs_get_process_cmdline()
     return cmdline;
 }
 
+unsigned long ucs_sys_get_pfn(uintptr_t address)
+{
+    static const char *pagemap_file = "/proc/self/pagemap";
+    static int initialized = 0;
+    static int pagemap_fd;
+    uint64_t data;
+    off_t offset;
+    ssize_t ret;
+
+    if (!initialized) {
+        pagemap_fd = open(pagemap_file, O_RDONLY);
+        if (pagemap_fd < 0) {
+            ucs_warn("failed to open %s: %m", pagemap_file);
+        }
+        initialized = 1;
+    }
+
+    if (pagemap_fd < 0) {
+        return 0; /* could not open file */
+    }
+
+    offset = (address / ucs_get_page_size()) * sizeof(data);
+    data   = 0;
+    ret    = pread(pagemap_fd, &data, sizeof(data), offset);
+    if (ret < 0) {
+        ucs_warn("pread(file=%s offset=%zu) failed: %m", pagemap_file, offset);
+        return 0;
+    }
+
+    if (!(data & UCS_BIT(63))) {
+        ucs_trace("address 0x%lx not present", address);
+        return 0;
+    }
+
+    return data & UCS_MASK(55);
+}
+
 ucs_status_t ucs_sys_fcntl_modfl(int fd, int add, int remove)
 {
     int oldfl, ret;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.h
index 0bdd3dd6f..43b8ba95e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/sys/sys.h
@@ -164,9 +164,9 @@ size_t ucs_get_page_size();
 
 
 /**
- * @return Huge page size on the system.
+ * @return Huge page size on the system, or -1 if unsupported.
  */
-size_t ucs_get_huge_page_size();
+ssize_t ucs_get_huge_page_size();
 
 
 /**
@@ -184,17 +184,18 @@ size_t ucs_get_phys_mem_size();
 /**
  * Allocate shared memory using SystemV API.
  *
- * @param size      Pointer to memory size to allocate, updated with actual size
- *                  (rounded up to huge page size or to regular page size).
- * @param max_size  maximal size to allocate. If need to allocate more than this,
- *                  the function fails and returns UCS_ERR_EXCEEDS_LIMIT.
- * @param address_p Filled with allocated memory address.
- * @param flags     Flags to indicate the permissions for the allocate memory.
- *                  (also, whether or not to allocate memory with huge pages).
- * @param shmid     Filled with the shmid from the shmget call in the function.
+ * @param size       Pointer to memory size to allocate, updated with actual size
+ *                   (rounded up to huge page size or to regular page size).
+ * @param max_size   maximal size to allocate. If need to allocate more than this,
+ *                   the function fails and returns UCS_ERR_EXCEEDS_LIMIT.
+ * @param address_p  Filled with allocated memory address.
+ * @param flags      Flags to indicate the permissions for the allocate memory.
+ *                   (also, whether or not to allocate memory with huge pages).
+ * @param alloc_name Name of memory allocation, for debug/error reporting purposes.
+ * @param shmid      Filled with the shmid from the shmget call in the function.
  */
 ucs_status_t ucs_sysv_alloc(size_t *size, size_t max_size, void **address_p,
-                            int flags, int *shimd UCS_MEMTRACK_ARG);
+                            int flags, const char *alloc_name, int *shimd);
 
 
 /**
@@ -237,6 +238,17 @@ ucs_status_t ucs_mmap_free(void *address, size_t length);
 int ucs_get_mem_prot(unsigned long start, unsigned long end);
 
 
+/**
+ * Returns the physical page frame number of a given virtual page address.
+ * If the page map file is non-readable (for example, due to permissions), or
+ * the page is not present, this function returns 0.
+ *
+ * @param address  Virtual address to get the PFN for
+ * @return PFN number, or 0 if failed.
+ */
+unsigned long ucs_sys_get_pfn(uintptr_t address);
+
+
 /**
  * Modify file descriptor flags via fcntl().
  *
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/time/timerq.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/time/timerq.c
index 3fe810cdd..311761e0f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/time/timerq.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/time/timerq.c
@@ -9,6 +9,7 @@
 #include <ucs/debug/log.h>
 #include <ucs/debug/memtrack.h>
 #include <ucs/sys/math.h>
+#include <stdlib.h>
 
 
 ucs_status_t ucs_timerq_init(ucs_timer_queue_t *timerq)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/class.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/class.h
index c5352bf61..97010ccb4 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/class.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/class.h
@@ -184,8 +184,6 @@ struct ucs_class {
  */
 #define UCS_CLASS_CALL_SUPER_INIT(_superclass, ...) \
     { \
-        ucs_assert(*_init_count >= 1); \
-        ucs_assert(&_UCS_CLASS_DECL_NAME(_superclass) == _myclass->superclass); \
         { \
             ucs_status_t status = _UCS_CLASS_INIT_NAME(_superclass)\
                     (&self->super, _myclass->superclass, _init_count, ## __VA_ARGS__); \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/component.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/component.h
index e0122bb62..29cef0b46 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/component.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/component.h
@@ -45,6 +45,13 @@ typedef struct ucs_component {
     static void UCS_F_CTOR UCS_PP_APPEND_UNIQUE_ID(ucs_initializer)()
 
 
+/*
+ * Define code which runs at global destructor phase
+ */
+#define UCS_STATIC_CLEANUP \
+    static void UCS_F_DTOR UCS_PP_APPEND_UNIQUE_ID(ucs_initializer)()
+
+
 /**
  * Define a list of components for specific base type.
  *
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.c
new file mode 100644
index 000000000..da17059a2
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.c
@@ -0,0 +1,40 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "spinlock.h"
+
+#include <ucs/debug/log.h>
+#include <string.h>
+
+
+ucs_status_t ucs_spinlock_init(ucs_spinlock_t *lock)
+{
+    int ret;
+
+    ret = pthread_spin_init(&lock->lock, 0);
+    if (ret != 0) {
+        return UCS_ERR_IO_ERROR;
+    }
+
+    lock->count = 0;
+    lock->owner = 0xfffffffful;
+    return UCS_OK;
+}
+
+void ucs_spinlock_destroy(ucs_spinlock_t *lock)
+{
+    int ret;
+
+    if (lock->count != 0) {
+        ucs_warn("destroying spinlock %p with use count %d (owner: 0x%lx)",
+                 lock, lock->count, lock->owner);
+    }
+
+    ret = pthread_spin_destroy(&lock->lock);
+    if (ret != 0) {
+        ucs_warn("failed to destroy spinlock %p: %s", lock, strerror(ret));
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.h
index 2a7d77b55..45ee8d705 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/spinlock.h
@@ -22,31 +22,9 @@ typedef struct ucs_spinlock {
 } ucs_spinlock_t;
 
 
-static inline ucs_status_t ucs_spinlock_init(ucs_spinlock_t *lock)
-{
-    int ret;
-
-    ret = pthread_spin_init(&lock->lock, 0);
-    if (ret != 0) {
-        return UCS_ERR_IO_ERROR;
-    }
-
-    lock->count = 0;
-    lock->owner = 0xfffffffful;
-    return UCS_OK;
-}
+ucs_status_t ucs_spinlock_init(ucs_spinlock_t *lock);
 
-static inline ucs_status_t ucs_spinlock_destroy(ucs_spinlock_t *lock)
-{
-    int ret;
-
-    ret = pthread_spin_destroy(&lock->lock);
-    if (ret != 0) {
-        return UCS_ERR_IO_ERROR;
-    }
-
-    return UCS_OK;
-}
+void ucs_spinlock_destroy(ucs_spinlock_t *lock);
 
 static inline int ucs_spin_is_owner(ucs_spinlock_t *lock, pthread_t self)
 {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.c b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.c
index 79b974265..09867872e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.c
@@ -62,6 +62,8 @@ const char *ucs_status_string(ucs_status_t status)
         return "User-defined limit was reached";
     case UCS_ERR_UNSUPPORTED:
         return "Unsupported operation";
+    case UCS_ERR_REJECTED:
+        return "Operation rejected by remote peer";
     case UCS_ERR_ENDPOINT_TIMEOUT:
         return "Endpoint timeout";
     default:
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.h b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.h
index 2ab165297..e414fe556 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/ucs/type/status.h
@@ -70,6 +70,7 @@ typedef enum {
     UCS_ERR_TIMED_OUT              = -20,
     UCS_ERR_EXCEEDS_LIMIT          = -21,
     UCS_ERR_UNSUPPORTED            = -22,
+    UCS_ERR_REJECTED               = -23,
 
     UCS_ERR_FIRST_LINK_FAILURE     = -40,
     UCS_ERR_LAST_LINK_FAILURE      = -59,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/src/uct/Makefile.am
index 7ef685711..1ca5befc0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/Makefile.am
@@ -29,9 +29,7 @@ noinst_HEADERS = \
 	base/uct_iface.h \
 	base/uct_log.h \
 	base/uct_worker.h \
-	sm/self/self_md.h \
-	sm/self/self_iface.h \
-	sm/self/self_ep.h \
+	sm/self/self.h \
 	tcp/tcp.h
 
 libuct_la_SOURCES = \
@@ -39,9 +37,7 @@ libuct_la_SOURCES = \
 	base/uct_mem.c \
 	base/uct_iface.c \
 	base/uct_worker.c \
-	sm/self/self_md.c \
-	sm/self/self_iface.c \
-	sm/self/self_ep.c \
+	sm/self/self.c \
 	tcp/tcp_ep.c \
 	tcp/tcp_iface.c \
 	tcp/tcp_md.c \
@@ -69,14 +65,21 @@ libuct_la_CPPFLAGS += \
 
 libuct_la_SOURCES += \
 	ib/mlx5/ib_mlx5_log.c \
-	ib/mlx5/ib_mlx5.c
+	ib/mlx5/ib_mlx5.c \
+	ib/mlx5/ib_mlx5_dv.c \
+	ib/mlx5/ib_mlx5_hw.c
 
 noinst_HEADERS += \
 	ib/mlx5/ib_mlx5_log.h \
 	ib/mlx5/ib_mlx5.h \
-	ib/mlx5/ib_mlx5.inl
-endif
+	ib/mlx5/ib_mlx5.inl \
+	ib/mlx5/ib_mlx5_dv.h \
+	ib/mlx5/ib_mlx5_hw.h
 
+if HAVE_MLX5_DV
+libuct_la_LDFLAGS += $(LIB_MLX5)
+endif
+endif
 
 if HAVE_TL_RC
 noinst_HEADERS += \
@@ -108,15 +111,21 @@ endif
 if HAVE_TL_DC
 noinst_HEADERS += \
 	ib/dc/base/dc_iface.h \
-	ib/dc/base/dc_ep.h \
-	ib/dc/verbs/dc_verbs.h 
+	ib/dc/base/dc_ep.h
 
 libuct_la_SOURCES += \
 	ib/dc/base/dc_iface.c \
-	ib/dc/base/dc_ep.c \
+	ib/dc/base/dc_ep.c
+
+if HAVE_DC_EXP
+noinst_HEADERS += \
+	ib/dc/verbs/dc_verbs.h
+
+libuct_la_SOURCES += \
 	ib/dc/verbs/dc_verbs.c
+endif
 
-if HAVE_MLX5_HW
+if HAVE_MLX5_HW_DC
 noinst_HEADERS += \
 	ib/dc/accel/dc_mlx5.h
 
@@ -141,7 +150,7 @@ noinst_HEADERS += \
 	ib/ud/base/ud_inl.h \
 	ib/ud/verbs/ud_verbs.h 
 
-if HAVE_MLX5_HW
+if HAVE_MLX5_HW_UD
 noinst_HEADERS += \
 	ib/ud/accel/ud_mlx5_common.h \
 	ib/ud/accel/ud_mlx5.h
@@ -225,18 +234,36 @@ libuct_la_SOURCES += \
     cuda/gdr_copy/gdr_copy_ep.c
 endif
 noinst_HEADERS += \
+    cuda/base/cuda_md.h \
+    cuda/base/cuda_iface.h \
     cuda/cuda_copy/cuda_copy_md.h \
     cuda/cuda_copy/cuda_copy_iface.h \
     cuda/cuda_copy/cuda_copy_ep.h
 
 libuct_la_SOURCES += \
+    cuda/base/cuda_md.c \
     cuda/cuda_copy/cuda_copy_md.c \
     cuda/cuda_copy/cuda_copy_iface.c \
     cuda/cuda_copy/cuda_copy_ep.c
 
+noinst_HEADERS += \
+    cuda/cuda_ipc/cuda_ipc_md.h \
+    cuda/cuda_ipc/cuda_ipc_iface.h \
+    cuda/cuda_ipc/cuda_ipc_ep.h \
+    cuda/cuda_ipc/cuda_ipc_cache.h
+
+libuct_la_SOURCES += \
+    cuda/cuda_ipc/cuda_ipc_md.c \
+    cuda/cuda_ipc/cuda_ipc_iface.c \
+    cuda/cuda_ipc/cuda_ipc_ep.c \
+    cuda/cuda_ipc/cuda_ipc_cache.c
+
 endif
 
 if HAVE_ROCM
+libuct_la_CPPFLAGS += $(ROCM_CPPFLAGS)
+libuct_la_LDFLAGS +=  $(ROCM_LDFLAGS) $(ROCM_LIBS)
+
 noinst_HEADERS += \
     rocm/rocm_cma_md.h \
     rocm/rocm_cma_iface.h \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/tl.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/tl.h
index c6d078ced..2f51e3aee 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/tl.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/tl.h
@@ -42,6 +42,9 @@ typedef struct uct_iface_ops {
 
     /* endpoint - get */
 
+    ucs_status_t (*ep_get_short)(uct_ep_h ep, void *buffer, unsigned length,
+                                 uint64_t remote_addr, uct_rkey_t rkey);
+
     ucs_status_t (*ep_get_bcopy)(uct_ep_h ep, uct_unpack_callback_t unpack_cb,
                                  void *arg, size_t length,
                                  uint64_t remote_addr, uct_rkey_t rkey,
@@ -67,36 +70,28 @@ typedef struct uct_iface_ops {
 
     /* endpoint - atomics */
 
-    ucs_status_t (*ep_atomic_add64)(uct_ep_h ep, uint64_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey);
-
-    ucs_status_t (*ep_atomic_fadd64)(uct_ep_h ep, uint64_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp);
-
-    ucs_status_t (*ep_atomic_swap64)(uct_ep_h ep, uint64_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp);
-
     ucs_status_t (*ep_atomic_cswap64)(uct_ep_h ep, uint64_t compare, uint64_t swap,
                                       uint64_t remote_addr, uct_rkey_t rkey,
                                       uint64_t *result, uct_completion_t *comp);
 
-    ucs_status_t (*ep_atomic_add32)(uct_ep_h ep, uint32_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey);
-
-    ucs_status_t (*ep_atomic_fadd32)(uct_ep_h ep, uint32_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp);
-
-    ucs_status_t (*ep_atomic_swap32)(uct_ep_h ep, uint32_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp);
-
     ucs_status_t (*ep_atomic_cswap32)(uct_ep_h ep, uint32_t compare, uint32_t swap,
                                       uint64_t remote_addr, uct_rkey_t rkey,
                                       uint32_t *result, uct_completion_t *comp);
 
+    ucs_status_t (*ep_atomic32_post)(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey);
+
+    ucs_status_t (*ep_atomic64_post)(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey);
+
+    ucs_status_t (*ep_atomic32_fetch)(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                      uint32_t *result, uint64_t remote_addr,
+                                      uct_rkey_t rkey, uct_completion_t *comp);
+
+    ucs_status_t (*ep_atomic64_fetch)(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                      uint64_t *result, uint64_t remote_addr,
+                                      uct_rkey_t rkey, uct_completion_t *comp);
+
     /* endpoint - tagged operations */
 
     ucs_status_t (*ep_tag_eager_short)(uct_ep_h ep, uct_tag_t tag,
@@ -137,7 +132,8 @@ typedef struct uct_iface_ops {
 
     /* endpoint - pending queue */
 
-    ucs_status_t (*ep_pending_add)(uct_ep_h ep, uct_pending_req_t *n);
+    ucs_status_t (*ep_pending_add)(uct_ep_h ep, uct_pending_req_t *n,
+                                   unsigned flags);
 
     void         (*ep_pending_purge)(uct_ep_h ep, uct_pending_purge_callback_t cb,
                                      void *arg);
@@ -162,8 +158,8 @@ typedef struct uct_iface_ops {
 
     ucs_status_t (*ep_create_sockaddr)(uct_iface_h iface,
                                        const ucs_sock_addr_t *sockaddr,
-                                       const void *priv_data, size_t length,
-                                       uct_ep_h *ep_p);
+                                       uct_sockaddr_priv_pack_callback_t pack_cb,
+                                       void *arg, uint32_t cb_flags, uct_ep_h *ep_p);
 
     void         (*ep_destroy)(uct_ep_h ep);
 
@@ -173,6 +169,12 @@ typedef struct uct_iface_ops {
                                      const uct_device_addr_t *dev_addr,
                                      const uct_ep_addr_t *ep_addr);
 
+    ucs_status_t (*iface_accept)(uct_iface_h iface,
+                                 uct_conn_request_h conn_request);
+
+    ucs_status_t (*iface_reject)(uct_iface_h iface,
+                                 uct_conn_request_h conn_request);
+
     /* interface - synchronization */
 
     ucs_status_t (*iface_flush)(uct_iface_h iface, unsigned flags,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct.h
index 9993cad6f..12cbfc73f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct.h
@@ -149,6 +149,24 @@ typedef struct uct_tl_resource_desc {
 #define UCT_TL_RESOURCE_DESC_ARG(_resource)   (_resource)->tl_name, (_resource)->dev_name
 
 
+/**
+ * @brief Atomic operation requested for uct_ep_atomic32_post, uct_ep_atomic64_post,
+ * uct_ep_atomic32_fetch and uct_ep_atomic64_fetch.
+ *
+ * This enumeration defines which atomic memory operation should be
+ * performed by the uct_ep_atomic family of fuctions.
+ */
+typedef enum uct_atomic_op {
+    UCT_ATOMIC_OP_ADD,   /**< Atomic add  */
+    UCT_ATOMIC_OP_AND,   /**< Atomic and  */
+    UCT_ATOMIC_OP_OR,    /**< Atomic or   */
+    UCT_ATOMIC_OP_XOR,   /**< Atomic xor  */
+    UCT_ATOMIC_OP_SWAP,  /**< Atomic swap */
+    UCT_ATOMIC_OP_CSWAP, /**< Atomic compare-and-swap */
+    UCT_ATOMIC_OP_LAST
+} uct_atomic_op_t;
+
+
 /**
  * @defgroup UCT_RESOURCE_IFACE_CAP   UCT interface operations and capabilities
  * @ingroup UCT_RESOURCE
@@ -176,16 +194,6 @@ typedef struct uct_tl_resource_desc {
 #define UCT_IFACE_FLAG_GET_BCOPY      UCS_BIT(9)  /**< Buffered get */
 #define UCT_IFACE_FLAG_GET_ZCOPY      UCS_BIT(10) /**< Zero-copy get */
 
-        /* Atomic operations capabilities */
-#define UCT_IFACE_FLAG_ATOMIC_ADD32   UCS_BIT(16) /**< 32bit atomic add */
-#define UCT_IFACE_FLAG_ATOMIC_ADD64   UCS_BIT(17) /**< 64bit atomic add */
-#define UCT_IFACE_FLAG_ATOMIC_FADD32  UCS_BIT(18) /**< 32bit atomic fetch-and-add */
-#define UCT_IFACE_FLAG_ATOMIC_FADD64  UCS_BIT(19) /**< 64bit atomic fetch-and-add */
-#define UCT_IFACE_FLAG_ATOMIC_SWAP32  UCS_BIT(20) /**< 32bit atomic swap */
-#define UCT_IFACE_FLAG_ATOMIC_SWAP64  UCS_BIT(21) /**< 64bit atomic swap */
-#define UCT_IFACE_FLAG_ATOMIC_CSWAP32 UCS_BIT(22) /**< 32bit atomic compare-and-swap */
-#define UCT_IFACE_FLAG_ATOMIC_CSWAP64 UCS_BIT(23) /**< 64bit atomic compare-and-swap */
-
         /* Atomic operations domain */
 #define UCT_IFACE_FLAG_ATOMIC_CPU     UCS_BIT(30) /**< Atomic communications are consistent
                                                        with respect to CPU operations. */
@@ -330,22 +338,23 @@ enum uct_msg_flags {
  * @brief Callback flags.
  *
  * List of flags for a callback.
- * A callback must have either the SYNC or ASYNC flag set.
  */
 enum uct_cb_flags {
-    UCT_CB_FLAG_SYNC  = UCS_BIT(1), /**< Callback is always invoked from the context (thread, process)
-                                         that called uct_iface_progress(). An interface must
-                                         have the @ref UCT_IFACE_FLAG_CB_SYNC flag set to support sync
-                                         callback invocation. */
-
-    UCT_CB_FLAG_ASYNC = UCS_BIT(2)  /**< Callback may be invoked from any context. For example,
-                                         it may be called from a transport async progress thread. To guarantee
-                                         async invocation, the interface must have the @ref UCT_IFACE_FLAG_CB_ASYNC
-                                         flag set.
-                                         If async callback is requested on an interface
-                                         which only supports sync callback
-                                         (i.e., only the @ref UCT_IFACE_FLAG_CB_SYNC flag is set),
-                                         it will behave exactly like a sync callback.  */
+    UCT_CB_FLAG_RESERVED = UCS_BIT(1), /**< Reserved for future use. */
+    UCT_CB_FLAG_ASYNC    = UCS_BIT(2)  /**< Callback may be invoked from any
+                                            context (thread, process). For
+                                            example, it may be called from a
+                                            transport async progress thread. To
+                                            guarantee async invocation, the
+                                            interface must have the @ref
+                                            UCT_IFACE_FLAG_CB_ASYNC flag set. If
+                                            async callback is requested on an
+                                            interface which only supports sync
+                                            callback (i.e., only the @ref
+                                            UCT_IFACE_FLAG_CB_SYNC flag is set),
+                                            the callback may be invoked only
+                                            from the context that called @ref
+                                            uct_iface_progress). */
 };
 
 
@@ -406,6 +415,7 @@ enum {
 typedef enum {
     UCT_MD_MEM_TYPE_HOST = 0,      /**< Default system memory */
     UCT_MD_MEM_TYPE_CUDA,          /**< NVIDIA CUDA memory */
+    UCT_MD_MEM_TYPE_CUDA_MANAGED,  /**< NVIDIA CUDA managed (or unified) memory*/
     UCT_MD_MEM_TYPE_LAST
 } uct_memory_type_t;
 
@@ -415,17 +425,21 @@ typedef enum {
  * @brief  Memory allocation/registration flags.
  */
 enum uct_md_mem_flags {
-    UCT_MD_MEM_FLAG_NONBLOCK = UCS_BIT(0), /**< Hint to perform non-blocking
-                                                allocation/registration: page
-                                                mapping may be deferred until
-                                                it is accessed by the CPU or a
-                                                transport. */
-    UCT_MD_MEM_FLAG_FIXED    = UCS_BIT(1), /**< Place the mapping at exactly
-                                                defined address */
-    UCT_MD_MEM_FLAG_LOCK     = UCS_BIT(2), /**< Registered memory should be
-                                                locked. May incur extra cost for
-                                                registration, but memory access
-                                                is usually faster. */
+    UCT_MD_MEM_FLAG_NONBLOCK    = UCS_BIT(0), /**< Hint to perform non-blocking
+                                                   allocation/registration: page
+                                                   mapping may be deferred until
+                                                   it is accessed by the CPU or a
+                                                   transport. */
+    UCT_MD_MEM_FLAG_FIXED       = UCS_BIT(1), /**< Place the mapping at exactly
+                                                   defined address */
+    UCT_MD_MEM_FLAG_LOCK        = UCS_BIT(2), /**< Registered memory should be
+                                                   locked. May incur extra cost for
+                                                   registration, but memory access
+                                                   is usually faster. */
+    UCT_MD_MEM_FLAG_HIDE_ERRORS = UCS_BIT(3), /**< Hide errors on memory registration.
+                                                   In some cases registration failure
+                                                   is not an error (e. g. for merged
+                                                   memory regions). */
 
     /* memory access flags */
     UCT_MD_MEM_ACCESS_REMOTE_PUT    = UCS_BIT(5), /**< enable remote put access */
@@ -494,6 +508,7 @@ struct uct_iface_attr {
         } put;                           /**< Attributes for PUT operations */
 
         struct {
+            size_t           max_short;  /**< Maximal size for get_short */
             size_t           max_bcopy;  /**< Maximal size for get_bcopy */
             size_t           min_zcopy;  /**< Minimal size for get_zcopy (total
                                               of @ref uct_iov_t::length of the
@@ -561,6 +576,11 @@ struct uct_iface_attr {
             } rndv;                      /**< Attributes related to rendezvous protocol */
         } tag;                           /**< Attributes for TAG operations */
 
+        struct {
+            uint64_t         op_flags;   /**< Attributes for atomic-post operations */
+            uint64_t         fop_flags;  /**< Attributes for atomic-fetch operations */
+        } atomic32, atomic64;            /**< Attributes for atomic operations */
+
         uint64_t             flags;      /**< Flags from @ref UCT_RESOURCE_IFACE_CAP */
     } cap;                               /**< Interface capabilities */
 
@@ -633,6 +653,9 @@ struct uct_iface_params {
     void                                         *err_handler_arg;
     /** The callback to handle transport level error.*/
     uct_error_handler_t                          err_handler;
+    /** Callback flags to indicate where the @a err_handler callback can be
+     * invoked from. @ref uct_cb_flags */
+    uint32_t                                     err_handler_flags;
 
     /** These callbacks are only relevant for HW Tag Matching */
     void                                         *eager_arg;
@@ -889,9 +912,7 @@ void uct_release_tl_resource_list(uct_tl_resource_desc_t *resources);
  *  Transports can allocate separate communication resources for every worker,
  * so that every worker can be progressed independently of others.
  *
- * @param [in]  async         Context for async event handlers.
-  *                            Can be NULL, which means that event handlers will
- *                             not have particular context.
+ * @param [in]  async         Context for async event handlers. Must not be NULL.
  * @param [in]  thread_mode   Thread access mode to the worker and all interfaces
  *                             and endpoints associated with it.
  * @param [out] worker_p      Filled with a pointer to the worker object.
@@ -1004,19 +1025,6 @@ ucs_status_t uct_md_iface_config_read(uct_md_h md, const char *tl_name,
 void uct_config_release(void *config);
 
 
-/**
- * @ingroup UCT_RESOURCE
- * @brief Print interface/MD configuration to a stream.
- *
- * @param [in]  config        Configuration to print.
- * @param [in]  stream        Output stream to print to.
- * @param [in]  title         Title to the output.
- * @param [in]  print_flags   Controls how the configuration is printed.
- */
-void uct_config_print(const void *config, FILE *stream, const char *title,
-                      ucs_config_print_flags_t print_flags);
-
-
 /**
  * @ingroup UCT_CONTEXT
  * @brief Get value by name from interface/MD configuration.
@@ -1161,8 +1169,8 @@ ucs_status_t uct_ep_check(const uct_ep_h ep, unsigned flags,
  * @ingroup UCT_RESOURCE
  * @brief Obtain a notification file descriptor for polling.
  *
- * Only interfaces supporting the @ref UCT_IFACE_FLAG_EVENT_FD implement this
- * function.
+ * Only interfaces that support at least one of the UCT_IFACE_FLAG_EVENT* flags
+ * will implement this function.
  *
  * @param [in]  iface      Interface to get the notification descriptor.
  * @param [out] fd_p       Location to write the notification file descriptor.
@@ -1260,6 +1268,37 @@ ucs_status_t uct_iface_set_am_tracer(uct_iface_h iface, uct_am_tracer_t tracer,
                                      void *arg);
 
 
+/**
+ * @ingroup UCT_RESOURCE
+ * @brief Accept connection request.
+ *
+ * @param [in] iface        Transport interface which generated connection
+ *                          request @a conn_request.
+ * @param [in] conn_request Connection establishment request passed as parameter
+ *                          of @ref uct_sockaddr_conn_request_callback_t.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t uct_iface_accept(uct_iface_h iface,
+                              uct_conn_request_h conn_request);
+
+
+/**
+ * @ingroup UCT_RESOURCE
+ * @brief Reject connection request. Will invoke an error handler @ref
+ *        uct_error_handler_t on the remote transport interface, if set.
+ *
+ * @param [in] iface        Interface which generated connection establishment
+ *                          request @a conn_request.
+ * @param [in] conn_request Connection establishment request passed as parameter
+ *                          of @ref uct_sockaddr_conn_request_callback_t.
+ *
+ * @return Error code as defined by @ref ucs_status_t
+ */
+ucs_status_t uct_iface_reject(uct_iface_h iface,
+                              uct_conn_request_h conn_request);
+
+
 /**
  * @ingroup UCT_RESOURCE
  * @brief Create new endpoint.
@@ -1327,32 +1366,39 @@ ucs_status_t uct_ep_connect_to_ep(uct_ep_h ep, const uct_device_addr_t *dev_addr
  *
  * This routine will create an endpoint for a connection to the remote peer,
  * specified by its socket address.
- * The user may provide private data to be sent on a connection request to the
- * remote peer.
+ * The user may provide a callback function which will be used to fill the
+ * private data that will be sent on a connection request to the remote peer.
+ *
+ * @note It is never guaranteed that the callaback will be called.
+ * If, for example, the endpoint goes into error state before issuing the
+ * connection request, the callback will not be invoked.
  *
  * @note The interface in this routine requires the
  * @ref UCT_IFACE_FLAG_CONNECT_TO_SOCKADDR capability.
  *
- * @param [in]  iface            Interface to create the endpoint on.
- * @param [in]  sockaddr         The sockaddr to connect to on the remote peer.
- * @param [in]  priv_data        User's private data for connecting to the
- *                               remote peer.
- * @param [in]  length           Length of the private data.
- * @param [out] ep_p             Handle to the created endpoint.
- *
- * @return UCS_OK              - Connection request was sent to the server.
- *                               This does not guarantee that the server has
- *                               received the message; in case of failure, the
- *                               error will be reported to the interface error
- *                               handler callback provided to @ref uct_iface_open
- *                               via @ref uct_iface_params_t.err_handler.
- *
- * @return error code          - In case of an error. (@ref ucs_status_t)
+ * @param [in]  iface              Interface to create the endpoint on.
+ * @param [in]  sockaddr           The sockaddr to connect to on the remote peer.
+ * @param [in]  pack_cb            Callback for filling the user's private data.
+ * @param [in]  arg                User defined argument for the callback.
+ * @param [in]  cb_flags           Required @ref uct_cb_flags "callback flags" to
+ *                                 indicate where the
+ *                                 @ref uct_sockaddr_priv_pack_callback_t
+ *                                 callback can be invoked from.
+ * @param [out] ep_p               Handle to the created endpoint.
+ *
+ * @return UCS_OK                  Connection request was sent to the server.
+ *                                 This does not guarantee that the server has
+ *                                 received the message; in case of failure, the
+ *                                 error will be reported to the interface error
+ *                                 handler callback provided to @ref uct_iface_open
+ *                                 via @ref uct_iface_params_t.err_handler.
+ *
+ * @return error code              In case of an error. (@ref ucs_status_t)
  */
 ucs_status_t uct_ep_create_sockaddr(uct_iface_h iface,
                                     const ucs_sock_addr_t *sockaddr,
-                                    const void *priv_data, size_t length,
-                                    uct_ep_h *ep_p);
+                                    uct_sockaddr_priv_pack_callback_t pack_cb,
+                                    void *arg, uint32_t cb_flags, uct_ep_h *ep_p);
 
 
 /**
@@ -1452,7 +1498,7 @@ ucs_status_t uct_md_mem_dereg(uct_md_h md, uct_mem_h memh);
  *  @return Nonzero if memory is owned, 0 if not owned
  *
  * @param [in]     md        Memory domain to detect if memory belongs to.
- * @param [in]     address   Memory address to detect.
+ * @param [in]     addr      Memory address to detect.
  * @param [in]     length    Size of memory
  */
 int uct_md_is_mem_type_owned(uct_md_h md, void *addr, size_t length);
@@ -1463,7 +1509,7 @@ int uct_md_is_mem_type_owned(uct_md_h md, void *addr, size_t length);
  *
  * Allocate potentially registered memory. Every one of the provided allocation
  * methods will be used, in turn, to perform the allocation, until one succeeds.
- *  Whenever the MD method is encountered, every one of the provided MDs will be
+ * Whenever the MD method is encountered, every one of the provided MDs will be
  * used, in turn, to allocate the memory, until one succeeds, or they are
  * exhausted. In this case the next allocation method from the initial list will
  * be attempted.
@@ -1761,6 +1807,17 @@ UCT_INLINE_API ucs_status_t uct_ep_put_zcopy(uct_ep_h ep,
 }
 
 
+/**
+ * @ingroup UCT_RMA
+ * @brief
+ */
+UCT_INLINE_API ucs_status_t uct_ep_get_short(uct_ep_h ep, void *buffer, unsigned length,
+                                             uint64_t remote_addr, uct_rkey_t rkey)
+{
+    return ep->iface->ops.ep_get_short(ep, buffer, length, remote_addr, rkey);
+}
+
+
 /**
  * @ingroup UCT_RMA
  * @brief
@@ -1882,34 +1939,11 @@ UCT_INLINE_API ucs_status_t uct_ep_am_zcopy(uct_ep_h ep, uint8_t id,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_add64(uct_ep_h ep, uint64_t add,
-                                                uint64_t remote_addr, uct_rkey_t rkey)
-{
-    return ep->iface->ops.ep_atomic_add64(ep, add, remote_addr, rkey);
-}
-
-
-/**
- * @ingroup UCT_AMO
- * @brief
- */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_fadd64(uct_ep_h ep, uint64_t add,
-                                                 uint64_t remote_addr, uct_rkey_t rkey,
-                                                 uint64_t *result, uct_completion_t *comp)
-{
-    return ep->iface->ops.ep_atomic_fadd64(ep, add, remote_addr, rkey, result, comp);
-}
-
-
-/**
- * @ingroup UCT_AMO
- * @brief
- */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_swap64(uct_ep_h ep, uint64_t swap,
-                                                 uint64_t remote_addr, uct_rkey_t rkey,
-                                                 uint64_t *result, uct_completion_t *comp)
+UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap64(uct_ep_h ep, uint64_t compare, uint64_t swap,
+                                                  uint64_t remote_addr, uct_rkey_t rkey,
+                                                  uint64_t *result, uct_completion_t *comp)
 {
-    return ep->iface->ops.ep_atomic_swap64(ep, swap, remote_addr, rkey, result, comp);
+    return ep->iface->ops.ep_atomic_cswap64(ep, compare, swap, remote_addr, rkey, result, comp);
 }
 
 
@@ -1917,11 +1951,11 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_swap64(uct_ep_h ep, uint64_t swap,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap64(uct_ep_h ep, uint64_t compare, uint64_t swap,
+UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap32(uct_ep_h ep, uint32_t compare, uint32_t swap,
                                                   uint64_t remote_addr, uct_rkey_t rkey,
-                                                  uint64_t *result, uct_completion_t *comp)
+                                                  uint32_t *result, uct_completion_t *comp)
 {
-    return ep->iface->ops.ep_atomic_cswap64(ep, compare, swap, remote_addr, rkey, result, comp);
+    return ep->iface->ops.ep_atomic_cswap32(ep, compare, swap, remote_addr, rkey, result, comp);
 }
 
 
@@ -1929,10 +1963,11 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap64(uct_ep_h ep, uint64_t compare,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_add32(uct_ep_h ep, uint32_t add,
-                                                uint64_t remote_addr, uct_rkey_t rkey)
+UCT_INLINE_API ucs_status_t uct_ep_atomic32_post(uct_ep_h ep, uct_atomic_op_t opcode,
+                                                 uint32_t value, uint64_t remote_addr,
+                                                 uct_rkey_t rkey)
 {
-    return ep->iface->ops.ep_atomic_add32(ep, add, remote_addr, rkey);
+    return ep->iface->ops.ep_atomic32_post(ep, opcode, value, remote_addr, rkey);
 }
 
 
@@ -1940,11 +1975,11 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_add32(uct_ep_h ep, uint32_t add,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_fadd32(uct_ep_h ep, uint32_t add,
-                                                 uint64_t remote_addr, uct_rkey_t rkey,
-                                                 uint32_t *result, uct_completion_t *comp)
+UCT_INLINE_API ucs_status_t uct_ep_atomic64_post(uct_ep_h ep, uct_atomic_op_t opcode,
+                                                 uint64_t value, uint64_t remote_addr,
+                                                 uct_rkey_t rkey)
 {
-    return ep->iface->ops.ep_atomic_fadd32(ep, add, remote_addr, rkey, result, comp);
+    return ep->iface->ops.ep_atomic64_post(ep, opcode, value, remote_addr, rkey);
 }
 
 
@@ -1952,11 +1987,13 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_fadd32(uct_ep_h ep, uint32_t add,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_swap32(uct_ep_h ep, uint32_t swap,
-                                                 uint64_t remote_addr, uct_rkey_t rkey,
-                                                 uint32_t *result, uct_completion_t *comp)
+UCT_INLINE_API ucs_status_t uct_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                                  uint32_t value, uint32_t *result,
+                                                  uint64_t remote_addr, uct_rkey_t rkey,
+                                                  uct_completion_t *comp)
 {
-    return ep->iface->ops.ep_atomic_swap32(ep, swap, remote_addr, rkey, result, comp);
+    return ep->iface->ops.ep_atomic32_fetch(ep, opcode, value, result,
+                                            remote_addr, rkey, comp);
 }
 
 
@@ -1964,11 +2001,13 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_swap32(uct_ep_h ep, uint32_t swap,
  * @ingroup UCT_AMO
  * @brief
  */
-UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap32(uct_ep_h ep, uint32_t compare, uint32_t swap,
+UCT_INLINE_API ucs_status_t uct_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                                  uint64_t value, uint64_t *result,
                                                   uint64_t remote_addr, uct_rkey_t rkey,
-                                                  uint32_t *result, uct_completion_t *comp)
+                                                  uct_completion_t *comp)
 {
-    return ep->iface->ops.ep_atomic_cswap32(ep, compare, swap, remote_addr, rkey, result, comp);
+    return ep->iface->ops.ep_atomic64_fetch(ep, opcode, value, result,
+                                            remote_addr, rkey, comp);
 }
 
 
@@ -1985,15 +2024,18 @@ UCT_INLINE_API ucs_status_t uct_ep_atomic_cswap32(uct_ep_h ep, uint32_t compare,
  *                    the "func" field.
  *                    After passed to the function, the request is owned by UCT,
  *                    until the callback is called and returns UCS_OK.
+ * @param [in]  flags Reserved for future use.
  *
  * @return UCS_OK       - request added to pending queue
  *         UCS_ERR_BUSY - request was not added to pending queue, because send
  *                        resources are available now. The user is advised to
  *                        retry.
  */
-UCT_INLINE_API ucs_status_t uct_ep_pending_add(uct_ep_h ep, uct_pending_req_t *req)
+UCT_INLINE_API ucs_status_t uct_ep_pending_add(uct_ep_h ep,
+                                               uct_pending_req_t *req,
+                                               unsigned flags)
 {
-    return ep->iface->ops.ep_pending_add(ep, req);
+    return ep->iface->ops.ep_pending_add(ep, req, flags);
 }
 
 
@@ -2402,7 +2444,8 @@ UCT_INLINE_API void uct_iface_progress_disable(uct_iface_h iface, unsigned flags
 
 
 /**
- * Perform a progress on an interface.
+ * @ingroup UCT_RESOURCE
+ * @brief Perform a progress on an interface.
  */
 UCT_INLINE_API unsigned uct_iface_progress(uct_iface_h iface)
 {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct_def.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct_def.h
index 241618b9c..bf1f7f3b9 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct_def.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/api/uct_def.h
@@ -18,7 +18,7 @@
 #define UCT_MD_COMPONENT_NAME_MAX  8
 #define UCT_MD_NAME_MAX          16
 #define UCT_DEVICE_NAME_MAX      32
-#define UCT_PENDING_REQ_PRIV_LEN 32
+#define UCT_PENDING_REQ_PRIV_LEN 40
 #define UCT_TAG_PRIV_LEN         32
 #define UCT_AM_ID_BITS           5
 #define UCT_AM_ID_MAX            UCS_BIT(UCT_AM_ID_BITS)
@@ -91,6 +91,7 @@ typedef struct uct_ep_addr       uct_ep_addr_t;
 typedef struct uct_tag_context   uct_tag_context_t;
 typedef uint64_t                 uct_tag_t;  /* tag type - 64 bit */
 typedef int                      uct_worker_cb_id_t;
+typedef void*                    uct_conn_request_h;
 /**
  * @}
  */
@@ -139,7 +140,7 @@ typedef struct uct_iov {
  * @brief Callback to process incoming active message
  *
  * When the callback is called, @a flags indicates how @a data should be handled.
- * If @a flags contain @ref UCT_CB_FLAG_DESC value, it means @a data is part of
+ * If @a flags contain @ref UCT_CB_PARAM_FLAG_DESC value, it means @a data is part of
  * a descriptor which must be released later by @ref uct_iface_release_desc by
  * the user if the callback returns @ref UCS_INPROGRESS.
  *
@@ -156,7 +157,7 @@ typedef struct uct_iov {
  *                          by the caller.
  * @retval UCS_INPROGRESS - descriptor is owned by the callee, and would be
  *                          released later. Supported only if @a flags contain
- *                          @ref UCT_CB_FLAG_DESC value. Otherwise, this is
+ *                          @ref UCT_CB_PARAM_FLAG_DESC value. Otherwise, this is
  *                          an error.
  *
  */
@@ -280,23 +281,56 @@ typedef void (*uct_unpack_callback_t)(void *arg, const void *data, size_t length
  * Other than communication progress routines, it is allowed to call other UCT
  * communication routines from this callback.
  *
+ * @param [in]  iface            Transport interface.
  * @param [in]  arg              User defined argument for this callback.
+ * @param [in]  conn_request     Transport level connection request. The user
+ *                               should accept or reject the request by calling
+ *                               @ref uct_iface_accept or @ref uct_iface_reject
+ *                               routines respectively.
  * @param [in]  conn_priv_data   Points to the received data.
  *                               This is the private data that was passed to the
  *                               @ref uct_ep_create_sockaddr function on the
  *                               client side.
  * @param [in]  length           Length of the received data.
  *
- * @retval UCS_OK         - the server will accept the connection request from
- *                          the client.
- * @retval Otherwise      - the server will reject the connection request from
- *                          the client which will invoke the error handling flow
- *                          on the client side.
- *
  */
-typedef ucs_status_t (*uct_sockaddr_conn_request_callback_t)(void *arg,
-                                                             const void *conn_priv_data,
-                                                             size_t length);
+typedef void
+(*uct_sockaddr_conn_request_callback_t)(uct_iface_h iface, void *arg,
+                                        uct_conn_request_h conn_request,
+                                        const void *conn_priv_data,
+                                        size_t length);
+
+
+/**
+ * @ingroup UCT_RESOURCE
+ * @brief Callback to fill the user's private data on the client side.
+ *
+ * This callback routine will be invoked on the client side before sending the
+ * transport's connection request to the server.
+ * The callback routine must be set by the client when creating an endpoint.
+ * The user's private data should be placed inside the priv_data buffer to be
+ * sent to the server side.
+ * The maximal allowed length of the private data is indicated by the field
+ * max_conn_priv inside @ref uct_iface_attr.
+ * Communication progress routines should not be called from this callback.
+ * It is allowed to call other UCT communication routines from this callback.
+ *
+ * @param [in]  arg        User defined argument for this callback.
+ * @param [in]  dev_name   Device name. This routine may fill the user's private
+ *                         data according to the given device name.
+ *                         The device name that is passed to this routine,
+ *                         corresponds to the dev_name field inside
+ *                         @ref uct_tl_resource_desc_t as returned from
+ *                         @ref uct_md_query_tl_resources.
+ * @param [out] priv_data  User's private data to be passed to the server side.
+ *
+ * @return Negative value indicates an error according to @ref ucs_status_t.
+ *         On success, non-negative value indicates actual number of
+ *         bytes written to the @a priv_data buffer.
+ */
+typedef ssize_t (*uct_sockaddr_priv_pack_callback_t)(void *arg,
+                                                     const char *dev_name,
+                                                     void *priv_data);
 
 
 /**
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.c
index c62310356..e777e78bb 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.c
@@ -22,6 +22,9 @@ static ucs_stats_class_t uct_ep_stats_class = {
         [UCT_EP_STAT_PUT]         = "put",
         [UCT_EP_STAT_GET]         = "get",
         [UCT_EP_STAT_ATOMIC]      = "atomic",
+#if IBV_EXP_HW_TM
+        [UCT_EP_STAT_TAG]         = "tag",
+#endif
         [UCT_EP_STAT_BYTES_SHORT] = "bytes_short",
         [UCT_EP_STAT_BYTES_BCOPY] = "bytes_bcopy",
         [UCT_EP_STAT_BYTES_ZCOPY] = "bytes_zcopy",
@@ -81,20 +84,17 @@ ucs_status_t uct_iface_set_am_handler(uct_iface_h tl_iface, uint8_t id,
         return UCS_OK;
     }
 
-    if (!(flags & (UCT_CB_FLAG_SYNC|UCT_CB_FLAG_ASYNC))) {
-        ucs_error("invalid active message flags 0x%x", flags);
-        return UCS_ERR_INVALID_PARAM;
-    }
-
     status = uct_iface_query(tl_iface, &attr);
     if (status != UCS_OK) {
         return status;
     }
 
+    UCT_CB_FLAGS_CHECK(flags);
+
     /* If user wants a synchronous callback, it must be supported, or the
      * callback could be called from another thread.
      */
-    if ((flags & UCT_CB_FLAG_SYNC) && !(attr.cap.flags & UCT_IFACE_FLAG_CB_SYNC)) {
+    if (!(flags & UCT_CB_FLAG_ASYNC) && !(attr.cap.flags & UCT_IFACE_FLAG_CB_SYNC)) {
         ucs_error("Synchronous callback requested, but not supported");
         return UCS_ERR_INVALID_PARAM;
     }
@@ -329,19 +329,18 @@ ucs_status_t uct_set_ep_failed(ucs_class_t *cls, uct_ep_h tl_ep,
     ops->ep_put_short       = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_put_bcopy       = (void*)ucs_empty_function_return_bc_ep_timeout;
     ops->ep_put_zcopy       = (void*)ucs_empty_function_return_ep_timeout;
+    ops->ep_get_short       = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_get_bcopy       = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_get_zcopy       = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_am_short        = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_am_bcopy        = (void*)ucs_empty_function_return_bc_ep_timeout;
     ops->ep_am_zcopy        = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_add64    = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_fadd64   = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_swap64   = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_atomic_cswap64  = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_add32    = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_fadd32   = (void*)ucs_empty_function_return_ep_timeout;
-    ops->ep_atomic_swap32   = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_atomic_cswap32  = (void*)ucs_empty_function_return_ep_timeout;
+    ops->ep_atomic64_post   = (void*)ucs_empty_function_return_ep_timeout;
+    ops->ep_atomic32_post   = (void*)ucs_empty_function_return_ep_timeout;
+    ops->ep_atomic64_fetch  = (void*)ucs_empty_function_return_ep_timeout;
+    ops->ep_atomic32_fetch  = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_tag_eager_short = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_tag_eager_bcopy = (void*)ucs_empty_function_return_ep_timeout;
     ops->ep_tag_eager_zcopy = (void*)ucs_empty_function_return_ep_timeout;
@@ -410,13 +409,16 @@ UCS_CLASS_INIT_FUNC(uct_base_iface_t, uct_iface_ops_t *ops, uct_md_h md,
 
     UCS_CLASS_CALL_SUPER_INIT(uct_iface_t, ops);
 
-    self->md              = md;
-    self->worker          = ucs_derived_of(worker, uct_priv_worker_t);
-    self->am_tracer       = NULL;
-    self->am_tracer_arg   = NULL;
-    self->err_handler     = params->err_handler;
-    self->err_handler_arg = params->err_handler_arg;
-    self->progress_flags  = 0;
+    UCT_CB_FLAGS_CHECK(params->err_handler_flags);
+
+    self->md                = md;
+    self->worker            = ucs_derived_of(worker, uct_priv_worker_t);
+    self->am_tracer         = NULL;
+    self->am_tracer_arg     = NULL;
+    self->err_handler       = params->err_handler;
+    self->err_handler_flags = params->err_handler_flags;
+    self->err_handler_arg   = params->err_handler_arg;
+    self->progress_flags    = 0;
     uct_worker_progress_init(&self->prog);
 
     for (id = 0; id < UCT_AM_ID_MAX; ++id) {
@@ -452,6 +454,20 @@ static UCS_CLASS_CLEANUP_FUNC(uct_base_iface_t)
 UCS_CLASS_DEFINE(uct_base_iface_t, uct_iface_t);
 
 
+ucs_status_t uct_iface_accept(uct_iface_h iface,
+                              uct_conn_request_h conn_request)
+{
+    return iface->ops.iface_accept(iface, conn_request);
+}
+
+
+ucs_status_t uct_iface_reject(uct_iface_h iface,
+                              uct_conn_request_h conn_request)
+{
+    return iface->ops.iface_reject(iface, conn_request);
+}
+
+
 ucs_status_t uct_ep_create(uct_iface_h iface, uct_ep_h *ep_p)
 {
     return iface->ops.ep_create(iface, ep_p);
@@ -466,9 +482,11 @@ uct_ep_create_connected(uct_iface_h iface, const uct_device_addr_t *dev_addr,
 
 ucs_status_t
 uct_ep_create_sockaddr(uct_iface_h iface, const ucs_sock_addr_t *sockaddr,
-                       const void *priv_data, size_t length, uct_ep_h *ep_p)
+                       uct_sockaddr_priv_pack_callback_t pack_cb,
+                       void *arg, uint32_t cb_flags, uct_ep_h *ep_p)
 {
-    return iface->ops.ep_create_sockaddr(iface, sockaddr, priv_data, length, ep_p);
+    return iface->ops.ep_create_sockaddr(iface, sockaddr, pack_cb, arg,
+                                         cb_flags, ep_p);
 }
 
 void uct_ep_destroy(uct_ep_h ep)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.h
index 3be8800bb..ebbf781ff 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_iface.h
@@ -26,6 +26,9 @@ enum {
     UCT_EP_STAT_PUT,
     UCT_EP_STAT_GET,
     UCT_EP_STAT_ATOMIC,
+#if IBV_EXP_HW_TM
+    UCT_EP_STAT_TAG,
+#endif
     UCT_EP_STAT_BYTES_SHORT,
     UCT_EP_STAT_BYTES_BCOPY,
     UCT_EP_STAT_BYTES_ZCOPY,
@@ -76,6 +79,15 @@ enum {
     UCS_STATS_UPDATE_COUNTER((_iface)->stats, UCT_IFACE_STAT_TX_NO_DESC, 1);
 
 
+#define UCT_CB_FLAGS_CHECK(_flags) \
+    do { \
+        if ((_flags) & UCT_CB_FLAG_RESERVED) { \
+            ucs_error("Unsupported callback flag 0x%x", UCT_CB_FLAG_RESERVED); \
+            return UCS_ERR_INVALID_PARAM; \
+        } \
+    } while (0)
+
+
 /**
  * In release mode - do nothing.
  *
@@ -175,6 +187,7 @@ typedef struct uct_base_iface {
     void                    *am_tracer_arg;   /* Tracer argument */
     uct_error_handler_t     err_handler;      /* Error handler */
     void                    *err_handler_arg; /* Error handler argument */
+    uint32_t                err_handler_flags; /* Error handler callback flags */
     uct_worker_progress_t   prog;             /* Will be removed once all transports
                                                  support progress control */
     unsigned                progress_flags;   /* Which progress is currently enabled */
@@ -245,7 +258,9 @@ typedef struct uct_tl_component {
         .cfg_prefix          = _cfg_prefix, \
         .iface_config_table  = _cfg_table, \
         .iface_config_size   = sizeof(_cfg_struct) \
-    };
+    }; \
+    UCS_CONFIG_REGISTER_TABLE(_cfg_table, _name" transport", _cfg_prefix, \
+                              _cfg_struct)
 
 
 /**
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.c
index 8983a2c74..4c9712215 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.c
@@ -401,14 +401,6 @@ void uct_config_release(void *config)
     ucs_free(bundle);
 }
 
-void uct_config_print(const void *config, FILE *stream, const char *title,
-                      ucs_config_print_flags_t print_flags)
-{
-    uct_config_bundle_t *bundle = (uct_config_bundle_t *)config - 1;
-    ucs_config_parser_print_opts(stream, title, bundle->data, bundle->table,
-                                 bundle->table_prefix, print_flags);
-}
-
 ucs_status_t uct_config_get(void *config, const char *name, char *value,
                             size_t max)
 {
@@ -423,28 +415,6 @@ ucs_status_t uct_config_modify(void *config, const char *name, const char *value
     return ucs_config_parser_set_value(bundle->data, bundle->table, name, value);
 }
 
-void uct_md_component_config_print(ucs_config_print_flags_t print_flags)
-{
-    uct_md_component_t *mdc;
-    uct_md_config_t *md_config;
-    char cfg_title[UCT_TL_NAME_MAX + 128];
-    ucs_status_t status;
-
-    /* go over the list of md components and print the config table per each */
-    ucs_list_for_each(mdc, &uct_md_components_list, list)
-    {
-        snprintf(cfg_title, sizeof(cfg_title), "%s MD component configuration",
-                 mdc->name);
-        status = uct_md_config_read(mdc->name, NULL, NULL, &md_config);
-        if (status != UCS_OK) {
-            ucs_error("Failed to read md_config for MD component %s", mdc->name);
-            continue;
-        }
-        uct_config_print(md_config, stdout, cfg_title, print_flags);
-        uct_config_release(md_config);
-    }
-}
-
 ucs_status_t uct_md_mkey_pack(uct_md_h md, uct_mem_h memh, void *rkey_buffer)
 {
     void *rbuf = uct_md_fill_md_name(md, rkey_buffer);
@@ -522,7 +492,7 @@ ucs_status_t uct_md_mem_alloc(uct_md_h md, size_t *length_p, void **address_p,
         return status;
     }
 
-    return md->ops->mem_alloc(md, length_p, address_p, flags, memh_p UCS_MEMTRACK_VAL);
+    return md->ops->mem_alloc(md, length_p, address_p, flags, alloc_name, memh_p);
 }
 
 ucs_status_t uct_md_mem_free(uct_md_h md, uct_mem_h memh)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.h
index e1276880b..091e99fbf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_md.h
@@ -54,9 +54,8 @@ extern ucs_config_field_t uct_md_config_rcache_table[];
  * Specific MDs extend this structure.
  */
 struct uct_md_config {
-#ifdef __cplusplus
-    char __dummy;
-#endif
+    /* C standard prohibits empty structures */
+    char                   __dummy;
 };
 
 
@@ -102,7 +101,9 @@ typedef struct uct_md_registered_tl {
     }; \
     UCS_STATIC_INIT { \
         ucs_list_add_tail(&uct_md_components_list, &_mdc.list); \
-    }
+    } \
+    UCS_CONFIG_REGISTER_TABLE(_cfg_table, _name" memory domain", _cfg_prefix, \
+                              _cfg_struct)
 
 
 /**
@@ -129,7 +130,8 @@ struct uct_md_ops {
     ucs_status_t (*query)(uct_md_h md, uct_md_attr_t *md_attr);
 
     ucs_status_t (*mem_alloc)(uct_md_h md, size_t *length_p, void **address_p,
-                              unsigned flags, uct_mem_h *memh_p UCS_MEMTRACK_ARG);
+                              unsigned flags, const char *alloc_name,
+                              uct_mem_h *memh_p);
 
     ucs_status_t (*mem_free)(uct_md_h md, uct_mem_h memh);
     ucs_status_t (*mem_advise)(uct_md_h md, uct_mem_h memh, void *addr,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_mem.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_mem.c
index 4cb55cea3..cf8450402 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_mem.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_mem.c
@@ -8,7 +8,8 @@
 #include "uct_md.h"
 
 #include <ucs/arch/cpu.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
+#include <ucs/sys/math.h>
 
 
 typedef struct {
@@ -64,6 +65,7 @@ ucs_status_t uct_mem_alloc(void *addr, size_t min_length, unsigned flags,
     void *address;
     int shmid;
 #ifdef MADV_HUGEPAGE
+    ssize_t huge_page_size;
     int ret;
 #endif
 
@@ -119,9 +121,9 @@ ucs_status_t uct_mem_alloc(void *addr, size_t min_length, unsigned flags,
                 status = uct_md_mem_alloc(md, &alloc_length, &address, flags,
                                           alloc_name, &memh);
                 if (status != UCS_OK) {
-                    ucs_error("failed to allocate %zu bytes using md %s: %s",
+                    ucs_error("failed to allocate %zu bytes using md %s for %s: %s",
                               alloc_length, md->component->name,
-                              ucs_status_string(status));
+                              alloc_name, ucs_status_string(status));
                     return status;
                 }
 
@@ -136,22 +138,26 @@ ucs_status_t uct_mem_alloc(void *addr, size_t min_length, unsigned flags,
 
         case UCT_ALLOC_METHOD_THP:
 #ifdef MADV_HUGEPAGE
+            /* Fixed option is not supported for thp allocation*/
+            if (flags & UCT_MD_MEM_FLAG_FIXED) {
+                break;
+            }
+
             if (!ucs_is_thp_enabled()) {
                 break;
             }
 
-            /* Fixed option is not supported for thp allocation*/
-            if (flags & UCT_MD_MEM_FLAG_FIXED) {
+            huge_page_size = ucs_get_huge_page_size();
+            if (huge_page_size <= 0) {
                 break;
             }
 
-            alloc_length = ucs_align_up(min_length, ucs_get_huge_page_size());
+            alloc_length = ucs_align_up(min_length, huge_page_size);
             if (alloc_length >= 2 * min_length) {
                 break;
             }
 
-            address = ucs_memalign(ucs_get_huge_page_size(), alloc_length
-                                   UCS_MEMTRACK_VAL);
+            address = ucs_memalign(huge_page_size, alloc_length UCS_MEMTRACK_VAL);
             if (address == NULL) {
                 ucs_trace("failed to allocate %zu bytes using THP: %m", alloc_length);
             } else {
@@ -206,7 +212,7 @@ ucs_status_t uct_mem_alloc(void *addr, size_t min_length, unsigned flags,
             alloc_length = min_length;
             address = (flags & UCT_MD_MEM_FLAG_FIXED) ? addr : NULL;
             status = ucs_sysv_alloc(&alloc_length, min_length * 2, &address,
-                                    SHM_HUGETLB, &shmid UCS_MEMTRACK_VAL);
+                                    SHM_HUGETLB, alloc_name, &shmid);
             if (status == UCS_OK) {
                 goto allocated_without_md;
             }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.c
index a2c75b99f..e9da0de50 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.c
@@ -31,6 +31,10 @@ static UCS_CLASS_INIT_FUNC(uct_priv_worker_t, ucs_async_context_t *async,
 {
     UCS_CLASS_CALL_SUPER_INIT(uct_worker_t);
 
+    if (async == NULL) {
+        return UCS_ERR_INVALID_PARAM;
+    }
+
     self->async       = async;
     self->thread_mode = thread_mode;
     ucs_list_head_init(&self->tl_data);
@@ -101,8 +105,8 @@ void uct_worker_progress_register_safe(uct_worker_h tl_worker, ucs_callback_t fu
     if (*id_p == UCS_CALLBACKQ_ID_NULL) {
         UCS_ASYNC_BLOCK(worker->async);
         *id_p = ucs_callbackq_add_safe(&worker->super.progress_q, func, arg, flags);
-        UCS_ASYNC_UNBLOCK(worker->async);
         ucs_assert(*id_p != UCS_CALLBACKQ_ID_NULL);
+        UCS_ASYNC_UNBLOCK(worker->async);
     }
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.h
index 9342b1756..78abfb1a1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/base/uct_worker.h
@@ -19,7 +19,6 @@ typedef struct uct_worker_tl_data {
     ucs_list_link_t        list;
     uint32_t               refcount;
     uint32_t               key;
-    void                   *ptr;
 } uct_worker_tl_data_t;
 
 
@@ -40,6 +39,8 @@ typedef struct uct_worker_progress {
 #define uct_worker_tl_data_get(_worker, _key, _type, _cmp_fn, _init_fn, ...) \
     ({ \
         uct_worker_tl_data_t *data; \
+        _type *result; \
+        ucs_status_t status; \
         \
         ucs_list_for_each(data, &(_worker)->tl_data, list) { \
             if ((data->key == (_key)) && _cmp_fn(ucs_derived_of(data, _type), \
@@ -50,16 +51,26 @@ typedef struct uct_worker_progress {
             } \
         } \
         \
-        if (&data->list == &(_worker)->tl_data) { \
+        if (&data->list == &(_worker)->tl_data) { /* not found */ \
             data = ucs_malloc(sizeof(_type), UCS_PP_QUOTE(_type)); \
-            if (data != NULL) { \
+            if (data == NULL) { \
+                result = UCS_STATUS_PTR(UCS_ERR_NO_MEMORY); \
+            } else { \
                 data->key      = (_key); \
                 data->refcount = 1; \
-                _init_fn(ucs_derived_of(data, _type), ## __VA_ARGS__); \
-                ucs_list_add_tail(&(_worker)->tl_data, &data->list); \
+                status = _init_fn(ucs_derived_of(data, _type), ## __VA_ARGS__); \
+                if (status != UCS_OK) { \
+                    ucs_free(data); \
+                    result = UCS_STATUS_PTR(status); \
+                } else { \
+                    ucs_list_add_tail(&(_worker)->tl_data, &data->list); \
+                    result = ucs_derived_of(data, _type); \
+                } \
             } \
+        } else { \
+            result = ucs_derived_of(data, _type); \
         } \
-        ucs_derived_of(data, _type); \
+        result; \
     })
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_iface.h
new file mode 100644
index 000000000..4891182a4
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_iface.h
@@ -0,0 +1,47 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_CUDA_IFACE_H
+#define UCT_CUDA_IFACE_H
+
+#include <ucs/sys/preprocessor.h>
+#include <cuda_runtime.h>
+#include <cuda.h>
+
+#define UCT_CUDA_FUNC(_func)                                    \
+    ({                                                          \
+        ucs_status_t _status = UCS_OK;                          \
+        do {                                                    \
+            cudaError_t _result = (_func);                      \
+            if (cudaSuccess != _result) {                       \
+                ucs_error("%s is failed. ret:%s",               \
+                          UCS_PP_MAKE_STRING(_func),            \
+                          cudaGetErrorString(_result));         \
+                _status = UCS_ERR_IO_ERROR;                     \
+            }                                                   \
+        } while (0);                                            \
+        _status;                                                \
+    })
+
+
+#define UCT_CUDADRV_FUNC(_func)                                 \
+    ({                                                          \
+        ucs_status_t _status = UCS_OK;                          \
+        do {                                                    \
+            CUresult _result = (_func);                         \
+            const char *cu_err_str;                             \
+            if (CUDA_ERROR_NOT_READY == _result) {              \
+                _status = UCS_INPROGRESS;                       \
+            } else if (CUDA_SUCCESS != _result) {               \
+                cuGetErrorString(_result, &cu_err_str);         \
+                ucs_error("%s is failed. ret:%s",               \
+                          UCS_PP_MAKE_STRING(_func),cu_err_str);\
+                _status = UCS_ERR_IO_ERROR;                     \
+            }                                                   \
+        } while (0);                                            \
+        _status;                                                \
+    })
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.c
new file mode 100644
index 000000000..d9cf0f9ce
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.c
@@ -0,0 +1,26 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#include <uct/cuda/base/cuda_md.h>
+
+#include <cuda_runtime.h>
+#include <cuda.h>
+
+int uct_cuda_is_mem_type_owned(uct_md_h md, void *addr, size_t length)
+{
+    CUmemorytype memType = 0;
+    uint32_t isManaged = 0;
+    void *attrdata[] = {(void *)&memType, (void *)&isManaged};
+    CUpointer_attribute attributes[2] = {CU_POINTER_ATTRIBUTE_MEMORY_TYPE,
+                                         CU_POINTER_ATTRIBUTE_IS_MANAGED};
+    CUresult cu_err;
+
+    if (addr == NULL) {
+        return 0;
+    }
+
+    cu_err = cuPointerGetAttributes(2, attributes, attrdata, (CUdeviceptr)addr);
+    return ((cu_err == CUDA_SUCCESS) && (!isManaged && (memType == CU_MEMORYTYPE_DEVICE)));
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.h
new file mode 100644
index 000000000..050848a17
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/base/cuda_md.h
@@ -0,0 +1,13 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_CUDA_MD_H
+#define UCT_CUDA_MD_H
+
+#include <uct/base/uct_md.h>
+
+int uct_cuda_is_mem_type_owned(uct_md_h md, void *addr, size_t length);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.c
index 46cab05d8..a323e9ee6 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.c
@@ -127,3 +127,35 @@ ucs_status_t uct_cuda_copy_ep_put_zcopy(uct_ep_h tl_ep, const uct_iov_t *iov, si
     return status;
 
 }
+
+
+ucs_status_t uct_cuda_copy_ep_put_short(uct_ep_h tl_ep, const void *buffer,
+                                        unsigned length, uint64_t remote_addr,
+                                        uct_rkey_t rkey)
+{
+    ucs_status_t status;
+
+    status = UCT_CUDA_FUNC(cudaMemcpy((void *)remote_addr, buffer,
+                                      length, cudaMemcpyHostToDevice));
+
+    UCT_TL_EP_STAT_OP(ucs_derived_of(tl_ep, uct_base_ep_t), PUT, SHORT, length);
+    ucs_trace_data("PUT_SHORT size %d from %p to %p",
+                   length, buffer, (void *)remote_addr);
+    return status;
+}
+
+ucs_status_t uct_cuda_copy_ep_get_short(uct_ep_h tl_ep, void *buffer,
+                                        unsigned length, uint64_t remote_addr,
+                                        uct_rkey_t rkey)
+{
+    ucs_status_t status;
+
+    status = UCT_CUDA_FUNC(cudaMemcpy(buffer, (void *)remote_addr,
+                                      length, cudaMemcpyDeviceToHost));
+
+    UCT_TL_EP_STAT_OP(ucs_derived_of(tl_ep, uct_base_ep_t), GET, SHORT, length);
+    ucs_trace_data("GET_SHORT size %d from %p to %p",
+                   length, (void *)remote_addr, buffer);
+    return status;
+}
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.h
index e029a3929..ed0e88b3b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_ep.h
@@ -33,4 +33,13 @@ ucs_status_t uct_cuda_copy_ep_put_zcopy(uct_ep_h tl_ep,
                                         const uct_iov_t *iov, size_t iovcnt,
                                         uint64_t remote_addr, uct_rkey_t rkey,
                                         uct_completion_t *comp);
+
+ucs_status_t uct_cuda_copy_ep_put_short(uct_ep_h tl_ep, const void *buffer,
+                                        unsigned length, uint64_t remote_addr,
+                                        uct_rkey_t rkey);
+
+ucs_status_t uct_cuda_copy_ep_get_short(uct_ep_h tl_ep, void *buffer,
+                                        unsigned length, uint64_t remote_addr,
+                                        uct_rkey_t rkey);
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.c
index f5e393b58..7b877b7b1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.c
@@ -9,6 +9,7 @@
 
 #include <ucs/type/class.h>
 #include <ucs/sys/string.h>
+#include <ucs/arch/cpu.h>
 
 
 static ucs_config_field_t uct_cuda_copy_iface_config_table[] = {
@@ -19,7 +20,11 @@ static ucs_config_field_t uct_cuda_copy_iface_config_table[] = {
 
     {"MAX_POLL", "16",
      "Max number of event completions to pick during cuda events polling",
-      ucs_offsetof(uct_cuda_copy_iface_config_t, max_poll), UCS_CONFIG_TYPE_UINT},
+     ucs_offsetof(uct_cuda_copy_iface_config_t, max_poll), UCS_CONFIG_TYPE_UINT},
+
+    {"MAX_EVENTS", "1024",
+     "Max number of cuda events. -1 is infinite",
+     ucs_offsetof(uct_cuda_copy_iface_config_t, max_cuda_events), UCS_CONFIG_TYPE_UINT},
 
     {NULL}
 };
@@ -32,17 +37,20 @@ static void UCS_CLASS_DELETE_FUNC_NAME(uct_cuda_copy_iface_t)(uct_iface_t*);
 static ucs_status_t uct_cuda_copy_iface_get_address(uct_iface_h tl_iface,
                                                     uct_iface_addr_t *iface_addr)
 {
-    int *cuda_copy_addr = (int*)iface_addr;
+    uct_cuda_copy_iface_t *iface = ucs_derived_of(tl_iface, uct_cuda_copy_iface_t);
 
-    *cuda_copy_addr = 0;
+    *(uct_cuda_copy_iface_addr_t*)iface_addr = iface->id;
     return UCS_OK;
 }
 
-static int uct_cuda_copy_iface_is_reachable(const uct_iface_h iface,
+static int uct_cuda_copy_iface_is_reachable(const uct_iface_h tl_iface,
                                             const uct_device_addr_t *dev_addr,
                                             const uct_iface_addr_t *iface_addr)
 {
-    return 1;
+    uct_cuda_copy_iface_t  *iface = ucs_derived_of(tl_iface, uct_cuda_copy_iface_t);
+    uct_cuda_copy_iface_addr_t *addr = (uct_cuda_copy_iface_addr_t*)iface_addr;
+
+    return (addr != NULL) && (iface->id == *addr);
 }
 
 static ucs_status_t uct_cuda_copy_iface_query(uct_iface_h iface,
@@ -50,15 +58,17 @@ static ucs_status_t uct_cuda_copy_iface_query(uct_iface_h iface,
 {
     memset(iface_attr, 0, sizeof(uct_iface_attr_t));
 
-    iface_attr->iface_addr_len          = sizeof(int);
+    iface_attr->iface_addr_len          = sizeof(uct_cuda_copy_iface_addr_t);
     iface_attr->device_addr_len         = 0;
     iface_attr->ep_addr_len             = 0;
     iface_attr->cap.flags               = UCT_IFACE_FLAG_CONNECT_TO_IFACE |
+                                          UCT_IFACE_FLAG_GET_SHORT |
+                                          UCT_IFACE_FLAG_PUT_SHORT |
                                           UCT_IFACE_FLAG_GET_ZCOPY |
                                           UCT_IFACE_FLAG_PUT_ZCOPY |
                                           UCT_IFACE_FLAG_PENDING;
 
-    iface_attr->cap.put.max_short       = 0;
+    iface_attr->cap.put.max_short       = UINT_MAX;
     iface_attr->cap.put.max_bcopy       = 0;
     iface_attr->cap.put.min_zcopy       = 0;
     iface_attr->cap.put.max_zcopy       = SIZE_MAX;
@@ -66,6 +76,7 @@ static ucs_status_t uct_cuda_copy_iface_query(uct_iface_h iface,
     iface_attr->cap.put.align_mtu       = iface_attr->cap.put.opt_zcopy_align;
     iface_attr->cap.put.max_iov         = 1;
 
+    iface_attr->cap.get.max_short       = UINT_MAX;
     iface_attr->cap.get.max_bcopy       = 0;
     iface_attr->cap.get.min_zcopy       = 0;
     iface_attr->cap.get.max_zcopy       = SIZE_MAX;
@@ -151,6 +162,8 @@ static unsigned uct_cuda_copy_iface_progress(uct_iface_h tl_iface)
 }
 
 static uct_iface_ops_t uct_cuda_copy_iface_ops = {
+    .ep_get_short             = uct_cuda_copy_ep_get_short,
+    .ep_put_short             = uct_cuda_copy_ep_put_short,
     .ep_get_zcopy             = uct_cuda_copy_ep_get_zcopy,
     .ep_put_zcopy             = uct_cuda_copy_ep_put_zcopy,
     .ep_pending_add           = ucs_empty_function_return_busy,
@@ -220,7 +233,9 @@ static UCS_CLASS_INIT_FUNC(uct_cuda_copy_iface_t, uct_md_h md, uct_worker_h work
         return UCS_ERR_NO_DEVICE;
     }
 
-    self->config.max_poll = config->max_poll;
+    self->id                     = ucs_generate_uuid((uintptr_t)self);
+    self->config.max_poll        = config->max_poll;
+    self->config.max_cuda_events = config->max_cuda_events;
 
     status = ucs_mpool_init(&self->cuda_event_desc,
                             0,
@@ -228,7 +243,7 @@ static UCS_CLASS_INIT_FUNC(uct_cuda_copy_iface_t, uct_md_h md, uct_worker_h work
                             0,
                             UCS_SYS_CACHE_LINE_SIZE,
                             128,
-                            1024,
+                            self->config.max_cuda_events,
                             &uct_cuda_copy_event_desc_mpool_ops,
                             "CUDA EVENT objects");
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.h
index 47ce350c8..7c8c5de80 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_iface.h
@@ -7,37 +7,27 @@
 #define UCT_CUDA_COPY_IFACE_H
 
 #include <uct/base/uct_iface.h>
-#include <ucs/arch/cpu.h>
-#include <ucs/sys/preprocessor.h>
-#include <cuda_runtime.h>
-#include <cuda.h>
+#include <uct/cuda/base/cuda_iface.h>
 
 
 #define UCT_CUDA_COPY_TL_NAME    "cuda_copy"
-#define UCT_CUDA_DEV_NAME   "cudacopy0"
-
-#define UCT_CUDA_FUNC(_func)  ({                        \
-ucs_status_t _status = UCS_OK;                          \
-do {                                                    \
-    cudaError_t _result = (_func);                      \
-    if (cudaSuccess != _result) {                       \
-        ucs_error("%s failed with %d \n",               \
-                  UCS_PP_MAKE_STRING(_func), _result);  \
-        _status = UCS_ERR_IO_ERROR;                     \
-    }                                                   \
-} while (0);                                            \
-_status;                                                \
-})
+#define UCT_CUDA_DEV_NAME        "cudacopy0"
+
+
+typedef uint64_t uct_cuda_copy_iface_addr_t;
+
 
 typedef struct uct_cuda_copy_iface {
-    uct_base_iface_t        super;
-    ucs_mpool_t             cuda_event_desc;
-    ucs_queue_head_t        outstanding_d2h_cuda_event_q;
-    ucs_queue_head_t        outstanding_h2d_cuda_event_q;
-    cudaStream_t            stream_d2h;
-    cudaStream_t            stream_h2d;
+    uct_base_iface_t            super;
+    uct_cuda_copy_iface_addr_t  id;
+    ucs_mpool_t                 cuda_event_desc;
+    ucs_queue_head_t            outstanding_d2h_cuda_event_q;
+    ucs_queue_head_t            outstanding_h2d_cuda_event_q;
+    cudaStream_t                stream_d2h;
+    cudaStream_t                stream_h2d;
     struct {
-        unsigned            max_poll;
+        unsigned                max_poll;
+        unsigned                max_cuda_events;
     } config;
 } uct_cuda_copy_iface_t;
 
@@ -45,8 +35,10 @@ typedef struct uct_cuda_copy_iface {
 typedef struct uct_cuda_copy_iface_config {
     uct_iface_config_t      super;
     unsigned                max_poll;
+    unsigned                max_cuda_events;
 } uct_cuda_copy_iface_config_t;
 
+
 typedef struct uct_cuda_copy_event_desc {
     cudaEvent_t event;
     uct_completion_t *comp;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.c
index b255f84f0..af6e17b6d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.c
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2017.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2017-2018.  ALL RIGHTS RESERVED.
  * See file LICENSE for terms.
  */
 
@@ -92,33 +92,6 @@ static ucs_status_t uct_cuda_copy_mem_dereg(uct_md_h md, uct_mem_h memh)
     return UCS_OK;
 }
 
-static int uct_is_cuda_copy_mem_type_owned(uct_md_h md, void *addr, size_t length)
-{
-    int memory_type;
-    struct cudaPointerAttributes attributes;
-    cudaError_t cuda_err;
-    CUresult cu_err;
-
-    if (addr == NULL) {
-        return 0;
-    }
-
-    cu_err = cuPointerGetAttribute(&memory_type,
-                                   CU_POINTER_ATTRIBUTE_MEMORY_TYPE,
-                                   (CUdeviceptr)addr);
-    if (cu_err != CUDA_SUCCESS) {
-        cuda_err = cudaPointerGetAttributes (&attributes, addr);
-        if (cuda_err == cudaSuccess) {
-            if (attributes.memoryType == cudaMemoryTypeDevice) {
-                return 1;
-            }
-        }
-    } else if (memory_type == CU_MEMORYTYPE_DEVICE) {
-        return 1;
-    }
-    return 0;
-}
-
 static ucs_status_t uct_cuda_copy_query_md_resources(uct_md_resource_desc_t **resources_p,
                                                      unsigned *num_resources_p)
 {
@@ -148,7 +121,7 @@ static uct_md_ops_t md_ops = {
     .mkey_pack          = uct_cuda_copy_mkey_pack,
     .mem_reg            = uct_cuda_copy_mem_reg,
     .mem_dereg          = uct_cuda_copy_mem_dereg,
-    .is_mem_type_owned  = uct_is_cuda_copy_mem_type_owned,
+    .is_mem_type_owned  = uct_cuda_is_mem_type_owned,
 };
 
 static ucs_status_t uct_cuda_copy_md_open(const char *md_name, const uct_md_config_t *md_config,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.h
index 4af286317..bd50206e1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_copy/cuda_copy_md.h
@@ -7,6 +7,7 @@
 #define UCT_CUDA_COPY_MD_H
 
 #include <uct/base/uct_md.h>
+#include <uct/cuda/base/cuda_md.h>
 
 #define UCT_CUDA_COPY_MD_NAME           "cuda_cpy"
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.c
new file mode 100644
index 000000000..becb90a89
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.c
@@ -0,0 +1,263 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "cuda_ipc_cache.h"
+#include <ucs/debug/log.h>
+#include <ucs/debug/memtrack.h>
+#include <ucs/profile/profile.h>
+#include <ucs/sys/sys.h>
+#include <ucs/sys/math.h>
+
+static ucs_pgt_dir_t *uct_cuda_ipc_cache_pgt_dir_alloc(const ucs_pgtable_t *pgtable)
+{
+    return ucs_memalign(UCS_PGT_ENTRY_MIN_ALIGN, sizeof(ucs_pgt_dir_t),
+                        "cuda_ipc_cache_pgdir");
+}
+
+static void uct_cuda_ipc_cache_pgt_dir_release(const ucs_pgtable_t *pgtable,
+                                               ucs_pgt_dir_t *dir)
+{
+    ucs_free(dir);
+}
+
+static void
+uct_cuda_ipc_cache_region_collect_callback(const ucs_pgtable_t *pgtable,
+                                           ucs_pgt_region_t *pgt_region,
+                                           void *arg)
+{
+    ucs_list_link_t *list = arg;
+    uct_cuda_ipc_cache_region_t *region;
+
+    region = ucs_derived_of(pgt_region, uct_cuda_ipc_cache_region_t);
+    ucs_list_add_tail(list, &region->list);
+}
+
+static void uct_cuda_ipc_cache_purge(uct_cuda_ipc_cache_t *cache)
+{
+    uct_cuda_ipc_cache_region_t *region, *tmp;
+    ucs_list_link_t region_list;
+
+    ucs_list_head_init(&region_list);
+    ucs_pgtable_purge(&cache->pgtable, uct_cuda_ipc_cache_region_collect_callback,
+                      &region_list);
+    ucs_list_for_each_safe(region, tmp, &region_list, list) {
+        UCT_CUDADRV_FUNC(cuIpcCloseMemHandle((CUdeviceptr)region->mapped_addr));
+        ucs_free(region);
+    }
+    ucs_trace("%s: cuda ipc cache purged", cache->name);
+}
+
+static ucs_status_t uct_cuda_ipc_open_memhandle(CUipcMemHandle memh,
+                                                CUdeviceptr *mapped_addr)
+{
+    CUresult cuerr;
+
+    cuerr = cuIpcOpenMemHandle(mapped_addr, memh,
+                               CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS);
+    if (cuerr != CUDA_SUCCESS) {
+        if (cuerr == CUDA_ERROR_ALREADY_MAPPED) {
+            return UCS_ERR_ALREADY_EXISTS;
+        }
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    return UCS_OK;
+}
+
+static void uct_cuda_ipc_cache_invalidate_regions(uct_cuda_ipc_cache_t *cache,
+                                                  void *from, void *to)
+{
+    ucs_list_link_t region_list;
+    ucs_status_t status;
+    uct_cuda_ipc_cache_region_t *region, *tmp;
+
+    ucs_list_head_init(&region_list);
+    ucs_pgtable_search_range(&cache->pgtable, (ucs_pgt_addr_t)from,
+                             (ucs_pgt_addr_t)to,
+                             uct_cuda_ipc_cache_region_collect_callback,
+                             &region_list);
+    ucs_list_for_each_safe(region, tmp, &region_list, list) {
+        status = ucs_pgtable_remove(&cache->pgtable, &region->super);
+        if (status != UCS_OK) {
+            ucs_error("failed to remove address:%p from cache (%s)",
+                      (void *)region->key.d_bptr, ucs_status_string(status));
+        }
+        UCT_CUDADRV_FUNC(cuIpcCloseMemHandle((CUdeviceptr)region->mapped_addr));
+        ucs_free(region);
+    }
+    ucs_trace("%s: closed memhandles in the range [%p..%p]",
+              cache->name, from, to);
+}
+
+ucs_status_t uct_cuda_ipc_cache_map_memhandle(void *arg, uct_cuda_ipc_key_t *key,
+                                              void **mapped_addr)
+{
+    uct_cuda_ipc_cache_t *cache = (uct_cuda_ipc_cache_t *)arg;
+    ucs_status_t status;
+    ucs_pgt_region_t *pgt_region;
+    uct_cuda_ipc_cache_region_t *region;
+
+    pthread_rwlock_rdlock(&cache->lock);
+    pgt_region = UCS_PROFILE_CALL(ucs_pgtable_lookup,
+                                  &cache->pgtable, key->d_bptr);
+    if (ucs_likely(pgt_region != NULL)) {
+        region = ucs_derived_of(pgt_region, uct_cuda_ipc_cache_region_t);
+        if (memcmp((const void *)&key->ph, (const void *)&region->key.ph,
+                   sizeof(key->ph)) == 0) {
+            /*cache hit */
+            ucs_trace("%s: cuda_ipc cache hit addr:%p size:%lu region:"
+                      UCS_PGT_REGION_FMT, cache->name, (void *)key->d_bptr,
+                      key->b_len, UCS_PGT_REGION_ARG(&region->super));
+
+            *mapped_addr = region->mapped_addr;
+            pthread_rwlock_unlock(&cache->lock);
+            return UCS_OK;
+        } else {
+            ucs_trace("%s: cuda_ipc cache remove stale region:"
+                      UCS_PGT_REGION_FMT " new_addr:%p new_size:%lu",
+                      cache->name, UCS_PGT_REGION_ARG(&region->super),
+                      (void *)key->d_bptr, key->b_len);
+
+            status = ucs_pgtable_remove(&cache->pgtable, &region->super);
+            if (status != UCS_OK) {
+                ucs_error("%s: failed to remove address:%p from cache",
+                          cache->name, (void *)key->d_bptr);
+                goto err;
+            }
+
+            /* close memhandle */
+            UCT_CUDADRV_FUNC(cuIpcCloseMemHandle((CUdeviceptr)
+                                                 region->mapped_addr));
+            ucs_free(region);
+        }
+    }
+
+    status = uct_cuda_ipc_open_memhandle(key->ph, (CUdeviceptr *)mapped_addr);
+    if (ucs_unlikely(status != UCS_OK)) {
+        if (ucs_likely(status == UCS_ERR_ALREADY_EXISTS)) {
+            /* unmap all overlapping regions and retry*/
+            uct_cuda_ipc_cache_invalidate_regions(cache, (void *)key->d_bptr,
+                                                  (void *)key->d_bptr + key->b_len);
+            status = uct_cuda_ipc_open_memhandle(key->ph, (CUdeviceptr *)mapped_addr);
+            if (ucs_unlikely(status != UCS_OK)) {
+                if (ucs_likely(status == UCS_ERR_ALREADY_EXISTS)) {
+                    /* unmap all cache entries and retry */
+                    uct_cuda_ipc_cache_purge(cache);
+                    status = uct_cuda_ipc_open_memhandle(key->ph, (CUdeviceptr *)mapped_addr);
+                    if (status != UCS_OK) {
+                        ucs_fatal("%s: failed to open ipc mem handle. addr:%p "
+                                  "len:%lu (%s)", cache->name,
+                                  (void *)key->d_bptr, key->b_len,
+                                  ucs_status_string(status));
+                    }
+                } else {
+                    ucs_fatal("%s: failed to open ipc mem handle. addr:%p len:%lu",
+                              cache->name, (void *)key->d_bptr, key->b_len);
+                }
+            }
+        } else {
+            ucs_fatal("%s: failed to open ipc mem handle. addr:%p len:%lu",
+                      cache->name, (void *)key->d_bptr, key->b_len);
+        }
+    }
+
+    /*create new cache entry */
+    region = ucs_memalign(UCS_PGT_ENTRY_MIN_ALIGN,
+                          sizeof(uct_cuda_ipc_cache_region_t),
+                          "uct_cuda_ipc_cache_region");
+    if (region == NULL) {
+        ucs_warn("failed to allocate uct_cuda_ipc_cache region");
+        status = UCS_ERR_NO_MEMORY;
+        goto err;
+    }
+
+    region->super.start = ucs_align_down_pow2((uintptr_t)key->d_bptr,
+                                               UCS_PGT_ADDR_ALIGN);
+    region->super.end   = ucs_align_up_pow2  ((uintptr_t)key->d_bptr + key->b_len,
+                                               UCS_PGT_ADDR_ALIGN);
+    region->key         = *key;
+    region->mapped_addr = *mapped_addr;
+
+    status = UCS_PROFILE_CALL(ucs_pgtable_insert,
+                              &cache->pgtable, &region->super);
+    if (status == UCS_ERR_ALREADY_EXISTS) {
+        /* overlapped region means memory freed at source. remove and try insert */
+        uct_cuda_ipc_cache_invalidate_regions(cache,
+                                              (void *)region->super.start,
+                                              (void *)region->super.end);
+        status = UCS_PROFILE_CALL(ucs_pgtable_insert,
+                                  &cache->pgtable, &region->super);
+    }
+    if (status != UCS_OK) {
+
+        ucs_error("%s: failed to insert region:"UCS_PGT_REGION_FMT" size:%lu :%s",
+                  cache->name, UCS_PGT_REGION_ARG(&region->super), key->b_len,
+                  ucs_status_string(status));
+        ucs_free(region);
+        goto err;
+    }
+
+    ucs_trace("%s: cuda_ipc cache new region:"UCS_PGT_REGION_FMT" size:%lu",
+              cache->name, UCS_PGT_REGION_ARG(&region->super), key->b_len);
+
+    pthread_rwlock_unlock(&cache->lock);
+    return UCS_OK;
+err:
+    pthread_rwlock_unlock(&cache->lock);
+    return status;
+}
+
+ucs_status_t uct_cuda_ipc_create_cache(uct_cuda_ipc_cache_t **cache,
+                                       const char *name)
+{
+    ucs_status_t status;
+    uct_cuda_ipc_cache_t *cache_desc;
+    int ret;
+
+    cache_desc = ucs_malloc(sizeof(uct_cuda_ipc_cache_t), "uct_cuda_ipc_cache_t");
+    if (cache_desc == NULL) {
+        ucs_error("failed to allocate memory for cuda_ipc cache");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    ret = pthread_rwlock_init(&cache_desc->lock, NULL);
+    if (ret) {
+        ucs_error("pthread_rwlock_init() failed: %m");
+        status = UCS_ERR_INVALID_PARAM;
+        goto err;
+    }
+
+    status = ucs_pgtable_init(&cache_desc->pgtable,
+                              uct_cuda_ipc_cache_pgt_dir_alloc,
+                              uct_cuda_ipc_cache_pgt_dir_release);
+    if (status != UCS_OK) {
+        goto err_destroy_rwlock;
+    }
+
+    cache_desc->name = strdup(name);
+    if (cache_desc->name == NULL) {
+        status = UCS_ERR_NO_MEMORY;
+        goto err_destroy_rwlock;
+    }
+
+    *cache = cache_desc;
+    return UCS_OK;
+
+err_destroy_rwlock:
+    pthread_rwlock_destroy(&cache_desc->lock);
+err:
+    free(cache_desc);
+    return status;
+}
+
+void uct_cuda_ipc_destroy_cache(uct_cuda_ipc_cache_t *cache)
+{
+    uct_cuda_ipc_cache_purge(cache);
+    ucs_pgtable_cleanup(&cache->pgtable);
+    pthread_rwlock_destroy(&cache->lock);
+    free(cache->name);
+    ucs_free(cache);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.h
new file mode 100644
index 000000000..fa5f86777
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_cache.h
@@ -0,0 +1,48 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_CUDA_IPC_CACHE_H_
+#define UCT_CUDA_IPC_CACHE_H_
+
+#include <ucs/datastruct/pgtable.h>
+#include <ucs/datastruct/list.h>
+#include "cuda_ipc_md.h"
+#include <cuda.h>
+#include <cuda_runtime.h>
+
+
+typedef struct uct_cuda_ipc_cache         uct_cuda_ipc_cache_t;
+typedef struct uct_cuda_ipc_cache_region  uct_cuda_ipc_cache_region_t;
+
+
+typedef struct uct_cuda_ipc_rem_memh uct_cuda_ipc_rem_memh_t;
+
+
+struct uct_cuda_ipc_cache_region {
+    ucs_pgt_region_t        super;        /**< Base class - page table region */
+    ucs_list_link_t         list;         /**< List element */
+    uct_cuda_ipc_key_t      key;          /**< Remote memory key */
+    void                    *mapped_addr; /**< Local mapped address */
+};
+
+
+struct uct_cuda_ipc_cache {
+    pthread_rwlock_t      lock;       /**< protests the page table */
+    ucs_pgtable_t         pgtable;    /**< Page table to hold the regions */
+    char                  *name;      /**< Name */
+};
+
+
+ucs_status_t uct_cuda_ipc_create_cache(uct_cuda_ipc_cache_t **cache,
+                                       const char *name);
+
+
+void uct_cuda_ipc_destroy_cache(uct_cuda_ipc_cache_t *cache);
+
+
+ucs_status_t uct_cuda_ipc_cache_map_memhandle(void *arg, uct_cuda_ipc_key_t *key,
+                                              void **mapped_addr);
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.c
new file mode 100644
index 000000000..f3cf0b0c1
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.c
@@ -0,0 +1,168 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ * See file LICENSE for terms.
+ */
+
+#include "cuda_ipc_ep.h"
+#include "cuda_ipc_iface.h"
+#include "cuda_ipc_md.h"
+
+#include <uct/base/uct_log.h>
+#include <ucs/debug/memtrack.h>
+#include <ucs/type/class.h>
+
+#define UCT_CUDA_IPC_PUT 0
+#define UCT_CUDA_IPC_GET 1
+
+static UCS_CLASS_INIT_FUNC(uct_cuda_ipc_ep_t, uct_iface_t *tl_iface,
+                           const uct_device_addr_t *dev_addr,
+                           const uct_iface_addr_t *iface_addr)
+{
+    uct_cuda_ipc_iface_t *iface = ucs_derived_of(tl_iface, uct_cuda_ipc_iface_t);
+    ucs_status_t status;
+    char target_name[64];
+
+    UCS_CLASS_CALL_SUPER_INIT(uct_base_ep_t, &iface->super);
+    self->remote_memh_cache = NULL;
+
+    if (iface->config.enable_cache) {
+        snprintf(target_name, sizeof(target_name), "dest:%d", *(pid_t*)iface_addr);
+        status = uct_cuda_ipc_create_cache(&self->remote_memh_cache, target_name);
+        if (status != UCS_OK) {
+            ucs_error("could not create create cuda ipc cache: %s",
+                       ucs_status_string(status));
+            return status;
+        }
+    }
+
+    return UCS_OK;
+}
+
+static UCS_CLASS_CLEANUP_FUNC(uct_cuda_ipc_ep_t)
+{
+    if (self->remote_memh_cache) {
+        uct_cuda_ipc_destroy_cache(self->remote_memh_cache);
+    }
+}
+
+UCS_CLASS_DEFINE(uct_cuda_ipc_ep_t, uct_base_ep_t)
+UCS_CLASS_DEFINE_NEW_FUNC(uct_cuda_ipc_ep_t, uct_ep_t, uct_iface_t*,
+                          const uct_device_addr_t *, const uct_iface_addr_t *);
+UCS_CLASS_DEFINE_DELETE_FUNC(uct_cuda_ipc_ep_t, uct_ep_t);
+
+#define uct_cuda_ipc_trace_data(_addr, _rkey, _fmt, ...)     \
+    ucs_trace_data(_fmt " to %"PRIx64"(%+ld)", ## __VA_ARGS__, (_addr), (_rkey))
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+uct_cuda_ipc_post_cuda_async_copy(uct_ep_h tl_ep, uint64_t remote_addr,
+                                  const uct_iov_t *iov, uct_rkey_t rkey,
+                                  uct_completion_t *comp, int direction)
+{
+    uct_cuda_ipc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_cuda_ipc_iface_t);
+    uct_cuda_ipc_ep_t *ep       = ucs_derived_of(tl_ep, uct_cuda_ipc_ep_t);
+    uct_cuda_ipc_key_t *key     = (uct_cuda_ipc_key_t *) rkey;
+    void *mapped_rem_addr;
+    void *mapped_addr;
+    uct_cuda_ipc_event_desc_t *cuda_ipc_event;
+    ucs_queue_head_t *outstanding_queue;
+    ucs_status_t status;
+    CUdeviceptr dst, src;
+    CUdevice cu_device;
+    CUstream stream;
+    size_t offset;
+
+    if (0 == iov[0].length) {
+        ucs_trace_data("Zero length request: skip it");
+        return UCS_OK;
+    }
+
+    UCT_CUDA_IPC_GET_DEVICE(cu_device);
+
+    status = iface->map_memhandle((void *)ep->remote_memh_cache, key, &mapped_addr);
+    if (status != UCS_OK) {
+        return UCS_ERR_IO_ERROR;
+    }
+
+    offset          = (uintptr_t)remote_addr - (uintptr_t)key->d_bptr;
+    mapped_rem_addr = (void *) ((uintptr_t) mapped_addr + offset);
+    ucs_assert(offset <= key->b_len);
+
+    if (!iface->streams_initialized) {
+        status = uct_cuda_ipc_iface_init_streams(iface);
+        if (UCS_OK != status) {
+            return status;
+        }
+    }
+
+    stream            = iface->stream_d2d[key->dev_num];
+    outstanding_queue = &iface->outstanding_d2d_event_q;
+    cuda_ipc_event    = ucs_mpool_get(&iface->event_desc);
+
+    if (ucs_unlikely(cuda_ipc_event == NULL)) {
+        ucs_error("Failed to allocate cuda_ipc event object");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    dst = (CUdeviceptr)
+        ((direction == UCT_CUDA_IPC_PUT) ? mapped_rem_addr : iov[0].buffer);
+    src = (CUdeviceptr)
+        ((direction == UCT_CUDA_IPC_PUT) ? iov[0].buffer : mapped_rem_addr);
+
+    status = UCT_CUDADRV_FUNC(cuMemcpyDtoDAsync(dst, src, iov[0].length, stream));
+    if (UCS_OK != status) {
+        ucs_mpool_put(cuda_ipc_event);
+        return status;
+    }
+
+    status = UCT_CUDADRV_FUNC(cuEventRecord(cuda_ipc_event->event, stream));
+    if (UCS_OK != status) {
+        ucs_mpool_put(cuda_ipc_event);
+        return status;
+    }
+
+    ucs_queue_push(outstanding_queue, &cuda_ipc_event->queue);
+    cuda_ipc_event->comp        = comp;
+    cuda_ipc_event->mapped_addr = mapped_addr;
+    ucs_trace("cuMemcpyDtoDAsync issued :%p dst:%p, src:%p  len:%ld",
+             cuda_ipc_event, (void *) dst, (void *) src, iov[0].length);
+    return UCS_INPROGRESS;
+}
+
+ucs_status_t uct_cuda_ipc_ep_get_zcopy(uct_ep_h tl_ep, const uct_iov_t *iov, size_t iovcnt,
+                                       uint64_t remote_addr, uct_rkey_t rkey,
+                                       uct_completion_t *comp)
+{
+    ucs_status_t status;
+
+    status = uct_cuda_ipc_post_cuda_async_copy(tl_ep, remote_addr, iov,
+                                               rkey, comp, UCT_CUDA_IPC_GET);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    UCT_TL_EP_STAT_OP(ucs_derived_of(tl_ep, uct_base_ep_t), GET, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
+    uct_cuda_ipc_trace_data(remote_addr, rkey, "GET_ZCOPY [length %zu]",
+                            uct_iov_total_length(iov, iovcnt));
+    return status;
+}
+
+ucs_status_t uct_cuda_ipc_ep_put_zcopy(uct_ep_h tl_ep, const uct_iov_t *iov, size_t iovcnt,
+                                       uint64_t remote_addr, uct_rkey_t rkey,
+                                       uct_completion_t *comp)
+{
+    ucs_status_t status;
+
+    status = uct_cuda_ipc_post_cuda_async_copy(tl_ep, remote_addr, iov,
+                                               rkey, comp, UCT_CUDA_IPC_PUT);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    UCT_TL_EP_STAT_OP(ucs_derived_of(tl_ep, uct_base_ep_t), PUT, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
+    uct_cuda_ipc_trace_data(remote_addr, rkey, "PUT_ZCOPY [length %zu]",
+                                uct_iov_total_length(iov, iovcnt));
+    return status;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.h
new file mode 100644
index 000000000..464380686
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_ep.h
@@ -0,0 +1,37 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+* See file LICENSE for terms.
+*/
+
+#ifndef UCT_CUDA_IPC_EP_H
+#define UCT_CUDA_IPC_EP_H
+
+#include <uct/api/uct.h>
+#include <uct/base/uct_iface.h>
+#include <ucs/type/class.h>
+#include "cuda_ipc_md.h"
+#include "cuda_ipc_cache.h"
+
+typedef struct uct_cuda_ipc_ep_addr {
+    int                ep_id;
+} uct_cuda_ipc_ep_addr_t;
+
+typedef struct uct_cuda_ipc_ep {
+    uct_base_ep_t                   super;
+    uct_cuda_ipc_cache_t            *remote_memh_cache;
+} uct_cuda_ipc_ep_t;
+
+UCS_CLASS_DECLARE_NEW_FUNC(uct_cuda_ipc_ep_t, uct_ep_t, uct_iface_t*,
+                           const uct_device_addr_t *, const uct_iface_addr_t *);
+UCS_CLASS_DECLARE_DELETE_FUNC(uct_cuda_ipc_ep_t, uct_ep_t);
+
+ucs_status_t uct_cuda_ipc_ep_get_zcopy(uct_ep_h tl_ep,
+                                       const uct_iov_t *iov, size_t iovcnt,
+                                       uint64_t remote_addr, uct_rkey_t rkey,
+                                       uct_completion_t *comp);
+
+ucs_status_t uct_cuda_ipc_ep_put_zcopy(uct_ep_h tl_ep,
+                                       const uct_iov_t *iov, size_t iovcnt,
+                                       uint64_t remote_addr, uct_rkey_t rkey,
+                                       uct_completion_t *comp);
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.c
new file mode 100644
index 000000000..785bdc088
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.c
@@ -0,0 +1,358 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ * See file LICENSE for terms.
+ */
+
+#include "cuda_ipc_iface.h"
+#include "cuda_ipc_md.h"
+#include "cuda_ipc_ep.h"
+
+#include <ucs/type/class.h>
+#include <ucs/sys/string.h>
+
+
+static ucs_config_field_t uct_cuda_ipc_iface_config_table[] = {
+
+    {"", "", NULL,
+     ucs_offsetof(uct_cuda_ipc_iface_config_t, super),
+     UCS_CONFIG_TYPE_TABLE(uct_iface_config_table)},
+
+    {"MAX_POLL", "16",
+     "Max number of event completions to pick during cuda events polling",
+      ucs_offsetof(uct_cuda_ipc_iface_config_t, max_poll), UCS_CONFIG_TYPE_UINT},
+
+    {"CACHE", "y",
+     "Enable remote endpoint IPC memhandle mapping cache",
+     ucs_offsetof(uct_cuda_ipc_iface_config_t, enable_cache),
+     UCS_CONFIG_TYPE_BOOL},
+
+    {NULL}
+};
+
+
+/* Forward declaration for the delete function */
+static void UCS_CLASS_DELETE_FUNC_NAME(uct_cuda_ipc_iface_t)(uct_iface_t*);
+
+
+static uint64_t uct_cuda_ipc_iface_node_guid(uct_base_iface_t *iface)
+{
+    return ucs_machine_guid() *
+           ucs_string_to_id(iface->md->component->name);
+}
+
+ucs_status_t uct_cuda_ipc_iface_get_device_address(uct_iface_t *tl_iface,
+                                                   uct_device_addr_t *addr)
+{
+    uct_base_iface_t *iface = ucs_derived_of(tl_iface, uct_base_iface_t);
+
+    *(uint64_t*)addr = uct_cuda_ipc_iface_node_guid(iface);
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_iface_get_address(uct_iface_h tl_iface,
+                                                   uct_iface_addr_t *iface_addr)
+{
+    *(pid_t*)iface_addr = getpid();
+    return UCS_OK;
+}
+
+static int uct_cuda_ipc_iface_is_reachable(const uct_iface_h tl_iface,
+                                           const uct_device_addr_t *dev_addr,
+                                           const uct_iface_addr_t *iface_addr)
+{
+    uct_cuda_ipc_iface_t  *iface = ucs_derived_of(tl_iface, uct_cuda_ipc_iface_t);
+
+    return ((uct_cuda_ipc_iface_node_guid(&iface->super) ==
+            *((const uint64_t *)dev_addr)) && ((getpid() != *(pid_t *)iface_addr)));
+}
+
+static ucs_status_t uct_cuda_ipc_iface_query(uct_iface_h iface,
+                                             uct_iface_attr_t *iface_attr)
+{
+    memset(iface_attr, 0, sizeof(uct_iface_attr_t));
+    iface_attr->iface_addr_len          = sizeof(pid_t);
+    iface_attr->device_addr_len         = sizeof(uint64_t);
+    iface_attr->ep_addr_len             = 0;
+    iface_attr->max_conn_priv           = 0;
+    iface_attr->cap.flags               = UCT_IFACE_FLAG_CONNECT_TO_IFACE |
+                                          UCT_IFACE_FLAG_PENDING   |
+                                          UCT_IFACE_FLAG_GET_ZCOPY |
+                                          UCT_IFACE_FLAG_PUT_ZCOPY;
+
+    iface_attr->cap.put.max_short       = 0;
+    iface_attr->cap.put.max_bcopy       = 0;
+    iface_attr->cap.put.min_zcopy       = 0;
+    iface_attr->cap.put.max_zcopy       = UCT_CUDA_IPC_MAX_ALLOC_SZ;
+    iface_attr->cap.put.opt_zcopy_align = 1;
+    iface_attr->cap.put.align_mtu       = iface_attr->cap.put.opt_zcopy_align;
+    iface_attr->cap.put.max_iov         = 1;
+
+    iface_attr->cap.get.max_bcopy       = 0;
+    iface_attr->cap.get.min_zcopy       = 0;
+    iface_attr->cap.get.max_zcopy       = UCT_CUDA_IPC_MAX_ALLOC_SZ;
+    iface_attr->cap.get.opt_zcopy_align = 1;
+    iface_attr->cap.get.align_mtu       = iface_attr->cap.get.opt_zcopy_align;
+    iface_attr->cap.get.max_iov         = 1;
+
+    iface_attr->latency.overhead        = 1e-9;
+    iface_attr->latency.growth          = 0;
+    iface_attr->bandwidth               = 6911 * 1024.0 * 1024.0;
+    iface_attr->overhead                = 0;
+    iface_attr->priority                = 0;
+
+    return UCS_OK;
+}
+
+static ucs_status_t
+uct_cuda_ipc_iface_flush(uct_iface_h tl_iface, unsigned flags,
+                         uct_completion_t *comp)
+{
+    uct_cuda_ipc_iface_t *iface = ucs_derived_of(tl_iface, uct_cuda_ipc_iface_t);
+
+    if (comp != NULL) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    if (ucs_queue_is_empty(&iface->outstanding_d2d_event_q)) {
+        UCT_TL_IFACE_STAT_FLUSH(ucs_derived_of(tl_iface, uct_base_iface_t));
+        return UCS_OK;
+    }
+
+    UCT_TL_IFACE_STAT_FLUSH_WAIT(ucs_derived_of(tl_iface, uct_base_iface_t));
+    return UCS_INPROGRESS;
+}
+
+static UCS_F_ALWAYS_INLINE unsigned
+uct_cuda_ipc_progress_event_q(uct_cuda_ipc_iface_t *iface,
+                              ucs_queue_head_t *event_q, unsigned max_events)
+{
+    unsigned count = 0;
+    uct_cuda_ipc_event_desc_t *cuda_ipc_event;
+    ucs_queue_iter_t iter;
+    ucs_status_t status;
+
+    ucs_queue_for_each_safe(cuda_ipc_event, iter, event_q, queue) {
+        status = UCT_CUDADRV_FUNC(cuEventQuery(cuda_ipc_event->event));
+        if (UCS_INPROGRESS == status) {
+            continue;
+        } else if (UCS_OK != status) {
+            return status;
+        }
+
+        ucs_queue_del_iter(event_q, iter);
+        if (cuda_ipc_event->comp != NULL) {
+            uct_invoke_completion(cuda_ipc_event->comp, UCS_OK);
+        }
+
+        status = iface->unmap_memhandle(cuda_ipc_event->mapped_addr);
+        if (status != UCS_OK) {
+            ucs_fatal("failed to unmap addr:%p", cuda_ipc_event->mapped_addr);
+        }
+
+        ucs_trace_poll("CUDA_IPC Event Done :%p", cuda_ipc_event);
+        ucs_mpool_put(cuda_ipc_event);
+        count++;
+
+        if (count >= max_events) {
+            break;
+        }
+    }
+
+    return count;
+}
+
+static unsigned uct_cuda_ipc_iface_progress(uct_iface_h tl_iface)
+{
+    uct_cuda_ipc_iface_t *iface = ucs_derived_of(tl_iface, uct_cuda_ipc_iface_t);
+    unsigned max_events         = iface->config.max_poll;
+
+    return uct_cuda_ipc_progress_event_q(iface, &iface->outstanding_d2d_event_q,
+                                         max_events);
+}
+
+static uct_iface_ops_t uct_cuda_ipc_iface_ops = {
+    .ep_get_zcopy             = uct_cuda_ipc_ep_get_zcopy,
+    .ep_put_zcopy             = uct_cuda_ipc_ep_put_zcopy,
+    .ep_pending_add           = ucs_empty_function_return_busy,
+    .ep_pending_purge         = ucs_empty_function,
+    .ep_flush                 = uct_base_ep_flush,
+    .ep_fence                 = uct_base_ep_fence,
+    .ep_create_connected      = UCS_CLASS_NEW_FUNC_NAME(uct_cuda_ipc_ep_t),
+    .ep_destroy               = UCS_CLASS_DELETE_FUNC_NAME(uct_cuda_ipc_ep_t),
+    .iface_flush              = uct_cuda_ipc_iface_flush,
+    .iface_fence              = uct_base_iface_fence,
+    .iface_progress_enable    = uct_base_iface_progress_enable,
+    .iface_progress_disable   = uct_base_iface_progress_disable,
+    .iface_progress           = uct_cuda_ipc_iface_progress,
+    .iface_close              = UCS_CLASS_DELETE_FUNC_NAME(uct_cuda_ipc_iface_t),
+    .iface_query              = uct_cuda_ipc_iface_query,
+    .iface_get_device_address = uct_cuda_ipc_iface_get_device_address,
+    .iface_get_address        = uct_cuda_ipc_iface_get_address,
+    .iface_is_reachable       = uct_cuda_ipc_iface_is_reachable,
+};
+
+static void uct_cuda_ipc_event_desc_init(ucs_mpool_t *mp, void *obj, void *chunk)
+{
+    uct_cuda_ipc_event_desc_t *base = (uct_cuda_ipc_event_desc_t *) obj;
+
+    memset(base, 0, sizeof(*base));
+    UCT_CUDADRV_FUNC(cuEventCreate(&base->event, CU_EVENT_DISABLE_TIMING));
+}
+
+static void uct_cuda_ipc_event_desc_cleanup(ucs_mpool_t *mp, void *obj)
+{
+    uct_cuda_ipc_event_desc_t *base = (uct_cuda_ipc_event_desc_t *) obj;
+
+    UCT_CUDADRV_FUNC(cuEventDestroy(base->event));
+}
+
+ucs_status_t uct_cuda_ipc_iface_init_streams(uct_cuda_ipc_iface_t *iface)
+{
+    ucs_status_t status;
+    int i;
+
+    for (i = 0; i < iface->device_count; i++) {
+        status = UCT_CUDADRV_FUNC(cuStreamCreate(&iface->stream_d2d[i],
+                                                 CU_STREAM_NON_BLOCKING));
+        if (UCS_OK != status) {
+            return status;
+        }
+    }
+
+    iface->streams_initialized = 1;
+
+    return UCS_OK;
+}
+
+static ucs_mpool_ops_t uct_cuda_ipc_event_desc_mpool_ops = {
+    .chunk_alloc   = ucs_mpool_chunk_malloc,
+    .chunk_release = ucs_mpool_chunk_free,
+    .obj_init      = uct_cuda_ipc_event_desc_init,
+    .obj_cleanup   = uct_cuda_ipc_event_desc_cleanup,
+};
+
+ucs_status_t uct_cuda_ipc_map_memhandle(void *arg, uct_cuda_ipc_key_t *key,
+                                        void **mapped_addr)
+{
+    return  UCT_CUDADRV_FUNC(cuIpcOpenMemHandle((CUdeviceptr *)mapped_addr,
+                             key->ph, CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS));
+}
+
+ucs_status_t uct_cuda_ipc_unmap_memhandle(void *mapped_addr)
+{
+    return UCT_CUDADRV_FUNC(cuIpcCloseMemHandle((CUdeviceptr)mapped_addr));
+}
+
+static UCS_CLASS_INIT_FUNC(uct_cuda_ipc_iface_t, uct_md_h md, uct_worker_h worker,
+                           const uct_iface_params_t *params,
+                           const uct_iface_config_t *tl_config)
+{
+    uct_cuda_ipc_iface_config_t *config = NULL;
+    ucs_status_t status;
+    int dev_count;
+
+    config = ucs_derived_of(tl_config, uct_cuda_ipc_iface_config_t);
+    UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &uct_cuda_ipc_iface_ops, md, worker,
+                              params, tl_config UCS_STATS_ARG(params->stats_root)
+                              UCS_STATS_ARG(UCT_CUDA_IPC_TL_NAME));
+
+    if (strncmp(params->mode.device.dev_name,
+                UCT_CUDA_IPC_DEV_NAME, strlen(UCT_CUDA_IPC_DEV_NAME)) != 0) {
+        ucs_error("No device was found: %s", params->mode.device.dev_name);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    status = UCT_CUDADRV_FUNC(cuDeviceGetCount(&dev_count));
+    if (UCS_OK != status) {
+        return status;
+    }
+    ucs_assert(dev_count <= UCT_CUDA_IPC_MAX_PEERS);
+
+    self->device_count        = dev_count;
+    self->config.max_poll     = config->max_poll;
+    self->config.enable_cache = config->enable_cache;
+
+    if (self->config.enable_cache) {
+        self->map_memhandle   = uct_cuda_ipc_cache_map_memhandle;
+        self->unmap_memhandle = ucs_empty_function_return_success;
+    } else {
+        self->map_memhandle   = uct_cuda_ipc_map_memhandle;
+        self->unmap_memhandle = uct_cuda_ipc_unmap_memhandle;
+    }
+
+    status = ucs_mpool_init(&self->event_desc,
+                            0,
+                            sizeof(uct_cuda_ipc_event_desc_t),
+                            0,
+                            UCS_SYS_CACHE_LINE_SIZE,
+                            128,
+                            1024,
+                            &uct_cuda_ipc_event_desc_mpool_ops,
+                            "CUDA_IPC EVENT objects");
+    if (UCS_OK != status) {
+        ucs_error("mpool creation failed");
+        return UCS_ERR_IO_ERROR;
+    }
+
+    self->streams_initialized = 0;
+    ucs_queue_head_init(&self->outstanding_d2d_event_q);
+    return UCS_OK;
+}
+
+static UCS_CLASS_CLEANUP_FUNC(uct_cuda_ipc_iface_t)
+{
+    ucs_status_t status;
+    int i;
+
+    if (self->streams_initialized) {
+        for (i = 0; i < self->device_count; i++) {
+            status = UCT_CUDADRV_FUNC(cuStreamDestroy(self->stream_d2d[i]));
+            if (UCS_OK != status) {
+                continue;
+            }
+        }
+        self->streams_initialized = 0;
+    }
+
+    uct_base_iface_progress_disable(&self->super.super,
+                                    UCT_PROGRESS_SEND | UCT_PROGRESS_RECV);
+    ucs_mpool_cleanup(&self->event_desc, 1);
+}
+
+UCS_CLASS_DEFINE(uct_cuda_ipc_iface_t, uct_base_iface_t);
+UCS_CLASS_DEFINE_NEW_FUNC(uct_cuda_ipc_iface_t, uct_iface_t, uct_md_h, uct_worker_h,
+                          const uct_iface_params_t*, const uct_iface_config_t*);
+static UCS_CLASS_DEFINE_DELETE_FUNC(uct_cuda_ipc_iface_t, uct_iface_t);
+
+static ucs_status_t uct_cuda_ipc_query_tl_resources(uct_md_h md,
+                                                    uct_tl_resource_desc_t **resource_p,
+                                                    unsigned *num_resources_p)
+{
+    uct_tl_resource_desc_t *resource;
+
+    resource = ucs_calloc(1, sizeof(uct_tl_resource_desc_t), "resource desc");
+    if (NULL == resource) {
+        ucs_error("Failed to allocate memory");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    ucs_snprintf_zero(resource->tl_name, sizeof(resource->tl_name), "%s",
+                      UCT_CUDA_IPC_TL_NAME);
+    ucs_snprintf_zero(resource->dev_name, sizeof(resource->dev_name), "%s",
+                      UCT_CUDA_IPC_DEV_NAME);
+    resource->dev_type = UCT_DEVICE_TYPE_ACC;
+
+    *num_resources_p = 1;
+    *resource_p      = resource;
+    return UCS_OK;
+}
+
+UCT_TL_COMPONENT_DEFINE(uct_cuda_ipc_tl,
+                        uct_cuda_ipc_query_tl_resources,
+                        uct_cuda_ipc_iface_t,
+                        UCT_CUDA_IPC_TL_NAME,
+                        "CUDA_IPC_",
+                        uct_cuda_ipc_iface_config_table,
+                        uct_cuda_ipc_iface_config_t);
+UCT_MD_REGISTER_TL(&uct_cuda_ipc_md_component, &uct_cuda_ipc_tl);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.h
new file mode 100644
index 000000000..a1e3060ca
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_iface.h
@@ -0,0 +1,57 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_CUDA_IPC_IFACE_H
+#define UCT_CUDA_IPC_IFACE_H
+
+#include <uct/base/uct_iface.h>
+#include <ucs/arch/cpu.h>
+#include <cuda_runtime.h>
+#include <cuda.h>
+
+#include "cuda_ipc_md.h"
+
+
+#define UCT_CUDA_IPC_TL_NAME    "cuda_ipc"
+#define UCT_CUDA_IPC_DEV_NAME   "cudaipc0"
+#define UCT_CUDA_IPC_MAX_PEERS  16
+
+
+typedef struct uct_cuda_ipc_iface {
+    uct_base_iface_t super;
+    ucs_mpool_t      event_desc;              /* cuda event desc */
+    ucs_queue_head_t outstanding_d2d_event_q; /* stream for outstanding d2d */
+    int              device_count;
+    int              streams_initialized;     /* indicates if stream created */
+    CUstream         stream_d2d[UCT_CUDA_IPC_MAX_PEERS];
+                                              /* per-peer stream */
+    struct {
+        unsigned     max_poll;                /* query attempts w.o success */
+        int          enable_cache;            /* enable/disable ipc handle cache */
+    } config;
+    ucs_status_t     (*map_memhandle)(void *context, uct_cuda_ipc_key_t *key,
+                                      void **map_addr);
+    ucs_status_t     (*unmap_memhandle)(void *map_addr);
+} uct_cuda_ipc_iface_t;
+
+
+typedef struct uct_cuda_ipc_iface_config {
+    uct_iface_config_t      super;
+    unsigned                max_poll;
+    int                     enable_cache;
+} uct_cuda_ipc_iface_config_t;
+
+
+typedef struct uct_cuda_ipc_event_desc {
+    CUevent           event;
+    void              *mapped_addr;
+    uct_completion_t  *comp;
+    ucs_queue_elem_t  queue;
+} uct_cuda_ipc_event_desc_t;
+
+
+ucs_status_t uct_cuda_ipc_iface_init_streams(uct_cuda_ipc_iface_t *iface);
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.c
new file mode 100644
index 000000000..23212640c
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.c
@@ -0,0 +1,184 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ * See file LICENSE for terms.
+ */
+
+#include "cuda_ipc_md.h"
+
+#include <string.h>
+#include <limits.h>
+#include <ucs/debug/log.h>
+#include <ucs/sys/sys.h>
+#include <ucs/debug/memtrack.h>
+#include <ucs/type/class.h>
+#include <sys/types.h>
+#include <unistd.h>
+
+
+static ucs_config_field_t uct_cuda_ipc_md_config_table[] = {
+    {"", "", NULL,
+     ucs_offsetof(uct_cuda_ipc_md_config_t, super), UCS_CONFIG_TYPE_TABLE(uct_md_config_table)},
+
+    {NULL}
+};
+
+static ucs_status_t uct_cuda_ipc_md_query(uct_md_h md, uct_md_attr_t *md_attr)
+{
+    md_attr->cap.flags         = UCT_MD_FLAG_REG |
+                                 UCT_MD_FLAG_NEED_RKEY;
+    md_attr->cap.reg_mem_types = UCS_BIT(UCT_MD_MEM_TYPE_CUDA);
+    md_attr->cap.mem_type      = UCT_MD_MEM_TYPE_CUDA;
+    md_attr->cap.max_alloc     = 0;
+    md_attr->cap.max_reg       = UCT_CUDA_IPC_MAX_ALLOC_SZ;
+    md_attr->rkey_packed_size  = sizeof(uct_cuda_ipc_key_t);
+    md_attr->reg_cost.overhead = 0;
+    md_attr->reg_cost.growth   = 0;
+    memset(&md_attr->local_cpus, 0xff, sizeof(md_attr->local_cpus));
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_mkey_pack(uct_md_h md, uct_mem_h memh,
+                                           void *rkey_buffer)
+{
+    uct_cuda_ipc_key_t *packed   = (uct_cuda_ipc_key_t *) rkey_buffer;
+    uct_cuda_ipc_key_t *mem_hndl = (uct_cuda_ipc_key_t *) memh;
+
+    *packed = *mem_hndl;
+
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_rkey_unpack(uct_md_component_t *mdc,
+                                             const void *rkey_buffer, uct_rkey_t *rkey_p,
+                                             void **handle_p)
+{
+    uct_cuda_ipc_key_t *packed = (uct_cuda_ipc_key_t *) rkey_buffer;
+    uct_cuda_ipc_key_t *key;
+    ucs_status_t status;
+    CUdevice cu_device;
+    int peer_accessble;
+
+    UCT_CUDA_IPC_GET_DEVICE(cu_device);
+
+    status = UCT_CUDADRV_FUNC(cuDeviceCanAccessPeer(&peer_accessble,
+                                                    cu_device, packed->dev_num));
+    if ((status != UCS_OK) || (peer_accessble == 0)) {
+        return UCS_ERR_UNREACHABLE;
+    }
+
+    key = ucs_malloc(sizeof(uct_cuda_ipc_key_t), "uct_cuda_ipc_key_t");
+    if (NULL == key) {
+        ucs_error("failed to allocate memory for uct_cuda_ipc_key_t");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    *key      = *packed;
+    *handle_p = NULL;
+    *rkey_p   = (uintptr_t) key;
+
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_rkey_release(uct_md_component_t *mdc, uct_rkey_t rkey,
+                                              void *handle)
+{
+    ucs_assert(NULL == handle);
+    ucs_free((void *)rkey);
+    return UCS_OK;
+}
+
+static ucs_status_t
+uct_cuda_ipc_mem_reg_internal(uct_md_h uct_md, void *addr, size_t length,
+                              unsigned flags, uct_cuda_ipc_key_t *key)
+{
+    CUdevice cu_device;
+    ucs_status_t status;
+
+    if (!length) {
+        return UCS_OK;
+    }
+
+    status = UCT_CUDADRV_FUNC(cuIpcGetMemHandle(&(key->ph), (CUdeviceptr) addr));
+    if (UCS_OK != status) {
+        return status;
+    }
+
+    UCT_CUDA_IPC_GET_DEVICE(cu_device);
+
+    UCT_CUDADRV_FUNC(cuMemGetAddressRange(&(key->d_bptr),
+                                          &(key->b_len),
+                                          (CUdeviceptr) addr));
+    key->dev_num  = (int) cu_device;
+    ucs_trace("registered memory:%p..%p length:%lu dev_num:%d",
+              addr, addr + length, length, (int) cu_device);
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_mem_reg(uct_md_h md, void *address, size_t length,
+                                         unsigned flags, uct_mem_h *memh_p)
+{
+    uct_cuda_ipc_key_t *key;
+
+    key = ucs_malloc(sizeof(uct_cuda_ipc_key_t), "uct_cuda_ipc_key_t");
+    if (NULL == key) {
+        ucs_error("failed to allocate memory for uct_cuda_ipc_key_t");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    if (UCS_OK != uct_cuda_ipc_mem_reg_internal(md, address, length, 0, key)) {
+        ucs_free(key);
+        return UCS_ERR_IO_ERROR;
+    }
+    *memh_p = key;
+
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_mem_dereg(uct_md_h md, uct_mem_h memh)
+{
+    ucs_free(memh);
+    return UCS_OK;
+}
+
+static ucs_status_t uct_cuda_ipc_query_md_resources(uct_md_resource_desc_t **resources_p,
+                                                    unsigned *num_resources_p)
+{
+    int num_gpus;
+    cudaError_t cudaErr;
+
+    cudaErr = cudaGetDeviceCount(&num_gpus);
+    if ((cudaErr!= cudaSuccess) || (num_gpus == 0)) {
+        ucs_debug("Not found cuda devices");
+        *resources_p     = NULL;
+        *num_resources_p = 0;
+        return UCS_OK;
+    }
+
+    return uct_single_md_resource(&uct_cuda_ipc_md_component, resources_p, num_resources_p);
+}
+
+static ucs_status_t uct_cuda_ipc_md_open(const char *md_name, const uct_md_config_t *md_config,
+                                         uct_md_h *md_p)
+{
+    static uct_md_ops_t md_ops = {
+        .close        = (void*)ucs_empty_function,
+        .query        = uct_cuda_ipc_md_query,
+        .mkey_pack    = uct_cuda_ipc_mkey_pack,
+        .mem_reg      = uct_cuda_ipc_mem_reg,
+        .mem_dereg    = uct_cuda_ipc_mem_dereg,
+        .is_mem_type_owned = uct_cuda_is_mem_type_owned,
+    };
+    static uct_md_t md = {
+        .ops          = &md_ops,
+        .component    = &uct_cuda_ipc_md_component
+    };
+
+    *md_p = &md;
+    return UCS_OK;
+}
+
+UCT_MD_COMPONENT_DEFINE(uct_cuda_ipc_md_component, UCT_CUDA_IPC_MD_NAME,
+                        uct_cuda_ipc_query_md_resources, uct_cuda_ipc_md_open, NULL,
+                        uct_cuda_ipc_rkey_unpack, uct_cuda_ipc_rkey_release, "CUDA_IPC_",
+                        uct_cuda_ipc_md_config_table, uct_cuda_ipc_md_config_t);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.h
new file mode 100644
index 000000000..b36fa4e36
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/cuda_ipc/cuda_ipc_md.h
@@ -0,0 +1,56 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ * Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_CUDA_IPC_MD_H
+#define UCT_CUDA_IPC_MD_H
+
+#include <uct/base/uct_md.h>
+#include <uct/cuda/base/cuda_md.h>
+#include <uct/cuda/base/cuda_iface.h>
+
+
+#define UCT_CUDA_IPC_MD_NAME      "cuda_ipc"
+#define UCT_CUDA_IPC_MAX_ALLOC_SZ (1 << 30)
+
+
+extern uct_md_component_t uct_cuda_ipc_md_component;
+
+
+/**
+ * @brief cuda ipc MD descriptor
+ */
+typedef struct uct_cuda_ipc_md {
+    struct uct_md super;   /**< Domain info */
+} uct_cuda_ipc_md_t;
+
+
+/**
+ * @brief cuda ipc domain configuration.
+ */
+typedef struct uct_cuda_ipc_md_config {
+    uct_md_config_t super;
+} uct_cuda_ipc_md_config_t;
+
+
+/**
+ * @brief cuda_ipc packed and remote key for put/get
+ */
+typedef struct uct_cuda_ipc_key {
+    CUipcMemHandle ph;           /* Memory handle of GPU memory */
+    CUdeviceptr    d_bptr;       /* Allocation base address */
+    size_t         b_len;        /* Allocation size */
+    int            dev_num;      /* GPU Device number */
+} uct_cuda_ipc_key_t;
+
+
+#define UCT_CUDA_IPC_GET_DEVICE(_cu_device)                             \
+    do {                                                                \
+        if (UCS_OK != UCT_CUDADRV_FUNC(cuCtxGetDevice(&_cu_device))) {  \
+            return UCS_ERR_IO_ERROR;                                    \
+        }                                                               \
+    } while(0);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.c
index 8cc1da497..3cd339ed7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.c
@@ -42,7 +42,7 @@ ucs_status_t uct_gdr_copy_ep_put_short(uct_ep_h tl_ep, const void *buffer,
 
     if (ucs_likely(length)) {
         bar_offset = remote_addr - gdr_copy_key->vaddr;
-        ret = gdr_copy_to_bar((gdr_copy_key->bar_ptr + bar_offset), buffer, length);
+        ret = gdr_copy_to_bar(gdr_copy_key->bar_ptr + bar_offset, buffer, length);
         if (ret) {
             ucs_error("gdr_copy_to_bar failed. ret:%d", ret);
             return UCS_ERR_IO_ERROR;
@@ -54,3 +54,26 @@ ucs_status_t uct_gdr_copy_ep_put_short(uct_ep_h tl_ep, const void *buffer,
                    length, buffer, (void *)remote_addr);
     return UCS_OK;
 }
+
+ucs_status_t uct_gdr_copy_ep_get_short(uct_ep_h tl_ep, void *buffer,
+                                       unsigned length, uint64_t remote_addr,
+                                       uct_rkey_t rkey)
+{
+    uct_gdr_copy_key_t *gdr_copy_key = (uct_gdr_copy_key_t *) rkey;
+    size_t bar_offset;
+    int ret;
+
+    if (ucs_likely(length)) {
+        bar_offset = remote_addr - gdr_copy_key->vaddr;
+        ret = gdr_copy_from_bar(buffer, gdr_copy_key->bar_ptr + bar_offset, length);
+        if (ret) {
+            ucs_error("gdr_copy_from_bar failed. ret:%d", ret);
+            return UCS_ERR_IO_ERROR;
+        }
+    }
+
+    UCT_TL_EP_STAT_OP(ucs_derived_of(tl_ep, uct_base_ep_t), GET, SHORT, length);
+    ucs_trace_data("GET_SHORT size %d from %p to %p",
+                   length, (void *)remote_addr, buffer);
+    return UCS_OK;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.h
index 3ce697cbb..2a09cdfbd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_ep.h
@@ -31,4 +31,8 @@ ucs_status_t uct_gdr_copy_ep_put_short(uct_ep_h tl_ep, const void *buffer,
                                        unsigned length, uint64_t remote_addr,
                                        uct_rkey_t rkey);
 
+ucs_status_t uct_gdr_copy_ep_get_short(uct_ep_h tl_ep, void *buffer,
+                                       unsigned length, uint64_t remote_addr,
+                                       uct_rkey_t rkey);
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.c
index 746b985f5..eae362cf1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.c
@@ -26,17 +26,20 @@ static void UCS_CLASS_DELETE_FUNC_NAME(uct_gdr_copy_iface_t)(uct_iface_t*);
 static ucs_status_t uct_gdr_copy_iface_get_address(uct_iface_h tl_iface,
                                                    uct_iface_addr_t *iface_addr)
 {
-    int *gdr_copy_addr = (int*)iface_addr;
+    uct_gdr_copy_iface_t *iface = ucs_derived_of(tl_iface, uct_gdr_copy_iface_t);
 
-    *gdr_copy_addr = 0;
+    *(uct_gdr_copy_iface_addr_t*)iface_addr = iface->id;
     return UCS_OK;
 }
 
-static int uct_gdr_copy_iface_is_reachable(const uct_iface_h iface,
+static int uct_gdr_copy_iface_is_reachable(const uct_iface_h tl_iface,
                                            const uct_device_addr_t *dev_addr,
                                            const uct_iface_addr_t *iface_addr)
 {
-    return 1;
+    uct_gdr_copy_iface_t  *iface = ucs_derived_of(tl_iface, uct_gdr_copy_iface_t);
+    uct_gdr_copy_iface_addr_t *addr = (uct_gdr_copy_iface_addr_t*)iface_addr;
+
+    return (addr != NULL) && (iface->id == *addr);
 }
 
 static ucs_status_t uct_gdr_copy_iface_query(uct_iface_h iface,
@@ -44,11 +47,12 @@ static ucs_status_t uct_gdr_copy_iface_query(uct_iface_h iface,
 {
     memset(iface_attr, 0, sizeof(uct_iface_attr_t));
 
-    iface_attr->iface_addr_len          = sizeof(int);
+    iface_attr->iface_addr_len          = sizeof(uct_gdr_copy_iface_addr_t);
     iface_attr->device_addr_len         = 0;
     iface_attr->ep_addr_len             = 0;
     iface_attr->cap.flags               = UCT_IFACE_FLAG_CONNECT_TO_IFACE |
-                                          UCT_IFACE_FLAG_PUT_SHORT;
+                                          UCT_IFACE_FLAG_PUT_SHORT |
+                                          UCT_IFACE_FLAG_GET_SHORT;
 
     iface_attr->cap.put.max_short       = UINT_MAX;
     iface_attr->cap.put.max_bcopy       = 0;
@@ -58,6 +62,7 @@ static ucs_status_t uct_gdr_copy_iface_query(uct_iface_h iface,
     iface_attr->cap.put.align_mtu       = iface_attr->cap.put.opt_zcopy_align;
     iface_attr->cap.put.max_iov         = 1;
 
+    iface_attr->cap.get.max_short       = UINT_MAX;
     iface_attr->cap.get.max_bcopy       = 0;
     iface_attr->cap.get.min_zcopy       = 0;
     iface_attr->cap.get.max_zcopy       = 0;
@@ -85,6 +90,7 @@ static ucs_status_t uct_gdr_copy_iface_query(uct_iface_h iface,
 
 static uct_iface_ops_t uct_gdr_copy_iface_ops = {
     .ep_put_short             = uct_gdr_copy_ep_put_short,
+    .ep_get_short             = uct_gdr_copy_ep_get_short,
     .ep_pending_add           = ucs_empty_function_return_busy,
     .ep_pending_purge         = ucs_empty_function,
     .ep_flush                 = uct_base_ep_flush,
@@ -117,6 +123,8 @@ static UCS_CLASS_INIT_FUNC(uct_gdr_copy_iface_t, uct_md_h md, uct_worker_h worke
         return UCS_ERR_NO_DEVICE;
     }
 
+    self->id = ucs_generate_uuid((uintptr_t)self);
+
     return UCS_OK;
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.h
index 3ecb41a14..882ec9c99 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_iface.h
@@ -10,11 +10,15 @@
 
 
 #define UCT_GDR_COPY_TL_NAME    "gdr_copy"
-#define UCT_CUDA_DEV_NAME   "gdrcopy0"
+#define UCT_CUDA_DEV_NAME       "gdrcopy0"
+
+
+typedef uint64_t uct_gdr_copy_iface_addr_t;
 
 
 typedef struct uct_gdr_copy_iface {
-    uct_base_iface_t        super;
+    uct_base_iface_t            super;
+    uct_gdr_copy_iface_addr_t   id;
 } uct_gdr_copy_iface_t;
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.c
index 70a7f36c1..10aea7d28 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.c
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2017.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2017-2018.  ALL RIGHTS RESERVED.
  * See file LICENSE for terms.
  */
 
@@ -9,10 +9,11 @@
 #include <limits.h>
 #include <ucs/debug/log.h>
 #include <ucs/sys/sys.h>
+#include <ucs/sys/math.h>
 #include <ucs/debug/memtrack.h>
 #include <ucs/type/class.h>
-#include <cuda_runtime.h>
-#include <cuda.h>
+#include <ucm/api/ucm.h>
+#include <uct/cuda/base/cuda_iface.h>
 
 #define UCT_GDR_COPY_MD_RCACHE_DEFAULT_ALIGN 65536
 
@@ -178,8 +179,7 @@ static ucs_status_t uct_gdr_copy_mem_reg(uct_md_h uct_md, void *address, size_t
                                          unsigned flags, uct_mem_h *memh_p)
 {
     uct_gdr_copy_mem_t *mem_hndl = NULL;
-    size_t reg_size;
-    void *ptr;
+    void *start, *end;
     ucs_status_t status;
 
     mem_hndl = ucs_malloc(sizeof(uct_gdr_copy_mem_t), "gdr_copy handle");
@@ -188,10 +188,11 @@ static ucs_status_t uct_gdr_copy_mem_reg(uct_md_h uct_md, void *address, size_t
         return UCS_ERR_NO_MEMORY;
     }
 
-    reg_size = (length + GPU_PAGE_SIZE - 1) & GPU_PAGE_MASK;
-    ptr = (void *) ((uintptr_t)address & GPU_PAGE_MASK);
+    start = ucs_align_down_pow2_ptr(address, GPU_PAGE_SIZE);
+    end   = ucs_align_up_pow2_ptr(address + length, GPU_PAGE_SIZE);
+    ucs_assert_always(start <= end);
 
-    status = uct_gdr_copy_mem_reg_internal(uct_md, ptr, reg_size, 0, mem_hndl);
+    status = uct_gdr_copy_mem_reg_internal(uct_md, start, end - start, 0, mem_hndl);
     if (status != UCS_OK) {
         ucs_free(mem_hndl);
         return status;
@@ -215,33 +216,6 @@ static ucs_status_t uct_gdr_copy_mem_dereg(uct_md_h uct_md, uct_mem_h memh)
     return status;
 }
 
-static int uct_is_gdr_copy_mem_type_owned(uct_md_h md, void *addr, size_t length)
-{
-    int memory_type;
-    struct cudaPointerAttributes attributes;
-    cudaError_t cuda_err;
-    CUresult cu_err;
-
-    if (addr == NULL) {
-        return 0;
-    }
-
-    cu_err = cuPointerGetAttribute(&memory_type,
-                                   CU_POINTER_ATTRIBUTE_MEMORY_TYPE,
-                                   (CUdeviceptr)addr);
-    if (cu_err != CUDA_SUCCESS) {
-        cuda_err = cudaPointerGetAttributes (&attributes, addr);
-        if (cuda_err == cudaSuccess) {
-            if (attributes.memoryType == cudaMemoryTypeDevice) {
-                return 1;
-            }
-        }
-    } else if (memory_type == CU_MEMORYTYPE_DEVICE) {
-        return 1;
-    }
-    return 0;
-}
-
 static ucs_status_t uct_gdr_copy_query_md_resources(uct_md_resource_desc_t **resources_p,
                                                     unsigned *num_resources_p)
 {
@@ -293,7 +267,7 @@ static uct_md_ops_t md_ops = {
     .mkey_pack          = uct_gdr_copy_mkey_pack,
     .mem_reg            = uct_gdr_copy_mem_reg,
     .mem_dereg          = uct_gdr_copy_mem_dereg,
-    .is_mem_type_owned  = uct_is_gdr_copy_mem_type_owned,
+    .is_mem_type_owned  = uct_cuda_is_mem_type_owned,
 };
 
 static inline uct_gdr_copy_rcache_region_t*
@@ -311,7 +285,7 @@ uct_gdr_copy_mem_rcache_reg(uct_md_h uct_md, void *address, size_t length,
     ucs_status_t status;
     uct_gdr_copy_mem_t *memh;
 
-    status = ucs_rcache_get(md->rcache, address, length, PROT_READ|PROT_WRITE,
+    status = ucs_rcache_get(md->rcache, (void *)address, length, PROT_READ|PROT_WRITE,
                             &flags, &rregion);
     if (status != UCS_OK) {
         return status;
@@ -338,12 +312,13 @@ static uct_md_ops_t md_rcache_ops = {
     .mkey_pack          = uct_gdr_copy_mkey_pack,
     .mem_reg            = uct_gdr_copy_mem_rcache_reg,
     .mem_dereg          = uct_gdr_copy_mem_rcache_dereg,
-    .is_mem_type_owned  = uct_is_gdr_copy_mem_type_owned,
+    .is_mem_type_owned  = uct_cuda_is_mem_type_owned,
 };
 
 static ucs_status_t
 uct_gdr_copy_rcache_mem_reg_cb(void *context, ucs_rcache_t *rcache,
-                               void *arg, ucs_rcache_region_t *rregion)
+                               void *arg, ucs_rcache_region_t *rregion,
+                               uint16_t rcache_mem_reg_flags)
 {
     uct_gdr_copy_md_t *md = context;
     int *flags = arg;
@@ -415,6 +390,7 @@ static ucs_status_t uct_gdr_copy_md_open(const char *md_name,
         rcache_params.region_struct_size = sizeof(uct_gdr_copy_rcache_region_t);
         rcache_params.alignment          = md_config->rcache.alignment;
         rcache_params.max_alignment      = UCT_GDR_COPY_MD_RCACHE_DEFAULT_ALIGN;
+        rcache_params.ucm_events         = UCM_EVENT_MEM_TYPE_FREE;
         rcache_params.ucm_event_priority = md_config->rcache.event_prio;
         rcache_params.context            = md;
         rcache_params.ops                = &uct_gdr_copy_rcache_ops;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.h
index f6fc5051f..f9417854a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/cuda/gdr_copy/gdr_copy_md.h
@@ -7,6 +7,7 @@
 #define UCT_GDR_COPY_MD_H
 
 #include <uct/base/uct_md.h>
+#include <uct/cuda/base/cuda_md.h>
 #include <ucs/sys/rcache.h>
 #include "gdrapi.h"
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.c
index 4d71c33b0..a93465548 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.c
@@ -18,6 +18,25 @@
 #include <sched.h>
 
 
+/* use both gid + lid data for key generarion (lid - ib based, gid - RoCE) */
+static UCS_F_ALWAYS_INLINE
+khint32_t uct_ib_kh_ah_hash_func(struct ibv_ah_attr attr)
+{
+    return kh_int64_hash_func(attr.grh.dgid.global.subnet_prefix ^
+                              attr.grh.dgid.global.interface_id  ^
+                              attr.dlid);
+}
+
+static UCS_F_ALWAYS_INLINE
+int uct_ib_kh_ah_hash_equal(struct ibv_ah_attr a, struct ibv_ah_attr b)
+{
+    return !memcmp(&a, &b, sizeof(a));
+}
+
+KHASH_IMPL(uct_ib_ah, struct ibv_ah_attr, struct ibv_ah*, 1,
+           uct_ib_kh_ah_hash_func, uct_ib_kh_ah_hash_equal)
+
+
 #if ENABLE_STATS
 static ucs_stats_class_t uct_ib_device_stats_class = {
     .name           = "",
@@ -57,9 +76,27 @@ static uct_ib_device_spec_t uct_ib_builtin_device_specs[] = {
   {0x02c9, 41682, "ConnectX-5",
    UCT_IB_DEVICE_FLAG_MELLANOX | UCT_IB_DEVICE_FLAG_MLX5_PRM |
    UCT_IB_DEVICE_FLAG_DC_V2, 37},
+  {0x02c9, 4122, "ConnectX-5",
+   UCT_IB_DEVICE_FLAG_MELLANOX | UCT_IB_DEVICE_FLAG_MLX5_PRM |
+   UCT_IB_DEVICE_FLAG_DC_V2, 36},
+  {0x02c9, 4123, "ConnectX-6",
+   UCT_IB_DEVICE_FLAG_MELLANOX | UCT_IB_DEVICE_FLAG_MLX5_PRM |
+   UCT_IB_DEVICE_FLAG_DC_V2, 50},
   {0, 0, "Generic HCA", 0, 0}
 };
 
+UCS_LIST_HEAD(uct_ib_device_init_list);
+
+#if HAVE_DECL_IBV_EXP_QUERY_GID_ATTR
+/* This struct defines the RoCE versions priorities */
+static uct_ib_roce_version_desc_t roce_versions_priorities[] = {
+        {IBV_EXP_ROCE_V2_GID_TYPE,    AF_INET,  1},
+        {IBV_EXP_ROCE_V2_GID_TYPE,    AF_INET6, 2},
+        {IBV_EXP_IB_ROCE_V1_GID_TYPE, AF_INET,  3},
+        {IBV_EXP_IB_ROCE_V1_GID_TYPE, AF_INET6, 4}
+};
+#endif
+
 static void uct_ib_device_get_locailty(const char *dev_name, cpu_set_t *cpu_mask,
                                        int *numa_node)
 {
@@ -212,6 +249,7 @@ ucs_status_t uct_ib_device_init(uct_ib_device_t *dev,
                                 struct ibv_device *ibv_device, int async_events
                                 UCS_STATS_ARG(ucs_stats_node_t *stats_parent))
 {
+    uct_ib_device_init_entry_t *init_entry;
     ucs_status_t status;
     uint8_t i;
     int ret;
@@ -221,7 +259,7 @@ ucs_status_t uct_ib_device_init(uct_ib_device_t *dev,
     /* Open verbs context */
     dev->ibv_context = ibv_open_device(ibv_device);
     if (dev->ibv_context == NULL) {
-        ucs_error("Failed to open %s: %m", ibv_get_device_name(ibv_device));
+        ucs_error("ibv_open_device(%s) failed: %m", ibv_get_device_name(ibv_device));
         status = UCS_ERR_IO_ERROR;
         goto err;
     }
@@ -249,6 +287,13 @@ ucs_status_t uct_ib_device_init(uct_ib_device_t *dev,
         break;
     }
 
+    ucs_list_for_each(init_entry, &uct_ib_device_init_list, list) {
+        status = init_entry->init(dev);
+        if (status != UCS_OK) {
+            goto err_free_context;
+        }
+    }
+
     if (dev->num_ports > UCT_IB_DEV_MAX_PORTS) {
         ucs_error("%s has %d ports, but only up to %d are supported",
                   ibv_get_device_name(ibv_device), dev->num_ports,
@@ -294,6 +339,9 @@ ucs_status_t uct_ib_device_init(uct_ib_device_t *dev,
         }
     }
 
+    kh_init_inplace(uct_ib_ah, &dev->ah_hash);
+    ucs_spinlock_init(&dev->ah_lock);
+
     ucs_debug("initialized device '%s' (%s) with %d ports", uct_ib_device_name(dev),
               ibv_node_type_str(ibv_device->node_type),
               dev->num_ports);
@@ -307,10 +355,20 @@ err:
     return status;
 }
 
+void uct_ib_device_cleanup_ah_cached(uct_ib_device_t *dev)
+{
+    struct ibv_ah *ah;
+
+    kh_foreach_value(&dev->ah_hash, ah, ibv_destroy_ah(ah));
+}
+
 void uct_ib_device_cleanup(uct_ib_device_t *dev)
 {
     ucs_debug("destroying ib device %s", uct_ib_device_name(dev));
 
+    kh_destroy_inplace(uct_ib_ah, &dev->ah_hash);
+    ucs_spinlock_destroy(&dev->ah_lock);
+
     if (dev->async_events) {
         ucs_async_remove_handler(dev->ibv_context->async_fd, 1);
     }
@@ -347,11 +405,23 @@ const uct_ib_device_spec_t* uct_ib_device_spec(uct_ib_device_t *dev)
                     default settings for unknown devices */
 }
 
+static size_t uct_ib_device_get_ib_gid_index(uct_ib_md_t *md)
+{
+    if (md->config.gid_index == UCS_CONFIG_ULUNITS_AUTO) {
+        return UCT_IB_MD_DEFAULT_GID_INDEX;
+    } else {
+        return md->config.gid_index;
+    }
+}
+
 ucs_status_t uct_ib_device_port_check(uct_ib_device_t *dev, uint8_t port_num,
                                       unsigned flags)
 {
+    uct_ib_md_t *md = ucs_container_of(dev, uct_ib_md_t, dev);
     const uct_ib_device_spec_t *dev_info;
     uint8_t required_dev_flags;
+    ucs_status_t status;
+    union ibv_gid gid;
 
     if (port_num < dev->first_port || port_num >= dev->first_port + dev->num_ports) {
         return UCS_ERR_NO_DEVICE;
@@ -370,7 +440,7 @@ ucs_status_t uct_ib_device_port_check(uct_ib_device_t *dev, uint8_t port_num,
     }
 
     if (flags & UCT_IB_DEVICE_FLAG_DC) {
-        if (!IBV_DEVICE_HAS_DC(&dev->dev_attr)) {
+        if (!IBV_DEVICE_HAS_DC(dev)) {
             ucs_trace("%s:%d does not support DC", uct_ib_device_name(dev), port_num);
             return UCS_ERR_UNSUPPORTED;
         }
@@ -386,9 +456,131 @@ ucs_status_t uct_ib_device_port_check(uct_ib_device_t *dev, uint8_t port_num,
         return UCS_ERR_UNSUPPORTED;
     }
 
+    if (md->check_subnet_filter && uct_ib_device_is_port_ib(dev, port_num)) {
+        status = uct_ib_device_query_gid(dev, port_num,
+                                         uct_ib_device_get_ib_gid_index(md), &gid);
+        if (status) {
+            return status;
+        }
+
+        if (md->subnet_filter != gid.global.subnet_prefix) {
+            ucs_trace("%s:%d subnet_prefix does not match",
+                      uct_ib_device_name(dev), port_num);
+            return UCS_ERR_UNSUPPORTED;
+        }
+    }
+
     return UCS_OK;
 }
 
+static int uct_ib_device_is_addr_ipv4_mcast(const struct in6_addr *raw,
+                                            const uint32_t addr_last_bits)
+{
+    /* IPv4 encoded multicast addresses */
+    return (raw->s6_addr32[0] == htonl(0xff0e0000)) &&
+           !(raw->s6_addr32[1] | addr_last_bits);
+}
+
+static int uct_ib_device_get_addr_family(union ibv_gid *gid, int gid_index)
+{
+    const struct in6_addr *raw    = (struct in6_addr *)gid->raw;
+    const uint32_t addr_last_bits = raw->s6_addr32[2] ^ htonl(0x0000ffff);
+    char p[128];
+
+    ucs_debug("testing addr_family on gid index %d: %s",
+              gid_index, inet_ntop(AF_INET6, gid, p, sizeof(p)));
+
+    if (!((raw->s6_addr32[0] | raw->s6_addr32[1]) | addr_last_bits) ||
+        uct_ib_device_is_addr_ipv4_mcast(raw, addr_last_bits)) {
+        return AF_INET;
+    } else {
+        return AF_INET6;
+    }
+}
+
+static ucs_status_t uct_ib_device_set_roce_gid_index(uct_ib_device_t *dev,
+                                                     uint8_t port_num,
+                                                     uint8_t *gid_index)
+{
+    int i, gid_tbl_len      = uct_ib_device_port_attr(dev, port_num)->gid_tbl_len;
+    ucs_status_t status     = UCS_OK;
+#if HAVE_DECL_IBV_EXP_QUERY_GID_ATTR
+    int priorities_arr_len  = ucs_static_array_size(roce_versions_priorities);
+    struct ibv_exp_gid_attr attr;
+    int prio_idx;
+#else
+    union ibv_gid gid;
+#endif
+
+#if HAVE_DECL_IBV_EXP_QUERY_GID_ATTR
+    for (prio_idx = 0; prio_idx < priorities_arr_len; prio_idx++) {
+        for (i = 0; i < gid_tbl_len; i++) {
+            attr.comp_mask = IBV_EXP_QUERY_GID_ATTR_TYPE | IBV_EXP_QUERY_GID_ATTR_GID;
+            if (ibv_exp_query_gid_attr(dev->ibv_context, port_num, i, &attr)) {
+                ucs_error("failed to query gid attributes "
+                          "(%s:%d gid_idx %d). %m", uct_ib_device_name(dev), port_num, i);
+                status = UCS_ERR_INVALID_PARAM;
+                goto out;
+            }
+
+            if ((roce_versions_priorities[prio_idx].type == attr.type) &&
+                (roce_versions_priorities[prio_idx].address_family ==
+                 uct_ib_device_get_addr_family(&attr.gid, i))) {
+                *gid_index = i;
+                goto out_print;
+            }
+        }
+    }
+
+    ucs_error("failed to find a gid index that matches one of the RoCE priorities");
+    status = UCS_ERR_INVALID_PARAM;
+    goto out;
+
+#else
+    for (i = 0; i < gid_tbl_len; i++) {
+        status = uct_ib_device_query_gid(dev, port_num, i, &gid);
+        if (status != UCS_OK) {
+            goto out;
+        }
+
+        /* assume RoCE v1 */
+        if (uct_ib_device_get_addr_family(&gid, i) == AF_INET) {
+            /* take the first gid that has IPv4 */
+            *gid_index = i;
+            goto out_print;
+        }
+    }
+
+    *gid_index = UCT_IB_MD_DEFAULT_GID_INDEX;
+#endif
+
+out_print:
+    ucs_debug("%s:%d set gid_index %d",
+              uct_ib_device_name(dev), port_num, *gid_index);
+out:
+    return status;
+}
+
+ucs_status_t uct_ib_device_select_gid_index(uct_ib_device_t *dev,
+                                            uint8_t port_num,
+                                            size_t md_config_index,
+                                            uint8_t *ib_gid_index)
+{
+    ucs_status_t status = UCS_OK;
+
+    if (md_config_index == UCS_CONFIG_ULUNITS_AUTO) {
+        if (uct_ib_device_is_port_ib(dev, port_num)) {
+            *ib_gid_index = UCT_IB_MD_DEFAULT_GID_INDEX;
+        } else {
+            status = uct_ib_device_set_roce_gid_index(dev, port_num, ib_gid_index);
+        }
+    } else {
+        *ib_gid_index = md_config_index;
+    }
+
+    return status;
+}
+
 const char *uct_ib_device_name(uct_ib_device_t *dev)
 {
     return ibv_get_device_name(dev->ibv_context->device);
@@ -445,150 +637,21 @@ uint8_t uct_ib_to_fabric_time(double time)
     }
 }
 
-uct_ib_address_type_t uct_ib_address_scope(uint64_t subnet_prefix)
-{
-    if (subnet_prefix == UCT_IB_LINK_LOCAL_PREFIX) {
-        return UCT_IB_ADDRESS_TYPE_LINK_LOCAL;
-    } else if ((subnet_prefix & UCT_IB_SITE_LOCAL_MASK) == UCT_IB_SITE_LOCAL_PREFIX) {
-        return UCT_IB_ADDRESS_TYPE_SITE_LOCAL;
-    } else {
-        return UCT_IB_ADDRESS_TYPE_GLOBAL;
-    }
-}
-
-size_t uct_ib_address_size(uct_ib_address_type_t type)
-{
-    switch (type) {
-    case UCT_IB_ADDRESS_TYPE_LINK_LOCAL:
-        return sizeof(uct_ib_address_t) +
-               sizeof(uint16_t); /* lid */
-    case UCT_IB_ADDRESS_TYPE_SITE_LOCAL:
-        return sizeof(uct_ib_address_t) +
-               sizeof(uint16_t) + /* lid */
-               sizeof(uint64_t) + /* if_id */
-               sizeof(uint16_t);  /* subnet16 */
-    case UCT_IB_ADDRESS_TYPE_GLOBAL:
-        return sizeof(uct_ib_address_t) +
-               sizeof(uint16_t) + /* lid */
-               sizeof(uint64_t) + /* if_id */
-               sizeof(uint64_t);  /* subnet64 */
-    case UCT_IB_ADDRESS_TYPE_ETH:
-        return sizeof(uct_ib_address_t) +
-               sizeof(union ibv_gid);  /* raw gid */
-    default:
-        ucs_fatal("Invalid IB address type: %d", type);
-    }
-}
-
-void uct_ib_address_pack(uct_ib_device_t *dev, uct_ib_address_type_t type,
-                         const union ibv_gid *gid, uint16_t lid,
-                         uct_ib_address_t *ib_addr)
-{
-    void *ptr = ib_addr + 1;
-
-    ib_addr->flags = 0;
-
-    if (type != UCT_IB_ADDRESS_TYPE_ETH) {
-        /* IB */
-        ib_addr->flags |= UCT_IB_ADDRESS_FLAG_LINK_LAYER_IB;
-
-        /* LID */
-        ib_addr->flags |= UCT_IB_ADDRESS_FLAG_LID;
-        *(uint16_t*) ptr = lid;
-        ptr += sizeof(uint16_t);
-
-        if (type >= UCT_IB_ADDRESS_TYPE_SITE_LOCAL) {
-            ib_addr->flags |= UCT_IB_ADDRESS_FLAG_IF_ID;
-            *(uint64_t*) ptr = gid->global.interface_id;
-            ptr += sizeof(uint64_t);
-
-            if (type >= UCT_IB_ADDRESS_TYPE_GLOBAL) {
-                /* Global */
-                ib_addr->flags |= UCT_IB_ADDRESS_FLAG_SUBNET64;
-                *(uint64_t*) ptr = gid->global.subnet_prefix;
-            } else {
-                /* Site-local */
-                ib_addr->flags |= UCT_IB_ADDRESS_FLAG_SUBNET16;
-                *(uint16_t*) ptr = gid->global.subnet_prefix >> 48;
-            }
-        }
-    } else {
-        /* RoCE */
-        ib_addr->flags |= UCT_IB_ADDRESS_FLAG_LINK_LAYER_ETH;
-        /* in this case we don't use the lid and set the GID flag */
-        ib_addr->flags |= UCT_IB_ADDRESS_FLAG_GID;
-        /* uint8_t raw[16]; */
-        memcpy(ptr, gid->raw, sizeof(gid->raw) * sizeof(uint8_t));
-    }
-
-}
-
-void uct_ib_address_unpack(const uct_ib_address_t *ib_addr, uint16_t *lid,
-                           uint8_t *is_global, union ibv_gid *gid)
-{
-    const void *ptr = ib_addr + 1;
-
-
-    gid->global.subnet_prefix = UCT_IB_LINK_LOCAL_PREFIX; /* Default prefix */
-    gid->global.interface_id  = 0;
-    *lid                      = 0;
-    *is_global                = 0;
-
-    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_GID) {
-        memcpy(gid->raw, ptr, sizeof(gid->raw) * sizeof(uint8_t)); /* uint8_t raw[16]; */
-        *is_global = 1;
-    }
-
-    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LID) {
-        *lid = *(uint16_t*)ptr;
-        ptr += sizeof(uint16_t);
-    }
-
-    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_IF_ID) {
-        gid->global.interface_id = *(uint64_t*)ptr;
-        ptr += sizeof(uint64_t);
-    }
-
-    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_SUBNET16) {
-        gid->global.subnet_prefix = UCT_IB_SITE_LOCAL_PREFIX |
-                                    ((uint64_t) *(uint16_t*) ptr << 48);
-        *is_global = 1;
-        ptr += sizeof(uint16_t);
-    }
-
-    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_SUBNET64) {
-        gid->global.subnet_prefix = *(uint64_t*) ptr;
-        *is_global = 1;
-        ptr += sizeof(uint64_t);
-    }
-}
-
-const char *uct_ib_address_str(const uct_ib_address_t *ib_addr, char *buf,
-                               size_t max)
+ucs_status_t uct_ib_modify_qp(struct ibv_qp *qp, enum ibv_qp_state state)
 {
-    union ibv_gid gid;
-    uint8_t is_global;
-    uint16_t lid;
-    char *p, *endp;
+    struct ibv_qp_attr qp_attr;
 
-    uct_ib_address_unpack(ib_addr, &lid, &is_global, &gid);
-
-    if (is_global) {
-        p    = buf;
-        endp = buf + max;
-        if (lid != 0) {
-            snprintf(p, endp - p, "lid %d ", lid);
-            p += strlen(p);
-        }
-        inet_ntop(AF_INET6, &gid, p, endp - p);
-    } else {
-        snprintf(buf, max, "lid %d", lid);
+    ucs_debug("modify QP 0x%x to state %d", qp->qp_num, state);
+    memset(&qp_attr, 0, sizeof(qp_attr));
+    qp_attr.qp_state = state;
+    if (ibv_modify_qp(qp, &qp_attr, IBV_QP_STATE)) {
+        ucs_warn("modify qp 0x%x to state %d failed: %m", qp->qp_num, state);
+        return UCS_ERR_IO_ERROR;
     }
 
-    return buf;
+    return UCS_OK;
 }
 
-
 ucs_status_t uct_ib_device_query_tl_resources(uct_ib_device_t *dev,
                                               const char *tl_name, unsigned flags,
                                               uct_tl_resource_desc_t **resources_p,
@@ -802,7 +865,7 @@ size_t uct_ib_device_odp_max_size(uct_ib_device_t *dev)
         return 0;
     }
 
-    if (IBV_DEVICE_HAS_DC(dev_attr)
+    if (IBV_DEVICE_HAS_DC(dev)
 #  if HAVE_STRUCT_IBV_EXP_DEVICE_ATTR_ODP_CAPS_PER_TRANSPORT_CAPS_DC_ODP_CAPS
         && !ucs_test_all_flags(IBV_EXP_ODP_CAPS(dev_attr, dc), required_rc_odp_caps)
 #  endif
@@ -873,3 +936,84 @@ int uct_ib_device_odp_has_global_mr(uct_ib_device_t *dev)
 
     return 1;
 }
+
+const char *uct_ib_wc_status_str(enum ibv_wc_status wc_status)
+{
+    return ibv_wc_status_str(wc_status);
+}
+
+static ucs_status_t uct_ib_device_create_ah(uct_ib_device_t *dev,
+                                            struct ibv_ah_attr *ah_attr,
+                                            struct ibv_pd *pd,
+                                            struct ibv_ah **ah_p)
+{
+    char buf[128];
+    char *p, *endp;
+    struct ibv_ah *ah;
+
+    ah = ibv_create_ah(pd, ah_attr);
+    if (ah == NULL) {
+        p    = buf;
+        endp = buf + sizeof(buf);
+        snprintf(p, endp - p, "dlid=%d sl=%d port=%d src_path_bits=%d",
+                 ah_attr->dlid, ah_attr->sl,
+                 ah_attr->port_num, ah_attr->src_path_bits);
+        p += strlen(p);
+
+        if (ah_attr->is_global) {
+            snprintf(p, endp - p, " dgid=");
+            p += strlen(p);
+            inet_ntop(AF_INET6, &ah_attr->grh.dgid, p, endp - p);
+            p += strlen(p);
+            snprintf(p, endp - p, " sgid_index=%d traffic_class=%d",
+                     ah_attr->grh.sgid_index, ah_attr->grh.traffic_class);
+        }
+
+        ucs_error("ibv_create_ah(%s) failed: %m", buf);
+        return UCS_ERR_INVALID_ADDR;
+    }
+
+    *ah_p = ah;
+    return UCS_OK;
+}
+
+ucs_status_t uct_ib_device_create_ah_cached(uct_ib_device_t *dev,
+                                            struct ibv_ah_attr *ah_attr,
+                                            struct ibv_pd *pd,
+                                            struct ibv_ah **ah_p)
+{
+    ucs_status_t status = UCS_OK;
+    khiter_t iter;
+    int ret;
+
+    ucs_spin_lock(&dev->ah_lock);
+
+    /* looking for existing AH with same attributes */
+    iter = kh_get(uct_ib_ah, &dev->ah_hash, *ah_attr);
+    if (iter == kh_end(&dev->ah_hash)) {
+        /* new AH */
+        status = uct_ib_device_create_ah(dev, ah_attr, pd, ah_p);
+        if (status != UCS_OK) {
+            goto unlock;
+        }
+
+        /* store AH in hash */
+        iter = kh_put(uct_ib_ah, &dev->ah_hash, *ah_attr, &ret);
+
+        /* failed to store - rollback */
+        if (iter == kh_end(&dev->ah_hash)) {
+            ibv_destroy_ah(*ah_p);
+            status = UCS_ERR_NO_MEMORY;
+            goto unlock;
+        }
+
+        kh_value(&dev->ah_hash, iter) = *ah_p;
+    } else {
+        /* found existing AH */
+        *ah_p = kh_value(&dev->ah_hash, iter);
+    }
+
+unlock:
+    ucs_spin_unlock(&dev->ah_lock);
+    return status;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.h
index 5e6395796..73fd9f18d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_device.h
@@ -12,6 +12,8 @@
 #include <uct/api/uct.h>
 #include <ucs/stats/stats.h>
 #include <ucs/debug/assert.h>
+#include <ucs/datastruct/khash.h>
+#include <ucs/type/spinlock.h>
 
 #include <endian.h>
 
@@ -61,15 +63,6 @@ enum {
 };
 
 
-typedef enum {
-    UCT_IB_ADDRESS_TYPE_LINK_LOCAL,   /* Subnet-local address */
-    UCT_IB_ADDRESS_TYPE_SITE_LOCAL,   /* Site local, 16-bit subnet prefix */
-    UCT_IB_ADDRESS_TYPE_GLOBAL,       /* Global, 64-bit subnet prefix */
-    UCT_IB_ADDRESS_TYPE_ETH,          /* RoCE  address */
-    UCT_IB_ADDRESS_TYPE_LAST
-} uct_ib_address_type_t;
-
-
 /**
  * Flags which specify which address fields are present
  */
@@ -113,6 +106,8 @@ typedef struct uct_ib_device_spec {
 } uct_ib_device_spec_t;
 
 
+KHASH_TYPE(uct_ib_ah, struct ibv_ah_attr, struct ibv_ah*);
+
 /**
  * IB device (corresponds to HCA)
  */
@@ -124,11 +119,46 @@ typedef struct uct_ib_device {
     cpu_set_t                   local_cpus;      /* CPUs local to device */
     int                         numa_node;       /* NUMA node of the device */
     int                         async_events;    /* Whether async events are handled */
+    int                         max_zcopy_log_sge; /* Maximum sges log for zcopy am */
     UCS_STATS_NODE_DECLARE(stats);
     struct ibv_exp_port_attr    port_attr[UCT_IB_DEV_MAX_PORTS]; /* Cached port attributes */
+    unsigned                    flags;
+    /* AH hash */
+    khash_t(uct_ib_ah)          ah_hash;
+    ucs_spinlock_t              ah_lock;
 } uct_ib_device_t;
 
 
+/**
+ * IB device private initializer.
+ */
+typedef struct uct_ib_device_init_entry {
+    ucs_list_link_t             list;
+    ucs_status_t                (*init)(uct_ib_device_t *dev);
+} uct_ib_device_init_entry_t;
+
+#define UCT_IB_DEVICE_INIT(_init_fn) \
+    UCS_STATIC_INIT { \
+        extern ucs_list_link_t uct_ib_device_init_list; \
+        static uct_ib_device_init_entry_t init_entry = { \
+            .init = _init_fn, \
+        }; \
+        ucs_list_add_tail(&uct_ib_device_init_list, &init_entry.list); \
+    }
+
+
+#if HAVE_DECL_IBV_EXP_QUERY_GID_ATTR
+/**
+ * RoCE version description
+ */
+typedef struct uct_ib_roce_version_desc {
+    enum ibv_exp_roce_gid_type type;
+    int address_family;
+    int priority;
+} uct_ib_roce_version_desc_t;
+#endif
+
+
 /**
  * Check if a port on a device is active and supports the given flags.
  */
@@ -164,6 +194,20 @@ void uct_ib_device_cleanup(uct_ib_device_t *dev);
 const uct_ib_device_spec_t* uct_ib_device_spec(uct_ib_device_t *dev);
 
 
+/**
+ * Select the IB gid index to use.
+ *
+ * @param dev                   IB device.
+ * @param port_num              Port number.
+ * @param md_config_index       Gid index from the md configuration.
+ * @param ib_gid_index          Filled with the selected gid index.
+ */
+ucs_status_t uct_ib_device_select_gid_index(uct_ib_device_t *dev,
+                                            uint8_t port_num,
+                                            size_t md_config_index,
+                                            uint8_t *ib_gid_index);
+
+
 /**
  * @return device name.
  */
@@ -195,49 +239,9 @@ size_t uct_ib_mtu_value(enum ibv_mtu mtu);
 
 
 /**
- * @return IB address scope of a given subnet prefix (according to IBTA 4.1.1 12).
- */
-uct_ib_address_type_t uct_ib_address_scope(uint64_t subnet_prefix);
-
-
-/**
- * @return IB address size of the given link scope.
- */
-size_t uct_ib_address_size(uct_ib_address_type_t type);
-
-
-/**
- * Pack IB address.
- *
- * @param [in]  dev        IB device. TODO remove this.
- * @param [in]  scope      Address scope.
- * @param [in]  gid        GID address to pack.
- * @param [in]  lid        LID address to pack.
- * @param [out] ib_addr    Filled with packed ib address. Size of the structure
- *                         must be at least what @ref uct_ib_address_size() returns
- *                         for the given scope.
- */
-void uct_ib_address_pack(uct_ib_device_t *dev, uct_ib_address_type_t scope,
-                         const union ibv_gid *gid, uint16_t lid,
-                         uct_ib_address_t *ib_addr);
-
-
-/**
- * Unpack IB address.
- *
- * @param [in]  ib_addr    IB address to unpack.
- * @param [out] lid        Filled with address LID, or 0 if not present.
- * @param [out] is_global  Filled with 0, or 1 if the address is IB global
- */
-void uct_ib_address_unpack(const uct_ib_address_t *ib_addr, uint16_t *lid,
-                           uint8_t *is_global, union ibv_gid *gid);
-
-
-/**
- * Convert IB address to a human-readable string.
+ * Modify QP to a given state and check for error
  */
-const char *uct_ib_address_str(const uct_ib_address_t *ib_addr, char *buf,
-                               size_t max);
+ucs_status_t uct_ib_modify_qp(struct ibv_qp *qp, enum ibv_qp_state state);
 
 
 /**
@@ -258,6 +262,15 @@ size_t uct_ib_device_odp_max_size(uct_ib_device_t *dev);
 
 int uct_ib_device_odp_has_global_mr(uct_ib_device_t *dev);
 
+const char *uct_ib_wc_status_str(enum ibv_wc_status wc_status);
+
+ucs_status_t uct_ib_device_create_ah_cached(uct_ib_device_t *dev,
+                                            struct ibv_ah_attr *ah_attr,
+                                            struct ibv_pd *pd,
+                                            struct ibv_ah **ah_p);
+
+void uct_ib_device_cleanup_ah_cached(uct_ib_device_t *dev);
+
 static inline struct ibv_exp_port_attr*
 uct_ib_device_port_attr(uct_ib_device_t *dev, uint8_t port_num)
 {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.c
index a4a6ac274..0a81b1449 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.c
@@ -18,7 +18,13 @@
 #include <string.h>
 #include <stdlib.h>
 #include <poll.h>
+#if HAVE_CUDA
+#include <limits.h>
+#include <cuda.h>
+#endif
+#include <ctype.h>
 
+#define MAXPATHSIZE 1024
 
 static UCS_CONFIG_DEFINE_ARRAY(path_bits_spec,
                                sizeof(ucs_range_spec_t),
@@ -34,6 +40,11 @@ const char *uct_ib_mtu_values[] = {
 };
 
 enum {
+    UCT_IB_ADDRESS_TYPE_LINK_LOCAL,
+    UCT_IB_ADDRESS_TYPE_SITE_LOCAL,
+    UCT_IB_ADDRESS_TYPE_GLOBAL,
+    UCT_IB_ADDRESS_TYPE_ETH,
+    UCT_IB_ADDRESS_TYPE_LAST,
     UCT_IB_IFACE_ADDRESS_TYPE_AUTO  = UCT_IB_ADDRESS_TYPE_LAST,
     UCT_IB_IFACE_ADDRESS_TYPE_LAST
 };
@@ -80,7 +91,7 @@ ucs_config_field_t uct_ib_iface_config_table[] = {
    ucs_offsetof(uct_ib_iface_config_t, tx.min_sge), UCS_CONFIG_TYPE_UINT},
 
   {"TX_CQ_MODERATION", "64",
-   "Number of send WQEs for which completion is requested.",
+   "Maximum number of send WQEs which can be posted without requesting a completion.",
    ucs_offsetof(uct_ib_iface_config_t, tx.cq_moderation), UCS_CONFIG_TYPE_UINT},
 
 #if HAVE_DECL_IBV_EXP_CQ_MODERATION
@@ -129,24 +140,29 @@ ucs_config_field_t uct_ib_iface_config_table[] = {
 
   {"ADDR_TYPE", "auto",
    "Set the interface address type. \"auto\" mode detects the type according to\n"
-   "link layer type and IB subnet prefix.",
+   "link layer type and IB subnet prefix.\n"
+   "Deprecated. To force use of global routing use IS_GLOBAL.",
    ucs_offsetof(uct_ib_iface_config_t, addr_type),
    UCS_CONFIG_TYPE_ENUM(uct_ib_iface_addr_types)},
 
-  {"GID_INDEX", "0",
-   "Port GID index to use.",
-   ucs_offsetof(uct_ib_iface_config_t, gid_index), UCS_CONFIG_TYPE_UINT},
+  {"IS_GLOBAL", "n",
+   "Force interface to use global routing.",
+   ucs_offsetof(uct_ib_iface_config_t, is_global), UCS_CONFIG_TYPE_BOOL},
 
   {"SL", "0",
-   "Which IB service level to use.\n",
+   "IB Service Level / RoCEv2 Ethernet Priority.\n",
    ucs_offsetof(uct_ib_iface_config_t, sl), UCS_CONFIG_TYPE_UINT},
 
   {"TRAFFIC_CLASS", "0",
-   "Which IB traffic class to use.\n",
+   "IB Traffic Class / RoCEv2 Differentiated Services Code Point (DSCP)\n",
    ucs_offsetof(uct_ib_iface_config_t, traffic_class), UCS_CONFIG_TYPE_UINT},
 
+  {"HOP_LIMIT", "255",
+   "IB Hop limit / RoCEv2 Time to Live. Should be between 0 and 255.\n",
+   ucs_offsetof(uct_ib_iface_config_t, hop_limit), UCS_CONFIG_TYPE_UINT},
+
   {"LID_PATH_BITS", "0-17",
-   "list of IB Path bits separated by comma (a,b,c) "
+   "List of IB Path bits separated by comma (a,b,c) "
    "which will be the low portion of the LID, according to the LMC in the fabric.",
    ucs_offsetof(uct_ib_iface_config_t, lid_path_bits), UCS_CONFIG_TYPE_ARRAY(path_bits_spec)},
 
@@ -154,6 +170,12 @@ ucs_config_field_t uct_ib_iface_config_table[] = {
    "Which pkey value to use. Should be between 0 and 0x7fff.",
    ucs_offsetof(uct_ib_iface_config_t, pkey_value), UCS_CONFIG_TYPE_HEX},
 
+#if HAVE_IBV_EXP_RES_DOMAIN
+  {"RESOURCE_DOMAIN", "y",
+   "Enable multiple resource domains (experimental).",
+   ucs_offsetof(uct_ib_iface_config_t, enable_res_domain), UCS_CONFIG_TYPE_BOOL},
+#endif
+
 
   {NULL}
 };
@@ -198,12 +220,130 @@ void uct_ib_iface_release_desc(uct_recv_desc_t *self, void *desc)
     ucs_mpool_put_inline(ib_desc);
 }
 
+size_t uct_ib_address_size(uct_ib_iface_t *iface)
+{
+    if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface))) {
+        return sizeof(uct_ib_address_t) +
+               sizeof(union ibv_gid);  /* raw gid */
+    } else if ((iface->gid.global.subnet_prefix == UCT_IB_LINK_LOCAL_PREFIX) &&
+               !iface->is_global_addr) {
+        return sizeof(uct_ib_address_t) +
+               sizeof(uint16_t); /* lid */
+    } else if (((iface->gid.global.subnet_prefix & UCT_IB_SITE_LOCAL_MASK) ==
+                                                  UCT_IB_SITE_LOCAL_PREFIX) &&
+               !iface->is_global_addr) {
+        return sizeof(uct_ib_address_t) +
+               sizeof(uint16_t) + /* lid */
+               sizeof(uint64_t) + /* if_id */
+               sizeof(uint16_t);  /* subnet16 */
+    } else {
+        return sizeof(uct_ib_address_t) +
+               sizeof(uint16_t) + /* lid */
+               sizeof(uint64_t) + /* if_id */
+               sizeof(uint64_t);  /* subnet64 */
+    }
+}
+
+void uct_ib_address_pack(uct_ib_iface_t *iface,
+                         const union ibv_gid *gid, uint16_t lid,
+                         uct_ib_address_t *ib_addr)
+{
+    void *ptr = ib_addr + 1;
+
+    if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface))) {
+        /* RoCE, in this case we don't use the lid and set the GID flag */
+        ib_addr->flags = UCT_IB_ADDRESS_FLAG_LINK_LAYER_ETH |
+                         UCT_IB_ADDRESS_FLAG_GID;
+        /* uint8_t raw[16]; */
+        memcpy(ptr, gid->raw, sizeof(gid->raw) * sizeof(uint8_t));
+    } else {
+        /* IB, LID */
+        ib_addr->flags = UCT_IB_ADDRESS_FLAG_LINK_LAYER_IB |
+                         UCT_IB_ADDRESS_FLAG_LID;
+        *(uint16_t*) ptr = lid;
+        ptr += sizeof(uint16_t);
+
+        if ((gid->global.subnet_prefix != UCT_IB_LINK_LOCAL_PREFIX) ||
+            iface->is_global_addr) {
+            ib_addr->flags |= UCT_IB_ADDRESS_FLAG_IF_ID;
+            *(uint64_t*) ptr = gid->global.interface_id;
+            ptr += sizeof(uint64_t);
+
+            if (((gid->global.subnet_prefix & UCT_IB_SITE_LOCAL_MASK) ==
+                                              UCT_IB_SITE_LOCAL_PREFIX) &&
+                !iface->is_global_addr) {
+                /* Site-local */
+                ib_addr->flags |= UCT_IB_ADDRESS_FLAG_SUBNET16;
+                *(uint16_t*) ptr = gid->global.subnet_prefix >> 48;
+            } else {
+                /* Global */
+                ib_addr->flags |= UCT_IB_ADDRESS_FLAG_SUBNET64;
+                *(uint64_t*) ptr = gid->global.subnet_prefix;
+            }
+        }
+    }
+}
+
+void uct_ib_address_unpack(const uct_ib_address_t *ib_addr, uint16_t *lid,
+                           union ibv_gid *gid)
+{
+    const void *ptr = ib_addr + 1;
+
+    gid->global.subnet_prefix = UCT_IB_LINK_LOCAL_PREFIX; /* Default prefix */
+    gid->global.interface_id  = 0;
+    *lid                      = 0;
+
+    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_GID) {
+        memcpy(gid->raw, ptr, sizeof(gid->raw) * sizeof(uint8_t)); /* uint8_t raw[16]; */
+    }
+
+    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LID) {
+        *lid = *(uint16_t*)ptr;
+        ptr += sizeof(uint16_t);
+    }
+
+    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_IF_ID) {
+        gid->global.interface_id = *(uint64_t*)ptr;
+        ptr += sizeof(uint64_t);
+    }
+
+    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_SUBNET16) {
+        gid->global.subnet_prefix = UCT_IB_SITE_LOCAL_PREFIX |
+                                    ((uint64_t) *(uint16_t*) ptr << 48);
+        ptr += sizeof(uint16_t);
+    }
+
+    if (ib_addr->flags & UCT_IB_ADDRESS_FLAG_SUBNET64) {
+        gid->global.subnet_prefix = *(uint64_t*) ptr;
+        ptr += sizeof(uint64_t);
+    }
+}
+
+const char *uct_ib_address_str(const uct_ib_address_t *ib_addr, char *buf,
+                               size_t max)
+{
+    union ibv_gid gid;
+    uint16_t lid;
+    char *p, *endp;
+
+    uct_ib_address_unpack(ib_addr, &lid, &gid);
+
+    p    = buf;
+    endp = buf + max;
+    if (lid != 0) {
+        snprintf(p, endp - p, "lid %d ", lid);
+        p += strlen(p);
+    }
+    inet_ntop(AF_INET6, &gid, p, endp - p);
+
+    return buf;
+}
+
 ucs_status_t uct_ib_iface_get_device_address(uct_iface_h tl_iface,
                                              uct_device_addr_t *dev_addr)
 {
     uct_ib_iface_t *iface = ucs_derived_of(tl_iface, uct_ib_iface_t);
-    uct_ib_address_pack(uct_ib_iface_device(iface), iface->addr_type,
-                        &iface->gid, uct_ib_iface_port_attr(iface)->lid,
+    uct_ib_address_pack(iface, &iface->gid, uct_ib_iface_port_attr(iface)->lid,
                         (void*)dev_addr);
     return UCS_OK;
 }
@@ -212,33 +352,17 @@ int uct_ib_iface_is_reachable(const uct_iface_h tl_iface, const uct_device_addr_
                               const uct_iface_addr_t *iface_addr)
 {
     uct_ib_iface_t *iface = ucs_derived_of(tl_iface, uct_ib_iface_t);
+    int is_local_eth = IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface));
     const uct_ib_address_t *ib_addr = (const void*)dev_addr;
     union ibv_gid gid;
-    uint8_t is_global;
     uint16_t lid;
-    int is_local_ib;
-
-    uct_ib_address_unpack(ib_addr, &lid, &is_global, &gid);
-
-    ucs_assert(iface->addr_type < UCT_IB_ADDRESS_TYPE_LAST);
-    switch (iface->addr_type) {
-    case UCT_IB_ADDRESS_TYPE_LINK_LOCAL:
-    case UCT_IB_ADDRESS_TYPE_SITE_LOCAL:
-    case UCT_IB_ADDRESS_TYPE_GLOBAL:
-         is_local_ib = 1;
-         break;
-    case UCT_IB_ADDRESS_TYPE_ETH:
-         is_local_ib = 0;
-         break;
-    default:
-         ucs_fatal("Unknown address type %d", iface->addr_type);
-         break;
-    }
 
-    if (is_local_ib && (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LINK_LAYER_IB)) {
+    uct_ib_address_unpack(ib_addr, &lid, &gid);
+
+    if (!is_local_eth && (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LINK_LAYER_IB)) {
         /* same subnet prefix */
         return gid.global.subnet_prefix == iface->gid.global.subnet_prefix;
-    } else if (!is_local_ib && (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LINK_LAYER_ETH)) {
+    } else if (is_local_eth && (ib_addr->flags & UCT_IB_ADDRESS_FLAG_LINK_LAYER_ETH)) {
         /* there shouldn't be a lid and the gid flag should be on */
         ucs_assert(ib_addr->flags & UCT_IB_ADDRESS_FLAG_GID);
         ucs_assert(!(ib_addr->flags & UCT_IB_ADDRESS_FLAG_LID));
@@ -253,36 +377,8 @@ ucs_status_t uct_ib_iface_create_ah(uct_ib_iface_t *iface,
                                     struct ibv_ah_attr *ah_attr,
                                     struct ibv_ah **ah_p)
 {
-    struct ibv_ah *ah;
-    char buf[128];
-    char *p, *endp;
-
-    ah = ibv_create_ah(uct_ib_iface_md(iface)->pd, ah_attr);
-
-    if (ah == NULL) {
-        p    = buf;
-        endp = buf + sizeof(buf);
-        snprintf(p, endp - p, "dlid=%d sl=%d port=%d src_path_bits=%d",
-                 ah_attr->dlid, ah_attr->sl,
-                 ah_attr->port_num, ah_attr->src_path_bits);
-        p += strlen(p);
-
-        if (ah_attr->is_global) {
-            snprintf(p, endp - p, " dgid=");
-            p += strlen(p);
-            inet_ntop(AF_INET6, &ah_attr->grh.dgid, p, endp - p);
-            p += strlen(p);
-            snprintf(p, endp - p, " sgid_index=%d traffic_class=%d",
-                     ah_attr->grh.sgid_index, ah_attr->grh.traffic_class);
-        }
-
-        ucs_error("ibv_create_ah(%s) on "UCT_IB_IFACE_FMT" failed: %m", buf,
-                  UCT_IB_IFACE_ARG(iface));
-        return UCS_ERR_INVALID_ADDR;
-    }
-
-    *ah_p        = ah;
-    return UCS_OK;
+    return uct_ib_device_create_ah_cached(uct_ib_iface_device(iface), ah_attr,
+                                          uct_ib_iface_md(iface)->pd, ah_p);
 }
 
 static ucs_status_t uct_ib_iface_init_pkey(uct_ib_iface_t *iface,
@@ -393,17 +489,174 @@ static ucs_status_t uct_ib_iface_init_lmc(uct_ib_iface_t *iface,
     return UCS_OK;
 }
 
+static const char *uct_ib_qp_type_str(int qp_type)
+{
+    switch (qp_type) {
+    case IBV_QPT_RC:
+        return "rc";
+    case IBV_QPT_UD:
+        return "ud";
+#if HAVE_TL_DC
+    case UCT_IB_QPT_DCI:
+        return "dci";
+#endif
+    default:
+        ucs_bug("invalid qp type: %d", qp_type);
+        return "unknown";
+    }
+}
+
+void uct_ib_iface_fill_attr(uct_ib_iface_t *iface, uct_ib_qp_attr_t *attr)
+{
+    attr->ibv.send_cq             = iface->cq[UCT_IB_DIR_TX];
+    attr->ibv.recv_cq             = iface->cq[UCT_IB_DIR_RX];
+
+    attr->ibv.srq                 = attr->srq;
+    attr->ibv.cap                 = attr->cap;
+    attr->ibv.qp_type             = attr->qp_type;
+    attr->ibv.sq_sig_all          = attr->sq_sig_all;
+
+#if HAVE_DECL_IBV_EXP_CREATE_QP
+    attr->ibv.comp_mask           = IBV_EXP_QP_INIT_ATTR_PD;
+    attr->ibv.pd                  = uct_ib_iface_qp_pd(iface);
+#elif HAVE_DECL_IBV_CREATE_QP_EX
+    attr->ibv.comp_mask           = IBV_QP_INIT_ATTR_PD;
+    attr->ibv.pd                  = uct_ib_iface_qp_pd(iface);
+#endif
+
+#if HAVE_IBV_EXP_RES_DOMAIN
+    if (iface->res_domain != NULL) {
+        attr->ibv.comp_mask      |= IBV_EXP_QP_INIT_ATTR_RES_DOMAIN;
+        attr->ibv.res_domain      = iface->res_domain->ibv_domain;
+    }
+#endif
+
+    if (attr->qp_type == IBV_QPT_UD) {
+        return;
+    }
+
+#if HAVE_IB_EXT_ATOMICS
+    attr->ibv.comp_mask          |= IBV_EXP_QP_INIT_ATTR_ATOMICS_ARG;
+    attr->ibv.max_atomic_arg      = UCT_IB_MAX_ATOMIC_SIZE;
+#endif
+
+#if HAVE_DECL_IBV_EXP_ATOMIC_HCA_REPLY_BE
+    if (uct_ib_iface_device(iface)->dev_attr.exp_atomic_cap ==
+                                     IBV_EXP_ATOMIC_HCA_REPLY_BE) {
+        attr->ibv.comp_mask       |= IBV_EXP_QP_INIT_ATTR_CREATE_FLAGS;
+        attr->ibv.exp_create_flags = IBV_EXP_QP_CREATE_ATOMIC_BE_REPLY;
+    }
+#endif
+
+#if HAVE_STRUCT_IBV_EXP_QP_INIT_ATTR_MAX_INL_RECV
+    attr->ibv.comp_mask           |= IBV_EXP_QP_INIT_ATTR_INL_RECV;
+    attr->ibv.max_inl_recv         = attr->max_inl_recv;
+#endif
+}
+
+ucs_status_t uct_ib_iface_create_qp(uct_ib_iface_t *iface,
+                                    uct_ib_qp_attr_t *attr,
+                                    struct ibv_qp **qp_p)
+{
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+    struct ibv_qp *qp;
+
+    uct_ib_iface_fill_attr(iface, attr);
+
+#if HAVE_DECL_IBV_EXP_CREATE_QP
+    qp = ibv_exp_create_qp(dev->ibv_context, &attr->ibv);
+#elif HAVE_DECL_IBV_CREATE_QP_EX
+    qp = ibv_create_qp_ex(dev->ibv_context, &attr->ibv);
+#else
+    qp = ibv_create_qp(uct_ib_iface_qp_pd(iface), &attr->ibv);
+#endif
+    if (qp == NULL) {
+        ucs_error("iface=%p: failed to create %s QP TX wr:%d sge:%d inl:%d RX wr:%d sge:%d inl %d: %m",
+                  iface, uct_ib_qp_type_str(attr->qp_type),
+                  attr->cap.max_send_wr, attr->cap.max_send_sge, attr->cap.max_inline_data,
+                  attr->cap.max_recv_wr, attr->cap.max_recv_sge, attr->max_inl_recv);
+        return UCS_ERR_IO_ERROR;
+    }
+
+    attr->cap  = attr->ibv.cap;
+    *qp_p      = qp;
+
+    ucs_debug("iface=%p: created %s QP 0x%x on %s:%d TX wr:%d sge:%d inl:%d RX wr:%d sge:%d inl %d",
+              iface, uct_ib_qp_type_str(attr->qp_type), qp->qp_num,
+              uct_ib_device_name(dev), iface->config.port_num,
+              attr->cap.max_send_wr, attr->cap.max_send_sge, attr->cap.max_inline_data,
+              attr->cap.max_recv_wr, attr->cap.max_recv_sge, attr->max_inl_recv);
+
+    return UCS_OK;
+}
+
+#if HAVE_DECL_IBV_EXP_SETENV
+static int uct_ib_max_cqe_size()
+{
+    static int max_cqe_size = -1;
+
+    if (max_cqe_size == -1) {
+#ifdef __aarch64__
+        char arm_board_vendor[128];
+        ucs_aarch64_cpuid_t cpuid;
+        ucs_aarch64_cpuid(&cpuid);
+
+        arm_board_vendor[0] = '\0';
+        ucs_read_file(arm_board_vendor, sizeof(arm_board_vendor), 1,
+                      "/sys/devices/virtual/dmi/id/board_vendor");
+        ucs_debug("arm_board_vendor is '%s'", arm_board_vendor);
+
+        max_cqe_size = ((strcasestr(arm_board_vendor, "Huawei")) &&
+                        (cpuid.implementer == 0x41) && (cpuid.architecture == 8) &&
+                        (cpuid.variant == 0)        && (cpuid.part == 0xd08)     &&
+                        (cpuid.revision == 2))
+                       ? 64 : 128;
+#else
+        max_cqe_size = 128;
+#endif
+        ucs_debug("max IB CQE size is %d", max_cqe_size);
+    }
+
+    return max_cqe_size;
+}
+#endif
+
+struct ibv_cq *uct_ib_create_cq(struct ibv_context *context, int cqe,
+                                struct ibv_comp_channel *channel,
+                                int comp_vector, int ignore_overrun)
+{
+    struct ibv_cq *cq;
+#if HAVE_DECL_IBV_CREATE_CQ_ATTR_IGNORE_OVERRUN
+    struct ibv_cq_init_attr_ex cq_attr = {};
+
+    cq_attr.cqe = cqe;
+    cq_attr.channel = channel;
+    cq_attr.comp_vector = comp_vector;
+    if (ignore_overrun) {
+        cq_attr.comp_mask = IBV_CQ_INIT_ATTR_MASK_FLAGS;
+        cq_attr.flags = IBV_CREATE_CQ_ATTR_IGNORE_OVERRUN;
+    }
+
+    cq = ibv_cq_ex_to_cq(ibv_create_cq_ex(context, &cq_attr));
+#else
+    cq = ibv_create_cq(context, cqe, NULL, channel, comp_vector);
+#endif
+    return cq;
+}
+
 static ucs_status_t uct_ib_iface_create_cq(uct_ib_iface_t *iface, int cq_length,
                                            size_t *inl, int preferred_cpu,
-                                           struct ibv_cq **cq_p)
+                                           int flags, struct ibv_cq **cq_p)
 {
-    static const char *cqe_size_env_var = "MLX5_CQE_SIZE";
     uct_ib_device_t *dev = uct_ib_iface_device(iface);
+    struct ibv_cq *cq;
+    size_t cqe_size = 64;
+    ucs_status_t status;
+#if HAVE_DECL_IBV_EXP_SETENV
+    static const char *cqe_size_env_var = "MLX5_CQE_SIZE";
     const char *cqe_size_env_value;
-    size_t cqe_size_min, cqe_size;
+    size_t cqe_size_min;
     char cqe_size_buf[32];
-    ucs_status_t status;
-    struct ibv_cq *cq;
     int env_var_added = 0;
     int ret;
 
@@ -415,8 +668,7 @@ static ucs_status_t uct_ib_iface_create_cq(uct_ib_iface_t *iface, int cq_length,
         if (cqe_size < cqe_size_min) {
             ucs_error("%s is set to %zu, but at least %zu is required (inl: %zu)",
                       cqe_size_env_var, cqe_size, cqe_size_min, *inl);
-            status = UCS_ERR_INVALID_PARAM;
-            goto out;
+            return UCS_ERR_INVALID_PARAM;
         }
     } else {
         /* CQE size is not defined by the environment, set it according to inline
@@ -424,7 +676,7 @@ static ucs_status_t uct_ib_iface_create_cq(uct_ib_iface_t *iface, int cq_length,
          */
         cqe_size = ucs_max(cqe_size_min, UCS_SYS_CACHE_LINE_SIZE);
         cqe_size = ucs_max(cqe_size, 64);  /* at least 64 */
-        cqe_size = ucs_min(cqe_size, 128); /* at most 128 */
+        cqe_size = ucs_min(cqe_size, uct_ib_max_cqe_size());
         snprintf(cqe_size_buf, sizeof(cqe_size_buf),"%zu", cqe_size);
         ucs_debug("%s: setting %s=%s", uct_ib_device_name(dev), cqe_size_env_var,
                   cqe_size_buf);
@@ -432,15 +684,14 @@ static ucs_status_t uct_ib_iface_create_cq(uct_ib_iface_t *iface, int cq_length,
         if (ret) {
             ucs_error("ibv_exp_setenv(%s=%s) failed: %m", cqe_size_env_var,
                       cqe_size_buf);
-            status = UCS_ERR_INVALID_PARAM;
-            goto out;
+            return UCS_ERR_INVALID_PARAM;
         }
 
         env_var_added = 1;
     }
-
-    cq = ibv_create_cq(dev->ibv_context, cq_length, NULL, iface->comp_channel,
-                       preferred_cpu);
+#endif
+    cq = uct_ib_create_cq(dev->ibv_context, cq_length, iface->comp_channel,
+                          preferred_cpu, flags & UCT_IB_CQ_IGNORE_OVERRUN);
     if (cq == NULL) {
         ucs_error("ibv_create_cq(cqe=%d) failed: %m", cq_length);
         status = UCS_ERR_IO_ERROR;
@@ -452,6 +703,7 @@ static ucs_status_t uct_ib_iface_create_cq(uct_ib_iface_t *iface, int cq_length,
     status = UCS_OK;
 
 out_unsetenv:
+#if HAVE_DECL_IBV_EXP_SETENV
     if (env_var_added) {
         /* if we created a new environment variable, remove it */
         ret = ibv_exp_unsetenv(dev->ibv_context, cqe_size_env_var);
@@ -459,7 +711,7 @@ out_unsetenv:
             ucs_warn("unsetenv(%s) failed: %m", cqe_size_env_var);
         }
     }
-out:
+#endif
     return status;
 }
 
@@ -505,19 +757,125 @@ static ucs_status_t uct_ib_iface_set_moderation(struct ibv_cq *cq,
     return UCS_OK;
 }
 
-/**
- * @param rx_headroom   Headroom requested by the user.
- * @param rx_priv_len   Length of transport private data to reserve (0 if unused)
- * @param rx_hdr_len    Length of transport network header.
- * @param mss           Maximal segment size (transport limit).
- */
+static int uct_ib_iface_res_domain_cmp(uct_ib_iface_res_domain_t *res_domain,
+                                       uct_ib_iface_t *iface)
+{
+#if HAVE_IBV_EXP_RES_DOMAIN
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+
+    return res_domain->ibv_domain->context == dev->ibv_context;
+#elif HAVE_DECL_IBV_ALLOC_TD
+    uct_ib_md_t     *md  = uct_ib_iface_md(iface);
+
+    return res_domain->pd == md->pd;
+#else
+    return 1;
+#endif
+}
+
+static ucs_status_t
+uct_ib_iface_res_domain_init(uct_ib_iface_res_domain_t *res_domain,
+                             uct_ib_iface_t *iface)
+{
+#if HAVE_IBV_EXP_RES_DOMAIN
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+    struct ibv_exp_res_domain_init_attr attr;
+
+    attr.comp_mask    = IBV_EXP_RES_DOMAIN_THREAD_MODEL |
+                        IBV_EXP_RES_DOMAIN_MSG_MODEL;
+    attr.msg_model    = IBV_EXP_MSG_LOW_LATENCY;
+
+    switch (iface->super.worker->thread_mode) {
+    case UCS_THREAD_MODE_SINGLE:
+        attr.thread_model = IBV_EXP_THREAD_SINGLE;
+        break;
+    case UCS_THREAD_MODE_SERIALIZED:
+        attr.thread_model = IBV_EXP_THREAD_UNSAFE;
+        break;
+    default:
+        attr.thread_model = IBV_EXP_THREAD_SAFE;
+        break;
+    }
+
+    res_domain->ibv_domain = ibv_exp_create_res_domain(dev->ibv_context, &attr);
+    if (res_domain->ibv_domain == NULL) {
+        ucs_error("ibv_exp_create_res_domain() on %s failed: %m",
+                  uct_ib_device_name(dev));
+        return UCS_ERR_IO_ERROR;
+    }
+#elif HAVE_DECL_IBV_ALLOC_TD
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+    uct_ib_md_t     *md  = uct_ib_iface_md(iface);
+    struct ibv_parent_domain_init_attr attr;
+    struct ibv_td_init_attr td_attr;
+
+    if (iface->super.worker->thread_mode == UCS_THREAD_MODE_MULTI) {
+        td_attr.comp_mask = 0;
+        res_domain->td = ibv_alloc_td(dev->ibv_context, &td_attr);
+        if (res_domain->td == NULL) {
+            ucs_error("ibv_alloc_td() on %s failed: %m",
+                      uct_ib_device_name(dev));
+            return UCS_ERR_IO_ERROR;
+        }
+    } else {
+        res_domain->td = NULL;
+        res_domain->ibv_domain = NULL;
+        res_domain->pd = md->pd;
+        return UCS_OK;
+    }
+
+    attr.td = res_domain->td;
+    attr.pd = md->pd;
+    attr.comp_mask = 0;
+    res_domain->ibv_domain = ibv_alloc_parent_domain(dev->ibv_context, &attr);
+    if (res_domain->ibv_domain == NULL) {
+        ucs_error("ibv_alloc_parent_domain() on %s failed: %m",
+                  uct_ib_device_name(dev));
+        ibv_dealloc_td(res_domain->td);
+        return UCS_ERR_IO_ERROR;
+    }
+    res_domain->pd = md->pd;
+#endif
+    return UCS_OK;
+}
+
+static void uct_ib_iface_res_domain_cleanup(uct_ib_iface_res_domain_t *res_domain)
+{
+#if HAVE_IBV_EXP_RES_DOMAIN
+    struct ibv_exp_destroy_res_domain_attr attr;
+    int ret;
+
+    attr.comp_mask = 0;
+    ret = ibv_exp_destroy_res_domain(res_domain->ibv_domain->context,
+                                     res_domain->ibv_domain, &attr);
+    if (ret != 0) {
+        ucs_warn("ibv_exp_destroy_res_domain() failed: %m");
+    }
+#elif HAVE_DECL_IBV_ALLOC_TD
+    int ret;
+
+    if (res_domain->ibv_domain != NULL) {
+        ret = ibv_dealloc_pd(res_domain->ibv_domain);
+        if (ret != 0) {
+            ucs_warn("ibv_dealloc_pd() failed: %m");
+            return;
+        }
+
+        ret = ibv_dealloc_td(res_domain->td);
+        if (ret != 0) {
+            ucs_warn("ibv_dealloc_td() failed: %m");
+        }
+    }
+#endif
+}
+
 UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
                     uct_worker_h worker, const uct_iface_params_t *params,
-                    unsigned rx_priv_len, unsigned rx_hdr_len,
-                    unsigned tx_cq_len, unsigned rx_cq_len, size_t mss,
-                    const uct_ib_iface_config_t *config)
+                    const uct_ib_iface_config_t *config,
+                    const uct_ib_iface_init_attr_t *init_attr)
 {
-    uct_ib_device_t *dev = &ucs_derived_of(md, uct_ib_md_t)->dev;
+    uct_ib_md_t *ib_md = ucs_derived_of(md, uct_ib_md_t);
+    uct_ib_device_t *dev = &ib_md->dev;
     int preferred_cpu = ucs_cpu_set_find_lcs(&params->cpu_mask);
     ucs_status_t status;
     uint8_t port_num;
@@ -525,48 +883,58 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
 
     ucs_assert(params->open_mode & UCT_IFACE_OPEN_MODE_DEVICE);
 
-    if (params->stats_root == NULL) {
-        UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &ops->super, md, worker,
-                                  params, &config->super
-                                  UCS_STATS_ARG(dev->stats)
-                                  UCS_STATS_ARG(params->mode.device.dev_name));
-    } else {
-        UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &ops->super, md, worker,
-                                  params, &config->super
-                                  UCS_STATS_ARG(params->stats_root)
-                                  UCS_STATS_ARG(params->mode.device.dev_name));
-    }
+    UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &ops->super, md, worker,
+                              params, &config->super
+                              UCS_STATS_ARG((params->stats_root == NULL) ?
+                                            dev->stats : params->stats_root)
+                              UCS_STATS_ARG(params->mode.device.dev_name));
 
     status = uct_ib_device_find_port(dev, params->mode.device.dev_name, &port_num);
     if (status != UCS_OK) {
         goto err;
     }
 
-    self->ops                      = ops;
-
-    self->config.rx_payload_offset = sizeof(uct_ib_iface_recv_desc_t) +
-                                     ucs_max(sizeof(uct_recv_desc_t) +
-                                             params->rx_headroom,
-                                             rx_priv_len + rx_hdr_len);
-    self->config.rx_hdr_offset     = self->config.rx_payload_offset - rx_hdr_len;
-    self->config.rx_headroom_offset= self->config.rx_payload_offset -
-                                     params->rx_headroom;
-    self->config.seg_size          = ucs_min(mss, config->super.max_bcopy);
-    self->config.tx_max_poll       = config->tx.max_poll;
-    self->config.rx_max_poll       = config->rx.max_poll;
-    self->config.rx_max_batch      = ucs_min(config->rx.max_batch,
-                                             config->rx.queue_len / 4);
-    self->config.port_num          = port_num;
-    self->config.sl                = config->sl;
-    self->config.traffic_class     = config->traffic_class;
-    self->config.gid_index         = config->gid_index;
-    self->release_desc.cb          = uct_ib_iface_release_desc;
+    self->ops                       = ops;
+
+    self->config.rx_payload_offset  = sizeof(uct_ib_iface_recv_desc_t) +
+                                      ucs_max(sizeof(uct_recv_desc_t) +
+                                              params->rx_headroom,
+                                              init_attr->rx_priv_len +
+                                              init_attr->rx_hdr_len);
+    self->config.rx_hdr_offset      = self->config.rx_payload_offset -
+                                      init_attr->rx_hdr_len;
+    self->config.rx_headroom_offset = self->config.rx_payload_offset -
+                                      params->rx_headroom;
+    self->config.seg_size           = init_attr->seg_size;
+    self->config.tx_max_poll        = config->tx.max_poll;
+    self->config.rx_max_poll        = config->rx.max_poll;
+    self->config.rx_max_batch       = ucs_min(config->rx.max_batch,
+                                              config->rx.queue_len / 4);
+    self->config.port_num           = port_num;
+    self->config.sl                 = config->sl;
+    self->config.traffic_class      = config->traffic_class;
+    self->config.hop_limit          = config->hop_limit;
+    self->release_desc.cb           = uct_ib_iface_release_desc;
+
+    self->config.enable_res_domain  = config->enable_res_domain;
+
+    if (ucs_derived_of(worker, uct_priv_worker_t)->thread_mode == UCS_THREAD_MODE_MULTI) {
+        ucs_error("IB transports do not support multi-threaded worker");
+        return UCS_ERR_INVALID_PARAM;
+    }
 
     status = uct_ib_iface_init_pkey(self, config);
     if (status != UCS_OK) {
         goto err;
     }
 
+    status = uct_ib_device_select_gid_index(dev, self->config.port_num,
+                                            ib_md->config.gid_index,
+                                            &self->config.gid_index);
+    if (status != UCS_OK) {
+        goto err;
+    }
+
     status = uct_ib_device_query_gid(dev, self->config.port_num,
                                      self->config.gid_index, &self->gid);
     if (status != UCS_OK) {
@@ -578,11 +946,27 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
         goto err;
     }
 
+    if ((init_attr->res_domain_key == UCT_IB_IFACE_NULL_RES_DOMAIN_KEY) ||
+        !self->config.enable_res_domain) {
+        self->res_domain = NULL;
+    } else {
+        self->res_domain = uct_worker_tl_data_get(self->super.worker,
+                                                  init_attr->res_domain_key,
+                                                  uct_ib_iface_res_domain_t,
+                                                  uct_ib_iface_res_domain_cmp,
+                                                  uct_ib_iface_res_domain_init,
+                                                  self);
+        if (UCS_PTR_IS_ERR(self->res_domain)) {
+            status = UCS_PTR_STATUS(self->res_domain);
+            goto err_free_path_bits;
+        }
+    }
+
     self->comp_channel = ibv_create_comp_channel(dev->ibv_context);
     if (self->comp_channel == NULL) {
         ucs_error("ibv_create_comp_channel() failed: %m");
         status = UCS_ERR_IO_ERROR;
-        goto err_free_path_bits;
+        goto err_put_res_domain;
     }
 
     status = ucs_sys_fcntl_modfl(self->comp_channel->fd, O_NONBLOCK, 0);
@@ -591,15 +975,16 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
     }
 
     inl = config->rx.inl;
-    status = uct_ib_iface_create_cq(self, tx_cq_len, &inl, preferred_cpu,
-                                    &self->send_cq);
+    status = uct_ib_iface_create_cq(self, init_attr->tx_cq_len, &inl,
+                                    preferred_cpu, init_attr->flags,
+                                    &self->cq[UCT_IB_DIR_TX]);
     if (status != UCS_OK) {
         goto err_destroy_comp_channel;
     }
     ucs_assert_always(inl <= UINT8_MAX);
     self->config.max_inl_resp = inl;
 
-    status = uct_ib_iface_set_moderation(self->send_cq,
+    status = uct_ib_iface_set_moderation(self->cq[UCT_IB_DIR_TX],
                                          config->tx.cq_moderation_count,
                                          config->tx.cq_moderation_period);
     if (status != UCS_OK) {
@@ -607,13 +992,14 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
     }
 
     inl = config->rx.inl;
-    status = uct_ib_iface_create_cq(self, rx_cq_len, &inl,
-                                    preferred_cpu, &self->recv_cq);
+    status = uct_ib_iface_create_cq(self, init_attr->rx_cq_len, &inl,
+                                    preferred_cpu, init_attr->flags,
+                                    &self->cq[UCT_IB_DIR_RX]);
     if (status != UCS_OK) {
         goto err_destroy_send_cq;
     }
 
-    status = uct_ib_iface_set_moderation(self->recv_cq,
+    status = uct_ib_iface_set_moderation(self->cq[UCT_IB_DIR_RX],
                                          config->rx.cq_moderation_count,
                                          config->rx.cq_moderation_period);
     if (status != UCS_OK) {
@@ -621,18 +1007,17 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
     }
 
     /* Address scope and size */
-    if (config->addr_type == UCT_IB_IFACE_ADDRESS_TYPE_AUTO) {
-        if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(self))) {
-            self->addr_type = UCT_IB_ADDRESS_TYPE_ETH;
-        } else {
-            self->addr_type = uct_ib_address_scope(self->gid.global.subnet_prefix);
-        }
+    if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(self)) ||
+        config->is_global ||
+        /* check ADDR_TYPE for backward compatibility */
+        (config->addr_type == UCT_IB_ADDRESS_TYPE_SITE_LOCAL) ||
+        (config->addr_type == UCT_IB_ADDRESS_TYPE_GLOBAL)) {
+        self->is_global_addr = 1;
     } else {
-        ucs_assert(config->addr_type < UCT_IB_ADDRESS_TYPE_LAST);
-        self->addr_type = config->addr_type;
+        self->is_global_addr = 0;
     }
 
-    self->addr_size  = uct_ib_address_size(self->addr_type);
+    self->addr_size  = uct_ib_address_size(self);
 
     ucs_debug("created uct_ib_iface_t headroom_ofs %d payload_ofs %d hdr_ofs %d data_sz %d",
               self->config.rx_headroom_offset, self->config.rx_payload_offset,
@@ -641,11 +1026,15 @@ UCS_CLASS_INIT_FUNC(uct_ib_iface_t, uct_ib_iface_ops_t *ops, uct_md_h md,
     return UCS_OK;
 
 err_destroy_recv_cq:
-    ibv_destroy_cq(self->recv_cq);
+    ibv_destroy_cq(self->cq[UCT_IB_DIR_RX]);
 err_destroy_send_cq:
-    ibv_destroy_cq(self->send_cq);
+    ibv_destroy_cq(self->cq[UCT_IB_DIR_TX]);
 err_destroy_comp_channel:
     ibv_destroy_comp_channel(self->comp_channel);
+err_put_res_domain:
+    if (self->res_domain != NULL) {
+        uct_worker_tl_data_put(self->res_domain, uct_ib_iface_res_domain_cleanup);
+    }
 err_free_path_bits:
     ucs_free(self->path_bits);
 err:
@@ -656,12 +1045,12 @@ static UCS_CLASS_CLEANUP_FUNC(uct_ib_iface_t)
 {
     int ret;
 
-    ret = ibv_destroy_cq(self->recv_cq);
+    ret = ibv_destroy_cq(self->cq[UCT_IB_DIR_RX]);
     if (ret != 0) {
         ucs_warn("ibv_destroy_cq(recv_cq) returned %d: %m", ret);
     }
 
-    ret = ibv_destroy_cq(self->send_cq);
+    ret = ibv_destroy_cq(self->cq[UCT_IB_DIR_TX]);
     if (ret != 0) {
         ucs_warn("ibv_destroy_cq(send_cq) returned %d: %m", ret);
     }
@@ -671,6 +1060,9 @@ static UCS_CLASS_CLEANUP_FUNC(uct_ib_iface_t)
         ucs_warn("ibv_destroy_comp_channel(comp_channel) returned %d: %m", ret);
     }
 
+    if (self->res_domain != NULL) {
+        uct_worker_tl_data_put(self->res_domain, uct_ib_iface_res_domain_cleanup);
+    }
     ucs_free(self->path_bits);
 }
 
@@ -758,6 +1150,140 @@ static ucs_status_t uct_ib_iface_get_numa_latency(uct_ib_iface_t *iface,
     return UCS_OK;
 }
 
+#if HAVE_CUDA
+static int uct_ib_iface_cuda_sysfs_path(int cuda_dev, char** path)
+{
+    char* cuda_rpath = NULL;
+    char bus_path[]  = "/sys/class/pci_bus/0000:00/device";
+    char bus_id[16];
+    char pathname[MAXPATHSIZE];
+    int i;
+    CUresult cu_err;
+
+    /* Initialize bus_id array to avoid valgrind complaints */
+    for (i = 0; i < 16; i++) {
+        bus_id[i] = 0;
+    }
+
+    cu_err = cuDeviceGetPCIBusId(bus_id, 16, cuda_dev);
+    if (CUDA_SUCCESS != cu_err) {
+        return UCS_ERR_IO_ERROR;
+    }
+
+    for (i = 0; i < 16; i++) {
+        bus_id[i] = tolower(bus_id[i]);
+    }
+
+    memcpy(bus_path + sizeof("/sys/class/pci_bus/") - 1, bus_id,
+           sizeof("0000:00") - 1);
+    cuda_rpath = realpath(bus_path, NULL);
+    snprintf(pathname, MAXPATHSIZE, "%s/%s", cuda_rpath, bus_id);
+
+    *path = realpath(pathname, NULL);
+    if (*path == NULL) {
+        ucs_error("Could not find real path of %s", pathname);
+        free(cuda_rpath);
+        return UCS_ERR_IO_ERROR;
+    }
+
+    free(cuda_rpath);
+
+    return UCS_OK;
+}
+
+static int uct_ib_iface_ibdev_sysfs_path(const char* ib_name, char** path)
+{
+    char device_path[MAXPATHSIZE];
+
+    snprintf(device_path, MAXPATHSIZE, "/sys/class/infiniband/%s/device",
+             ib_name);
+
+    *path = realpath(device_path, NULL);
+    if (*path == NULL) {
+        return UCS_ERR_IO_ERROR;
+    }
+
+    return UCS_OK;
+}
+
+static int uct_ib_iface_pci_distance(char* cuda_path, char* ibdev_path)
+{
+    int score = 0;
+    int depth = 0;
+    int same = 1;
+    int i;
+
+    /* Compare cuda and ibdev paths to determine distance between them
+       Example for UCT_IB_PATH_PIX:
+
+     ibdev_path:
+     /sys/devices/pci0000:00/0000:00:02.0/0000:03:00.0/0000:04:04.0/0000:05:00.0
+     cuda path:
+     /sys/devices/pci0000:00/0000:00:02.0/0000:03:00.0/0000:04:08.0
+     */
+    for (i = 0; i < strlen(cuda_path); i++) {
+        if (cuda_path[i] != ibdev_path[i]) same = 0;
+        if (cuda_path[i] == '/') {
+            depth++;
+            if (same) {
+                score++;
+            }
+        }
+    }
+
+    if (3 == score) {
+        return UCT_IB_PATH_SOC;
+    }
+
+    if (4 == score) {
+        return UCT_IB_PATH_PHB;
+    }
+
+    if ((depth - 1) == score) {
+        return UCT_IB_PATH_PIX;
+    }
+
+    return UCT_IB_PATH_PXB;
+}
+
+static ucs_status_t uct_ib_iface_get_cuda_latency(uct_ib_iface_t *iface,
+                                                  double *latency)
+{
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+    int pci_distance;
+    CUdevice cuda_device;
+    CUresult cu_err;
+    char     *cuda_dev_path;
+    char     *ibdev_path;
+
+    /* Find cuda dev that the current ctx is using and find it's path*/
+    cu_err = cuCtxGetDevice(&cuda_device);
+    if (CUDA_SUCCESS != cu_err) {
+        *latency = 0.0;
+        return UCS_OK;
+    }
+    uct_ib_iface_cuda_sysfs_path(cuda_device, &cuda_dev_path);
+
+    /* Find ibdev path for given iface */
+    uct_ib_iface_ibdev_sysfs_path(uct_ib_device_name(dev), &ibdev_path);
+
+    /* Obtain pci_distance from the cuda device and ibdev device pair */
+    pci_distance = uct_ib_iface_pci_distance(cuda_dev_path, ibdev_path);
+    ucs_debug("device = %d ib = %s pci_distance = %d\n",
+              (int) cuda_device, uct_ib_device_name(dev), pci_distance);
+
+    /* Assign latency as a factor of pci_distance */
+
+    *latency = 200e-9 * pci_distance;
+
+    /* release realpath resources */
+    free(cuda_dev_path);
+    free(ibdev_path);
+
+    return UCS_OK;
+}
+#endif
+
 ucs_status_t uct_ib_iface_query(uct_ib_iface_t *iface, size_t xport_hdr_len,
                                 uct_iface_attr_t *iface_attr)
 {
@@ -766,13 +1292,17 @@ ucs_status_t uct_ib_iface_query(uct_ib_iface_t *iface, size_t xport_hdr_len,
         [0] = 1,
         [1] = 4,
         [2] = 8,
-        [3] = 12
+        [3] = 12,
+        [4] = 16
     };
     uint8_t active_width, active_speed, active_mtu;
     double encoding, signal_rate, wire_speed;
     size_t mtu, width, extra_pkt_len;
     ucs_status_t status;
     double numa_latency;
+#if HAVE_CUDA
+    double cuda_latency;
+#endif
     
     active_width = uct_ib_iface_port_attr(iface)->active_width;
     active_speed = uct_ib_iface_port_attr(iface)->active_speed;
@@ -780,7 +1310,7 @@ ucs_status_t uct_ib_iface_query(uct_ib_iface_t *iface, size_t xport_hdr_len,
 
     /* Get active width */
     if (!ucs_is_pow2(active_width) ||
-        (active_width < 1) || (ucs_ilog2(active_width) > 3))
+        (active_width < 1) || (ucs_ilog2(active_width) > 4))
     {
         ucs_error("Invalid active_width on %s:%d: %d",
                   UCT_IB_IFACE_ARG(iface), active_width);
@@ -845,6 +1375,14 @@ ucs_status_t uct_ib_iface_query(uct_ib_iface_t *iface, size_t xport_hdr_len,
         return status;
     }
 
+#if HAVE_CUDA
+    status = uct_ib_iface_get_cuda_latency(iface, &cuda_latency);
+    if (status != UCS_OK) {
+        return status;
+    }
+    iface_attr->latency.overhead += cuda_latency;
+#endif
+
     iface_attr->latency.overhead += numa_latency;
     iface_attr->latency.growth    = 0;
 
@@ -860,6 +1398,7 @@ ucs_status_t uct_ib_iface_query(uct_ib_iface_t *iface, size_t xport_hdr_len,
 
     if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface))) {
         extra_pkt_len += UCT_IB_GRH_LEN + UCT_IB_ROCE_LEN;
+        iface_attr->latency.overhead += 200e-9;
     } else {
         /* TODO check if UCT_IB_DELIM_LEN is present in RoCE as well */
         extra_pkt_len += UCT_IB_LRH_LEN;
@@ -889,10 +1428,12 @@ ucs_status_t uct_ib_iface_pre_arm(uct_ib_iface_t *iface)
     do {
         res = ibv_get_cq_event(iface->comp_channel, &cq, &cq_context);
         if (0 == res) {
-            if (iface->send_cq == cq) {
+            if (iface->cq[UCT_IB_DIR_TX] == cq) {
+                iface->ops->event_cq(iface, UCT_IB_DIR_TX);
                 ++send_cq_count;
             }
-            if (iface->recv_cq == cq) {
+            if (iface->cq[UCT_IB_DIR_RX] == cq) {
+                iface->ops->event_cq(iface, UCT_IB_DIR_RX);
                 ++recv_cq_count;
             }
         }
@@ -903,11 +1444,11 @@ ucs_status_t uct_ib_iface_pre_arm(uct_ib_iface_t *iface)
     }
 
     if (send_cq_count > 0) {
-        ibv_ack_cq_events(iface->send_cq, send_cq_count);
+        ibv_ack_cq_events(iface->cq[UCT_IB_DIR_TX], send_cq_count);
     }
 
     if (recv_cq_count > 0) {
-        ibv_ack_cq_events(iface->recv_cq, recv_cq_count);
+        ibv_ack_cq_events(iface->cq[UCT_IB_DIR_RX], recv_cq_count);
     }
 
     /* avoid re-arming the interface if any events exists */
@@ -920,26 +1461,18 @@ ucs_status_t uct_ib_iface_pre_arm(uct_ib_iface_t *iface)
     return UCS_OK;
 }
 
-static ucs_status_t uct_ib_iface_arm_cq(uct_ib_iface_t *iface, struct ibv_cq *cq,
-                                        int solicited_only)
+ucs_status_t uct_ib_iface_arm_cq(uct_ib_iface_t *iface,
+                                 uct_ib_dir_t dir,
+                                 int solicited_only)
 {
     int ret;
 
-    ret = ibv_req_notify_cq(cq, solicited_only);
+    ret = ibv_req_notify_cq(iface->cq[dir], solicited_only);
     if (ret != 0) {
-        ucs_error("ibv_req_notify_cq("UCT_IB_IFACE_FMT", cq, sol=%d) failed: %m",
-                  UCT_IB_IFACE_ARG(iface), solicited_only);
+        ucs_error("ibv_req_notify_cq("UCT_IB_IFACE_FMT", %d, sol=%d) failed: %m",
+                  UCT_IB_IFACE_ARG(iface), dir, solicited_only);
         return UCS_ERR_IO_ERROR;
     }
     return UCS_OK;
 }
 
-ucs_status_t uct_ib_iface_arm_tx_cq(uct_ib_iface_t *iface)
-{
-    return uct_ib_iface_arm_cq(iface, iface->send_cq, 0);
-}
-
-ucs_status_t uct_ib_iface_arm_rx_cq(uct_ib_iface_t *iface, int solicited_only)
-{
-    return uct_ib_iface_arm_cq(iface, iface->recv_cq, solicited_only);
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.h
index e929e7a5d..c90c435a2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_iface.h
@@ -16,12 +16,15 @@
 #include <ucs/datastruct/mpool.inl>
 #include <ucs/sys/math.h>
 
-#define UCT_IB_MAX_IOV         8UL
+#define UCT_IB_MAX_IOV                     8UL
+#define UCT_IB_IFACE_NULL_RES_DOMAIN_KEY   0u
+#define UCT_IB_MAX_ATOMIC_SIZE             sizeof(uint64_t)
+
 
 /* Forward declarations */
 typedef struct uct_ib_iface_config   uct_ib_iface_config_t;
 typedef struct uct_ib_iface_ops      uct_ib_iface_ops_t;
-typedef struct uct_ib_iface         uct_ib_iface_t;
+typedef struct uct_ib_iface          uct_ib_iface_t;
 
 
 /**
@@ -37,6 +40,39 @@ typedef enum uct_ib_mtu {
 } uct_ib_mtu_t;
 
 
+/**
+ * Traffic direction.
+ */
+typedef enum {
+    UCT_IB_DIR_RX,
+    UCT_IB_DIR_TX,
+    UCT_IB_DIR_NUM
+} uct_ib_dir_t;
+
+enum {
+    UCT_IB_QPT_UNKNOWN,
+#if HAVE_DC_EXP
+    UCT_IB_QPT_DCI = IBV_EXP_QPT_DC_INI,
+#elif HAVE_DC_DV
+    UCT_IB_QPT_DCI = IBV_QPT_DRIVER,
+#endif
+};
+
+#if HAVE_CUDA
+enum {
+    /*
+    PHB = Connection traversing PCIe as well as a PCIe Host Bridge
+    PXB = Connection traversing multiple PCIe switches
+    PIX = Connection traversing a single PCIe switch
+    */
+
+    UCT_IB_PATH_PIX = 0,
+    UCT_IB_PATH_PXB = 1,
+    UCT_IB_PATH_PHB = 2,
+    UCT_IB_PATH_SOC = 3
+};
+#endif
+
 struct uct_ib_iface_config {
     uct_iface_config_t      super;
 
@@ -70,8 +106,8 @@ struct uct_ib_iface_config {
     /* Change the address type */
     int                     addr_type;
 
-    /* IB GID index to use  */
-    unsigned                gid_index;
+    /* Forice global routing */
+    int                     is_global;
 
     /* IB SL to use */
     unsigned                sl;
@@ -79,30 +115,68 @@ struct uct_ib_iface_config {
     /* IB Traffic Class to use */
     unsigned                traffic_class;
 
+    /* IB hop limit / TTL */
+    unsigned                hop_limit;
+
     /* Ranges of path bits */
     UCS_CONFIG_ARRAY_FIELD(ucs_range_spec_t, ranges) lid_path_bits;
 
     /* IB PKEY to use */
     unsigned                pkey_value;
+
+    /* Multiple resource domains */
+    int                     enable_res_domain;
 };
 
 
+typedef struct uct_ib_qp_attr {
+    int                         qp_type;
+    struct ibv_qp_cap           cap;
+    struct ibv_srq              *srq;
+    unsigned                    sq_sig_all;
+    unsigned                    max_inl_recv;
+#if HAVE_DECL_IBV_EXP_CREATE_QP
+    struct ibv_exp_qp_init_attr ibv;
+#elif HAVE_DECL_IBV_CREATE_QP_EX
+    struct ibv_qp_init_attr_ex  ibv;
+#else
+    struct ibv_qp_init_attr     ibv;
+#endif
+} uct_ib_qp_attr_t;
+
+
 struct uct_ib_iface_ops {
     uct_iface_ops_t         super;
-    ucs_status_t            (*arm_tx_cq)(uct_ib_iface_t *iface);
-    ucs_status_t            (*arm_rx_cq)(uct_ib_iface_t *iface, int solicited_only);
+    ucs_status_t            (*arm_cq)(uct_ib_iface_t *iface,
+                                      uct_ib_dir_t dir,
+                                      int solicited_only);
+    void                    (*event_cq)(uct_ib_iface_t *iface,
+                                        uct_ib_dir_t dir);
     void                    (*handle_failure)(uct_ib_iface_t *iface, void *arg,
                                               ucs_status_t status);
     ucs_status_t            (*set_ep_failed)(uct_ib_iface_t *iface, uct_ep_h ep,
                                              ucs_status_t status);
+    ucs_status_t            (*create_qp)(uct_ib_iface_t *iface, uct_ib_qp_attr_t *attr,
+                                         struct ibv_qp **qp_p);
 };
 
 
+typedef struct uct_ib_iface_res_domain {
+    uct_worker_tl_data_t        super;
+#if HAVE_IBV_EXP_RES_DOMAIN
+    struct ibv_exp_res_domain   *ibv_domain;
+#elif HAVE_DECL_IBV_ALLOC_TD
+    struct ibv_td               *td;
+    struct ibv_pd               *pd;
+    struct ibv_pd               *ibv_domain;
+#endif
+} uct_ib_iface_res_domain_t;
+
+
 struct uct_ib_iface {
     uct_base_iface_t        super;
 
-    struct ibv_cq           *send_cq;
-    struct ibv_cq           *recv_cq;
+    struct ibv_cq           *cq[UCT_IB_DIR_NUM];
     struct ibv_comp_channel *comp_channel;
     uct_recv_desc_t         release_desc;
 
@@ -110,9 +184,10 @@ struct uct_ib_iface {
     unsigned                path_bits_count;
     uint16_t                pkey_index;
     uint16_t                pkey_value;
-    uct_ib_address_type_t   addr_type;
+    uint8_t                 is_global_addr;
     uint8_t                 addr_size;
     union ibv_gid           gid;
+    uct_ib_iface_res_domain_t *res_domain;
 
     struct {
         unsigned            rx_payload_offset;   /* offset from desc to payload */
@@ -126,17 +201,35 @@ struct uct_ib_iface {
         uint8_t             port_num;
         uint8_t             sl;
         uint8_t             traffic_class;
-        uint8_t             gid_index;
+        uint8_t             hop_limit;
+        uint8_t             gid_index;           /* IB GID index to use  */
+        int                 enable_res_domain;   /* Disable multiple resource domains */
         size_t              max_iov;             /* Maximum buffers in IOV array */
     } config;
 
     uct_ib_iface_ops_t      *ops;
+};
 
+enum {
+    UCT_IB_CQ_IGNORE_OVERRUN         = UCS_BIT(0),
 };
-UCS_CLASS_DECLARE(uct_ib_iface_t, uct_ib_iface_ops_t*, uct_md_h, uct_worker_h,
-                  const uct_iface_params_t*, unsigned, unsigned, unsigned,
-                  unsigned, size_t, const uct_ib_iface_config_t*)
 
+typedef struct uct_ib_iface_init_attr {
+
+    unsigned    rx_priv_len;     /* Length of transport private data to reserve */
+    unsigned    rx_hdr_len;      /* Length of transport network header */
+    unsigned    tx_cq_len;       /* Send CQ length */
+    unsigned    rx_cq_len;       /* Receive CQ length */
+    size_t      seg_size;        /* Transport segment size */
+    uint32_t    res_domain_key;  /* Resource domain key */
+    int         tm_cap_bit;      /* Required HW tag-matching capabilities */
+    unsigned    fc_req_size;     /* Flow control request size */
+    int         flags;           /* Various flags (see enum) */
+} uct_ib_iface_init_attr_t;
+
+UCS_CLASS_DECLARE(uct_ib_iface_t, uct_ib_iface_ops_t*, uct_md_h, uct_worker_h,
+                  const uct_iface_params_t*, const uct_ib_iface_config_t*,
+                  const uct_ib_iface_init_attr_t*);
 
 /*
  * The offset to the payload is the maximum between user-requested headroom
@@ -213,6 +306,44 @@ uct_ib_iface_invoke_am_desc(uct_ib_iface_t *iface, uint8_t am_id, void *data,
     }
 }
 
+
+/**
+ * @return IB address size of the given link scope.
+ */
+size_t uct_ib_address_size(uct_ib_iface_t *iface);
+
+
+/**
+ * Pack IB address.
+ *
+ * @param [in]  dev        IB device. TODO remove this.
+ * @param [in]  gid        GID address to pack.
+ * @param [in]  lid        LID address to pack.
+ * @param [out] ib_addr    Filled with packed ib address. Size of the structure
+ *                         must be at least what @ref uct_ib_address_size() returns
+ *                         for the given scope.
+ */
+void uct_ib_address_pack(uct_ib_iface_t *iface,
+                         const union ibv_gid *gid, uint16_t lid,
+                         uct_ib_address_t *ib_addr);
+
+
+/**
+ * Unpack IB address.
+ *
+ * @param [in]  ib_addr    IB address to unpack.
+ * @param [out] lid        Filled with address LID, or 0 if not present.
+ */
+void uct_ib_address_unpack(const uct_ib_address_t *ib_addr, uint16_t *lid,
+                           union ibv_gid *gid);
+
+
+/**
+ * Convert IB address to a human-readable string.
+ */
+const char *uct_ib_address_str(const uct_ib_address_t *ib_addr, char *buf,
+                               size_t max);
+
 ucs_status_t uct_ib_iface_get_device_address(uct_iface_h tl_iface,
                                              uct_device_addr_t *dev_addr);
 
@@ -260,9 +391,6 @@ typedef struct uct_ib_recv_wr {
 int uct_ib_iface_prepare_rx_wrs(uct_ib_iface_t *iface, ucs_mpool_t *mp,
                                 uct_ib_recv_wr_t *wrs, unsigned n);
 
-void uct_ib_iface_fill_ah_attr(uct_ib_iface_t *iface, const uct_ib_address_t *ib_addr,
-                               uint8_t src_path_bits, struct ibv_ah_attr *ah_attr);
-
 ucs_status_t uct_ib_iface_create_ah(uct_ib_iface_t *iface,
                                     struct ibv_ah_attr *ah_attr,
                                     struct ibv_ah **ah_p);
@@ -271,16 +399,22 @@ ucs_status_t uct_ib_iface_pre_arm(uct_ib_iface_t *iface);
 
 ucs_status_t uct_ib_iface_event_fd_get(uct_iface_h iface, int *fd_p);
 
-ucs_status_t uct_ib_iface_arm_tx_cq(uct_ib_iface_t *iface);
-
-ucs_status_t uct_ib_iface_arm_rx_cq(uct_ib_iface_t *iface, int solicited_only);
-
+ucs_status_t uct_ib_iface_arm_cq(uct_ib_iface_t *iface,
+                                 uct_ib_dir_t dir,
+                                 int solicited_only);
 
 static inline uint8_t uct_ib_iface_get_atomic_mr_id(uct_ib_iface_t *iface)
 {
     return uct_ib_md_get_atomic_mr_id(ucs_derived_of(iface->super.md, uct_ib_md_t));
 }
 
+ucs_status_t uct_ib_iface_create_qp(uct_ib_iface_t *iface,
+                                    uct_ib_qp_attr_t *attr,
+                                    struct ibv_qp **qp_p);
+
+void uct_ib_iface_fill_attr(uct_ib_iface_t *iface,
+                            uct_ib_qp_attr_t *attr);
+
 
 #define UCT_IB_IFACE_FMT \
     "%s:%d"
@@ -291,7 +425,7 @@ static inline uint8_t uct_ib_iface_get_atomic_mr_id(uct_ib_iface_t *iface)
 #define UCT_IB_IFACE_VERBS_COMPLETION_ERR(_type, _iface, _i,  _wc) \
     ucs_fatal("%s completion[%d] with error on %s/%p: %s, vendor_err 0x%x wr_id 0x%lx", \
               _type, _i, uct_ib_device_name(uct_ib_iface_device(_iface)), _iface, \
-              ibv_wc_status_str(_wc[i].status), _wc[i].vendor_err, \
+              uct_ib_wc_status_str(_wc[i].status), _wc[i].vendor_err, \
               _wc[i].wr_id);
 
 #define UCT_IB_IFACE_VERBS_FOREACH_RXWQE(_iface, _i, _hdr, _wc, _wc_count) \
@@ -304,6 +438,9 @@ static inline uint8_t uct_ib_iface_get_atomic_mr_id(uct_ib_iface_t *iface)
         VALGRIND_MAKE_MEM_DEFINED(_hdr, _wc[i].byte_len); \
                1; }); ++_i)
 
+#define UCT_IB_MAX_ZCOPY_LOG_SGE(_iface) \
+    (uct_ib_iface_device(_iface)->max_zcopy_log_sge)
+
 /**
  * Fill ibv_sge data structure by data provided in uct_iov_t
  * The function avoids copying IOVs with zero length
@@ -369,15 +506,15 @@ void uct_ib_iface_fill_ah_attr_from_gid_lid(uct_ib_iface_t *iface, uint16_t lid,
     ah_attr->port_num          = iface->config.port_num;
     ah_attr->grh.traffic_class = iface->config.traffic_class;
 
-    if ((gid != NULL) &&
-        ((iface->addr_type == UCT_IB_ADDRESS_TYPE_ETH)    ||
-         (iface->addr_type == UCT_IB_ADDRESS_TYPE_GLOBAL) ||
-         (iface->gid.global.subnet_prefix != gid->global.subnet_prefix))) {
-        ah_attr->is_global = 1;
+    if (iface->is_global_addr ||
+        (iface->gid.global.subnet_prefix != gid->global.subnet_prefix)) {
+        ucs_assert_always(gid->global.interface_id != 0);
+        ah_attr->is_global      = 1;
+        ah_attr->grh.dgid       = *gid;
         ah_attr->grh.sgid_index = iface->config.gid_index;
-        ah_attr->grh.dgid = *gid;
+        ah_attr->grh.hop_limit  = iface->config.hop_limit;
     } else {
-        ah_attr->is_global = 0;
+        ah_attr->is_global      = 0;
     }
 }
 
@@ -387,18 +524,26 @@ void uct_ib_iface_fill_ah_attr_from_addr(uct_ib_iface_t *iface,
                                          uint8_t path_bits,
                                          struct ibv_ah_attr *ah_attr)
 {
-    union ibv_gid *gid_p = NULL;
     union ibv_gid  gid;
-    uint8_t        is_global;
     uint16_t       lid;
 
-    uct_ib_address_unpack(ib_addr, &lid, &is_global, &gid);
+    uct_ib_address_unpack(ib_addr, &lid, &gid);
 
-    if (is_global) {
-        gid_p = &gid;
-    }
+    uct_ib_iface_fill_ah_attr_from_gid_lid(iface, lid, &gid, path_bits, ah_attr);
+}
+
+static UCS_F_ALWAYS_INLINE
+struct ibv_pd *uct_ib_iface_qp_pd(uct_ib_iface_t *iface)
+{
+    struct ibv_pd *pd;
 
-    uct_ib_iface_fill_ah_attr_from_gid_lid(iface, lid, gid_p, path_bits, ah_attr);
+    pd = uct_ib_iface_md(iface)->pd;
+#if HAVE_DECL_IBV_ALLOC_TD
+    if (iface->res_domain && iface->res_domain->ibv_domain) {
+        pd = iface->res_domain->ibv_domain;
+    }
+#endif
+    return pd;
 }
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.c
index 9bafb933c..6230e5e39 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.c
@@ -9,10 +9,10 @@
 #include <ucs/sys/sys.h>
 
 
-void uct_ib_log_dump_opcode(uint32_t qp_num, uct_ib_opcode_t *op, int signal,
-                        int fence, int se, char *buf, size_t max)
+void uct_ib_log_dump_opcode(uint32_t qp_num, unsigned idx, uct_ib_opcode_t *op,
+                            int signal, int fence, int se, char *buf, size_t max)
 {
-    snprintf(buf, max, "%s qp 0x%x %c%c%c", op->name, qp_num,
+    snprintf(buf, max, "%s qp 0x%x %03d %c%c%c", op->name, qp_num, idx,
              signal ? 's' : '-', fence ? 'f' : '-', se ? 'e' : '-');
 }
 
@@ -39,16 +39,19 @@ void uct_ib_log_dump_sg_list(uct_ib_iface_t *iface, uct_am_trace_type_t type,
                      sg_list[i].addr, sg_list[i].length, sg_list[i].lkey);
         }
 
-        len = ucs_min(sg_list[i].length, (void*)data + sizeof(data) - md);
-        memcpy(md, (void*)sg_list[i].addr, len);
-
         s               += strlen(s);
-        md              += len;
-        total_len       += len;
-        total_valid_len += sg_list[i].length;
+
+        if (data_dump) {
+            len = ucs_min(sg_list[i].length, (void*)data + sizeof(data) - md);
+            memcpy(md, (void*)sg_list[i].addr, len);
+
+            md              += len;
+            total_len       += len;
+            total_valid_len += sg_list[i].length;
+        }
     }
 
-    if (data_dump != NULL) {
+    if (data_dump) {
         data_dump(&iface->super, type, data, total_len, total_valid_len, s, ends - s);
     }
 }
@@ -83,7 +86,7 @@ void uct_ib_log_dump_atomic_masked_cswap(int argsize, uint64_t compare, uint64_t
              argsize * 8, compare, compare_mask, swap, swap_mask);
 }
 
-void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, int qp_type,
                                      uint32_t local_qp, uint32_t sender_qp,
                                      uint16_t sender_lid, void *data, size_t length,
                                      uct_log_data_dump_func_t data_dump,
@@ -109,10 +112,11 @@ void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, enum ibv_qp_type qp_
     }
 }
 
-static void uct_ib_dump_wr_opcode(struct ibv_qp *qp, uct_ib_opcode_t *op,
-                                  int send_flags, char *buf, size_t max)
+static void uct_ib_dump_wr_opcode(struct ibv_qp *qp, uint64_t wr_id,
+                                  uct_ib_opcode_t *op, int send_flags,
+                                  char *buf, size_t max)
 {
-    uct_ib_log_dump_opcode(qp->qp_num, op,
+    uct_ib_log_dump_opcode(qp->qp_num, wr_id, op,
                            send_flags & IBV_SEND_SIGNALED,
                            send_flags & IBV_SEND_FENCE,
                            send_flags & IBV_SEND_SOLICITED,
@@ -153,7 +157,7 @@ static void uct_ib_dump_wr(struct ibv_qp *qp, uct_ib_opcode_t *op,
 }
 
 static void uct_ib_dump_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
-                                struct ibv_send_wr *wr,
+                                struct ibv_send_wr *wr, int max_sge,
                                 uct_log_data_dump_func_t data_dump,
                                 char *buf, size_t max)
 {
@@ -161,40 +165,42 @@ static void uct_ib_dump_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
         [IBV_WR_RDMA_WRITE]           = { "RDMA_WRITE", UCT_IB_OPCODE_FLAG_HAS_RADDR },
         [IBV_WR_RDMA_READ]            = { "RDMA_READ",  UCT_IB_OPCODE_FLAG_HAS_RADDR },
         [IBV_WR_SEND]                 = { "SEND",       0 },
-        [IBV_WR_ATOMIC_CMP_AND_SWP]   = { "CS",         UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
-        [IBV_WR_ATOMIC_FETCH_AND_ADD] = { "FA",         UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [IBV_WR_SEND_WITH_IMM]        = { "SEND_IMM",   0 },
+        [IBV_WR_ATOMIC_CMP_AND_SWP]   = { "CSWAP",      UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [IBV_WR_ATOMIC_FETCH_AND_ADD] = { "FETCH_ADD",  UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
    };
 
     char *s             = buf;
     char *ends          = buf + max;
     uct_ib_opcode_t *op = &opcodes[wr->opcode];
 
-    uct_ib_dump_wr_opcode(qp, op, wr->send_flags, s, ends - s);
+    uct_ib_dump_wr_opcode(qp, wr->wr_id, op, wr->send_flags, s, ends - s);
     s += strlen(s);
 
     uct_ib_dump_wr(qp, op, wr, s, ends - s);
     s += strlen(s);
 
-    uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND, wr->sg_list, wr->num_sge,
+    uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND, wr->sg_list,
+                            ucs_min(wr->num_sge, max_sge),
                             (wr->send_flags & IBV_SEND_INLINE) ? -1 : 0,
                             data_dump, s, ends - s);
 }
 
 void __uct_ib_log_post_send(const char *file, int line, const char *function,
                             uct_ib_iface_t *iface, struct ibv_qp *qp,
-                            struct ibv_send_wr *wr,
+                            struct ibv_send_wr *wr, int max_sge,
                             uct_log_data_dump_func_t data_dump_cb)
 {
     char buf[256] = {0};
     while (wr != NULL) {
-        uct_ib_dump_send_wr(iface, qp, wr, data_dump_cb, buf, sizeof(buf) - 1);
+        uct_ib_dump_send_wr(iface, qp, wr, max_sge, data_dump_cb, buf, sizeof(buf) - 1);
         uct_log_data(file, line, function, buf);
         wr = wr->next;
     }
 }
 
 void __uct_ib_log_recv_completion(const char *file, int line, const char *function,
-                                  uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+                                  uct_ib_iface_t *iface, int qp_type,
                                   uint32_t l_qp, uint32_t r_qp, uint16_t slid,
                                   void *data, size_t length,
                                   uct_log_data_dump_func_t packet_dump_cb)
@@ -214,7 +220,7 @@ void __uct_ib_log_recv_completion(const char *file, int line, const char *functi
 
 #if HAVE_DECL_IBV_EXP_POST_SEND
 static void uct_ib_dump_exp_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
-                                    struct ibv_exp_send_wr *wr,
+                                    struct ibv_exp_send_wr *wr, int max_sge,
                                     uct_log_data_dump_func_t data_dump_cb,
                                     char *buf, size_t max)
 {
@@ -225,13 +231,16 @@ static void uct_ib_dump_exp_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
         [IBV_EXP_WR_RDMA_WRITE]           = { "RDMA_WRITE", UCT_IB_OPCODE_FLAG_HAS_RADDR },
         [IBV_EXP_WR_RDMA_READ]            = { "RDMA_READ",  UCT_IB_OPCODE_FLAG_HAS_RADDR },
         [IBV_EXP_WR_SEND]                 = { "SEND",       0 },
-        [IBV_EXP_WR_ATOMIC_CMP_AND_SWP]   = { "CS",         UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
-        [IBV_EXP_WR_ATOMIC_FETCH_AND_ADD] = { "FA",         UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [IBV_EXP_WR_SEND_WITH_IMM]        = { "SEND_IMM",   0 },
+        [IBV_EXP_WR_ATOMIC_CMP_AND_SWP]   = { "CSWAP",      UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [IBV_EXP_WR_ATOMIC_FETCH_AND_ADD] = { "FETCH_ADD",  UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
 #if HAVE_DECL_IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP
-        [IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP]   = { "MASKED_CS", UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
+        [IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP]   = { "MASKED_CSWAP",
+                                                         UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
 #endif
 #if HAVE_DECL_IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD
-        [IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD] = { "MASKED_FA", UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
+        [IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD] = { "MASKED_FETCH_ADD",
+                                                         UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
 #endif
    };
 
@@ -243,7 +252,7 @@ static void uct_ib_dump_exp_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
    UCS_STATIC_ASSERT((int)IBV_SEND_SIGNALED  == (int)IBV_EXP_SEND_SIGNALED);
    UCS_STATIC_ASSERT((int)IBV_SEND_FENCE     == (int)IBV_EXP_SEND_FENCE);
    UCS_STATIC_ASSERT((int)IBV_SEND_SOLICITED == (int)IBV_EXP_SEND_SOLICITED);
-   uct_ib_dump_wr_opcode(qp, op, wr->exp_send_flags, s, ends - s);
+   uct_ib_dump_wr_opcode(qp, wr->wr_id, op, wr->exp_send_flags, s, ends - s);
    s += strlen(s);
 
    /* TODO DC address handle */
@@ -283,19 +292,21 @@ static void uct_ib_dump_exp_send_wr(uct_ib_iface_t *iface, struct ibv_qp *qp,
    }
 #endif
 
-   uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND, wr->sg_list, wr->num_sge,
+   uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND, wr->sg_list,
+                           ucs_min(wr->num_sge, max_sge),
                            (wr->exp_send_flags & IBV_EXP_SEND_INLINE) ? -1 : 0,
                            data_dump_cb, s, ends - s);
 }
 
 void __uct_ib_log_exp_post_send(const char *file, int line, const char *function,
                                 uct_ib_iface_t *iface, struct ibv_qp *qp,
-                                struct ibv_exp_send_wr *wr,
+                                struct ibv_exp_send_wr *wr, int max_sge,
                                 uct_log_data_dump_func_t packet_dump_cb)
 {
     char buf[256] = {0};
     while (wr != NULL) {
-        uct_ib_dump_exp_send_wr(iface, qp, wr, packet_dump_cb, buf, sizeof(buf) - 1);
+        uct_ib_dump_exp_send_wr(iface, qp, wr, max_sge, packet_dump_cb,
+                                buf, sizeof(buf) - 1);
         uct_log_data(file, line, function, buf);
         wr = wr->next;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.h
index 3f608fbc3..fc7d75630 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_log.h
@@ -28,8 +28,8 @@ typedef struct uct_ib_opcode {
 } uct_ib_opcode_t;
 
 
-void uct_ib_log_dump_opcode(uint32_t qp_num, uct_ib_opcode_t *op, int signal,
-                            int fence, int se, char *buf, size_t max);
+void uct_ib_log_dump_opcode(uint32_t qp_num, unsigned idx, uct_ib_opcode_t *op,
+                            int signal, int fence, int se, char *buf, size_t max);
 
 void uct_ib_log_dump_sg_list(uct_ib_iface_t *iface, uct_am_trace_type_t type,
                              struct ibv_sge *sg_list, int num_sge,
@@ -51,7 +51,7 @@ void uct_ib_log_dump_atomic_masked_cswap(int argsize, uint64_t compare, uint64_t
                                          uint64_t swap, uint64_t swap_mask,
                                          char *buf, size_t max);
 
-void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, int qp_type,
                                      uint32_t local_qp, uint32_t sender_qp,
                                      uint16_t sender_lid, void *data, size_t length,
                                      uct_log_data_dump_func_t data_dump,
@@ -59,25 +59,26 @@ void uct_ib_log_dump_recv_completion(uct_ib_iface_t *iface, enum ibv_qp_type qp_
 
 void __uct_ib_log_post_send(const char *file, int line, const char *function,
                             uct_ib_iface_t *iface, struct ibv_qp *qp,
-                            struct ibv_send_wr *wr,
+                            struct ibv_send_wr *wr, int max_sge,
                             uct_log_data_dump_func_t packet_dump_cb);
 
 void __uct_ib_log_recv_completion(const char *file, int line, const char *function,
-                                  uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+                                  uct_ib_iface_t *iface, int qp_type,
                                   uint32_t l_qp, uint32_t r_qp, uint16_t slid, void *data,
                                   size_t length, uct_log_data_dump_func_t packet_dump_cb);
 
 #if HAVE_DECL_IBV_EXP_POST_SEND
 void __uct_ib_log_exp_post_send(const char *file, int line, const char *function,
                                 uct_ib_iface_t *iface, struct ibv_qp *qp,
-                                struct ibv_exp_send_wr *wr,
+                                struct ibv_exp_send_wr *wr, int max_sge,
                                 uct_log_data_dump_func_t packet_dump_cb);
 #endif
 
 
-#define uct_ib_log_post_send(_iface, _qp, _wr, _dump_cb) \
+#define uct_ib_log_post_send(_iface, _qp, _wr, _max_sge, _dump_cb) \
     if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) { \
-        __uct_ib_log_post_send(__FILE__, __LINE__, __FUNCTION__, _iface, _qp, _wr, _dump_cb); \
+        __uct_ib_log_post_send(__FILE__, __LINE__, __FUNCTION__, \
+                                _iface, _qp, _wr, _max_sge, _dump_cb); \
     }
 
 /* Suitable for both: regular and exp wcs */
@@ -88,9 +89,10 @@ void __uct_ib_log_exp_post_send(const char *file, int line, const char *function
                                      _data, _length, _dump_cb, ## __VA_ARGS__); \
     }
 
-#define uct_ib_log_exp_post_send(_iface, _qp, _wr, _dump_cb) \
+#define uct_ib_log_exp_post_send(_iface, _qp, _wr, _max_sge,_dump_cb) \
     if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) { \
-        __uct_ib_log_exp_post_send(__FILE__, __LINE__, __FUNCTION__, _iface, _qp, _wr, _dump_cb); \
+        __uct_ib_log_exp_post_send(__FILE__, __LINE__, __FUNCTION__, \
+                                   _iface, _qp, _wr, _max_sge, _dump_cb); \
     }
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.c
index 791933e12..9df15e08f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.c
@@ -10,8 +10,12 @@
 #include "ib_device.h"
 
 #include <ucs/arch/atomic.h>
-#include <ucs/debug/profile.h>
+#include <ucs/profile/profile.h>
+#include <ucs/sys/math.h>
+#include <ucs/sys/string.h>
+#include <ucm/api/ucm.h>
 #include <pthread.h>
+#include <sys/resource.h>
 
 
 #define UCT_IB_MD_PREFIX         "ib"
@@ -98,6 +102,31 @@ static ucs_config_field_t uct_ib_md_config_table[] = {
      "cause stack smashing.\n",
      ucs_offsetof(uct_ib_md_config_t, ext.enable_contig_pages), UCS_CONFIG_TYPE_BOOL},
 
+    {"INDIRECT_ATOMIC", "y",
+     "Use indirect atomic\n",
+     ucs_offsetof(uct_ib_md_config_t, ext.enable_indirect_atomic), UCS_CONFIG_TYPE_BOOL},
+
+    {"GID_INDEX", "auto",
+     "Port GID index to use.",
+     ucs_offsetof(uct_ib_md_config_t, ext.gid_index), UCS_CONFIG_TYPE_ULUNITS},
+
+    {"SUBNET_PREFIX", "",
+     "Infiniband subnet prefix to filter ports by, empty means no filter. "
+     "Relevant for IB link layer only\n"
+     "For example a filter for the default subnet prefix can be specified as: fe80:0:0:0",
+     ucs_offsetof(uct_ib_md_config_t, subnet_prefix), UCS_CONFIG_TYPE_STRING},
+
+    {"GPU_DIRECT_RDMA", "try",
+     "Use GPU Direct RDMA for HCA to access GPU pages directly\n",
+     ucs_offsetof(uct_ib_md_config_t, ext.enable_gpudirect_rdma), UCS_CONFIG_TYPE_TERNARY},
+
+#if HAVE_EXP_UMR
+    {"MAX_INLINE_KLM_LIST", "inf",
+     "When posting a UMR, KLM lists shorter or equal to this value will be posted as inline.\n"
+     "The actual maximal length is also limited by device capabilities.",
+     ucs_offsetof(uct_ib_md_config_t, ext.max_inline_klm_list), UCS_CONFIG_TYPE_UINT},
+#endif
+
     {NULL}
 };
 
@@ -125,9 +154,18 @@ static ucs_status_t uct_ib_md_query(uct_md_h uct_md, uct_md_attr_t *md_attr)
     md_attr->cap.reg_mem_types = UCS_BIT(UCT_MD_MEM_TYPE_HOST);
 
 #if HAVE_CUDA
-    /* check if GDR driver is loaded */
-    if (!access("/sys/kernel/mm/memory_peers/nv_mem/version", F_OK)) {
-        md_attr->cap.reg_mem_types |= UCS_BIT(UCT_MD_MEM_TYPE_CUDA);
+    if (md->config.enable_gpudirect_rdma != UCS_NO) {
+        /* check if GDR driver is loaded */
+        if (!access("/sys/kernel/mm/memory_peers/nv_mem/version", F_OK)) {
+            md_attr->cap.reg_mem_types |= UCS_BIT(UCT_MD_MEM_TYPE_CUDA);
+            ucs_debug("%s: GPUDirect RDMA is enabled", uct_ib_device_name(&md->dev));
+        } else if (md->config.enable_gpudirect_rdma == UCS_YES) {
+            ucs_error("%s: Couldn't enable GPUDirect RDMA. Please make sure nv_peer_mem"
+                      " plugin installed correctly.", uct_ib_device_name(&md->dev));
+            return UCS_ERR_UNSUPPORTED;
+        } else {
+            ucs_debug("%s: GPUDirect RDMA is disabled", uct_ib_device_name(&md->dev));
+        }
     }
 #endif
 
@@ -157,7 +195,8 @@ static ucs_status_t uct_ib_md_umr_qp_create(uct_ib_md_t *md)
 
     ibdev = &md->dev;
 
-    if (!(ibdev->dev_attr.exp_device_cap_flags & IBV_EXP_DEVICE_UMR)) {
+    if (!(ibdev->dev_attr.exp_device_cap_flags & IBV_EXP_DEVICE_UMR) ||
+        !md->config.enable_indirect_atomic) {
         return UCS_ERR_UNSUPPORTED;
     }
 
@@ -173,6 +212,9 @@ static ucs_status_t uct_ib_md_umr_qp_create(uct_ib_md_t *md)
         goto err;
     }
 
+    md->config.max_inline_klm_list = ucs_min(md->config.max_inline_klm_list,
+                                             ibdev->dev_attr.umr_caps.max_send_wqe_inline_klms);
+
     qp_init_attr.qp_type             = IBV_QPT_RC;
     qp_init_attr.send_cq             = md->umr_cq;
     qp_init_attr.recv_cq             = md->umr_cq;
@@ -185,11 +227,7 @@ static ucs_status_t uct_ib_md_umr_qp_create(uct_ib_md_t *md)
     qp_init_attr.pd                  = md->pd;
     qp_init_attr.comp_mask           = IBV_EXP_QP_INIT_ATTR_PD|IBV_EXP_QP_INIT_ATTR_MAX_INL_KLMS;
     qp_init_attr.max_inl_recv        = 0;
-#if (HAVE_IBV_EXP_QP_CREATE_UMR_CAPS || HAVE_EXP_UMR_NEW_API)
-    qp_init_attr.max_inl_send_klms   = ibdev->dev_attr.umr_caps.max_send_wqe_inline_klms;
-#else
-    qp_init_attr.max_inl_send_klms   = ibdev->dev_attr.max_send_wqe_inline_klms;
-#endif
+    qp_init_attr.max_inl_send_klms   = md->config.max_inline_klm_list;
 
 #if HAVE_IBV_EXP_QP_CREATE_UMR
     qp_init_attr.comp_mask          |= IBV_EXP_QP_INIT_ATTR_CREATE_FLAGS;
@@ -224,7 +262,8 @@ static ucs_status_t uct_ib_md_umr_qp_create(uct_ib_md_t *md)
     qp_attr.ah_attr.port_num         = port_num;
     qp_attr.ah_attr.dlid             = port_attr->lid;
     qp_attr.ah_attr.is_global        = 1;
-    if (uct_ib_device_query_gid(ibdev, port_num, 0, &qp_attr.ah_attr.grh.dgid) != UCS_OK) {
+    if (uct_ib_device_query_gid(ibdev, port_num, UCT_IB_MD_DEFAULT_GID_INDEX,
+                                &qp_attr.ah_attr.grh.dgid) != UCS_OK) {
         goto err_destroy_qp;
     }
     qp_attr.rq_psn                   = 0;
@@ -254,6 +293,9 @@ static ucs_status_t uct_ib_md_umr_qp_create(uct_ib_md_t *md)
         ucs_error("Failed to modify UMR QP to RTS: %m");
         goto err_destroy_qp;
     }
+
+    ucs_debug("initialized UMR QP 0x%x, max_inline_klm_list %u",
+              md->umr_qp->qp_num, md->config.max_inline_klm_list);
     return UCS_OK;
 
 err_destroy_qp:
@@ -297,6 +339,29 @@ uint8_t uct_ib_md_get_atomic_mr_id(uct_ib_md_t *md)
 #endif
 }
 
+static void uct_ib_md_print_mem_reg_err_msg(ucs_log_level_t level, void *address,
+                                            size_t length, uint64_t exp_access,
+                                            const char *exp_prefix)
+{
+    char msg[200] = {0};
+    struct rlimit limit_info;
+
+    ucs_snprintf_zero(msg, sizeof(msg),
+                      "ibv_%sreg_mr(address=%p, length=%zu, %saccess=0x%lx) failed: %m",
+                      exp_prefix, address, length, exp_prefix, exp_access);
+
+    /* Check the value of the max locked memory which is set on the system
+     * (ulimit -l) */
+    if (!getrlimit(RLIMIT_MEMLOCK, &limit_info) &&
+        (limit_info.rlim_cur != RLIM_INFINITY)) {
+        ucs_snprintf_zero(msg + strlen(msg), sizeof(msg) - strlen(msg),
+                          ". Please set max locked memory (ulimit -l) to 'unlimited' "
+                          "(current: %llu kbytes)", limit_info.rlim_cur / UCS_KBYTE);
+    }
+
+    ucs_log(level, "%s", msg);
+}
+
 static ucs_status_t uct_ib_md_reg_mr(uct_ib_md_t *md, void *address,
                                      size_t length, uint64_t exp_access,
                                      int silent, struct ibv_mr **mr_p)
@@ -316,8 +381,8 @@ static ucs_status_t uct_ib_md_reg_mr(uct_ib_md_t *md, void *address,
 
         mr = UCS_PROFILE_CALL(ibv_exp_reg_mr, &in);
         if (mr == NULL) {
-            ucs_log(level, "ibv_exp_reg_mr(address=%p, length=%zu, exp_access=0x%lx) failed: %m",
-                    in.addr, in.length, in.exp_access);
+            uct_ib_md_print_mem_reg_err_msg(level, in.addr, in.length,
+                                            in.exp_access, "exp_");
             return UCS_ERR_IO_ERROR;
         }
 #else
@@ -327,8 +392,8 @@ static ucs_status_t uct_ib_md_reg_mr(uct_ib_md_t *md, void *address,
         mr = UCS_PROFILE_CALL(ibv_reg_mr, md->pd, address, length,
                               UCT_IB_MEM_ACCESS_FLAGS);
         if (mr == NULL) {
-            ucs_log(level, "ibv_reg_mr(address=%p, length=%zu, access=0x%x) failed: %m",
-                      address, length, UCT_IB_MEM_ACCESS_FLAGS);
+            uct_ib_md_print_mem_reg_err_msg(level, address, length,
+                                            UCT_IB_MEM_ACCESS_FLAGS, "");
             return UCS_ERR_IO_ERROR;
         }
     }
@@ -341,12 +406,14 @@ static ucs_status_t uct_ib_md_post_umr(uct_ib_md_t *md, struct ibv_mr *mr,
                                        off_t offset, struct ibv_mr **umr_p)
 {
 #if HAVE_EXP_UMR
-    struct ibv_exp_mem_region mem_reg;
+    struct ibv_exp_mem_region *mem_reg = NULL;
     struct ibv_exp_send_wr wr, *bad_wr;
     struct ibv_exp_create_mr_in mrin;
     ucs_status_t status;
     struct ibv_mr *umr;
     struct ibv_wc wc;
+    int i, list_size;
+    size_t reg_length;
     int ret;
 
     if (md->umr_qp == NULL) {
@@ -354,20 +421,56 @@ static ucs_status_t uct_ib_md_post_umr(uct_ib_md_t *md, struct ibv_mr *mr,
         goto err;
     }
 
-    /* Create memory key */
+    /* Create and fill memory key */
     memset(&mrin, 0, sizeof(mrin));
-    mrin.pd                       = md->pd;
+    memset(&wr, 0, sizeof(wr));
 
-#ifdef HAVE_EXP_UMR_NEW_API
-    mrin.attr.create_flags        = IBV_EXP_MR_INDIRECT_KLMS;
-    mrin.attr.exp_access_flags    = UCT_IB_MEM_ACCESS_FLAGS;
-    mrin.attr.max_klm_list_size   = 1;
+    mrin.pd                             = md->pd;
+    wr.exp_opcode                       = IBV_EXP_WR_UMR_FILL;
+    wr.exp_send_flags                   = IBV_EXP_SEND_SIGNALED;
+    wr.ext_op.umr.exp_access            = UCT_IB_MEM_ACCESS_FLAGS;
+
+    reg_length = UCT_IB_MD_MAX_MR_SIZE;
+#ifdef HAVE_EXP_UMR_KSM
+    if ((md->dev.dev_attr.comp_mask & IBV_EXP_DEVICE_ATTR_COMP_MASK_2) &&
+        (md->dev.dev_attr.comp_mask_2 & IBV_EXP_DEVICE_ATTR_UMR_FIXED_SIZE_CAPS) &&
+        (md->dev.dev_attr.exp_device_cap_flags & IBV_EXP_DEVICE_UMR_FIXED_SIZE))
+    {
+        reg_length                      = md->dev.dev_attr.umr_fixed_size_caps.max_entity_size;
+        list_size                       = ucs_div_round_up(mr->length, reg_length);
+    } else if (mr->length < reg_length) {
+        list_size                       = 1;
+    } else {
+        status                          = UCS_ERR_UNSUPPORTED;
+        goto err;
+    }
+
+    if (list_size > 1) {
+        mrin.attr.create_flags          = IBV_EXP_MR_FIXED_BUFFER_SIZE;
+        wr.ext_op.umr.umr_type          = IBV_EXP_UMR_MR_LIST_FIXED_SIZE;
+    } else {
+        mrin.attr.create_flags          = IBV_EXP_MR_INDIRECT_KLMS;
+        wr.ext_op.umr.umr_type          = IBV_EXP_UMR_MR_LIST;
+    }
 #else
-    mrin.attr.create_flags        = IBV_MR_NONCONTIG_MEM;
-    mrin.attr.access_flags        = UCT_IB_MEM_ACCESS_FLAGS;
-    mrin.attr.max_reg_descriptors = 1;
+    if (mr->length >= reg_length) {
+        status = UCS_ERR_UNSUPPORTED;
+        goto err;
+    }
+
+    list_size                           = 1;
+    mrin.attr.create_flags              = IBV_EXP_MR_INDIRECT_KLMS;
+    wr.ext_op.umr.umr_type              = IBV_EXP_UMR_MR_LIST;
 #endif
 
+    mrin.attr.exp_access_flags          = UCT_IB_MEM_ACCESS_FLAGS;
+    mrin.attr.max_klm_list_size         = list_size;
+    mem_reg                             = ucs_calloc(list_size, sizeof(mem_reg[0]), "mem_reg");
+    if (!mem_reg) {
+        status = UCS_ERR_NO_MEMORY;
+        goto err;
+    }
+
     umr = ibv_exp_create_mr(&mrin);
     if (!umr) {
         ucs_error("Failed to create modified_mr: %m");
@@ -375,44 +478,50 @@ static ucs_status_t uct_ib_md_post_umr(uct_ib_md_t *md, struct ibv_mr *mr,
         goto err;
     }
 
-    /* Fill memory list and UMR */
-    memset(&wr, 0, sizeof(wr));
-    memset(&mem_reg, 0, sizeof(mem_reg));
-
-    mem_reg.base_addr                              = (uintptr_t) mr->addr;
-    mem_reg.length                                 = mr->length;
-
-#ifdef HAVE_EXP_UMR_NEW_API
-    mem_reg.mr                                     = mr;
-    wr.ext_op.umr.umr_type                         = IBV_EXP_UMR_MR_LIST;
-    wr.ext_op.umr.mem_list.mem_reg_list            = &mem_reg;
-    wr.ext_op.umr.exp_access                       = UCT_IB_MEM_ACCESS_FLAGS;
-    wr.ext_op.umr.modified_mr                      = umr;
-    wr.ext_op.umr.base_addr                        = (uint64_t) (uintptr_t) mr->addr + offset;
-    wr.ext_op.umr.num_mrs                          = 1;
-    ucs_trace_data("UMR_FILL qp 0x%x lkey 0x%x base 0x%lx [addr %lx len %zu lkey 0x%x]",
-                   md->umr_qp->qp_num, wr.ext_op.umr.modified_mr->lkey,
-                   wr.ext_op.umr.base_addr, mem_reg.base_addr, mem_reg.length,
-                   mem_reg.mr->lkey);
-#else
-    mem_reg.m_key                                  = mr;
-    wr.ext_op.umr.memory_key.mkey_type             = IBV_EXP_UMR_MEM_LAYOUT_NONCONTIG;
-    wr.ext_op.umr.memory_key.mem_list.mem_reg_list = &mem_reg;
-    wr.ext_op.umr.memory_key.access                = UCT_IB_MEM_ACCESS_FLAGS;
-    wr.ext_op.umr.memory_key.modified_mr           = umr;
-    wr.ext_op.umr.memory_key.region_base_addr      = mr->addr + offset;
-    wr.num_sge                                     = 1;
-#endif
+    for (i = 0; i < list_size; i++) {
+        mem_reg[i].base_addr            = (uintptr_t) mr->addr + i * reg_length;
+        mem_reg[i].length               = reg_length;
+        mem_reg[i].mr                   = mr;
+    }
+
+    ucs_assert(list_size >= 1);
+    mem_reg[list_size - 1].length       = mr->length % reg_length;
+    wr.ext_op.umr.mem_list.mem_reg_list = mem_reg;
+    wr.ext_op.umr.base_addr             = (uint64_t) (uintptr_t) mr->addr + offset;
+    wr.ext_op.umr.num_mrs               = list_size;
+    wr.ext_op.umr.modified_mr           = umr;
+
+    /* If the list exceeds max inline size, allocate a container object */
+    if (list_size > md->config.max_inline_klm_list) {
+        struct ibv_exp_mkey_list_container_attr in = {
+            .pd                = md->pd,
+            .mkey_list_type    = IBV_EXP_MKEY_LIST_TYPE_INDIRECT_MR,
+            .max_klm_list_size = list_size
+        };
+
+        wr.ext_op.umr.memory_objects = ibv_exp_alloc_mkey_list_memory(&in);
+        if (wr.ext_op.umr.memory_objects == NULL) {
+            ucs_error("ibv_exp_alloc_mkey_list_memory(list_size=%d) failed: %m",
+                      list_size);
+            status = UCS_ERR_IO_ERROR;
+            goto err_free_umr;
+        }
+    } else {
+        wr.ext_op.umr.memory_objects = NULL;
+        wr.exp_send_flags           |= IBV_EXP_SEND_INLINE;
+    }
 
-    wr.exp_opcode                                  = IBV_EXP_WR_UMR_FILL;
-    wr.exp_send_flags                              = IBV_EXP_SEND_INLINE | IBV_EXP_SEND_SIGNALED;
+    ucs_trace_data("UMR_FILL qp 0x%x lkey 0x%x base 0x%lx [addr %lx len %zu lkey 0x%x] list_size %d",
+                   md->umr_qp->qp_num, wr.ext_op.umr.modified_mr->lkey,
+                   wr.ext_op.umr.base_addr, mem_reg[0].base_addr,
+                   mem_reg[0].length, mem_reg[0].mr->lkey, list_size);
 
     /* Post UMR */
     ret = ibv_exp_post_send(md->umr_qp, &wr, &bad_wr);
     if (ret) {
         ucs_error("ibv_exp_post_send(UMR_FILL) failed: %m");
         status = UCS_ERR_IO_ERROR;
-        goto err_free_umr;
+        goto err_free_klm_container;
     }
 
     /* Wait for send UMR completion */
@@ -421,29 +530,39 @@ static ucs_status_t uct_ib_md_post_umr(uct_ib_md_t *md, struct ibv_mr *mr,
         if (ret < 0) {
             ucs_error("ibv_exp_poll_cq(umr_cq) failed: %m");
             status = UCS_ERR_IO_ERROR;
-            goto err_free_umr;
+            goto err_free_klm_container;
         }
         if (ret == 1) {
             if (wc.status != IBV_WC_SUCCESS) {
                 ucs_error("UMR_FILL completed with error: %s vendor_err %d",
                           ibv_wc_status_str(wc.status), wc.vendor_err);
                 status = UCS_ERR_IO_ERROR;
-                goto err_free_umr;
+                goto err_free_klm_container;
             }
             break;
         }
     }
 
-    ucs_trace("UMR registered memory %p..%p offset 0x%lx on %s lkey 0x%x rkey 0x%x",
+    if (wr.ext_op.umr.memory_objects != NULL) {
+        ibv_exp_dealloc_mkey_list_memory(wr.ext_op.umr.memory_objects);
+    }
+
+    ucs_debug("UMR registered memory %p..%p offset 0x%lx on %s lkey 0x%x rkey 0x%x",
               mr->addr, mr->addr + mr->length, offset, uct_ib_device_name(&md->dev),
               umr->lkey, umr->rkey);
     *umr_p = umr;
 
+    ucs_free(mem_reg);
     return UCS_OK;
 
+err_free_klm_container:
+    if (wr.ext_op.umr.memory_objects != NULL) {
+        ibv_exp_dealloc_mkey_list_memory(wr.ext_op.umr.memory_objects);
+    }
 err_free_umr:
     UCS_PROFILE_CALL(ibv_dereg_mr, umr);
 err:
+    ucs_free(mem_reg);
     return status;
 #else
     return UCS_ERR_UNSUPPORTED;
@@ -639,7 +758,7 @@ static void uct_ib_mem_init(uct_ib_mem_t *memh, unsigned uct_flags,
 
 static ucs_status_t uct_ib_mem_alloc(uct_md_h uct_md, size_t *length_p,
                                      void **address_p, unsigned flags,
-                                     uct_mem_h *memh_p UCS_MEMTRACK_ARG)
+                                     const char *alloc_name, uct_mem_h *memh_p)
 {
 #if HAVE_DECL_IBV_EXP_ACCESS_ALLOCATE_MR
     uct_ib_md_t *md = ucs_derived_of(uct_md, uct_ib_md_t);
@@ -658,7 +777,7 @@ static ucs_status_t uct_ib_mem_alloc(uct_md_h uct_md, size_t *length_p,
         goto err;
     }
 
-    length     = ucs_memtrack_adjust_alloc_size(*length_p);
+    length     = *length_p;
     exp_access = uct_ib_md_access_flags(md, flags, length) |
                  IBV_EXP_ACCESS_ALLOCATE_MR;
     status = uct_ib_md_reg_mr(md, NULL, length, exp_access, 0, &memh->mr);
@@ -678,10 +797,11 @@ static ucs_status_t uct_ib_mem_alloc(uct_md_h uct_md, size_t *length_p,
     }
 
     UCS_STATS_UPDATE_COUNTER(md->stats, UCT_IB_MD_STAT_MEM_ALLOC, +1);
+    ucs_memtrack_allocated(memh->mr->addr, memh->mr->length UCS_MEMTRACK_VAL);
+
     *address_p = memh->mr->addr;
     *length_p  = memh->mr->length;
     *memh_p    = memh;
-    ucs_memtrack_allocated(address_p, length_p UCS_MEMTRACK_VAL);
     return UCS_OK;
 
 err_free_memh:
@@ -698,7 +818,7 @@ static ucs_status_t uct_ib_mem_free(uct_md_h md, uct_mem_h memh)
     uct_ib_mem_t *ib_memh = memh;
     ucs_status_t status;
 
-    ucs_memtrack_releasing_adjusted(ib_memh->mr->addr);
+    ucs_memtrack_releasing(ib_memh->mr->addr);
 
     status = UCS_PROFILE_CALL(uct_ib_memh_dereg, memh);
     if (status != UCS_OK) {
@@ -841,8 +961,6 @@ static ucs_status_t uct_ib_rkey_unpack(uct_md_component_t *mdc,
     return UCS_OK;
 }
 
-static void uct_ib_md_close(uct_md_h md);
-
 static uct_md_ops_t uct_ib_md_ops = {
     .close             = uct_ib_md_close,
     .query             = uct_ib_md_query,
@@ -910,16 +1028,19 @@ static uct_md_ops_t uct_ib_md_rcache_ops = {
 };
 
 static ucs_status_t uct_ib_rcache_mem_reg_cb(void *context, ucs_rcache_t *rcache,
-                                             void *arg, ucs_rcache_region_t *rregion)
+                                             void *arg, ucs_rcache_region_t *rregion,
+                                             uint16_t rcache_mem_reg_flags)
 {
     uct_ib_rcache_region_t *region = ucs_derived_of(rregion, uct_ib_rcache_region_t);
     uct_ib_md_t *md = context;
-    int *flags = arg;
+    int *flags      = arg;
+    int silent      = (rcache_mem_reg_flags & UCS_RCACHE_MEM_REG_HIDE_ERRORS) ||
+                      (*flags & UCT_MD_MEM_FLAG_HIDE_ERRORS);
     ucs_status_t status;
 
     status = uct_ib_mem_reg_internal(&md->super, (void*)region->super.super.start,
                                      region->super.super.end - region->super.super.start,
-                                     *flags, 1, &region->memh);
+                                     *flags, silent, &region->memh);
     if (status != UCS_OK) {
         return status;
     }
@@ -1012,10 +1133,10 @@ static uct_md_ops_t UCS_V_UNUSED uct_ib_md_global_odp_ops = {
     .is_mem_type_owned = (void*)ucs_empty_function_return_zero,
 };
 
-static void uct_ib_make_md_name(char md_name[UCT_MD_NAME_MAX], struct ibv_device *device)
+void uct_ib_make_md_name(char md_name[UCT_MD_NAME_MAX], struct ibv_device *device)
 {
-    snprintf(md_name, UCT_MD_NAME_MAX, "%s/", UCT_IB_MD_PREFIX);
-    strncat(md_name, device->name, UCT_MD_NAME_MAX - strlen(device->name) - 1);
+    snprintf(md_name, UCT_MD_NAME_MAX, "%s/%s", UCT_IB_MD_PREFIX,
+             ibv_get_device_name(device));
 }
 
 static ucs_status_t uct_ib_query_md_resources(uct_md_resource_desc_t **resources_p,
@@ -1086,7 +1207,8 @@ static void uct_ib_md_release_device_config(uct_ib_md_t *md)
 }
 
 static ucs_status_t
-uct_ib_md_parse_reg_methods(uct_ib_md_t *md, const uct_ib_md_config_t *md_config)
+uct_ib_md_parse_reg_methods(uct_ib_md_t *md, uct_md_attr_t *md_attr,
+                            const uct_ib_md_config_t *md_config)
 {
     ucs_rcache_params_t rcache_params;
     ucs_status_t status;
@@ -1097,6 +1219,10 @@ uct_ib_md_parse_reg_methods(uct_ib_md_t *md, const uct_ib_md_config_t *md_config
             rcache_params.region_struct_size = sizeof(uct_ib_rcache_region_t);
             rcache_params.alignment          = md_config->rcache.alignment;
             rcache_params.max_alignment      = ucs_get_page_size();
+            rcache_params.ucm_events         = UCM_EVENT_VM_UNMAPPED;
+            if (md_attr->cap.reg_mem_types & ~UCS_BIT(UCT_MD_MEM_TYPE_HOST)) {
+                rcache_params.ucm_events     |= UCM_EVENT_MEM_TYPE_FREE;
+            }
             rcache_params.ucm_event_priority = md_config->rcache.event_prio;
             rcache_params.context            = md;
             rcache_params.ops                = &uct_ib_rcache_ops;
@@ -1234,6 +1360,29 @@ static void uct_ib_md_release_reg_method(uct_ib_md_t *md)
 }
 
 static ucs_status_t
+uct_ib_md_parse_subnet_prefix(const char *subnet_prefix_str,
+                              uint64_t *subnet_prefix)
+{
+    uint16_t pfx[4] = {0};
+    uint64_t pfx64 = 0;
+    int res, i;
+
+    res = sscanf(subnet_prefix_str, "%hx:%hx:%hx:%hx",
+                 &pfx[0], &pfx[1], &pfx[2], &pfx[3]);
+    if (res != 4) {
+        ucs_error("subnet filter '%s' is invalid", subnet_prefix_str);
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    for (i = 0; i < 4; i++) {
+        pfx64 = pfx[i] + (pfx64 << 16);
+    }
+
+    *subnet_prefix = htobe64(pfx64);
+    return UCS_OK;
+}
+
+ucs_status_t
 uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md_h *md_p)
 {
     const uct_ib_md_config_t *md_config = ucs_derived_of(uct_md_config, uct_ib_md_config_t);
@@ -1242,6 +1391,9 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
     ucs_status_t status;
     int i, num_devices, ret;
     uct_ib_md_t *md;
+    uct_md_attr_t md_attr;
+
+    ucs_trace("opening IB device %s", md_name);
 
     /* Get device list from driver */
     ib_device_list = ibv_get_device_list(&num_devices);
@@ -1260,6 +1412,7 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
         }
     }
     if (ib_device == NULL) {
+        ucs_debug("IB device %s not found", md_name);
         status = UCS_ERR_NO_DEVICE;
         goto out_free_dev_list;
     }
@@ -1270,6 +1423,7 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
         goto out_free_dev_list;
     }
 
+    md->super.ops             = &uct_ib_md_ops;
     md->super.component       = &uct_ib_mdc;
     md->config                = md_config->ext;
 
@@ -1313,6 +1467,17 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
         md->config.odp.max_size = uct_ib_device_odp_max_size(&md->dev);
     }
 
+    if (strlen(md_config->subnet_prefix) > 0) {
+        status = uct_ib_md_parse_subnet_prefix(md_config->subnet_prefix,
+                                               &md->subnet_filter);
+
+        if (status != UCS_OK) {
+            goto err_cleanup_device;
+        }
+
+        md->check_subnet_filter = 1;
+    }
+
     /* Allocate memory domain */
     md->pd = ibv_alloc_pd(md->dev.ibv_context);
     if (md->pd == NULL) {
@@ -1329,7 +1494,12 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
         goto err_dealloc_pd;
     }
 
-    status = uct_ib_md_parse_reg_methods(md, md_config);
+    status = uct_md_query(&md->super, &md_attr);
+    if (status != UCS_OK) {
+        goto err_destroy_umr_qp;
+    }
+
+    status = uct_ib_md_parse_reg_methods(md, &md_attr, md_config);
     if (status != UCS_OK) {
         goto err_destroy_umr_qp;
     }
@@ -1339,6 +1509,11 @@ uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md
         goto err_release_reg_method;
     }
 
+    md->dev.max_zcopy_log_sge = INT_MAX;
+    if (md_attr.cap.reg_mem_types & ~UCS_BIT(UCT_MD_MEM_TYPE_HOST)) {
+        md->dev.max_zcopy_log_sge = 1;
+    }
+
     *md_p = &md->super;
     status = UCS_OK;
 
@@ -1362,13 +1537,14 @@ err_free_md:
     goto out_free_dev_list;
 }
 
-static void uct_ib_md_close(uct_md_h uct_md)
+void uct_ib_md_close(uct_md_h uct_md)
 {
     uct_ib_md_t *md = ucs_derived_of(uct_md, uct_ib_md_t);
 
     uct_ib_md_release_device_config(md);
     uct_ib_md_release_reg_method(md);
     uct_ib_md_umr_qp_destroy(md);
+    uct_ib_device_cleanup_ah_cached(&md->dev);
     ibv_dealloc_pd(md->pd);
     uct_ib_device_cleanup(&md->dev);
     UCS_STATS_NODE_FREE(md->stats);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.h
index 2e3148b60..bc23f30d5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_md.h
@@ -16,9 +16,10 @@
 #include <ucs/sys/numa.h>
 #include <ucs/sys/rcache.h>
 
-
+#define UCT_IB_MD_MAX_MR_SIZE       0x80000000UL
 #define UCT_IB_MD_PACKED_RKEY_SIZE  sizeof(uint64_t)
 
+#define UCT_IB_MD_DEFAULT_GID_INDEX 0   /**< The gid index used by default for an IB/RoCE port */
 
 /**
  * IB MD statistics counters
@@ -47,6 +48,11 @@ typedef struct uct_ib_md_ext_config {
     int                      prefer_nearest_device; /**< Give priority for near
                                                          device */
     int                      enable_contig_pages; /** Enable contiguous pages */
+    int                      enable_indirect_atomic; /** Enable indirect atomic */
+    int                      enable_gpudirect_rdma; /** Enable GPUDirect RDMA */
+#if HAVE_EXP_UMR
+    unsigned                 max_inline_klm_list; /* Maximal length of inline KLM list */
+#endif
 
     struct {
         ucs_numa_policy_t    numa_policy;  /**< NUMA policy flags for ODP */
@@ -55,6 +61,7 @@ typedef struct uct_ib_md_ext_config {
         size_t               max_size;     /**< Maximal memory region size for ODP */
     } odp;
 
+    size_t                   gid_index;    /**< IB GID index to use  */
 } uct_ib_md_ext_config_t;
 
 
@@ -84,6 +91,8 @@ typedef struct uct_ib_md {
         uct_ib_device_spec_t *specs;    /* Custom device specifications */
         unsigned             count;     /* Number of custom devices */
     } custom_devices;
+    int                      check_subnet_filter;
+    uint64_t                 subnet_filter;
 } uct_ib_md_t;
 
 
@@ -106,6 +115,7 @@ typedef struct uct_ib_md_config {
 
     UCS_CONFIG_STRING_ARRAY_FIELD(spec) custom_devices; /**< Custom device specifications */
 
+    char                     *subnet_prefix; /**< Filter of subnet_prefix for IB ports */
 } uct_ib_md_config_t;
 
 
@@ -172,4 +182,12 @@ static inline uint16_t uct_ib_md_atomic_offset(uint8_t atomic_mr_id)
     return 8 * atomic_mr_id;
 }
 
+
+void uct_ib_make_md_name(char md_name[UCT_MD_NAME_MAX], struct ibv_device *device);
+
+ucs_status_t
+uct_ib_md_open(const char *md_name, const uct_md_config_t *uct_md_config, uct_md_h *md_p);
+
+void uct_ib_md_close(uct_md_h uct_md);
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_verbs.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_verbs.h
index 449f3db71..b56d18033 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_verbs.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/base/ib_verbs.h
@@ -60,7 +60,14 @@
 
 #else
 #  define IBV_SHARED_MR_ACCESS_FLAGS(_shared_mr)    ((_shared_mr)->access)
+#if HAVE_DECL_IBV_EXP_DEVICE_ATTR_RESERVED_2
+#  define IBV_EXP_DEVICE_ATTR_SET_COMP_MASK(_attr)  do { \
+            (_attr)->comp_mask = 0xffffffff; \
+            (_attr)->comp_mask_2 = (IBV_EXP_DEVICE_ATTR_RESERVED_2 - 1); \
+        } while (0)
+#else
 #  define IBV_EXP_DEVICE_ATTR_SET_COMP_MASK(_attr)  (_attr)->comp_mask = (IBV_EXP_DEVICE_ATTR_RESERVED - 1)
+#endif /* HAVE_DECL_IBV_EXP_DEVICE_ATTR_RESERVED_2 */
 #  define IBV_EXP_PORT_ATTR_SET_COMP_MASK(_attr)    (_attr)->comp_mask = 0
 #endif /* HAVE_VERBS_EXP_H */
 
@@ -97,12 +104,7 @@
 /*
  * DC support
  */
-#if HAVE_DECL_IBV_EXP_DEVICE_DC_TRANSPORT && HAVE_STRUCT_IBV_EXP_DEVICE_ATTR_EXP_DEVICE_CAP_FLAGS
-#  define IBV_DEVICE_HAS_DC(_attr)                  ((_attr)->exp_device_cap_flags & IBV_EXP_DEVICE_DC_TRANSPORT)
-#else
-#  define IBV_DEVICE_HAS_DC(_attr)                  0
-#endif /* HAVE_DECL_IBV_EXP_DEVICE_DC_TRANSPORT */
-
+#define IBV_DEVICE_HAS_DC(dev)                      (dev->flags & UCT_IB_DEVICE_FLAG_DC)
 
 /*
  * NOP support
@@ -142,11 +144,13 @@
 #if HAVE_DECL_IBV_EXP_CQ_IGNORE_OVERRUN
 static inline int ibv_exp_cq_ignore_overrun(struct ibv_cq *cq)
 {
-    struct ibv_exp_cq_attr cq_attr = {0};
+    struct ibv_exp_cq_attr cq_attr = {};
     cq_attr.comp_mask    = IBV_EXP_CQ_ATTR_CQ_CAP_FLAGS;
     cq_attr.cq_cap_flags = IBV_EXP_CQ_IGNORE_OVERRUN;
     return ibv_exp_modify_cq(cq, &cq_attr, IBV_EXP_CQ_CAP_FLAGS);
 }
+#elif HAVE_DECL_IBV_CREATE_CQ_ATTR_IGNORE_OVERRUN
+static inline int ibv_exp_cq_ignore_overrun(struct ibv_cq *cq) { return 0; }
 #else
 static inline int ibv_exp_cq_ignore_overrun(struct ibv_cq *cq)
 {
@@ -204,8 +208,6 @@ static inline int ibv_exp_cq_ignore_overrun(struct ibv_cq *cq)
 #  define ibv_exp_create_srq_attr           ibv_srq_init_attr
 #endif
 
-
-
 typedef uint8_t uct_ib_uint24_t[3];
 
 static inline void uct_ib_pack_uint24(uct_ib_uint24_t buf, const uint32_t qp_num)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm.h
index bfd88555a..6783ac60b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm.h
@@ -50,6 +50,7 @@ typedef struct uct_cm_iface {
     struct ib_cm_id          *listen_id;       /* Listening "socket" */
     ucs_queue_head_t          notify_q;        /* Notification queue */
     uint32_t                  num_outstanding; /* Number of outstanding sends */
+    uint32_t                  num_completions; /* Number of completed sends */
     ucs_queue_head_t          outstanding_q;   /* Outstanding operations queue */
     uct_worker_cb_id_t        slow_prog_id;    /* Callback id for slowpath progress */
 
@@ -67,7 +68,6 @@ typedef struct uct_cm_iface {
 typedef struct uct_cm_ep {
     uct_base_ep_t          super;
     uint16_t               dlid;
-    uint8_t                is_global;
     uint32_t               dest_service_id;
     union ibv_gid          dgid;
 } uct_cm_ep_t;
@@ -104,7 +104,8 @@ ucs_status_t uct_cm_iface_flush_do(uct_cm_iface_t *iface, uct_completion_t *comp
 ssize_t uct_cm_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id, uct_pack_callback_t pack_cb,
                            void *arg, unsigned flags);
 
-ucs_status_t uct_cm_ep_pending_add(uct_ep_h ep, uct_pending_req_t *req);
+ucs_status_t uct_cm_ep_pending_add(uct_ep_h ep, uct_pending_req_t *req,
+                                   unsigned flags);
 void uct_cm_ep_pending_purge(uct_ep_h ep, uct_pending_purge_callback_t cb,
                              void *arg);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_ep.c
index 16a389fbf..63fb7c577 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_ep.c
@@ -30,7 +30,7 @@ static UCS_CLASS_INIT_FUNC(uct_cm_ep_t, uct_iface_t *tl_iface,
     UCS_CLASS_CALL_SUPER_INIT(uct_base_ep_t, &iface->super.super);
 
     uct_ib_address_unpack((const uct_ib_address_t*)dev_addr, &self->dlid,
-                          &self->is_global, &self->dgid);
+                          &self->dgid);
     self->dest_service_id = *(const uint32_t*)iface_addr;
     return UCS_OK;
 }
@@ -54,9 +54,10 @@ static ucs_status_t uct_cm_ep_fill_path_rec(uct_cm_ep_t *ep,
     path->sgid                      = iface->super.gid;
     path->dlid                      = htons(ep->dlid);
     path->slid                      = htons(uct_ib_iface_port_attr(&iface->super)->lid);
-    if (ep->is_global) {
+    if (iface->super.is_global_addr) {
+        ucs_assert_always(ep->dgid.global.interface_id != 0);
         path->dgid                  = ep->dgid;
-        path->hop_limit             = 64;
+        path->hop_limit             = iface->super.config.hop_limit;
     } else {
         memset(&path->dgid, 0, sizeof(path->dgid));
         path->hop_limit             = 0;
@@ -198,7 +199,8 @@ err:
     return status;
 }
 
-ucs_status_t uct_cm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req)
+ucs_status_t uct_cm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req,
+                                   unsigned flags)
 {
     uct_cm_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_cm_iface_t);
     uct_cm_ep_t *ep = ucs_derived_of(tl_ep, uct_cm_ep_t);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_iface.c
index ad2a914a7..3d1b81c91 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/cm/cm_iface.c
@@ -56,6 +56,10 @@ static unsigned uct_cm_iface_progress(void *arg)
         ++count;
     }
 
+    /* we are in the progress() context. Now it is safe to release resources. */
+    iface->num_outstanding -= iface->num_completions;
+    iface->num_completions  = 0;
+
     /* Dispatch pending operations */
     uct_pending_queue_dispatch(priv, &iface->notify_q,
                                iface->num_outstanding < iface->config.max_outstanding);
@@ -154,7 +158,19 @@ static void uct_cm_iface_outstanding_remove(uct_cm_iface_t* iface,
     ucs_queue_for_each_safe(op, iter, &iface->outstanding_q, queue) {
         if (op->is_id && (op->id == id)) {
             ucs_queue_del_iter(&iface->outstanding_q, iter);
-            --iface->num_outstanding;
+            /* Must not release resources from the async context
+             * because it will break pending op ordering.
+             * For example bcopy() may succeed while there are queued
+             * pending ops:
+             * bcopy() -> no resources
+             * pending_add() -> ok
+             * <-- async event: resources available
+             * bcopy() --> ok. oops this is out of order send
+             *
+             * save the number and do actual release in the
+             * progress() context.
+             */
+            ++iface->num_completions;
             ucs_free(op);
             return;
         }
@@ -255,17 +271,20 @@ static UCS_CLASS_INIT_FUNC(uct_cm_iface_t, uct_md_h md, uct_worker_h worker,
                            const uct_iface_config_t *tl_config)
 {
     uct_cm_iface_config_t *config = ucs_derived_of(tl_config, uct_cm_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     ucs_status_t status;
     int ret;
 
     ucs_trace_func("");
 
+    init_attr.tx_cq_len      = 1;
+    init_attr.rx_cq_len      = config->super.rx.queue_len;
+    init_attr.seg_size       = ucs_min(IB_CM_SIDR_REQ_PRIVATE_DATA_SIZE,
+                                       config->super.super.max_bcopy);
+    init_attr.res_domain_key = UCT_IB_IFACE_NULL_RES_DOMAIN_KEY;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_ib_iface_t, &uct_cm_iface_ops, md, worker,
-                              params, 0 /* rx_priv_len */, 0 /* rx_hdr_len */,
-                              1 /* tx_cq_len */,
-                              config->super.rx.queue_len /* rx_cq_len */,
-                              IB_CM_SIDR_REQ_PRIVATE_DATA_SIZE, /* mss */
-                              &config->super);
+                              params, &config->super, &init_attr);
 
     if (self->super.super.worker->async == NULL) {
         ucs_error("cm must have async!=NULL");
@@ -273,6 +292,7 @@ static UCS_CLASS_INIT_FUNC(uct_cm_iface_t, uct_md_h md, uct_worker_h worker,
     }
 
     self->num_outstanding     = 0;
+    self->num_completions     = 0;
     self->service_id          = 0;
     self->config.timeout_ms   = (int)(config->timeout * 1e3 + 0.5);
     self->config.max_outstanding = config->max_outstanding;
@@ -432,8 +452,7 @@ static uct_ib_iface_ops_t uct_cm_iface_ops = {
     .iface_get_address        = uct_cm_iface_get_address,
     .iface_is_reachable       = uct_ib_iface_is_reachable
     },
-    .arm_tx_cq                = (void*)ucs_empty_function_return_success,
-    .arm_rx_cq                = (void*)ucs_empty_function_return_success
+    .arm_cq                   = (void*)ucs_empty_function_return_success,
 };
 
 static ucs_status_t uct_cm_query_resources(uct_md_h md,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.c
index 0b25d4feb..d763b9730 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.c
@@ -26,6 +26,10 @@ static ucs_config_field_t uct_dc_mlx5_iface_config_table[] = {
    ucs_offsetof(uct_dc_mlx5_iface_config_t, ud_common),
    UCS_CONFIG_TYPE_TABLE(uct_ud_mlx5_iface_common_config_table)},
 
+  {"", "", NULL,
+   ucs_offsetof(uct_dc_mlx5_iface_config_t, mlx5_common),
+   UCS_CONFIG_TYPE_TABLE(uct_ib_mlx5_iface_config_table)},
+
   {NULL}
 };
 
@@ -35,16 +39,30 @@ static UCS_CLASS_CLEANUP_FUNC(uct_dc_mlx5_ep_t)
     ucs_trace_func("");
 }
 
+static void uct_dc_mlx5_iface_set_av_sport(uct_dc_mlx5_iface_t *iface,
+                                           uct_ib_mlx5_base_av_t *av,
+                                           uint32_t remote_dctn)
+{
+    uct_ib_mlx5_iface_set_av_sport(&iface->super.super.super, av,
+                                   remote_dctn ^ uct_dc_get_dct_num(&iface->super));
+}
+
 static UCS_CLASS_INIT_FUNC(uct_dc_mlx5_ep_t, uct_dc_mlx5_iface_t *dc_iface,
                            const uct_dc_iface_addr_t *if_addr,
                            const uct_ib_mlx5_base_av_t *av)
 {
+    uint32_t remote_dctn;
+
     ucs_trace_func("");
 
     UCS_CLASS_CALL_SUPER_INIT(uct_dc_ep_t, &dc_iface->super, if_addr);
 
+    remote_dctn = uct_ib_unpack_uint24(if_addr->qp_num);
+
     memcpy(&self->av, av, sizeof(*av));
-    self->av.dqp_dct |= htonl(uct_ib_unpack_uint24(if_addr->qp_num));
+    self->av.dqp_dct |= htonl(remote_dctn);
+    uct_dc_mlx5_iface_set_av_sport(dc_iface, &self->av, remote_dctn);
+
     return UCS_OK;
 }
 
@@ -116,18 +134,30 @@ static void uct_dc_mlx5_ep_destroy(uct_ep_h tl_ep)
 static ucs_status_t uct_dc_mlx5_iface_query(uct_iface_h tl_iface, uct_iface_attr_t *iface_attr)
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_iface, uct_dc_mlx5_iface_t);
+    size_t max_am_inline       = UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE);
+    size_t max_put_inline      = UCT_IB_MLX5_PUT_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE);
     ucs_status_t status;
 
+#if HAVE_IBV_EXP_DM
+    if (iface->mlx5_common.dm.dm != NULL) {
+        max_am_inline  = ucs_max(iface->mlx5_common.dm.dm->seg_len,
+                                 UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE));
+        max_put_inline = ucs_max(iface->mlx5_common.dm.dm->seg_len,
+                                 UCT_IB_MLX5_PUT_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE));
+    }
+#endif
+
     status = uct_dc_iface_query(&iface->super, iface_attr,
-                                UCT_IB_MLX5_PUT_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE),
-                                UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE),
+                                max_put_inline,
+                                max_am_inline,
                                 UCT_IB_MLX5_AM_ZCOPY_MAX_HDR(UCT_IB_MLX5_AV_FULL_SIZE),
-                                UCT_IB_MLX5_AM_ZCOPY_MAX_IOV);
+                                UCT_IB_MLX5_AM_ZCOPY_MAX_IOV,
+                                UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(UCT_IB_MLX5_AV_FULL_SIZE));
     if (status != UCS_OK) {
         return status;
     }
 
-    uct_rc_mlx5_iface_common_query(iface_attr);
+    uct_rc_mlx5_iface_common_query(&iface->super.super.super, iface_attr);
     return UCS_OK;
 }
 
@@ -148,19 +178,21 @@ uct_dc_mlx5_iface_bcopy_post(uct_dc_mlx5_iface_t *iface, uct_dc_mlx5_ep_t *ep,
                             unsigned opcode, unsigned length,
                             /* RDMA */ uint64_t rdma_raddr, uct_rkey_t rdma_rkey,
                             uct_rc_iface_send_desc_t *desc, uint8_t send_flags,
-                            uint32_t imm_val_be)
+                            uint32_t imm_val_be, const void *buffer,
+                            uct_ib_log_sge_t *log_sge)
 {
     UCT_DC_MLX5_TXQP_DECL(txqp, txwq);
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
     desc->super.sn = txwq->sw_pi;
-    uct_rc_mlx5_txqp_dptr_post(&iface->super.super, IBV_EXP_QPT_DC_INI, txqp, txwq,
-                               opcode, desc + 1, length, &desc->lkey,
+    uct_rc_mlx5_txqp_dptr_post(&iface->super.super, UCT_IB_QPT_DCI, txqp, txwq,
+                               opcode, buffer, length, &desc->lkey,
                                rdma_raddr, uct_ib_md_direct_rkey(rdma_rkey),
-                               0, 0, 0,
+                               0, 0, 0, 0,
                                &ep->av, uct_dc_mlx5_ep_get_grh(ep),
                                uct_ib_mlx5_wqe_av_size(&ep->av),
-                               MLX5_WQE_CTRL_CQ_UPDATE | send_flags, imm_val_be);
+                               MLX5_WQE_CTRL_CQ_UPDATE | send_flags, imm_val_be, INT_MAX,
+                               log_sge);
     uct_rc_txqp_add_send_op(txqp, &desc->super);
 }
 
@@ -179,14 +211,15 @@ uct_dc_mlx5_iface_zcopy_post(uct_dc_mlx5_iface_t *iface, uct_dc_mlx5_ep_t *ep,
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
     sn = txwq->sw_pi;
-    uct_rc_mlx5_txqp_dptr_post_iov(&iface->super.super, IBV_EXP_QPT_DC_INI, txqp,
+    uct_rc_mlx5_txqp_dptr_post_iov(&iface->super.super, UCT_IB_QPT_DCI, txqp,
                                    txwq, opcode, iov, iovcnt,
                                    am_id, am_hdr, am_hdr_len,
                                    rdma_raddr, uct_ib_md_direct_rkey(rdma_rkey),
                                    tag, app_ctx, ib_imm_be,
                                    &ep->av, uct_dc_mlx5_ep_get_grh(ep),
                                    uct_ib_mlx5_wqe_av_size(&ep->av),
-                                   MLX5_WQE_CTRL_CQ_UPDATE | send_flags);
+                                   MLX5_WQE_CTRL_CQ_UPDATE | send_flags,
+                                   UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
 
     uct_rc_txqp_add_send_comp(&iface->super.super, txqp, comp, sn);
 }
@@ -195,7 +228,8 @@ static UCS_F_ALWAYS_INLINE void
 uct_dc_mlx5_iface_atomic_post(uct_dc_mlx5_iface_t *iface, uct_dc_mlx5_ep_t *ep,
                               unsigned opcode, uct_rc_iface_send_desc_t *desc, unsigned length,
                               uint64_t remote_addr, uct_rkey_t rkey,
-                              uint64_t compare_mask, uint64_t compare, uint64_t swap_add)
+                              uint64_t compare_mask, uint64_t compare,
+                              uint64_t swap_mask, uint64_t swap_add)
 {
     uint32_t ib_rkey = uct_ib_resolve_atomic_rkey(rkey, ep->super.atomic_mr_offset,
                                                   &remote_addr);
@@ -204,138 +238,147 @@ uct_dc_mlx5_iface_atomic_post(uct_dc_mlx5_iface_t *iface, uct_dc_mlx5_ep_t *ep,
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
     desc->super.sn = txwq->sw_pi;
-    uct_rc_mlx5_txqp_dptr_post(&iface->super.super, IBV_EXP_QPT_DC_INI, txqp, txwq,
+    uct_rc_mlx5_txqp_dptr_post(&iface->super.super, UCT_IB_QPT_DCI, txqp, txwq,
                                opcode, desc + 1, length, &desc->lkey,
                                remote_addr, ib_rkey,
-                               compare_mask, compare, swap_add,
+                               compare_mask, compare, swap_mask, swap_add,
                                &ep->av, uct_dc_mlx5_ep_get_grh(ep),
                                uct_ib_mlx5_wqe_av_size(&ep->av),
-                               MLX5_WQE_CTRL_CQ_UPDATE, 0);
+                               MLX5_WQE_CTRL_CQ_UPDATE, 0, INT_MAX, NULL);
 
     UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
     uct_rc_txqp_add_send_op(txqp, &desc->super);
 }
 
-static inline void uct_dc_mlx5_iface_add_send_comp(uct_dc_mlx5_iface_t *iface,
-                                                   uct_dc_mlx5_ep_t *ep,
-                                                   uct_completion_t *comp)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_atomic_op_post(uct_ep_h tl_ep, unsigned opcode, unsigned size,
+                              uint64_t value, uint64_t remote_addr, uct_rkey_t rkey)
 {
-    UCT_DC_MLX5_TXQP_DECL(txqp, txwq);
-
-    UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
-    uct_rc_txqp_add_send_comp(&iface->super.super, txqp, comp, txwq->sig_pi);
-}
-
-ucs_status_t uct_dc_mlx5_ep_atomic_add(uct_ep_h tl_ep,
-                                         int opcode, unsigned length,
-                                         uint64_t add, uint64_t remote_addr, uct_rkey_t rkey)
-{
-
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
-    uct_dc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    uct_dc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
     uct_rc_iface_send_desc_t *desc;
+    int op;
+    uint64_t compare_mask;
+    uint64_t compare;
+    uint64_t swap_mask;
+    uint64_t swap;
+    int      ext; /* not used here */
+    ucs_status_t status;
 
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super.super, &iface->mlx5_common.tx.atomic_desc_mp, desc);
-    uct_dc_mlx5_iface_atomic_post(iface, ep, opcode, desc, length,
-                                  remote_addr, rkey, 0, 0, add);
+    UCT_RC_MLX5_CHECK_ATOMIC_OPS(opcode, size, UCT_RC_MLX5_ATOMIC_OPS);
+
+    status = uct_rc_mlx5_iface_common_atomic_data(opcode, size, value, &op, &compare_mask,
+                                                  &compare, &swap_mask, &swap, &ext);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
+
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->mlx5_common.tx.atomic_desc_mp, desc);
+    uct_dc_mlx5_iface_atomic_post(iface, ep, op, desc, size, remote_addr, rkey,
+                                  compare_mask, compare, swap_mask, swap);
     return UCS_OK;
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
-uct_dc_mlx5_ep_atomic(uct_dc_mlx5_ep_t *ep, int opcode, void *result, int ext,
-                      unsigned length, uint64_t remote_addr, uct_rkey_t rkey,
-                      uint64_t compare_mask, uint64_t compare,
-                      uint64_t swap_add, uct_completion_t *comp)
+uct_dc_mlx5_ep_atomic_fop(uct_dc_mlx5_ep_t *ep, int opcode, void *result, int ext,
+                          unsigned length, uint64_t remote_addr, uct_rkey_t rkey,
+                          uint64_t compare_mask, uint64_t compare,
+                          uint64_t swap_mask, uint64_t swap_add, uct_completion_t *comp)
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface, uct_dc_mlx5_iface_t);
     uct_rc_iface_send_desc_t *desc;
 
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->mlx5_common.tx.atomic_desc_mp, desc,
-                                    uct_rc_iface_atomic_handler(&iface->super.super, ext, length),
-                                    result, comp);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super.super, &iface->mlx5_common.tx.atomic_desc_mp,
+                                          desc, uct_rc_iface_atomic_handler(&iface->super.super,
+                                                                            ext, length),
+                                          result, comp);
     uct_dc_mlx5_iface_atomic_post(iface, ep, opcode, desc, length, remote_addr, rkey,
-                                  compare_mask, compare, swap_add);
+                                  compare_mask, compare, swap_mask, swap_add);
     return UCS_INPROGRESS;
 }
 
-ucs_status_t uct_dc_mlx5_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_atomic_fop_post(uct_ep_h tl_ep, unsigned opcode, unsigned size,
+                               uint64_t value, void *result,
+                               uint64_t remote_addr, uct_rkey_t rkey,
+                               uct_completion_t *comp)
 {
-    return uct_dc_mlx5_ep_atomic_add(tl_ep, MLX5_OPCODE_ATOMIC_FA, sizeof(uint64_t),
-                                     htobe64(add), remote_addr, rkey);
-}
+    uct_dc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    int op;
+    uint64_t compare_mask;
+    uint64_t compare;
+    uint64_t swap_mask;
+    uint64_t swap;
+    int      ext;
+    ucs_status_t status;
 
-ucs_status_t uct_dc_mlx5_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp)
-{
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_FA, result, 0, sizeof(uint64_t),
-                                 remote_addr, rkey, 0, 0, htobe64(add), comp);
-}
+    UCT_RC_MLX5_CHECK_ATOMIC_OPS(opcode, size, UCT_RC_MLX5_ATOMIC_FOPS);
 
-ucs_status_t uct_dc_mlx5_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp)
-{
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint64_t), remote_addr, rkey, 0, 0,
-                                 htobe64(swap), comp);
+    status = uct_rc_mlx5_iface_common_atomic_data(opcode, size, value, &op, &compare_mask,
+                                                  &compare, &swap_mask, &swap, &ext);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
+
+    return uct_dc_mlx5_ep_atomic_fop(ep, op, result, ext, size, remote_addr, rkey,
+                                     compare_mask, compare, swap_mask, swap, comp);
 }
 
 ucs_status_t uct_dc_mlx5_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
                                            uint64_t remote_addr, uct_rkey_t rkey,
                                            uint64_t *result, uct_completion_t *comp)
 {
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_CS, result, 0, sizeof(uint64_t),
-                                 remote_addr, rkey, 0, htobe64(compare), htobe64(swap),
-                                 comp);
+    return uct_dc_mlx5_ep_atomic_fop(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
+                                     MLX5_OPCODE_ATOMIC_CS, result, 0, sizeof(uint64_t),
+                                     remote_addr, rkey, 0, htobe64(compare), -1,
+                                     htobe64(swap), comp);
+}
+
+ucs_status_t uct_dc_mlx5_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uint32_t *result, uct_completion_t *comp)
+{
+    return uct_dc_mlx5_ep_atomic_fop(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
+                                     MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
+                                     sizeof(uint32_t), remote_addr, rkey, UCS_MASK(32),
+                                     htonl(compare), -1, htonl(swap), comp);
 }
 
-ucs_status_t uct_dc_mlx5_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_dc_mlx5_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey)
 {
-    return uct_dc_mlx5_ep_atomic_add(tl_ep, MLX5_OPCODE_ATOMIC_MASKED_FA,
-                                     sizeof(uint32_t), htonl(add), remote_addr,
-                                     rkey);
+    return uct_dc_mlx5_ep_atomic_op_post(ep, opcode, sizeof(value), value, remote_addr, rkey);
 }
 
-ucs_status_t uct_dc_mlx5_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_dc_mlx5_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey)
 {
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_FA, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, 0, 0,
-                                 htonl(add), comp);
+    return uct_dc_mlx5_ep_atomic_op_post(ep, opcode, sizeof(value), value, remote_addr, rkey);
 }
 
-ucs_status_t uct_dc_mlx5_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_dc_mlx5_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint64_t value, uint64_t *result,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uct_completion_t *comp)
 {
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, 0, 0,
-                                 htonl(swap), comp);
+    return uct_dc_mlx5_ep_atomic_fop_post(ep, opcode, sizeof(value), value, result,
+                                          remote_addr, rkey, comp);
 }
 
-ucs_status_t uct_dc_mlx5_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+ucs_status_t uct_dc_mlx5_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint32_t value, uint32_t *result,
                                            uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp)
+                                           uct_completion_t *comp)
 {
-    return uct_dc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, UCS_MASK(32),
-                                 htonl(compare), htonl(swap), comp);
+    return uct_dc_mlx5_ep_atomic_fop_post(ep, opcode, sizeof(value), value, result,
+                                          remote_addr, rkey, comp);
 }
 
-ucs_status_t uct_dc_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
-                                     const void *buffer, unsigned length)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_am_short_inline(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
+                               const void *buffer, unsigned length)
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
     uct_dc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
@@ -346,20 +389,86 @@ ucs_status_t uct_dc_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
-    uct_rc_mlx5_txqp_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+    uct_rc_mlx5_txqp_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                  txqp, txwq,
                                  MLX5_OPCODE_SEND,
                                  buffer, length, id, hdr, 0,
                                  0, 0,
                                  &ep->av, uct_dc_mlx5_ep_get_grh(ep),
                                  uct_ib_mlx5_wqe_av_size(&ep->av),
-                                 MLX5_WQE_CTRL_SOLICITED);
+                                 MLX5_WQE_CTRL_SOLICITED, INT_MAX);
 
     UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(hdr) + length);
     return UCS_OK;
 }
 
+#if HAVE_IBV_EXP_DM
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_short_dm(uct_dc_mlx5_ep_t *ep, uct_rc_mlx5_dm_copy_data_t *cache,
+                        size_t hdr_len, const void *payload, unsigned length,
+                        unsigned opcode, uint8_t fm_ce_se,
+                        uint64_t rdma_raddr, uct_rkey_t rdma_rkey)
+{
+    uct_dc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface, uct_dc_mlx5_iface_t);
+    uct_rc_iface_send_desc_t *desc;
+    void *buffer;
+    ucs_status_t status;
+    uct_ib_log_sge_t log_sge;
+
+    status = uct_rc_mlx5_common_dm_make_data(&iface->mlx5_common, &iface->super.super,
+                                             cache, hdr_len, payload, length, &desc,
+                                             &buffer, &log_sge);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
+
+    uct_dc_mlx5_iface_bcopy_post(iface, ep, opcode,
+                                 hdr_len + length,
+                                 rdma_raddr, rdma_rkey,
+                                 desc, fm_ce_se, 0, buffer,
+                                 log_sge.num_sge ? &log_sge : NULL);
+    return UCS_OK;
+}
+#endif
+
+ucs_status_t uct_dc_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
+                                     const void *buffer, unsigned length)
+{
+#if HAVE_IBV_EXP_DM
+    uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
+    uct_dc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    ucs_status_t status;
+    uct_rc_mlx5_dm_copy_data_t cache;
+
+    if (ucs_likely((sizeof(uct_rc_am_short_hdr_t) + length <=
+                    UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_dc_mlx5_ep_am_short_inline(tl_ep, id, hdr, buffer, length);
+#if HAVE_IBV_EXP_DM
+    }
+
+    UCT_CHECK_LENGTH(length + sizeof(uct_rc_am_short_hdr_t), 0,
+                     iface->mlx5_common.dm.seg_len, "am_short");
+    UCT_DC_CHECK_RES_AND_FC(&iface->super, &ep->super);
+
+    uct_rc_am_hdr_fill(&cache.am_hdr.rc_hdr, id);
+    cache.am_hdr.am_hdr = hdr;
+
+    status = uct_dc_mlx5_ep_short_dm(ep, &cache, sizeof(cache.am_hdr), buffer, length,
+                                     MLX5_OPCODE_SEND,
+                                     MLX5_WQE_CTRL_SOLICITED | MLX5_WQE_CTRL_CQ_UPDATE,
+                                     0, 0);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+    UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(cache.am_hdr) + length);
+    UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
+    return UCS_OK;
+#endif
+}
+
 ssize_t uct_dc_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
                                 uct_pack_callback_t pack_cb, void *arg,
                                 unsigned flags)
@@ -375,7 +484,7 @@ ssize_t uct_dc_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
 
     uct_dc_mlx5_iface_bcopy_post(iface, ep, MLX5_OPCODE_SEND,
                                  sizeof(uct_rc_hdr_t) + length, 0, 0, desc,
-                                 MLX5_WQE_CTRL_SOLICITED, 0);
+                                 MLX5_WQE_CTRL_SOLICITED, 0, desc + 1, NULL);
 
     UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, BCOPY, length);
@@ -410,9 +519,10 @@ ucs_status_t uct_dc_mlx5_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *hea
 }
 
 
-ucs_status_t uct_dc_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer,
-                                      unsigned length, uint64_t remote_addr,
-                                      uct_rkey_t rkey)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_put_short_inline(uct_ep_h tl_ep, const void *buffer,
+                                unsigned length, uint64_t remote_addr,
+                                uct_rkey_t rkey)
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
     uct_dc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
@@ -422,19 +532,49 @@ ucs_status_t uct_dc_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer,
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
-    uct_rc_mlx5_txqp_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+    uct_rc_mlx5_txqp_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                  txqp, txwq,
                                  MLX5_OPCODE_RDMA_WRITE,
                                  buffer, length, 0, 0, 0,
                                  remote_addr, uct_ib_md_direct_rkey(rkey),
                                  &ep->av, uct_dc_mlx5_ep_get_grh(ep),
-                                 uct_ib_mlx5_wqe_av_size(&ep->av), 0);
+                                 uct_ib_mlx5_wqe_av_size(&ep->av), 0, INT_MAX);
 
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
 
     return UCS_OK;
 }
 
+ucs_status_t uct_dc_mlx5_ep_put_short(uct_ep_h tl_ep, const void *payload,
+                                      unsigned length, uint64_t remote_addr,
+                                      uct_rkey_t rkey)
+{
+#if HAVE_IBV_EXP_DM
+    uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
+    uct_dc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    ucs_status_t status;
+
+    if (ucs_likely((length <= UCT_IB_MLX5_PUT_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_dc_mlx5_ep_put_short_inline(tl_ep, payload, length, remote_addr, rkey);
+#if HAVE_IBV_EXP_DM
+    }
+
+    UCT_CHECK_LENGTH(length, 0, iface->mlx5_common.dm.seg_len, "put_short");
+    UCT_DC_CHECK_RES(&iface->super, &ep->super);
+    status = uct_dc_mlx5_ep_short_dm(ep, NULL, 0, payload, length,
+                                     MLX5_OPCODE_RDMA_WRITE,
+                                     MLX5_WQE_CTRL_CQ_UPDATE,
+                                     remote_addr, rkey);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+    UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
+    return UCS_OK;
+#endif
+}
+
 ssize_t uct_dc_mlx5_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
                                  void *arg, uint64_t remote_addr, uct_rkey_t rkey)
 {
@@ -447,7 +587,7 @@ ssize_t uct_dc_mlx5_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
     UCT_RC_IFACE_GET_TX_PUT_BCOPY_DESC(&iface->super.super, &iface->super.super.tx.mp,
                                        desc, pack_cb, arg, length);
     uct_dc_mlx5_iface_bcopy_post(iface, ep, MLX5_OPCODE_RDMA_WRITE, length,
-                                 remote_addr, rkey, desc, 0, 0);
+                                 remote_addr, rkey, desc, 0, 0, desc + 1, NULL);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, BCOPY, length);
     return length;
 }
@@ -489,7 +629,7 @@ ucs_status_t uct_dc_mlx5_ep_get_bcopy(uct_ep_h tl_ep,
     UCT_RC_IFACE_GET_TX_GET_BCOPY_DESC(&iface->super.super, &iface->super.super.tx.mp,
                                        desc, unpack_cb, comp, arg, length);
     uct_dc_mlx5_iface_bcopy_post(iface, ep, MLX5_OPCODE_RDMA_READ, length,
-                                 remote_addr, rkey, desc, 0, 0);
+                                 remote_addr, rkey, desc, 0, 0, desc + 1, NULL);
     UCT_TL_EP_STAT_OP(&ep->super.super, GET, BCOPY, length);
     return UCS_INPROGRESS;
 }
@@ -521,8 +661,9 @@ ucs_status_t uct_dc_mlx5_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completion
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface,
                                                 uct_dc_mlx5_iface_t);
-    uct_dc_mlx5_ep_t    *ep    = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    uct_dc_ep_t         *ep    = ucs_derived_of(tl_ep, uct_dc_ep_t);
     ucs_status_t        status;
+    UCT_DC_MLX5_TXQP_DECL(txqp, txwq);
 
     status = uct_dc_ep_flush(tl_ep, flags, comp);
     if (status == UCS_OK) {
@@ -530,8 +671,10 @@ ucs_status_t uct_dc_mlx5_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completion
     }
 
     if (status == UCS_INPROGRESS) {
-        ucs_assert(ep->super.dci != UCT_DC_EP_NO_DCI);
-        uct_dc_mlx5_iface_add_send_comp(iface, ep, comp);
+        ucs_assert(ep->dci != UCT_DC_EP_NO_DCI);
+        UCT_DC_MLX5_IFACE_TXQP_GET(iface, ep, txqp, txwq);
+        status = uct_rc_txqp_add_flush_comp(&iface->super.super, txqp, comp,
+                                            txwq->sig_pi);
     }
     return status;
 }
@@ -542,10 +685,11 @@ uct_dc_mlx5_poll_tx(uct_dc_mlx5_iface_t *iface)
     uint8_t dci;
     struct mlx5_cqe64 *cqe;
     uint32_t qp_num;
-    uint16_t hw_ci, bb_num;
+    uint16_t hw_ci;
     UCT_DC_MLX5_TXQP_DECL(txqp, txwq);
 
-    cqe = uct_ib_mlx5_poll_cq(&iface->super.super.super, &iface->mlx5_common.tx.cq);
+    cqe = uct_ib_mlx5_poll_cq(&iface->super.super.super,
+                              &iface->mlx5_common.cq[UCT_IB_DIR_TX]);
     if (cqe == NULL) {
         return 0;
     }
@@ -562,10 +706,7 @@ uct_dc_mlx5_poll_tx(uct_dc_mlx5_iface_t *iface)
     ucs_trace_poll("dc_mlx5 iface %p tx_cqe: dci[%d] qpn 0x%x txqp %p hw_ci %d",
                    iface, dci, qp_num, txqp, hw_ci);
 
-    bb_num = uct_ib_mlx5_txwq_update_bb(txwq, hw_ci) - uct_rc_txqp_available(txqp);
-    uct_rc_txqp_available_add(txqp, bb_num);
-    iface->super.super.tx.cq_available += bb_num;
-
+    uct_rc_mlx5_common_update_tx_res(&iface->super.super, txwq, txqp, hw_ci);
     uct_dc_iface_dci_put(&iface->super, dci);
     uct_rc_mlx5_txqp_process_tx_cqe(txqp, cqe, hw_ci);
 
@@ -588,8 +729,9 @@ static unsigned uct_dc_mlx5_iface_progress(void *arg)
 
 
 #if IBV_EXP_HW_TM_DC
-ucs_status_t uct_dc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
-                                            const void *data, size_t length)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_dc_mlx5_ep_tag_eager_short_inline(uct_ep_h tl_ep, uct_tag_t tag,
+                                      const void *data, size_t length)
 {
     uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
     uct_dc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
@@ -602,16 +744,53 @@ ucs_status_t uct_dc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
-    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                      txqp, txwq, MLX5_OPCODE_SEND, data, length,
                                      NULL, tag, 0, IBV_EXP_TMH_EAGER, 0,
                                      &ep->av, uct_dc_mlx5_ep_get_grh(ep),
                                      uct_ib_mlx5_wqe_av_size(&ep->av), NULL, 0,
                                      MLX5_WQE_CTRL_SOLICITED);
 
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
+
     return UCS_OK;
 }
 
+ucs_status_t uct_dc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
+                                            const void *data, size_t length)
+{
+#if HAVE_IBV_EXP_DM
+    uct_dc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_mlx5_iface_t);
+    uct_dc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
+    uct_rc_mlx5_dm_copy_data_t cache;
+    ucs_status_t status;
+
+    if (ucs_likely((sizeof(struct ibv_exp_tmh) + length <=
+                    UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_dc_mlx5_ep_tag_eager_short_inline(tl_ep, tag, data, length);
+#if HAVE_IBV_EXP_DM
+    }
+
+    UCT_CHECK_LENGTH(length + sizeof(struct ibv_exp_tmh), 0,
+                     iface->mlx5_common.dm.seg_len, "tag_short");
+    UCT_DC_CHECK_RES(&iface->super, &ep->super);
+
+    uct_rc_mlx5_fill_tmh(ucs_unaligned_ptr(&cache.tm_hdr), tag, 0, IBV_EXP_TMH_EAGER);
+
+    status = uct_dc_mlx5_ep_short_dm(ep, &cache, sizeof(cache.tm_hdr), data,
+                                     length, MLX5_OPCODE_SEND,
+                                     MLX5_WQE_CTRL_SOLICITED | MLX5_WQE_CTRL_CQ_UPDATE,
+                                     0, 0);
+    if (!UCS_STATUS_IS_ERR(status)) {
+        UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
+    }
+
+    return status;
+#endif
+}
+
 ssize_t uct_dc_mlx5_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                        uint64_t imm,
                                        uct_pack_callback_t pack_cb,
@@ -628,12 +807,15 @@ ssize_t uct_dc_mlx5_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
 
     UCT_RC_IFACE_FILL_TM_IMM(imm, app_ctx, ib_imm, opcode, MLX5_OPCODE_SEND, _IMM);
 
-    UCT_RC_IFACE_GET_TM_BCOPY_DESC(&iface->super.super, &iface->super.super.tx.mp,
-                                   desc, tag, app_ctx, pack_cb, arg, length);
+    UCT_RC_MLX5_IFACE_GET_TM_BCOPY_DESC(&iface->super.super,
+                                        &iface->super.super.tx.mp, desc, tag,
+                                        app_ctx, pack_cb, arg, length);
 
     uct_dc_mlx5_iface_bcopy_post(iface, ep, opcode,
                                  sizeof(struct ibv_exp_tmh) + length,
-                                 0, 0, desc, MLX5_WQE_CTRL_SOLICITED, ib_imm);
+                                 0, 0, desc, MLX5_WQE_CTRL_SOLICITED, ib_imm, desc + 1, NULL);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, BCOPY, length);
 
     return length;
 }
@@ -648,7 +830,8 @@ ucs_status_t uct_dc_mlx5_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     uint32_t app_ctx, ib_imm;
     int opcode;
 
-    UCT_CHECK_IOV_SIZE(iovcnt, 1ul, "uct_dc_mlx5_ep_tag_eager_zcopy");
+    UCT_CHECK_IOV_SIZE(iovcnt, UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(UCT_IB_MLX5_AV_FULL_SIZE),
+                       "uct_dc_mlx5_ep_tag_eager_zcopy");
     UCT_RC_CHECK_ZCOPY_DATA(sizeof(struct ibv_exp_tmh),
                             uct_iov_total_length(iov, iovcnt),
                             iface->super.super.super.config.seg_size);
@@ -660,6 +843,9 @@ ucs_status_t uct_dc_mlx5_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                  iov, iovcnt, 0, "", 0, 0, 0, tag, app_ctx,
                                  ib_imm, comp, MLX5_WQE_CTRL_SOLICITED);
 
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
+
     return UCS_INPROGRESS;
 }
 
@@ -687,11 +873,11 @@ ucs_status_ptr_t uct_dc_mlx5_ep_tag_rndv_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
 
     op_index = uct_rc_iface_tag_get_op_id(&iface->super.super, comp);
 
-    uct_dc_iface_fill_ravh(&ravh, iface->super.rx.dct->dct_num);
+    uct_dc_iface_fill_ravh(&ravh, uct_dc_get_dct_num(&iface->super));
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
-    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                      txqp, txwq, MLX5_OPCODE_SEND, header,
                                      header_length, iov, tag, op_index,
                                      IBV_EXP_TMH_RNDV, 0, &ep->av,
@@ -718,7 +904,7 @@ ucs_status_t uct_dc_mlx5_ep_tag_rndv_request(uct_ep_h tl_ep, uct_tag_t tag,
 
     UCT_DC_MLX5_IFACE_TXQP_GET(iface, &ep->super, txqp, txwq);
 
-    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+    uct_rc_mlx5_txqp_tag_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                      txqp, txwq, MLX5_OPCODE_SEND_IMM, header,
                                      header_length, NULL, tag, 0,
                                      IBV_EXP_TMH_EAGER, 0, &ep->av,
@@ -766,6 +952,13 @@ static unsigned uct_dc_mlx5_iface_progress_tm(void *arg)
 }
 #endif
 
+static void uct_dc_mlx5_iface_progress_enable(uct_iface_h tl_iface, unsigned flags)
+{
+    uct_rc_iface_t *iface = ucs_derived_of(tl_iface, uct_rc_iface_t);
+
+    uct_base_iface_progress_enable_cb(&iface->super.super, iface->progress, flags);
+}
+
 static void uct_dc_mlx5_iface_handle_failure(uct_ib_iface_t *ib_iface,
                                              void *arg, ucs_status_t status)
 {
@@ -778,7 +971,7 @@ static void uct_dc_mlx5_iface_handle_failure(uct_ib_iface_t *ib_iface,
         log_lvl = ib_iface->super.config.failure_level;
     }
 
-    uct_ib_mlx5_completion_with_err(arg, log_lvl);
+    uct_ib_mlx5_completion_with_err(ib_iface, arg, log_lvl);
 }
 
 static ucs_status_t uct_dc_mlx5_ep_set_failed(uct_ib_iface_t *ib_iface,
@@ -823,7 +1016,7 @@ ucs_status_t uct_dc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
         /* TODO: look at common code with uct_ud_mlx5_iface_get_av */
         if (dc_req->sender.global.is_global) {
             uct_ib_iface_fill_ah_attr_from_gid_lid(ib_iface, dc_req->lid,
-                                                   (void*)&dc_req->sender.global.gid,
+                                                   ucs_unaligned_ptr(&dc_req->sender.global.gid),
                                                    ib_iface->path_bits[0], &ah_attr);
 
             status = uct_ib_iface_create_ah(ib_iface, &ah_attr, &ah);
@@ -832,8 +1025,6 @@ ucs_status_t uct_dc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
             }
 
             uct_ib_mlx5_get_av(ah, &mlx5_av);
-
-            ibv_destroy_ah(ah);
         }
 
         /* Note av initialization is copied from exp verbs */
@@ -841,19 +1032,21 @@ ucs_status_t uct_dc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
         av.fl_mlid      = ib_iface->path_bits[0] & 0x7f;
 
         /* lid in dc_req is in BE already  */
-        av.rlid         = dc_req->lid | htons(ib_iface->path_bits[0]);
+        av.rlid         = IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(ib_iface)) ?
+                          0 : (dc_req->lid | htons(ib_iface->path_bits[0]));
         av.dqp_dct      = htonl(dc_req->dct_num);
+        uct_dc_mlx5_iface_set_av_sport(iface, &av, dc_req->dct_num);
 
         if (!iface->ud_common.config.compact_av || ah_attr.is_global) {
             av.dqp_dct |= UCT_IB_MLX5_EXTENDED_UD_AV;
         }
 
-        uct_rc_mlx5_txqp_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+        uct_rc_mlx5_txqp_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                      txqp, txwq, MLX5_OPCODE_SEND,
                                      &av /*dummy*/, 0, op, sender_ep, 0,
                                      0, 0,
                                      &av, ah_attr.is_global ? mlx5_av_grh(&mlx5_av) : NULL,
-                                     uct_ib_mlx5_wqe_av_size(&av), 0);
+                                     uct_ib_mlx5_wqe_av_size(&av), 0, INT_MAX);
     } else {
         ucs_assert(op == UCT_RC_EP_FC_FLAG_HARD_REQ);
         dc_mlx5_ep              = ucs_derived_of(tl_ep, uct_dc_mlx5_ep_t);
@@ -864,21 +1057,20 @@ ucs_status_t uct_dc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
         UCS_STATS_UPDATE_COUNTER(dc_ep->fc.stats,
                                  UCT_RC_FC_STAT_TX_HARD_REQ, 1);
 
-        uct_rc_mlx5_txqp_inline_post(&iface->super.super, IBV_EXP_QPT_DC_INI,
+        uct_rc_mlx5_txqp_inline_post(&iface->super.super, UCT_IB_QPT_DCI,
                                      txqp, txwq, MLX5_OPCODE_SEND_IMM,
                                      &sender.global, sizeof(sender.global), op, sender.ep,
-                                     iface->super.rx.dct->dct_num,
+                                     uct_dc_get_dct_num(&iface->super),
                                      0, 0,
                                      &dc_mlx5_ep->av,
                                      uct_dc_mlx5_ep_get_grh(dc_mlx5_ep),
                                      uct_ib_mlx5_wqe_av_size(&dc_mlx5_ep->av),
-                                     MLX5_WQE_CTRL_SOLICITED);
+                                     MLX5_WQE_CTRL_SOLICITED, INT_MAX);
     }
 
     return UCS_OK;
 }
 
-
 static void UCS_CLASS_DELETE_FUNC_NAME(uct_dc_mlx5_iface_t)(uct_iface_t*);
 
 static ucs_status_t uct_dc_mlx5_iface_reset_dci(uct_dc_iface_t *dc_iface, int dci)
@@ -893,16 +1085,199 @@ static ucs_status_t uct_dc_mlx5_iface_reset_dci(uct_dc_iface_t *dc_iface, int dc
      */
     uct_rc_mlx5_iface_common_update_cqs_ci(&iface->mlx5_common,
                                            &iface->super.super.super);
-    status = uct_rc_modify_qp(&iface->super.tx.dcis[dci].txqp, IBV_QPS_RESET);
+    status = uct_ib_modify_qp(iface->super.tx.dcis[dci].txqp.qp, IBV_QPS_RESET);
     uct_rc_mlx5_iface_common_sync_cqs_ci(&iface->mlx5_common,
                                          &iface->super.super.super);
 
+    uct_rc_mlx5_iface_commom_clean(&iface->mlx5_common.cq[UCT_IB_DIR_TX], NULL,
+                                   iface->super.tx.dcis[dci].txqp.qp->qp_num);
+
     /* Resume posting from to the beginning of the QP */
     uct_ib_mlx5_txwq_reset(&iface->dci_wqs[dci]);
 
     return status;
 }
 
+static void uct_dc_mlx5_iface_event_cq(uct_ib_iface_t *ib_iface,
+                                       uct_ib_dir_t dir)
+{
+    uct_dc_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_dc_mlx5_iface_t);
+
+    iface->mlx5_common.cq[dir].cq_sn++;
+}
+
+#if HAVE_DC_DV
+static ucs_status_t uct_dc_mlx5_iface_create_qp(uct_ib_iface_t *iface,
+                                                uct_ib_qp_attr_t *attr,
+                                                struct ibv_qp **qp_p)
+{
+    uct_ib_device_t *dev               = uct_ib_iface_device(iface);
+    struct mlx5dv_qp_init_attr dv_attr = {};
+    struct ibv_qp *qp;
+
+    uct_ib_iface_fill_attr(iface, attr);
+    attr->ibv.cap.max_recv_sge          = 0;
+
+    dv_attr.comp_mask                   = MLX5DV_QP_INIT_ATTR_MASK_DC;
+    dv_attr.dc_init_attr.dc_type        = MLX5DV_DCTYPE_DCI;
+    dv_attr.dc_init_attr.dct_access_key = UCT_IB_KEY;
+    qp = mlx5dv_create_qp(dev->ibv_context, &attr->ibv, &dv_attr);
+    if (qp == NULL) {
+        ucs_error("iface=%p: failed to create DCI: %m", iface);
+        return UCS_ERR_IO_ERROR;
+    }
+
+    attr->cap = attr->ibv.cap;
+    *qp_p     = qp;
+
+    return UCS_OK;
+}
+
+ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface,
+                                      uct_rc_txqp_t *dci)
+{
+    struct ibv_qp_attr attr;
+    long attr_mask;
+
+    memset(&attr, 0, sizeof(attr));
+    attr.qp_state        = IBV_QPS_INIT;
+    attr.pkey_index      = 0;
+    attr.qp_access_flags = 0;
+    attr.port_num        = iface->super.super.config.port_num;
+
+    if (ibv_modify_qp(dci->qp, &attr,
+                      IBV_QP_STATE        |
+                      IBV_QP_PKEY_INDEX   |
+                      IBV_QP_PORT)) {
+        ucs_error("error modifying QP to INIT : %m");
+        return UCS_ERR_IO_ERROR;
+    }
+
+    /* Move QP to the RTR state */
+    memset(&attr, 0, sizeof(attr));
+    attr.qp_state                   = IBV_QPS_RTR;
+    attr.path_mtu                   = iface->super.config.path_mtu;
+    attr.min_rnr_timer              = iface->super.config.min_rnr_timer;
+    attr.max_dest_rd_atomic         = 1;
+    attr.ah_attr.is_global          = iface->super.super.is_global_addr;
+    attr.ah_attr.sl                 = iface->super.super.config.sl;
+    attr_mask                       = IBV_QP_STATE     |
+                                      IBV_QP_PATH_MTU;
+
+    if (ibv_modify_qp(dci->qp, &attr, attr_mask)) {
+        ucs_error("error modifying DCI QP to RTR: %m");
+        return UCS_ERR_IO_ERROR;
+    }
+
+    /* Move QP to the RTS state */
+    memset(&attr, 0, sizeof(attr));
+    attr.qp_state       = IBV_QPS_RTS;
+    attr.timeout        = iface->super.config.timeout;
+    attr.rnr_retry      = iface->super.config.rnr_retry;
+    attr.retry_cnt      = iface->super.config.retry_cnt;
+    attr.max_rd_atomic  = iface->super.config.max_rd_atomic;
+    attr_mask           = IBV_QP_STATE      |
+                          IBV_QP_SQ_PSN     |
+                          IBV_QP_TIMEOUT    |
+                          IBV_QP_RETRY_CNT  |
+                          IBV_QP_RNR_RETRY  |
+                          IBV_QP_MAX_QP_RD_ATOMIC;
+
+    if (ibv_modify_qp(dci->qp, &attr, attr_mask)) {
+        ucs_error("error modifying DCI QP to RTS: %m");
+        return UCS_ERR_IO_ERROR;
+    }
+
+    return UCS_OK;
+}
+
+ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface)
+{
+    uct_dc_mlx5_iface_t *mlx5 = ucs_derived_of(iface, uct_dc_mlx5_iface_t);
+    uct_ib_device_t *dev = uct_ib_iface_device(&iface->super.super);
+    struct mlx5dv_qp_init_attr dv_init_attr = {};
+    struct ibv_qp_init_attr_ex init_attr = {};
+    struct ibv_qp_attr attr = {};
+    int ret;
+
+    init_attr.comp_mask             = IBV_QP_INIT_ATTR_PD;
+    init_attr.pd                    = uct_ib_iface_md(&iface->super.super)->pd;
+    init_attr.recv_cq               = iface->super.super.cq[UCT_IB_DIR_RX];
+    /* DCT can't send, but send_cq have to point to valid CQ */
+    init_attr.send_cq               = iface->super.super.cq[UCT_IB_DIR_RX];
+    init_attr.srq                   = iface->super.rx.srq.srq;
+    init_attr.qp_type               = IBV_QPT_DRIVER;
+    init_attr.cap.max_inline_data   = iface->super.config.rx_inline;
+
+    dv_init_attr.comp_mask                   = MLX5DV_QP_INIT_ATTR_MASK_DC;
+    dv_init_attr.dc_init_attr.dc_type        = MLX5DV_DCTYPE_DCT;
+    dv_init_attr.dc_init_attr.dct_access_key = UCT_IB_KEY;
+
+    mlx5->rx_dct = mlx5dv_create_qp(dev->ibv_context,
+                                    &init_attr, &dv_init_attr);
+    if (mlx5->rx_dct == NULL) {
+        ucs_error("Failed to created DC target %m");
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    attr.qp_state        = IBV_QPS_INIT;
+    attr.port_num        = iface->super.super.config.port_num;
+    attr.qp_access_flags = IBV_ACCESS_REMOTE_WRITE |
+                           IBV_ACCESS_REMOTE_READ  |
+                           IBV_ACCESS_REMOTE_ATOMIC;
+
+    ret = ibv_modify_qp(mlx5->rx_dct, &attr, IBV_QP_STATE |
+                                             IBV_QP_PKEY_INDEX |
+                                             IBV_QP_PORT |
+                                             IBV_QP_ACCESS_FLAGS);
+
+    if (ret) {
+         ucs_error("error modifying DCT to INIT: %m");
+         goto err;
+    }
+
+    attr.qp_state                  = IBV_QPS_RTR;
+    attr.path_mtu                  = iface->super.config.path_mtu;
+    attr.min_rnr_timer             = iface->super.config.min_rnr_timer;
+    attr.ah_attr.grh.hop_limit     = iface->super.super.config.hop_limit;
+    attr.ah_attr.grh.traffic_class = iface->super.super.config.traffic_class;
+    attr.ah_attr.grh.sgid_index    = uct_ib_iface_md(&iface->super.super)->config.gid_index;
+    attr.ah_attr.port_num          = iface->super.super.config.port_num;
+
+    ret = ibv_modify_qp(mlx5->rx_dct, &attr, IBV_QP_STATE |
+                                             IBV_QP_MIN_RNR_TIMER |
+                                             IBV_QP_AV |
+                                             IBV_QP_PATH_MTU);
+    if (ret) {
+         ucs_error("error modifying DCT to RTR: %m");
+         goto err;
+    }
+
+    return UCS_OK;
+
+err:
+    ibv_destroy_qp(mlx5->rx_dct);
+    return UCS_ERR_IO_ERROR;
+}
+
+int uct_dc_get_dct_num(uct_dc_iface_t *iface)
+{
+    uct_dc_mlx5_iface_t *mlx5 = ucs_derived_of(iface, uct_dc_mlx5_iface_t);
+
+    return mlx5->rx_dct->qp_num;
+}
+
+void uct_dc_destroy_dct(uct_dc_iface_t *iface)
+{
+    uct_dc_mlx5_iface_t *mlx5 = ucs_derived_of(iface, uct_dc_mlx5_iface_t);
+
+    if (mlx5->rx_dct != NULL) {
+        ibv_destroy_qp(mlx5->rx_dct);
+    }
+    mlx5->rx_dct = NULL;
+}
+#endif
+
 static uct_dc_iface_ops_t uct_dc_mlx5_iface_ops = {
     {
     {
@@ -915,14 +1290,12 @@ static uct_dc_iface_ops_t uct_dc_mlx5_iface_ops = {
     .ep_am_short              = uct_dc_mlx5_ep_am_short,
     .ep_am_bcopy              = uct_dc_mlx5_ep_am_bcopy,
     .ep_am_zcopy              = uct_dc_mlx5_ep_am_zcopy,
-    .ep_atomic_add64          = uct_dc_mlx5_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_dc_mlx5_ep_atomic_fadd64,
-    .ep_atomic_swap64         = uct_dc_mlx5_ep_atomic_swap64,
     .ep_atomic_cswap64        = uct_dc_mlx5_ep_atomic_cswap64,
-    .ep_atomic_add32          = uct_dc_mlx5_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_dc_mlx5_ep_atomic_fadd32,
-    .ep_atomic_swap32         = uct_dc_mlx5_ep_atomic_swap32,
     .ep_atomic_cswap32        = uct_dc_mlx5_ep_atomic_cswap32,
+    .ep_atomic64_post         = uct_dc_mlx5_ep_atomic64_post,
+    .ep_atomic32_post         = uct_dc_mlx5_ep_atomic32_post,
+    .ep_atomic64_fetch        = uct_dc_mlx5_ep_atomic64_fetch,
+    .ep_atomic32_fetch        = uct_dc_mlx5_ep_atomic32_fetch,
     .ep_pending_add           = uct_dc_ep_pending_add,
     .ep_pending_purge         = uct_dc_ep_pending_purge,
     .ep_flush                 = uct_dc_mlx5_ep_flush,
@@ -939,7 +1312,7 @@ static uct_dc_iface_ops_t uct_dc_mlx5_iface_ops = {
 #endif
     .iface_flush              = uct_dc_iface_flush,
     .iface_fence              = uct_base_iface_fence,
-    .iface_progress_enable    = uct_base_iface_progress_enable,
+    .iface_progress_enable    = uct_dc_mlx5_iface_progress_enable,
     .iface_progress_disable   = uct_base_iface_progress_disable,
     .iface_progress           = uct_rc_iface_do_progress,
     .iface_event_fd_get       = uct_ib_iface_event_fd_get,
@@ -952,10 +1325,15 @@ static uct_dc_iface_ops_t uct_dc_mlx5_iface_ops = {
     .iface_is_reachable       = uct_dc_iface_is_reachable,
     .iface_get_address        = uct_dc_iface_get_address,
     },
-    .arm_tx_cq                = uct_ib_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_ib_iface_arm_rx_cq,
+    .arm_cq                   = uct_ib_iface_arm_cq,
+    .event_cq                 = uct_dc_mlx5_iface_event_cq,
     .handle_failure           = uct_dc_mlx5_iface_handle_failure,
-    .set_ep_failed            = uct_dc_mlx5_ep_set_failed
+    .set_ep_failed            = uct_dc_mlx5_ep_set_failed,
+#if HAVE_DC_DV
+    .create_qp                = uct_dc_mlx5_iface_create_qp
+#else
+    .create_qp                = uct_ib_iface_create_qp
+#endif
     },
     .fc_ctrl                  = uct_dc_mlx5_ep_fc_ctrl,
     .fc_handler               = uct_dc_iface_fc_handler,
@@ -964,7 +1342,8 @@ static uct_dc_iface_ops_t uct_dc_mlx5_iface_ops = {
 };
 
 
-static ucs_status_t uct_dc_mlx5_iface_init_dcis(uct_dc_mlx5_iface_t *iface)
+static ucs_status_t uct_dc_mlx5_iface_init_dcis(uct_dc_mlx5_iface_t *iface,
+                                                uct_ib_mlx5_mmio_mode_t mmio_mode)
 {
     ucs_status_t status;
     uint16_t bb_max;
@@ -973,7 +1352,7 @@ static ucs_status_t uct_dc_mlx5_iface_init_dcis(uct_dc_mlx5_iface_t *iface)
     bb_max = 0;
     for (i = 0; i < iface->super.tx.ndci; i++) {
         status = uct_ib_mlx5_txwq_init(iface->super.super.super.super.worker,
-                                       &iface->dci_wqs[i],
+                                       mmio_mode, &iface->dci_wqs[i],
                                        iface->super.tx.dcis[i].txqp.qp);
         if (status != UCS_OK) {
             return status;
@@ -988,8 +1367,17 @@ static ucs_status_t uct_dc_mlx5_iface_init_dcis(uct_dc_mlx5_iface_t *iface)
     return UCS_OK;
 }
 
+static void uct_dc_mlx5_iface_cleanup_dcis(uct_dc_mlx5_iface_t *iface)
+{
+    int i;
+
+    for (i = 0; i < iface->super.tx.ndci; i++) {
+        uct_ib_mlx5_txwq_cleanup(&iface->dci_wqs[i]);
+    }
+}
+
 static ucs_status_t uct_dc_mlx5_iface_tag_init(uct_dc_mlx5_iface_t *iface,
-                                               uct_rc_iface_config_t *rc_config)
+                                               uct_dc_mlx5_iface_config_t *config)
 {
 #if IBV_EXP_HW_TM_DC
     if (UCT_RC_IFACE_TM_ENABLED(&iface->super.super)) {
@@ -1000,7 +1388,9 @@ static ucs_status_t uct_dc_mlx5_iface_tag_init(uct_dc_mlx5_iface_t *iface,
         uct_dc_iface_fill_xrq_init_attrs(&iface->super.super, &srq_init_attr, &dc_op);
 
         status = uct_rc_mlx5_iface_common_tag_init(&iface->mlx5_common,
-                                                   &iface->super.super, rc_config,
+                                                   &iface->super.super,
+                                                   &config->super.super,
+                                                   &config->mlx5_common,
                                                    &srq_init_attr,
                                                    sizeof(struct ibv_exp_tmh_rvh) +
                                                    sizeof(struct ibv_exp_tmh_ravh));
@@ -1027,8 +1417,7 @@ static ucs_status_t uct_dc_mlx5_iface_tag_init(uct_dc_mlx5_iface_t *iface,
 static void uct_dc_mlx5_iface_tag_cleanup(uct_dc_mlx5_iface_t *iface)
 {
     if (UCT_RC_IFACE_TM_ENABLED(&iface->super.super)) {
-        ibv_exp_destroy_dct(iface->super.rx.dct);
-        iface->super.rx.dct = NULL;
+        uct_dc_destroy_dct(&iface->super);
     }
 
     uct_rc_mlx5_iface_common_tag_cleanup(&iface->mlx5_common, &iface->super.super);
@@ -1040,20 +1429,25 @@ static UCS_CLASS_INIT_FUNC(uct_dc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
 {
     uct_dc_mlx5_iface_config_t *config = ucs_derived_of(tl_config,
                                                         uct_dc_mlx5_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     ucs_status_t status;
 
     ucs_trace_func("");
+
+    init_attr.res_domain_key = UCT_IB_MLX5_RES_DOMAIN_KEY;
+    init_attr.tm_cap_bit     = IBV_EXP_TM_CAP_DC;
+    init_attr.flags          = UCT_IB_CQ_IGNORE_OVERRUN;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_dc_iface_t, &uct_dc_mlx5_iface_ops, md,
-                              worker, params, 0, &config->super,
-                              IBV_EXP_TM_CAP_DC);
+                              worker, params, &config->super, &init_attr);
 
-    status = uct_dc_mlx5_iface_tag_init(self, &config->super.super);
+    status = uct_dc_mlx5_iface_tag_init(self, config);
     if (status != UCS_OK) {
         goto err;
     }
 
     status = uct_rc_mlx5_iface_common_init(&self->mlx5_common, &self->super.super,
-                                           &config->super.super);
+                                           &config->super.super, &config->mlx5_common);
     if (status != UCS_OK) {
         goto err_rc_mlx5_tag_cleanup;
     }
@@ -1064,7 +1458,7 @@ static UCS_CLASS_INIT_FUNC(uct_dc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
         goto err_rc_mlx5_common_cleanup;
     }
 
-    status = uct_dc_mlx5_iface_init_dcis(self);
+    status = uct_dc_mlx5_iface_init_dcis(self, config->mlx5_common.mmio_mode);
     if (status != UCS_OK) {
         goto err_rc_mlx5_common_cleanup;
     }
@@ -1072,7 +1466,7 @@ static UCS_CLASS_INIT_FUNC(uct_dc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
     uct_dc_iface_set_quota(&self->super, &config->super);
     /* Set max_iov for put_zcopy and get_zcopy */
     uct_ib_iface_set_max_iov(&self->super.super.super,
-                             ((UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB) -
+                             (UCT_IB_MLX5_MAX_SEND_WQE_SIZE -
                              sizeof(struct mlx5_wqe_raddr_seg) -
                              sizeof(struct mlx5_wqe_ctrl_seg) -
                              UCT_IB_MLX5_AV_FULL_SIZE) /
@@ -1097,6 +1491,7 @@ static UCS_CLASS_CLEANUP_FUNC(uct_dc_mlx5_iface_t)
     ucs_trace_func("");
     uct_base_iface_progress_disable(&self->super.super.super.super.super,
                                     UCT_PROGRESS_SEND | UCT_PROGRESS_RECV);
+    uct_dc_mlx5_iface_cleanup_dcis(self);
     uct_rc_mlx5_iface_common_cleanup(&self->mlx5_common);
     uct_dc_mlx5_iface_tag_cleanup(self);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.h
index a82474a7d..1e89c0a86 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/accel/dc_mlx5.h
@@ -16,6 +16,7 @@
 typedef struct uct_dc_mlx5_iface_config {
     uct_dc_iface_config_t               super;
     uct_ud_mlx5_iface_common_config_t   ud_common;
+    uct_ib_mlx5_iface_config_t          mlx5_common;
 } uct_dc_mlx5_iface_config_t;
 
 
@@ -24,6 +25,7 @@ typedef struct uct_dc_mlx5_iface {
     uct_rc_mlx5_iface_common_t          mlx5_common;
     uct_ud_mlx5_iface_common_t          ud_common;
     uct_ib_mlx5_txwq_t                  dci_wqs[UCT_DC_IFACE_MAX_DCIS];
+    struct ibv_qp                       *rx_dct;
 } uct_dc_mlx5_iface_t;
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.c
index 8b1175dcf..9967df82a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.c
@@ -77,7 +77,8 @@ void uct_dc_ep_release(uct_dc_ep_t *ep)
    currently pending code supports only dcs policy
    support hash/random policies
  */
-ucs_status_t uct_dc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *r)
+ucs_status_t uct_dc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *r,
+                                   unsigned flags)
 {
     uct_dc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_iface_t);
     uct_dc_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_ep_t);
@@ -129,11 +130,9 @@ uct_dc_iface_dci_do_pending_wait(ucs_arbiter_t *arbiter,
     uct_dc_ep_t *ep = ucs_container_of(ucs_arbiter_elem_group(elem), uct_dc_ep_t, arb_group);
     uct_dc_iface_t *iface = ucs_derived_of(ep->super.super.iface, uct_dc_iface_t);
 
-    /**
-     * stop if dci can not be allocated
-     * else move group to the dci arbiter
-     */
-    ucs_assert_always(ep->dci == UCT_DC_EP_NO_DCI);
+    if (ep->dci != UCT_DC_EP_NO_DCI) {
+        return UCS_ARBITER_CB_RESULT_DESCHED_GROUP;
+    }
 
     if (!uct_dc_iface_dci_can_alloc(iface)) {
         return UCS_ARBITER_CB_RESULT_STOP;
@@ -254,10 +253,6 @@ ucs_status_t uct_dc_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completion_t *c
         return UCS_OK;
     }
 
-    /* If waiting for FC grant, return NO_RESOURCE to prevent ep destruction.
-     * Otherwise grant for destroyed ep will arrive and there will be a
-     * segfault when we will try to access the ep by address from the grant
-     * message. */
     if (!uct_rc_iface_has_tx_resources(&iface->super)) {
         return UCS_ERR_NO_RESOURCE;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.h
index 79ff47e07..ce68e7a53 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_ep.h
@@ -53,7 +53,8 @@ uct_dc_iface_dci_do_pending_tx(ucs_arbiter_t *arbiter,
                                ucs_arbiter_elem_t *elem,
                                void *arg);
 
-ucs_status_t uct_dc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *r);
+ucs_status_t uct_dc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *r,
+                                   unsigned flags);
 void uct_dc_ep_pending_purge(uct_ep_h tl_ep, uct_pending_purge_callback_t cb, void *arg);
 
 void uct_dc_ep_cleanup(uct_ep_h tl_ep, ucs_class_t *cls);
@@ -105,7 +106,7 @@ enum uct_dc_ep_flags {
     UCT_DC_EP_FLAG_TX_WAIT  = UCS_BIT(0), /* ep is in the tx_wait state. See
                                              description of the dcs+quota dci
                                              selection policy above */
-    UCT_DC_EP_FLAG_GRH      = UCS_BIT(1), /* ep has GRH address. Used by 
+    UCT_DC_EP_FLAG_GRH      = UCS_BIT(1), /* ep has GRH address. Used by
                                              dc_mlx5 endpoint */
     UCT_DC_EP_FLAG_VALID    = UCS_BIT(2)  /* ep is a valid endpoint */
 };
@@ -212,6 +213,9 @@ static inline void uct_dc_iface_dci_put_dcs(uct_dc_iface_t *iface, uint8_t dci)
     }
     iface->tx.stack_top--;
     iface->tx.dcis_stack[iface->tx.stack_top] = dci;
+#if ENABLE_ASSERT
+    iface->tx.dcis[dci].flags = 0;
+#endif
 
     if (ucs_unlikely(ep == NULL)) {
         return;
@@ -221,9 +225,7 @@ static inline void uct_dc_iface_dci_put_dcs(uct_dc_iface_t *iface, uint8_t dci)
     ep->dci    = UCT_DC_EP_NO_DCI;
     ep->flags &= ~UCT_DC_EP_FLAG_TX_WAIT;
     iface->tx.dcis[dci].ep = NULL;
-#if ENABLE_ASSERT
-    iface->tx.dcis[dci].flags = 0;
-#endif
+
     /* it is possible that dci is released while ep still has scheduled pending ops.
      * move the group to the 'wait for dci alloc' state
      */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.c
index 50bd0c0b1..296256003 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.c
@@ -10,7 +10,6 @@
 #include <uct/ib/base/ib_device.h>
 #include <ucs/async/async.h>
 
-
 const static char *uct_dc_tx_policy_names[] = {
     [UCT_DC_TX_POLICY_DCS]           = "dcs",
     [UCT_DC_TX_POLICY_DCS_QUOTA]     = "dcs_quota",
@@ -52,6 +51,7 @@ ucs_config_field_t uct_dc_iface_config_table[] = {
     {NULL}
 };
 
+#if HAVE_DC_EXP
 ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface)
 {
     struct ibv_exp_dct_init_attr init_attr;
@@ -59,7 +59,7 @@ ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface)
     memset(&init_attr, 0, sizeof(init_attr));
 
     init_attr.pd               = uct_ib_iface_md(&iface->super.super)->pd;
-    init_attr.cq               = iface->super.super.recv_cq;
+    init_attr.cq               = iface->super.super.cq[UCT_IB_DIR_RX];
     init_attr.srq              = iface->super.rx.srq.srq;
     init_attr.dc_key           = UCT_IB_KEY;
     init_attr.port             = iface->super.super.config.port_num;
@@ -68,7 +68,9 @@ ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface)
                                  IBV_EXP_ACCESS_REMOTE_READ |
                                  IBV_EXP_ACCESS_REMOTE_ATOMIC;
     init_attr.min_rnr_timer    = iface->super.config.min_rnr_timer;
-    init_attr.hop_limit        = 1;
+    init_attr.tclass           = iface->super.super.config.traffic_class;
+    init_attr.hop_limit        = iface->super.super.config.hop_limit;
+    init_attr.gid_index        = iface->super.super.config.gid_index;
     init_attr.inline_size      = iface->super.config.rx_inline;
 
 #if HAVE_DECL_IBV_EXP_DCT_OOO_RW_DATA_PLACEMENT
@@ -81,19 +83,18 @@ ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface)
     }
 #endif
 
-    iface->rx.dct = ibv_exp_create_dct(uct_ib_iface_device(&iface->super.super)->ibv_context,
+    iface->rx_dct = ibv_exp_create_dct(uct_ib_iface_device(&iface->super.super)->ibv_context,
                                        &init_attr);
-    if (iface->rx.dct == NULL) {
+    if (iface->rx_dct == NULL) {
         ucs_error("Failed to created DC target %m");
         return UCS_ERR_INVALID_PARAM;
     }
-
     return UCS_OK;
 }
 
 /* take dc qp to rts state */
-static ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface,
-                                             uct_rc_txqp_t *dci)
+ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface,
+                                      uct_rc_txqp_t *dci)
 {
     struct ibv_exp_qp_attr attr;
     long attr_mask;
@@ -121,11 +122,7 @@ static ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface,
     attr.path_mtu                   = iface->super.config.path_mtu;
     attr.min_rnr_timer              = 0;
     attr.max_dest_rd_atomic         = 1;
-    if ((iface->super.super.addr_type == UCT_IB_ADDRESS_TYPE_ETH) ||
-        (iface->super.super.addr_type == UCT_IB_ADDRESS_TYPE_GLOBAL))
-    {
-        attr.ah_attr.is_global      = 1;
-    }
+    attr.ah_attr.is_global          = iface->super.super.is_global_addr;
     attr.ah_attr.sl                 = iface->super.super.config.sl;
     attr_mask                       = IBV_EXP_QP_STATE     |
                                       IBV_EXP_QP_PATH_MTU  |
@@ -167,6 +164,33 @@ static ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface,
     return UCS_OK;
 }
 
+int uct_dc_get_dct_num(uct_dc_iface_t *iface)
+{
+    return iface->rx_dct->dct_num;
+}
+
+void uct_dc_destroy_dct(uct_dc_iface_t *iface)
+{
+    if (iface->rx_dct != NULL) {
+        ibv_exp_destroy_dct(iface->rx_dct);
+    }
+    iface->rx_dct = NULL;
+}
+
+static ucs_status_t uct_dc_device_init(uct_ib_device_t *dev)
+{
+#if HAVE_DECL_IBV_EXP_DEVICE_DC_TRANSPORT && HAVE_STRUCT_IBV_EXP_DEVICE_ATTR_EXP_DEVICE_CAP_FLAGS
+    if (dev->dev_attr.exp_device_cap_flags & IBV_EXP_DEVICE_DC_TRANSPORT) {
+        dev->flags |= UCT_IB_DEVICE_FLAG_DC;
+    }
+#endif
+    return UCS_OK;
+}
+
+UCT_IB_DEVICE_INIT(uct_dc_device_init);
+
+#endif
+
 static void uct_dc_iface_dcis_destroy(uct_dc_iface_t *iface, int max)
 {
     int i;
@@ -187,7 +211,7 @@ static ucs_status_t uct_dc_iface_create_dcis(uct_dc_iface_t *iface,
     iface->tx.stack_top = 0;
     for (i = 0; i < iface->tx.ndci; i++) {
         status = uct_rc_txqp_init(&iface->tx.dcis[i].txqp, &iface->super,
-                                  IBV_EXP_QPT_DC_INI, &cap
+                                  UCT_IB_QPT_DCI, &cap
                                   UCS_STATS_ARG(iface->super.stats));
         if (status != UCS_OK) {
             goto err;
@@ -241,15 +265,16 @@ static void uct_dc_iface_init_version(uct_dc_iface_t *iface, uct_md_h md)
 
 UCS_CLASS_INIT_FUNC(uct_dc_iface_t, uct_dc_iface_ops_t *ops, uct_md_h md,
                     uct_worker_h worker, const uct_iface_params_t *params,
-                    unsigned rx_priv_len, uct_dc_iface_config_t *config,
-                    int tm_cap_bit)
+                    uct_dc_iface_config_t *config,
+                    uct_ib_iface_init_attr_t *init_attr)
 {
     ucs_status_t status;
     ucs_trace_func("");
 
+    init_attr->fc_req_size = sizeof(uct_dc_fc_request_t);
+
     UCS_CLASS_CALL_SUPER_INIT(uct_rc_iface_t, &ops->super, md, worker, params,
-                              &config->super, rx_priv_len,
-                              sizeof(uct_dc_fc_request_t), tm_cap_bit);
+                              &config->super, init_attr);
     if (config->ndci < 1) {
         ucs_error("dc interface must have at least 1 dci (requested: %d)",
                   config->ndci);
@@ -287,7 +312,7 @@ UCS_CLASS_INIT_FUNC(uct_dc_iface_t, uct_dc_iface_ops_t *ops, uct_md_h md,
     ucs_debug("dc iface %p: using '%s' policy with %d dcis, dct 0x%x", self,
               uct_dc_tx_policy_names[self->tx.policy], self->tx.ndci,
               UCT_RC_IFACE_TM_ENABLED(&self->super) ?
-              0 : self->rx.dct->dct_num);
+              0 : uct_dc_get_dct_num(self));
 
     /* Create fake endpoint which will be used for sending FC grants */
     uct_dc_iface_init_fc_ep(self);
@@ -297,7 +322,7 @@ UCS_CLASS_INIT_FUNC(uct_dc_iface_t, uct_dc_iface_ops_t *ops, uct_md_h md,
 
 err_destroy_dct:
     if (!UCT_RC_IFACE_TM_ENABLED(&self->super)) {
-        ibv_exp_destroy_dct(self->rx.dct);
+        uct_dc_destroy_dct(self);
     }
 err:
     return status;
@@ -308,15 +333,13 @@ static UCS_CLASS_CLEANUP_FUNC(uct_dc_iface_t)
     uct_dc_ep_t *ep, *tmp;
 
     ucs_trace_func("");
-    if (self->rx.dct != NULL) {
-        ibv_exp_destroy_dct(self->rx.dct);
-    }
+    uct_dc_destroy_dct(self);
     ucs_list_for_each_safe(ep, tmp, &self->tx.gc_list, list) {
         uct_dc_ep_release(ep);
     }
     uct_dc_iface_dcis_destroy(self, self->tx.ndci);
-    ucs_arbiter_cleanup(&self->tx.dci_arbiter);
     uct_dc_iface_cleanup_fc_ep(self);
+    ucs_arbiter_cleanup(&self->tx.dci_arbiter);
 }
 
 UCS_CLASS_DEFINE(uct_dc_iface_t, uct_rc_iface_t);
@@ -324,12 +347,13 @@ UCS_CLASS_DEFINE(uct_dc_iface_t, uct_rc_iface_t);
 ucs_status_t uct_dc_iface_query(uct_dc_iface_t *iface,
                                 uct_iface_attr_t *iface_attr,
                                 size_t put_max_short, size_t max_inline,
-                                size_t am_max_hdr, size_t am_max_iov)
+                                size_t am_max_hdr, size_t am_max_iov,
+                                size_t tag_max_iov)
 {
     ucs_status_t status;
 
     status = uct_rc_iface_query(&iface->super, iface_attr, put_max_short,
-                                max_inline, am_max_hdr, am_max_iov);
+                                max_inline, am_max_hdr, am_max_iov, tag_max_iov);
     if (status != UCS_OK) {
         return status;
     }
@@ -367,7 +391,7 @@ uct_dc_iface_get_address(uct_iface_h tl_iface, uct_iface_addr_t *iface_addr)
     uct_dc_iface_t      *iface = ucs_derived_of(tl_iface, uct_dc_iface_t);
     uct_dc_iface_addr_t *addr  = (uct_dc_iface_addr_t *)iface_addr;
 
-    uct_ib_pack_uint24(addr->qp_num, iface->rx.dct->dct_num);
+    uct_ib_pack_uint24(addr->qp_num, uct_dc_get_dct_num(iface));
     addr->atomic_mr_id = uct_ib_iface_get_atomic_mr_id(&iface->super.super);
     addr->flags        = iface->version_flag;
     if (UCT_RC_IFACE_TM_ENABLED(&iface->super)) {
@@ -395,7 +419,7 @@ static inline ucs_status_t uct_dc_iface_flush_dcis(uct_dc_iface_t *iface)
     for (i = 0; i < iface->tx.ndci; i++) {
         if ((iface->tx.dcis[i].ep != NULL) &&
             uct_dc_ep_fc_wait_for_grant(iface->tx.dcis[i].ep)) {
-            return UCS_ERR_NO_RESOURCE;
+            return UCS_INPROGRESS;
         }
         if (uct_dc_iface_flush_dci(iface, i) != UCS_OK) {
             is_flush_done = 0;
@@ -517,7 +541,8 @@ ucs_status_t uct_dc_iface_fc_handler(uct_rc_iface_t *rc_iface, unsigned qp_num,
 
         status = uct_dc_iface_fc_grant(&dc_req->super.super);
         if (status == UCS_ERR_NO_RESOURCE){
-            status = uct_ep_pending_add(&ep->super.super, &dc_req->super.super);
+            status = uct_ep_pending_add(&ep->super.super, &dc_req->super.super,
+                                        0);
         }
         ucs_assertv_always(status == UCS_OK, "Failed to send FC grant msg: %s",
                            ucs_status_string(status));
@@ -590,8 +615,18 @@ ucs_status_t uct_dc_handle_failure(uct_ib_iface_t *ib_iface, uint32_t qp_num,
     uct_dc_iface_dci_put(iface, dci);
     ucs_assert_always(ep->dci == UCT_DC_EP_NO_DCI);
 
-    ep_status = iface->super.super.ops->set_ep_failed(ib_iface,
-                                                      &ep->super.super, status);
+    if (ep == iface->tx.fc_ep) {
+        /* Cannot handle errors on flow-control endpoint, so ignore them */
+        ucs_error("got error on DC flow-control endpoint, iface %p: %s", iface,
+                  ucs_status_string(status));
+        ep_status = UCS_OK;
+    } else {
+        ep_status = iface->super.super.ops->set_ep_failed(ib_iface,
+                                                          &ep->super.super, status);
+        if (ep_status != UCS_OK) {
+            return ep_status;
+        }
+   }
 
     status = dc_ops->reset_dci(iface, dci);
     if (status != UCS_OK) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.h
index 4959449ca..6f494bb91 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/base/dc_iface.h
@@ -118,17 +118,17 @@ struct uct_dc_iface {
         ucs_list_link_t           gc_list;
     } tx;
 
-    struct {
-        struct ibv_exp_dct        *dct;
-    } rx;
+#if HAVE_DC_EXP
+    struct ibv_exp_dct            *rx_dct;
+#endif
 
     uint8_t                       version_flag;
 };
 
 
 UCS_CLASS_DECLARE(uct_dc_iface_t, uct_dc_iface_ops_t*, uct_md_h,
-                  uct_worker_h, const uct_iface_params_t*, unsigned,
-                  uct_dc_iface_config_t*, int)
+                  uct_worker_h, const uct_iface_params_t*,
+                  uct_dc_iface_config_t*, uct_ib_iface_init_attr_t*)
 
 extern ucs_config_field_t uct_dc_iface_config_table[];
 
@@ -136,7 +136,8 @@ ucs_status_t uct_dc_iface_create_dct(uct_dc_iface_t *iface);
 
 ucs_status_t uct_dc_iface_query(uct_dc_iface_t *iface, uct_iface_attr_t *iface_attr,
                                 size_t put_max_short, size_t max_inline,
-                                size_t am_max_hdr, size_t am_max_iov);
+                                size_t am_max_hdr, size_t am_max_iov,
+                                size_t tag_max_iov);
 
 int uct_dc_iface_is_reachable(const uct_iface_h tl_iface,
                               const uct_device_addr_t *dev_addr,
@@ -155,6 +156,8 @@ void uct_dc_iface_set_quota(uct_dc_iface_t *iface, uct_dc_iface_config_t *config
 
 ucs_status_t uct_dc_iface_init_fc_ep(uct_dc_iface_t *iface);
 
+ucs_status_t uct_dc_iface_dci_connect(uct_dc_iface_t *iface, uct_rc_txqp_t *dci);
+
 void uct_dc_iface_cleanup_fc_ep(uct_dc_iface_t *iface);
 
 ucs_status_t uct_dc_iface_fc_grant(uct_pending_req_t *self);
@@ -166,6 +169,10 @@ ucs_status_t uct_dc_iface_fc_handler(uct_rc_iface_t *rc_iface, unsigned qp_num,
 ucs_status_t uct_dc_handle_failure(uct_ib_iface_t *ib_iface, uint32_t qp_num,
                                    ucs_status_t status);
 
+int uct_dc_get_dct_num(uct_dc_iface_t *iface);
+
+void uct_dc_destroy_dct(uct_dc_iface_t *iface);
+
 #if IBV_EXP_HW_TM_DC
 void uct_dc_iface_fill_xrq_init_attrs(uct_rc_iface_t *rc_iface,
                                       struct ibv_exp_create_srq_attr *srq_attr,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/verbs/dc_verbs.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/verbs/dc_verbs.c
index 139278736..dfac71622 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/verbs/dc_verbs.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/dc/verbs/dc_verbs.c
@@ -56,7 +56,6 @@ static UCS_CLASS_INIT_FUNC(uct_dc_verbs_ep_t,
 static UCS_CLASS_CLEANUP_FUNC(uct_dc_verbs_ep_t)
 {
     ucs_trace_func("");
-    ibv_destroy_ah(self->ah);
 }
 
 UCS_CLASS_DEFINE(uct_dc_verbs_ep_t, uct_dc_ep_t);
@@ -78,6 +77,7 @@ static ucs_status_t uct_dc_verbs_iface_query(uct_iface_h tl_iface, uct_iface_att
                                 verbs_common->config.max_inline,
                                 verbs_common->config.max_inline,
                                 verbs_common->config.short_desc_size,
+                                uct_ib_iface_get_max_iov(&iface->super.super.super) - 1,
                                 uct_ib_iface_get_max_iov(&iface->super.super.super) - 1);
     if (status != UCS_OK) {
         return status;
@@ -93,7 +93,8 @@ static UCS_F_ALWAYS_INLINE void
 uct_dc_verbs_iface_post_send_to_dci(uct_dc_verbs_iface_t* iface,
                                     struct ibv_exp_send_wr *wr,
                                     uint8_t dci, struct ibv_ah *ah,
-                                    uint32_t dct_num, uint64_t send_flags)
+                                    uint32_t dct_num, uint64_t send_flags,
+                                    int max_log_sge)
 {
     struct ibv_exp_send_wr *bad_wr;
     int ret;
@@ -109,9 +110,10 @@ uct_dc_verbs_iface_post_send_to_dci(uct_dc_verbs_iface_t* iface,
     wr->dc.dct_number     = dct_num;
     wr->dc.dct_access_key = UCT_IB_KEY;
 
-    uct_ib_log_exp_post_send(&iface->super.super.super, txqp->qp, wr,
-                             (wr->exp_opcode == IBV_EXP_WR_SEND) ?
-                             uct_rc_ep_am_packet_dump : NULL);
+    uct_ib_log_exp_post_send(&iface->super.super.super, txqp->qp, wr, max_log_sge,
+                             ((wr->exp_opcode == IBV_EXP_WR_SEND) ||
+                              (wr->exp_opcode == IBV_EXP_WR_SEND_WITH_IMM)) ?
+                             uct_rc_verbs_common_packet_dump : NULL);
 
     ret = ibv_exp_post_send(txqp->qp, wr, &bad_wr);
     if (ret != 0) {
@@ -124,20 +126,22 @@ uct_dc_verbs_iface_post_send_to_dci(uct_dc_verbs_iface_t* iface,
 
 static UCS_F_ALWAYS_INLINE void
 uct_dc_verbs_iface_post_send(uct_dc_verbs_iface_t* iface, uct_dc_verbs_ep_t *ep,
-                             struct ibv_exp_send_wr *wr, uint64_t send_flags)
+                             struct ibv_exp_send_wr *wr, uint64_t send_flags,
+                             int max_log_sge)
 {
     uct_dc_verbs_iface_post_send_to_dci(iface, wr, ep->super.dci, ep->ah,
-                                        ep->dest_qpn, send_flags);
+                                        ep->dest_qpn, send_flags, max_log_sge);
 }
 
 static UCS_F_ALWAYS_INLINE void
 uct_dc_verbs_iface_post_send_desc(uct_dc_verbs_iface_t *iface,
                                   uct_dc_verbs_ep_t *ep,
                                   struct ibv_exp_send_wr *wr,
-                                  uct_rc_iface_send_desc_t *desc, uint64_t send_flags)
+                                  uct_rc_iface_send_desc_t *desc,
+                                  uint64_t send_flags, int max_log_sge)
 {
     UCT_RC_VERBS_FILL_DESC_WR(wr, desc);
-    uct_dc_verbs_iface_post_send(iface, ep, wr, send_flags);
+    uct_dc_verbs_iface_post_send(iface, ep, wr, send_flags, max_log_sge);
     uct_rc_txqp_add_send_op_sn(&iface->super.tx.dcis[ep->super.dci].txqp, &desc->super,
                                iface->dcis_txcnt[ep->super.dci].pi);
 }
@@ -170,7 +174,8 @@ uct_dc_verbs_ep_rdma_zcopy(uct_dc_verbs_ep_t *ep, const uct_iov_t *iov,
                                   remote_addr, rkey);
     wr.next = NULL;
 
-    uct_dc_verbs_iface_post_send(iface, ep, &wr, IBV_SEND_SIGNALED);
+    uct_dc_verbs_iface_post_send(iface, ep, &wr, IBV_SEND_SIGNALED,
+                                 UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
     uct_dc_verbs_iface_add_send_comp(iface, ep, comp);
     return UCS_INPROGRESS;
 }
@@ -195,7 +200,7 @@ ucs_status_t uct_dc_verbs_ep_get_bcopy(uct_ep_h tl_ep,
     UCT_RC_VERBS_FILL_RDMA_WR(wr, wr.exp_opcode, IBV_EXP_WR_RDMA_READ, sge,
                               length, remote_addr, rkey);
     UCT_TL_EP_STAT_OP(&ep->super.super, GET, BCOPY, length);
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SIGNALED);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SIGNALED, INT_MAX);
 
     return UCS_INPROGRESS;
 }
@@ -236,7 +241,7 @@ ucs_status_t uct_dc_verbs_ep_put_short(uct_ep_h tl_ep, const void *buffer,
     UCT_RC_VERBS_FILL_INL_PUT_WR(iface, remote_addr, rkey, buffer, length);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
     uct_dc_verbs_iface_post_send(iface, ep, &iface->inl_rwrite_wr,
-                                 IBV_SEND_INLINE|IBV_SEND_SIGNALED);
+                                 IBV_SEND_INLINE|IBV_SEND_SIGNALED, INT_MAX);
 
     return UCS_OK;
 }
@@ -257,7 +262,7 @@ ssize_t uct_dc_verbs_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
     UCT_RC_VERBS_FILL_RDMA_WR(wr, wr.exp_opcode, IBV_EXP_WR_RDMA_WRITE, sge,
                               length, remote_addr, rkey);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, BCOPY, length);
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SIGNALED);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SIGNALED, INT_MAX);
 
     return length;
 }
@@ -295,7 +300,7 @@ ucs_status_t uct_dc_verbs_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
     uct_rc_verbs_iface_fill_inl_am_sge(verbs_common, id, hdr, buffer, length);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(hdr) + length);
     uct_dc_verbs_iface_post_send(iface, ep, &iface->inl_am_wr,
-                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED);
+                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED, INT_MAX);
     UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
 
     return UCS_OK;
@@ -321,7 +326,7 @@ ssize_t uct_dc_verbs_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
     UCT_RC_VERBS_FILL_AM_BCOPY_WR(wr, sge, length + sizeof(uct_rc_hdr_t),
                                   wr.exp_opcode);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, BCOPY, length);
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SOLICITED);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SOLICITED, INT_MAX);
     UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
 
     return length;
@@ -360,7 +365,8 @@ ucs_status_t uct_dc_verbs_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *he
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, ZCOPY,
                       header_length + uct_iov_total_length(iov, iovcnt));
     uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc,
-                                      send_flags | IBV_SEND_SOLICITED);
+                                      send_flags | IBV_SEND_SOLICITED,
+                                      UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
     UCT_RC_UPDATE_FC_WND(&iface->super.super, &ep->super.fc);
 
     return UCS_INPROGRESS;
@@ -381,7 +387,7 @@ uct_dc_verbs_iface_atomic_post(uct_dc_verbs_iface_t *iface, uct_dc_verbs_ep_t *e
     UCT_RC_VERBS_FILL_ATOMIC_WR(wr, wr.exp_opcode, sge, opcode, compare_add,
                                 swap, remote_addr, ib_rkey);
     UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, force_sig);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, force_sig, INT_MAX);
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
@@ -394,9 +400,9 @@ uct_dc_verbs_ep_atomic(uct_dc_verbs_ep_t *ep, int opcode, void *result,
     uct_rc_iface_send_desc_t *desc;
 
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc,
-                                    iface->super.super.config.atomic64_handler,
-                                    result, comp);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp,
+                                          desc, iface->super.super.config.atomic64_handler,
+                                          result, comp);
     uct_dc_verbs_iface_atomic_post(iface, ep, opcode, compare_add, swap, remote_addr,
                                    rkey, desc, IBV_SEND_SIGNALED);
     return UCS_INPROGRESS;
@@ -415,7 +421,8 @@ uct_dc_verbs_iface_ext_atomic_post(uct_dc_verbs_iface_t *iface, uct_dc_verbs_ep_
     uct_rc_verbs_fill_ext_atomic_wr(&wr, &sge, opcode, length, compare_mask,
                                     compare_add, swap, remote_addr, rkey, ep->super.atomic_mr_offset);
     UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, force_sig|IBV_EXP_SEND_EXT_ATOMIC_INLINE);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc,
+                                      force_sig|IBV_EXP_SEND_EXT_ATOMIC_INLINE, INT_MAX);
 }
 
 
@@ -431,8 +438,8 @@ uct_dc_verbs_ep_ext_atomic(uct_dc_verbs_ep_t *ep, int opcode, void *result,
     uct_rc_send_handler_t handler = uct_rc_iface_atomic_handler(&iface->super.super, 1, length);
 
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc,
-                                    handler, result, comp);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp,
+                                          desc, handler, result, comp);
     uct_dc_verbs_iface_ext_atomic_post(iface, ep, opcode, length, compare_mask,
                                        compare_add, swap, remote_addr,
                                        rkey, desc, IBV_SEND_SIGNALED);
@@ -440,74 +447,44 @@ uct_dc_verbs_ep_ext_atomic(uct_dc_verbs_ep_t *ep, int opcode, void *result,
 }
 #endif
 
-ucs_status_t uct_dc_verbs_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_dc_verbs_ep_atomic64_post(uct_ep_h tl_ep, unsigned opcode, uint64_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey)
 {
-
     uct_dc_verbs_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_verbs_iface_t);
-    uct_dc_verbs_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_verbs_ep_t);
+    uct_dc_verbs_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_verbs_ep_t);
     uct_rc_iface_send_desc_t *desc;
 
-    /* TODO don't allocate descriptor - have dummy buffer */
+    if (opcode != UCT_ATOMIC_OP_ADD) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc);
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc);
 
     uct_dc_verbs_iface_atomic_post(iface, ep,
-                                   IBV_WR_ATOMIC_FETCH_AND_ADD, add, 0,
+                                   IBV_WR_ATOMIC_FETCH_AND_ADD, value, 0,
                                    remote_addr, rkey, desc,
                                    IBV_SEND_SIGNALED);
     return UCS_OK;
 }
 
-ucs_status_t uct_dc_verbs_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp)
-{
-
-    return uct_dc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
-                                  IBV_WR_ATOMIC_FETCH_AND_ADD, result, add, 0,
-                                  remote_addr, rkey, comp);
-}
-
-
-ucs_status_t uct_dc_verbs_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp)
-{
-#if HAVE_IB_EXT_ATOMICS
-    return uct_dc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
-                                      result, sizeof(uint64_t), 0, 0, swap, remote_addr,
-                                      rkey, comp);
-#else
-    return UCS_ERR_UNSUPPORTED;
-#endif
-}
-
-ucs_status_t uct_dc_verbs_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
-                                            uint64_t remote_addr, uct_rkey_t rkey,
-                                            uint64_t *result, uct_completion_t *comp)
-{
-    return uct_dc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
-                                  IBV_WR_ATOMIC_CMP_AND_SWP, result, compare, swap,
-                                  remote_addr, rkey, comp);
-}
-
-
-ucs_status_t uct_dc_verbs_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_dc_verbs_ep_atomic32_post(uct_ep_h tl_ep, unsigned opcode, uint32_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey)
 {
 #if HAVE_IB_EXT_ATOMICS
     uct_dc_verbs_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_dc_verbs_iface_t);
-    uct_dc_verbs_ep_t *ep = ucs_derived_of(tl_ep, uct_dc_verbs_ep_t);
+    uct_dc_verbs_ep_t *ep       = ucs_derived_of(tl_ep, uct_dc_verbs_ep_t);
     uct_rc_iface_send_desc_t *desc;
 
+    if (opcode != UCT_ATOMIC_OP_ADD) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
     UCT_DC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc);
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super.super, &iface->verbs_common.short_desc_mp, desc);
 
-    /* TODO don't allocate descriptor - have dummy buffer */
     uct_dc_verbs_iface_ext_atomic_post(iface, ep, IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD,
-                                       sizeof(uint32_t), 0, add, 0, remote_addr,
+                                       sizeof(uint32_t), 0, value, 0, remote_addr,
                                        rkey, desc, IBV_EXP_SEND_SIGNALED);
     return UCS_OK;
 #else
@@ -515,34 +492,61 @@ ucs_status_t uct_dc_verbs_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
 #endif
 }
 
-ucs_status_t uct_dc_verbs_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_dc_verbs_ep_atomic64_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint64_t value, uint64_t *result,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uct_completion_t *comp)
 {
-#if HAVE_IB_EXT_ATOMICS
-    return uct_dc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD,
-                                      result, sizeof(uint32_t), 0, add, 0,
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_dc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
+                                      IBV_WR_ATOMIC_FETCH_AND_ADD, result, value, 0,
                                       remote_addr, rkey, comp);
-#else
-    return UCS_ERR_UNSUPPORTED;
+#if HAVE_IB_EXT_ATOMICS
+    case UCT_ATOMIC_OP_SWAP:
+        return uct_dc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
+                                          IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
+                                          result, sizeof(uint64_t), 0, 0, value, remote_addr,
+                                          rkey, comp);
 #endif
+    default:
+        return UCS_ERR_UNSUPPORTED;
+    }
 }
 
-ucs_status_t uct_dc_verbs_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_dc_verbs_ep_atomic32_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint32_t value, uint32_t *result,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uct_completion_t *comp)
 {
 #if HAVE_IB_EXT_ATOMICS
-    return uct_dc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
-                                   result, sizeof(uint32_t), 0, 0, swap,
-                                   remote_addr, rkey, comp);
+    int op;
+    uint32_t add;
+    uint32_t swap;
+    ucs_status_t status;
+
+    status = uct_rc_verbs_ep_atomic32_data(opcode, value, &op, &add, &swap);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    return uct_dc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t), op,
+                                      result, sizeof(uint32_t), 0, add, swap,
+                                      remote_addr, rkey, comp);
 #else
     return UCS_ERR_UNSUPPORTED;
 #endif
 }
 
+ucs_status_t uct_dc_verbs_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uint64_t *result, uct_completion_t *comp)
+{
+    return uct_dc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_dc_verbs_ep_t),
+                                  IBV_WR_ATOMIC_CMP_AND_SWP, result, compare, swap,
+                                  remote_addr, rkey, comp);
+}
+
 ucs_status_t uct_dc_verbs_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
                                             uint64_t remote_addr, uct_rkey_t rkey,
                                             uint32_t *result, uct_completion_t *comp)
@@ -557,13 +561,13 @@ ucs_status_t uct_dc_verbs_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, ui
 #endif
 }
 
-
 ucs_status_t uct_dc_verbs_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completion_t *comp)
 {
     uct_dc_verbs_iface_t *iface = ucs_derived_of(tl_ep->iface,
                                                  uct_dc_verbs_iface_t);
     uct_dc_verbs_ep_t    *ep    = ucs_derived_of(tl_ep, uct_dc_verbs_ep_t);
     ucs_status_t         status;
+    uint8_t              dci;
 
     status = uct_dc_ep_flush(tl_ep, flags, comp);
     if (status == UCS_OK) {
@@ -571,9 +575,13 @@ ucs_status_t uct_dc_verbs_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completio
     }
 
     if (status == UCS_INPROGRESS) {
-        ucs_assert(ep->super.dci != UCT_DC_EP_NO_DCI);
-        uct_dc_verbs_iface_add_send_comp(iface, ep, comp);
+        dci = ep->super.dci;
+        ucs_assert(dci != UCT_DC_EP_NO_DCI);
+        status = uct_rc_txqp_add_flush_comp(&iface->super.super,
+                                            &iface->super.tx.dcis[dci].txqp,
+                                            comp, iface->dcis_txcnt[dci].pi);
     }
+
     return status;
 }
 
@@ -624,7 +632,7 @@ static ucs_status_t uct_dc_verbs_reset_dci(uct_dc_iface_t *dc_iface, int dci)
               iface->dcis_txcnt[dci].ci, new_ci);
     iface->dcis_txcnt[dci].ci = new_ci;
 
-    return uct_rc_modify_qp(&dc_iface->tx.dcis[dci].txqp, IBV_QPS_RESET);
+    return uct_ib_modify_qp(dc_iface->tx.dcis[dci].txqp.qp, IBV_QPS_RESET);
 }
 
 /* Send either request for grants or grant message. Request includes ep
@@ -647,7 +655,7 @@ ucs_status_t uct_dc_verbs_ep_fc_ctrl(uct_ep_h tl_ep, unsigned op,
     uct_dc_fc_sender_data_t sender = {
         .ep               = (uintptr_t)dc_ep,
         .global.gid       = ib_iface->gid,
-        .global.is_global = ib_iface->addr_type != UCT_IB_ADDRESS_TYPE_LINK_LOCAL};
+        .global.is_global = ib_iface->is_global_addr};
 
     ucs_assert(sizeof(*hdr) + sizeof(sender) <=
                iface->verbs_common.config.max_inline);
@@ -670,7 +678,7 @@ ucs_status_t uct_dc_verbs_ep_fc_ctrl(uct_ep_h tl_ep, unsigned op,
 
         status = uct_dc_verbs_iface_create_ah(
                     &iface->super, dc_req->lid,
-                    dc_req->sender.global.is_global ? (void*)&dc_req->sender.global.gid : NULL,
+                    ucs_unaligned_ptr(&dc_req->sender.global.gid),
                     &ah);
         if (status != UCS_OK) {
             return status;
@@ -680,8 +688,7 @@ ucs_status_t uct_dc_verbs_ep_fc_ctrl(uct_ep_h tl_ep, unsigned op,
         iface->verbs_common.inl_sge[1].length = sizeof(dc_req->sender.ep);
         uct_dc_verbs_iface_post_send_to_dci(iface, &wr, dc_ep->dci, ah,
                                             dc_req->dct_num,
-                                            IBV_SEND_INLINE | IBV_SEND_SIGNALED);
-        ibv_destroy_ah(ah);
+                                            IBV_SEND_INLINE | IBV_SEND_SIGNALED, INT_MAX);
     } else {
         ucs_assert(op == UCT_RC_EP_FC_FLAG_HARD_REQ);
         wr.exp_opcode                         = IBV_EXP_WR_SEND_WITH_IMM;
@@ -690,12 +697,12 @@ ucs_status_t uct_dc_verbs_ep_fc_ctrl(uct_ep_h tl_ep, unsigned op,
 
         /* Send out DCT number to the peer, so it will be able
          * to send grants back */
-        wr.ex.imm_data                        = iface->super.rx.dct->dct_num;
+        wr.ex.imm_data                        = iface->super.rx_dct->dct_num;
 
         dc_verbs_ep = ucs_derived_of(dc_ep, uct_dc_verbs_ep_t);
         uct_dc_verbs_iface_post_send(iface, dc_verbs_ep, &wr,
                                      IBV_SEND_INLINE | IBV_SEND_SIGNALED |
-                                     IBV_SEND_SOLICITED) ;
+                                     IBV_SEND_SOLICITED, INT_MAX) ;
         UCS_STATS_UPDATE_COUNTER(dc_ep->fc.stats,
                                  UCT_RC_FC_STAT_TX_HARD_REQ, 1);
     }
@@ -765,7 +772,9 @@ ucs_status_t uct_dc_verbs_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
     uct_rc_verbs_iface_fill_inl_sge(&iface->verbs_common, &tmh, sizeof(tmh),
                                     data, length);
     uct_dc_verbs_iface_post_send(iface, ep, &iface->inl_am_wr,
-                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED);
+                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED, INT_MAX);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
 
     return UCS_OK;
 }
@@ -792,7 +801,9 @@ ssize_t uct_dc_verbs_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                    &iface->super.super.tx.mp, desc,
                                    tag, app_ctx, pack_cb, arg, length);
     UCT_RC_VERBS_FILL_SGE(wr, sge, length + sizeof(struct ibv_exp_tmh));
-    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SOLICITED);
+    uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc, IBV_SEND_SOLICITED, INT_MAX);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, BCOPY, length);
 
     return length;
 }
@@ -812,7 +823,9 @@ ucs_status_t uct_dc_verbs_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     size_t sge_cnt;
     uint32_t app_ctx;
 
-    UCT_CHECK_IOV_SIZE(iovcnt, 1ul, "uct_dc_verbs_ep_tag_eager_zcopy");
+    UCT_CHECK_IOV_SIZE(iovcnt,
+                       uct_ib_iface_get_max_iov(&iface->super.super.super) - 1,
+                       "uct_dc_verbs_ep_tag_eager_zcopy");
     UCT_RC_CHECK_ZCOPY_DATA(sizeof(struct ibv_exp_tmh),
                             uct_iov_total_length(iov, iovcnt),
                             iface->super.super.super.config.seg_size);
@@ -829,7 +842,11 @@ ucs_status_t uct_dc_verbs_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     wr.num_sge = sge_cnt + 1;
     wr.sg_list = sge;
     uct_dc_verbs_iface_post_send_desc(iface, ep, &wr, desc,
-                                      send_flags | IBV_SEND_SOLICITED);
+                                      send_flags | IBV_SEND_SOLICITED,
+                                      UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
 
     return UCS_INPROGRESS;
 }
@@ -864,10 +881,11 @@ ucs_status_ptr_t uct_dc_verbs_ep_tag_rndv_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                                  tmh_len, tag, iov, comp);
 
     uct_dc_iface_fill_ravh(rvh + sizeof(struct ibv_exp_tmh_rvh),
-                           iface->super.rx.dct->dct_num);
+                           iface->super.rx_dct->dct_num);
 
     uct_dc_verbs_iface_post_send(iface, ep, &iface->inl_am_wr,
-                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED);
+                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED,
+                                 UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
     return (ucs_status_ptr_t)((uint64_t)rndv_idx);
 }
 
@@ -898,8 +916,8 @@ ucs_status_t uct_dc_verbs_ep_tag_rndv_request(uct_ep_h tl_ep, uct_tag_t tag,
     uct_rc_iface_fill_tmh(&tmh, tag, app_ctx, IBV_EXP_TMH_EAGER);
     uct_rc_verbs_iface_fill_inl_sge(&iface->verbs_common, &tmh, sizeof(tmh),
                                     header, header_length);
-    uct_dc_verbs_iface_post_send(iface, ep, &wr,
-                                 IBV_SEND_INLINE | IBV_SEND_SOLICITED);
+    uct_dc_verbs_iface_post_send(iface, ep, &wr, IBV_SEND_INLINE | IBV_SEND_SOLICITED,
+                                 UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super.super));
 
     return UCS_OK;
 }
@@ -990,8 +1008,8 @@ uct_dc_verbs_iface_tag_init(uct_dc_verbs_iface_t *iface,
 static void uct_dc_verbs_iface_tag_cleanup(uct_dc_verbs_iface_t *iface)
 {
     if (UCT_RC_IFACE_TM_ENABLED(&iface->super.super)) {
-        ibv_exp_destroy_dct(iface->super.rx.dct);
-        iface->super.rx.dct = NULL;
+        ibv_exp_destroy_dct(iface->super.rx_dct);
+        iface->super.rx_dct = NULL;
     }
 
     uct_rc_iface_tag_cleanup(&iface->super.super);
@@ -1021,14 +1039,12 @@ static uct_dc_iface_ops_t uct_dc_verbs_iface_ops = {
     .ep_am_short              = uct_dc_verbs_ep_am_short,
     .ep_am_bcopy              = uct_dc_verbs_ep_am_bcopy,
     .ep_am_zcopy              = uct_dc_verbs_ep_am_zcopy,
-    .ep_atomic_add64          = uct_dc_verbs_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_dc_verbs_ep_atomic_fadd64,
-    .ep_atomic_swap64         = uct_dc_verbs_ep_atomic_swap64,
     .ep_atomic_cswap64        = uct_dc_verbs_ep_atomic_cswap64,
-    .ep_atomic_add32          = uct_dc_verbs_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_dc_verbs_ep_atomic_fadd32,
-    .ep_atomic_swap32         = uct_dc_verbs_ep_atomic_swap32,
+    .ep_atomic64_post         = uct_dc_verbs_ep_atomic64_post,
+    .ep_atomic64_fetch        = uct_dc_verbs_ep_atomic64_fetch,
     .ep_atomic_cswap32        = uct_dc_verbs_ep_atomic_cswap32,
+    .ep_atomic32_post         = uct_dc_verbs_ep_atomic32_post,
+    .ep_atomic32_fetch        = uct_dc_verbs_ep_atomic32_fetch,
     .ep_pending_add           = uct_dc_ep_pending_add,
     .ep_pending_purge         = uct_dc_ep_pending_purge,
     .ep_flush                 = uct_dc_verbs_ep_flush,
@@ -1058,10 +1074,11 @@ static uct_dc_iface_ops_t uct_dc_verbs_iface_ops = {
     .iface_is_reachable       = uct_dc_iface_is_reachable,
     .iface_get_address        = uct_dc_iface_get_address
     },
-    .arm_tx_cq                = uct_ib_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_ib_iface_arm_rx_cq,
+    .arm_cq                   = uct_ib_iface_arm_cq,
+    .event_cq                 = (void*)ucs_empty_function,
     .handle_failure           = uct_dc_verbs_handle_failure,
-    .set_ep_failed            = uct_dc_verbs_ep_set_failed
+    .set_ep_failed            = uct_dc_verbs_ep_set_failed,
+    .create_qp                = uct_ib_iface_create_qp
     },
     .fc_ctrl                  = uct_dc_verbs_ep_fc_ctrl,
     .fc_handler               = uct_dc_iface_fc_handler,
@@ -1094,6 +1111,7 @@ static UCS_CLASS_INIT_FUNC(uct_dc_verbs_iface_t, uct_md_h md, uct_worker_h worke
 {
     uct_dc_verbs_iface_config_t *config = ucs_derived_of(tl_config,
                                                          uct_dc_verbs_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     struct ibv_qp_init_attr dci_init_attr;
     struct ibv_qp_attr dci_attr;
     ucs_status_t status;
@@ -1102,9 +1120,11 @@ static UCS_CLASS_INIT_FUNC(uct_dc_verbs_iface_t, uct_md_h md, uct_worker_h worke
 
     ucs_trace_func("");
 
+    init_attr.res_domain_key = UCT_IB_IFACE_NULL_RES_DOMAIN_KEY;
+    init_attr.tm_cap_bit     = IBV_EXP_TM_CAP_DC;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_dc_iface_t, &uct_dc_verbs_iface_ops, md,
-                              worker, params, 0, &config->super,
-                              IBV_EXP_TM_CAP_DC);
+                              worker, params, &config->super, &init_attr);
 
     uct_dc_verbs_iface_init_wrs(self);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.c
index ceed2f8e4..6e65f55db 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.c
@@ -17,166 +17,67 @@
 #include <string.h>
 
 
-typedef struct uct_ib_mlx5_qp_info {
-    uint32_t           qpn;           /* QP number */
-    volatile uint32_t  *dbrec;        /* QP doorbell record in RAM */
-
-    struct {
-            void       *buf;          /* Work queue buffer */
-            unsigned   wqe_cnt;       /* Number of WQEs in the work queue */
-            unsigned   stride;        /* Size of each WQE */
-    } sq, rq;
-
-    struct {
-            void       *reg;          /* BlueFlame register */
-            unsigned   size;          /* BlueFlame register size (0 - unsupported) */
-    } bf;
-} uct_ib_mlx5_qp_info_t;
-
-
-typedef struct uct_ib_mlx5_srq_info {
-    void               *buf;          /* SRQ queue buffer */
-    volatile uint32_t  *dbrec;        /* SRQ doorbell record in RAM */
-    unsigned           stride;        /* Size of each WQE */
-    unsigned           head;
-    unsigned           tail;
-} uct_ib_mlx5_srq_info_t;
-
-static void UCS_F_MAYBE_UNUSED uct_ib_mlx5_obj_error(const char *obj_name)
-{
-    ucs_error("Failed to get mlx5 %s information. Please make sure the installed "
-              "libmlx5 version matches the one UCX was compiled with (%s)",
-              obj_name, LIB_MLX5_VER);
-}
-
-static ucs_status_t uct_ib_mlx5_get_qp_info(struct ibv_qp *qp, uct_ib_mlx5_qp_info_t *qp_info)
-{
-#if HAVE_DECL_IBV_MLX5_EXP_GET_QP_INFO
-    struct ibv_mlx5_qp_info ibv_qp_info;
-    int ret;
-
-    ret = ibv_mlx5_exp_get_qp_info(qp, &ibv_qp_info);
-    if (ret != 0) {
-        uct_ib_mlx5_obj_error("qp");
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    qp_info->qpn        = ibv_qp_info.qpn;
-    qp_info->dbrec      = ibv_qp_info.dbrec;
-    qp_info->sq.buf     = ibv_qp_info.sq.buf;
-    qp_info->sq.wqe_cnt = ibv_qp_info.sq.wqe_cnt;
-    qp_info->sq.stride  = ibv_qp_info.sq.stride;
-    qp_info->rq.buf     = ibv_qp_info.rq.buf;
-    qp_info->rq.wqe_cnt = ibv_qp_info.rq.wqe_cnt;
-    qp_info->rq.stride  = ibv_qp_info.rq.stride;
-    qp_info->bf.reg     = ibv_qp_info.bf.reg;
-    qp_info->bf.size    = ibv_qp_info.bf.size;
-#else
-    struct mlx5_qp *mqp = ucs_container_of(qp, struct mlx5_qp, verbs_qp.qp);
-
-    if ((mqp->sq.cur_post != 0) || (mqp->rq.head != 0)) {
-        ucs_warn("cur_post=%d head=%d need_lock=%d", mqp->sq.cur_post,
-                 mqp->rq.head, mqp->bf->need_lock);
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    qp_info->qpn        = qp->qp_num;
-    qp_info->dbrec      = mqp->db;
-    qp_info->sq.buf     = mqp->buf.buf + mqp->sq.offset;
-    qp_info->sq.wqe_cnt = mqp->sq.wqe_cnt;
-    qp_info->sq.stride  = 1 << mqp->sq.wqe_shift;
-    qp_info->rq.buf     = mqp->buf.buf + mqp->rq.offset;
-    qp_info->rq.wqe_cnt = mqp->rq.wqe_cnt;
-    qp_info->rq.stride  = 1 << mqp->rq.wqe_shift;
-    qp_info->bf.reg     = mqp->bf->reg;
-
-    if (mqp->bf->uuarn > 0) {
-        qp_info->bf.size = mqp->bf->buf_size;
-    } else {
-        qp_info->bf.size = 0; /* No BF */
-    }
+static const char *uct_ib_mlx5_mmio_modes[] = {
+    [UCT_IB_MLX5_MMIO_MODE_BF_POST]    = "bf_post",
+    [UCT_IB_MLX5_MMIO_MODE_BF_POST_MT] = "bf_post_mt",
+    [UCT_IB_MLX5_MMIO_MODE_DB]         = "db",
+    [UCT_IB_MLX5_MMIO_MODE_AUTO]       = "auto",
+    [UCT_IB_MLX5_MMIO_MODE_LAST]       = NULL
+};
+
+ucs_config_field_t uct_ib_mlx5_iface_config_table[] = {
+#if HAVE_IBV_EXP_DM
+    {"DM_SIZE", "2k",
+     "Device Memory segment size (0 - disabled)",
+     ucs_offsetof(uct_ib_mlx5_iface_config_t, dm.seg_len), UCS_CONFIG_TYPE_MEMUNITS},
+    {"DM_COUNT", "1",
+     "Device Memory segments count (0 - disabled)",
+     ucs_offsetof(uct_ib_mlx5_iface_config_t, dm.count), UCS_CONFIG_TYPE_UINT},
 #endif
-    return UCS_OK;
-}
 
-static ucs_status_t uct_ib_mlx5_get_srq_info(struct ibv_srq *srq,
-                                             uct_ib_mlx5_srq_info_t *srq_info)
-{
-#if HAVE_DECL_IBV_MLX5_EXP_GET_SRQ_INFO
-    struct ibv_mlx5_srq_info ibv_srq_info;
-    int ret;
+    {"MMIO_MODE", "auto",
+     "How to write to MMIO register when posting sends on a QP. One of the following:\n"
+     " bf_post    - BlueFlame post, write the WQE fully to MMIO register.\n"
+     " bf_post_mt - Thread-safe BlueFlame, same as bf_post but same MMIO register can be used\n"
+     "              by multiple threads.\n"
+     " db         - Doorbell mode, write only 8 bytes to MMIO register, followed by a memory\n"
+     "              store fence, which makes sure the doorbell goes out on the bus.\n"
+     " auto       - Select best according to worker thread mode.",
+     ucs_offsetof(uct_ib_mlx5_iface_config_t, mmio_mode),
+     UCS_CONFIG_TYPE_ENUM(uct_ib_mlx5_mmio_modes)},
 
-    ret = ibv_mlx5_exp_get_srq_info(srq, &ibv_srq_info);
-    if (ret != 0) {
-        uct_ib_mlx5_obj_error("srq");
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    srq_info->buf    = ibv_srq_info.buf;
-    srq_info->dbrec  = ibv_srq_info.dbrec;
-    srq_info->stride = ibv_srq_info.stride;
-    srq_info->head   = ibv_srq_info.head;
-    srq_info->tail   = ibv_srq_info.tail;
-#else
-    struct mlx5_srq *msrq;
-
-    if (srq->handle == LEGACY_XRC_SRQ_HANDLE) {
-        srq = (struct ibv_srq *)(((struct ibv_srq_legacy *)srq)->ibv_srq);
-    }
-
-    msrq = ucs_container_of(srq, struct mlx5_srq, vsrq.srq);
-
-    if (msrq->counter != 0) {
-        ucs_error("SRQ counter is not 0 (%d)", msrq->counter);
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    srq_info->buf    = msrq->buf.buf;
-    srq_info->dbrec  = msrq->db;
-    srq_info->stride = 1 << msrq->wqe_shift;
-    srq_info->head   = msrq->head;
-    srq_info->tail   = msrq->tail;
-#endif
-    return UCS_OK;
-}
+    {NULL}
+};
 
 ucs_status_t uct_ib_mlx5_get_cq(struct ibv_cq *cq, uct_ib_mlx5_cq_t *mlx5_cq)
 {
+    uct_ib_mlx5dv_cq_t dcq = {};
+    uct_ib_mlx5dv_t obj = {};
+    struct mlx5_cqe64 *cqe;
     unsigned cqe_size;
-#if HAVE_DECL_IBV_MLX5_EXP_GET_CQ_INFO
-    struct ibv_mlx5_cq_info ibv_cq_info;
-    int ret;
+    ucs_status_t status;
+    int ret, i;
 
-    ret = ibv_mlx5_exp_get_cq_info(cq, &ibv_cq_info);
-    if (ret != 0) {
-        uct_ib_mlx5_obj_error("cq");
-        return UCS_ERR_NO_DEVICE;
+    obj.dv.cq.in = cq;
+    obj.dv.cq.out = &dcq.dv;
+    status = uct_ib_mlx5dv_init_obj(&obj, MLX5DV_OBJ_CQ);
+    if (status != UCS_OK) {
+        return UCS_ERR_IO_ERROR;
     }
 
-    mlx5_cq->cq_buf    = ibv_cq_info.buf;
+    mlx5_cq->cq_buf    = dcq.dv.buf;
     mlx5_cq->cq_ci     = 0;
-    mlx5_cq->cq_length = ibv_cq_info.cqe_cnt;
-#if ENABLE_DEBUG_DATA
-    mlx5_cq->cq_num    = ibv_cq_info.cqn;
-#endif
-    cqe_size           = ibv_cq_info.cqe_size;
+    mlx5_cq->cq_sn     = 0;
+    mlx5_cq->cq_length = dcq.dv.cqe_cnt;
+    mlx5_cq->cq_num    = dcq.dv.cqn;
+#if HAVE_STRUCT_MLX5DV_CQ_CQ_UAR
+    mlx5_cq->uar       = dcq.dv.cq_uar;
 #else
-    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
-    int ret;
-
-    if (mcq->cons_index != 0) {
-        ucs_error("CQ consumer index is not 0 (%d)", mcq->cons_index);
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    mlx5_cq->cq_buf      = mcq->active_buf->buf;
-    mlx5_cq->cq_ci       = 0;
-    mlx5_cq->cq_length   = mcq->ibv_cq.cqe + 1;
-#if ENABLE_DEBUG_DATA
-    mlx5_cq->cq_num      = mcq->cqn;
-#endif
-    cqe_size             = mcq->cqe_sz;
+    /* coverity[var_deref_model] */
+    mlx5_cq->uar       = uct_dv_get_info_uar0(dcq.dv.uar);
 #endif
+    mlx5_cq->dbrec     = dcq.dv.dbrec;
+    cqe_size           = dcq.dv.cqe_size;
 
     /* Move buffer forward for 128b CQE, so we would get pointer to the 2nd
      * 64b when polling.
@@ -191,28 +92,18 @@ ucs_status_t uct_ib_mlx5_get_cq(struct ibv_cq *cq, uct_ib_mlx5_cq_t *mlx5_cq)
 
     mlx5_cq->cqe_size_log = ucs_ilog2(cqe_size);
     ucs_assert_always((1<<mlx5_cq->cqe_size_log) == cqe_size);
-    return UCS_OK;
-}
 
-void uct_ib_mlx5_update_cq_ci(struct ibv_cq *cq, unsigned cq_ci)
-{
-#if HAVE_DECL_IBV_MLX5_EXP_UPDATE_CQ_CI
-    ibv_mlx5_exp_update_cq_ci(cq, cq_ci);
-#else
-    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
-    mcq->cons_index = cq_ci;
-#endif
-}
+    /* Set owner bit for all CQEs, so that CQE would look like it is in HW
+     * ownership. In this case CQ polling functions will return immediately if
+     * no any CQE ready, there is no need to check opcode for
+     * MLX5_CQE_INVALID value anymore. */
+    for (i = 0; i < mlx5_cq->cq_length; ++i) {
+        cqe = uct_ib_mlx5_get_cqe(mlx5_cq, i);
+        cqe->op_own |= MLX5_CQE_OWNER_MASK;
+    }
 
-unsigned uct_ib_mlx5_get_cq_ci(struct ibv_cq *cq)
-{
-    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
-    return mcq->cons_index;
-}
 
-void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av)
-{
-    memcpy(av, &ucs_container_of(ah, struct mlx5_ah, ibv_ah)->av, sizeof(*av));
+    return UCS_OK;
 }
 
 ucs_status_t uct_ib_mlx5_get_compact_av(uct_ib_iface_t *iface, int *compact_av)
@@ -233,13 +124,13 @@ ucs_status_t uct_ib_mlx5_get_compact_av(uct_ib_iface_t *iface, int *compact_av)
     }
 
     uct_ib_iface_fill_ah_attr_from_addr(iface, ib_addr, iface->path_bits[0], &ah_attr);
+    ah_attr.is_global = iface->is_global_addr;
     status = uct_ib_iface_create_ah(iface, &ah_attr, &ah);
     if (status != UCS_OK) {
         return status;
     }
 
     uct_ib_mlx5_get_av(ah, &mlx5_av);
-    ibv_destroy_ah(ah);
 
     /* copy MLX5_EXTENDED_UD_AV from the driver, if the flag is not present then
      * the device supports compact address vector. */
@@ -247,7 +138,6 @@ ucs_status_t uct_ib_mlx5_get_compact_av(uct_ib_iface_t *iface, int *compact_av)
     return UCS_OK;
 }
 
-
 void uct_ib_mlx5_check_completion(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq,
                                   struct mlx5_cqe64 *cqe)
 {
@@ -258,14 +148,15 @@ void uct_ib_mlx5_check_completion(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq,
         /* update ci before invoking error callback, since it can poll on cq */
         UCS_STATIC_ASSERT(MLX5_CQE_REQ_ERR & (UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK >> 4));
         ++cq->cq_ci;
-        status = uct_ib_mlx5_completion_with_err((void*)cqe, UCS_LOG_LEVEL_DEBUG);
+        status = uct_ib_mlx5_completion_with_err(iface, (void*)cqe,
+                                                 UCS_LOG_LEVEL_DEBUG);
         iface->ops->handle_failure(iface, cqe, status);
         return;
     case MLX5_CQE_RESP_ERR:
         /* Local side failure - treat as fatal */
         UCS_STATIC_ASSERT(MLX5_CQE_RESP_ERR & (UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK >> 4));
         ++cq->cq_ci;
-        uct_ib_mlx5_completion_with_err((void*)cqe, UCS_LOG_LEVEL_FATAL);
+        uct_ib_mlx5_completion_with_err(iface, (void*)cqe, UCS_LOG_LEVEL_FATAL);
         return;
     default:
         /* CQE might have been updated by HW. Skip it now, and it would be handled
@@ -274,74 +165,106 @@ void uct_ib_mlx5_check_completion(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq,
     }
 }
 
-static int uct_ib_mlx5_bf_cmp(uct_ib_mlx5_bf_t *bf, uintptr_t addr, unsigned bf_size)
+static int uct_ib_mlx5_mmio_cmp(uct_ib_mlx5_mmio_reg_t *reg, uintptr_t addr,
+                                unsigned bf_size)
 {
-    return ((bf->reg.addr & ~UCT_IB_MLX5_BF_REG_SIZE) == (addr & ~UCT_IB_MLX5_BF_REG_SIZE));
+    return (reg->addr.uint & ~UCT_IB_MLX5_BF_REG_SIZE) ==
+           (addr & ~UCT_IB_MLX5_BF_REG_SIZE);
 }
 
-static void uct_ib_mlx5_bf_init(uct_ib_mlx5_bf_t *bf, uintptr_t addr, unsigned bf_size)
+static ucs_status_t uct_ib_mlx5_mmio_init(uct_ib_mlx5_mmio_reg_t *reg,
+                                          uintptr_t addr,
+                                          uct_ib_mlx5_mmio_mode_t mmio_mode)
 {
-    bf->reg.addr  = addr;
-    bf->enable_bf = bf_size;
+    reg->addr.uint = addr;
+    reg->mode      = mmio_mode;
+    return UCS_OK;
 }
 
-static void uct_ib_mlx5_bf_cleanup(uct_ib_mlx5_bf_t *bf)
+static void uct_ib_mlx5_mmio_cleanup(uct_ib_mlx5_mmio_reg_t *reg)
 {
 }
 
 void uct_ib_mlx5_txwq_reset(uct_ib_mlx5_txwq_t *txwq)
 {
-    txwq->curr     = txwq->qstart;
-    txwq->sw_pi    = txwq->prev_sw_pi = 0;
+    txwq->curr       = txwq->qstart;
+    txwq->sw_pi      = 0;
+    txwq->prev_sw_pi = -1;
 #if ENABLE_ASSERT
-    txwq->hw_ci    = 0xFFFF;
+    txwq->hw_ci      = 0xFFFF;
 #endif
     memset(txwq->qstart, 0, txwq->qend - txwq->qstart);
 }
 
 ucs_status_t uct_ib_mlx5_txwq_init(uct_priv_worker_t *worker,
+                                   uct_ib_mlx5_mmio_mode_t cfg_mmio_mode,
                                    uct_ib_mlx5_txwq_t *txwq,
                                    struct ibv_qp *verbs_qp)
 {
-    uct_ib_mlx5_qp_info_t qp_info;
+    uct_ib_mlx5_mmio_mode_t mmio_mode;
+    uct_ib_mlx5dv_qp_t qp_info = {};
+    uct_ib_mlx5dv_t obj = {};
     ucs_status_t status;
 
-    status = uct_ib_mlx5_get_qp_info(verbs_qp, &qp_info);
+    obj.dv.qp.in = verbs_qp;
+    obj.dv.qp.out = &qp_info.dv;
+
+    status = uct_ib_mlx5dv_init_obj(&obj, MLX5DV_OBJ_QP);
     if (status != UCS_OK) {
         return UCS_ERR_IO_ERROR;
     }
 
-    if ((qp_info.sq.stride != MLX5_SEND_WQE_BB) || !ucs_is_pow2(qp_info.sq.wqe_cnt) ||
-        ((qp_info.bf.size != 0) && (qp_info.bf.size != UCT_IB_MLX5_BF_REG_SIZE)))
+    if ((qp_info.dv.sq.stride != MLX5_SEND_WQE_BB) || !ucs_is_pow2(qp_info.dv.sq.wqe_cnt) ||
+        ((qp_info.dv.bf.size != 0) && (qp_info.dv.bf.size != UCT_IB_MLX5_BF_REG_SIZE)))
     {
         ucs_error("mlx5 device parameters not suitable for transport "
                   "bf.size(%d) %d, sq.stride(%d) %d, wqe_cnt %d",
-                  UCT_IB_MLX5_BF_REG_SIZE, qp_info.bf.size,
-                  MLX5_SEND_WQE_BB, qp_info.sq.stride, qp_info.sq.wqe_cnt);
+                  UCT_IB_MLX5_BF_REG_SIZE, qp_info.dv.bf.size,
+                  MLX5_SEND_WQE_BB, qp_info.dv.sq.stride, qp_info.dv.sq.wqe_cnt);
         return UCS_ERR_IO_ERROR;
     }
 
-    ucs_debug("tx wq %d bytes [bb=%d, nwqe=%d]",
-              qp_info.sq.stride * qp_info.sq.wqe_cnt,
-              qp_info.sq.stride, qp_info.sq.wqe_cnt);
+    if (cfg_mmio_mode != UCT_IB_MLX5_MMIO_MODE_AUTO) {
+        mmio_mode = cfg_mmio_mode;
+    } else if (qp_info.dv.bf.size > 0) {
+        if (worker->thread_mode == UCS_THREAD_MODE_SINGLE) {
+            mmio_mode = UCT_IB_MLX5_MMIO_MODE_BF_POST;
+        } else if (worker->thread_mode == UCS_THREAD_MODE_SERIALIZED) {
+            mmio_mode = UCT_IB_MLX5_MMIO_MODE_BF_POST_MT;
+        } else {
+            ucs_error("unsupported thread mode for mlx5: %d", worker->thread_mode);
+            return UCS_ERR_UNSUPPORTED;
+        }
+    } else {
+        mmio_mode = UCT_IB_MLX5_MMIO_MODE_DB;
+    }
 
-    txwq->qstart   = qp_info.sq.buf;
-    txwq->qend     = qp_info.sq.buf + (qp_info.sq.stride * qp_info.sq.wqe_cnt);
-    txwq->bf       = uct_worker_tl_data_get(worker,
+    ucs_debug("tx wq %d bytes [bb=%d, nwqe=%d] mmio_mode %s",
+              qp_info.dv.sq.stride * qp_info.dv.sq.wqe_cnt,
+              qp_info.dv.sq.stride, qp_info.dv.sq.wqe_cnt,
+              uct_ib_mlx5_mmio_modes[mmio_mode]);
+
+    txwq->qstart   = qp_info.dv.sq.buf;
+    txwq->qend     = qp_info.dv.sq.buf + (qp_info.dv.sq.stride * qp_info.dv.sq.wqe_cnt);
+    txwq->reg      = uct_worker_tl_data_get(worker,
                                             UCT_IB_MLX5_WORKER_BF_KEY,
-                                            uct_ib_mlx5_bf_t,
-                                            uct_ib_mlx5_bf_cmp,
-                                            uct_ib_mlx5_bf_init,
-                                            (uintptr_t)qp_info.bf.reg,
-                                            qp_info.bf.size);
-    txwq->dbrec    = &qp_info.dbrec[MLX5_SND_DBR];
+                                            uct_ib_mlx5_mmio_reg_t,
+                                            uct_ib_mlx5_mmio_cmp,
+                                            uct_ib_mlx5_mmio_init,
+                                            (uintptr_t)qp_info.dv.bf.reg,
+                                            mmio_mode);
+    if (UCS_PTR_IS_ERR(txwq->reg)) {
+        return UCS_PTR_STATUS(txwq->reg);
+    }
+
+    txwq->dbrec    = &qp_info.dv.dbrec[MLX5_SND_DBR];
     /* need to reserve 2x because:
      *  - on completion we only get the index of last wqe and we do not
      *    really know how many bb is there (but no more than max bb
      *  - on send we check that there is at least one bb. We know
      *  exact number of bbs once we actually are sending.
      */
-    txwq->bb_max   = qp_info.sq.wqe_cnt - 2 * UCT_IB_MLX5_MAX_BB;
+    txwq->bb_max   = qp_info.dv.sq.wqe_cnt - 2 * UCT_IB_MLX5_MAX_BB;
     ucs_assert_always(txwq->bb_max > 0);
 
     uct_ib_mlx5_txwq_reset(txwq);
@@ -350,32 +273,36 @@ ucs_status_t uct_ib_mlx5_txwq_init(uct_priv_worker_t *worker,
 
 void uct_ib_mlx5_txwq_cleanup(uct_ib_mlx5_txwq_t* txwq)
 {
-    uct_worker_tl_data_put(txwq->bf, uct_ib_mlx5_bf_cleanup);
+    uct_worker_tl_data_put(txwq->reg, uct_ib_mlx5_mmio_cleanup);
 }
 
 ucs_status_t uct_ib_mlx5_get_rxwq(struct ibv_qp *verbs_qp, uct_ib_mlx5_rxwq_t *rxwq)
 {
-    uct_ib_mlx5_qp_info_t qp_info;
+    uct_ib_mlx5dv_qp_t qp_info = {};
+    uct_ib_mlx5dv_t obj = {};
     ucs_status_t status;
 
-    status = uct_ib_mlx5_get_qp_info(verbs_qp, &qp_info);
+    obj.dv.qp.in = verbs_qp;
+    obj.dv.qp.out = &qp_info.dv;
+
+    status = uct_ib_mlx5dv_init_obj(&obj, MLX5DV_OBJ_QP);
     if (status != UCS_OK) {
         return UCS_ERR_IO_ERROR;
     }
 
-    if (!ucs_is_pow2(qp_info.rq.wqe_cnt) ||
-        qp_info.rq.stride != sizeof(struct mlx5_wqe_data_seg)) {
+    if (!ucs_is_pow2(qp_info.dv.rq.wqe_cnt) ||
+        qp_info.dv.rq.stride != sizeof(struct mlx5_wqe_data_seg)) {
         ucs_error("mlx5 rx wq [count=%d stride=%d] has invalid parameters",
-                  qp_info.rq.wqe_cnt,
-                  qp_info.rq.stride);
+                  qp_info.dv.rq.wqe_cnt,
+                  qp_info.dv.rq.stride);
         return UCS_ERR_IO_ERROR;
     }
-    rxwq->wqes            = qp_info.rq.buf;
+    rxwq->wqes            = qp_info.dv.rq.buf;
     rxwq->rq_wqe_counter  = 0;
     rxwq->cq_wqe_counter  = 0;
-    rxwq->mask            = qp_info.rq.wqe_cnt - 1;
-    rxwq->dbrec           = &qp_info.dbrec[MLX5_RCV_DBR];
-    memset(rxwq->wqes, 0, qp_info.rq.wqe_cnt * sizeof(struct mlx5_wqe_data_seg));
+    rxwq->mask            = qp_info.dv.rq.wqe_cnt - 1;
+    rxwq->dbrec           = &qp_info.dv.dbrec[MLX5_RCV_DBR];
+    memset(rxwq->wqes, 0, qp_info.dv.rq.wqe_cnt * sizeof(struct mlx5_wqe_data_seg));
 
     return UCS_OK;
 }
@@ -383,41 +310,45 @@ ucs_status_t uct_ib_mlx5_get_rxwq(struct ibv_qp *verbs_qp, uct_ib_mlx5_rxwq_t *r
 ucs_status_t uct_ib_mlx5_srq_init(uct_ib_mlx5_srq_t *srq, struct ibv_srq *verbs_srq,
                                   size_t sg_byte_count)
 {
-    uct_ib_mlx5_srq_info_t srq_info;
+    uct_ib_mlx5dv_srq_t srq_info = {};
     uct_ib_mlx5_srq_seg_t *seg;
+    uct_ib_mlx5dv_t obj = {};
     ucs_status_t status;
     unsigned i;
 
-    status = uct_ib_mlx5_get_srq_info(verbs_srq, &srq_info);
+    obj.dv.srq.in = verbs_srq;
+    obj.dv.srq.out = &srq_info.dv;
+
+    status = uct_ib_mlx5dv_init_obj(&obj, MLX5DV_OBJ_SRQ);
     if (status != UCS_OK) {
         return status;
     }
 
-    if (srq_info.head != 0) {
-        ucs_error("SRQ head is not 0 (%d)", srq_info.head);
+    if (srq_info.dv.head != 0) {
+        ucs_error("SRQ head is not 0 (%d)", srq_info.dv.head);
         return UCS_ERR_NO_DEVICE;
     }
 
-    if (srq_info.stride != UCT_IB_MLX5_SRQ_STRIDE) {
+    if (srq_info.dv.stride != UCT_IB_MLX5_SRQ_STRIDE) {
         ucs_error("SRQ stride is not %lu (%d)", UCT_IB_MLX5_SRQ_STRIDE,
-                  srq_info.stride);
+                  srq_info.dv.stride);
         return UCS_ERR_NO_DEVICE;
     }
 
-    if (!ucs_is_pow2(srq_info.tail + 1)) {
-        ucs_error("SRQ length is not power of 2 (%d)", srq_info.tail + 1);
+    if (!ucs_is_pow2(srq_info.dv.tail + 1)) {
+        ucs_error("SRQ length is not power of 2 (%d)", srq_info.dv.tail + 1);
         return UCS_ERR_NO_DEVICE;
     }
 
-    srq->buf             = srq_info.buf;
-    srq->db              = srq_info.dbrec;
-    srq->free_idx        = srq_info.tail;
+    srq->buf             = srq_info.dv.buf;
+    srq->db              = srq_info.dv.dbrec;
+    srq->free_idx        = srq_info.dv.tail;
     srq->ready_idx       = -1;
     srq->sw_pi           = -1;
-    srq->mask            = srq_info.tail;
-    srq->tail            = srq_info.tail;
+    srq->mask            = srq_info.dv.tail;
+    srq->tail            = srq_info.dv.tail;
 
-    for (i = srq_info.head; i <= srq_info.tail; ++i) {
+    for (i = srq_info.dv.head; i <= srq_info.dv.tail; ++i) {
         seg = uct_ib_mlx5_srq_get_wqe(srq, i);
         seg->srq.free        = 0;
         seg->srq.desc        = NULL;
@@ -429,11 +360,15 @@ ucs_status_t uct_ib_mlx5_srq_init(uct_ib_mlx5_srq_t *srq, struct ibv_srq *verbs_
 
 void uct_ib_mlx5_srq_cleanup(uct_ib_mlx5_srq_t *srq, struct ibv_srq *verbs_srq)
 {
-    uct_ib_mlx5_srq_info_t srq_info;
+    uct_ib_mlx5dv_srq_t srq_info = {};
+    uct_ib_mlx5dv_t obj = {};
     ucs_status_t status;
 
-    status = uct_ib_mlx5_get_srq_info(verbs_srq, &srq_info);
+    obj.dv.srq.in = verbs_srq;
+    obj.dv.srq.out = &srq_info.dv;
+
+    status = uct_ib_mlx5dv_init_obj(&obj, MLX5DV_OBJ_SRQ);
     ucs_assert_always(status == UCS_OK);
-    ucs_assertv_always(srq->tail == srq_info.tail, "srq->tail=%d srq_info.tail=%d",
-                       srq->tail, srq_info.tail);
+    ucs_assertv_always(srq->tail == srq_info.dv.tail, "srq->tail=%d srq_info.tail=%d",
+                       srq->tail, srq_info.dv.tail);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.h
index 372fc68fc..10c76720f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.h
@@ -16,7 +16,30 @@
 #include <ucs/debug/log.h>
 #include <ucs/type/status.h>
 
-#include <infiniband/mlx5_hw.h>
+/**
+ * When using a clang version that is higher than 3.0, the GNUC_MINOR is set
+ * to 2, which affects the offset of several fields that are used by UCX from
+ * the liblmlx5 library (from the mlx5_qp struct).
+ * According to libmlx5, resetting the GNUC_MINOR version to 3, will make the
+ * offset of these fields inside libmlx5 (when compiled with GCC) the same as
+ * the one used by UCX (when compiled with clang).
+ */
+#ifdef __clang__
+#  define CLANG_VERSION ( __clang_major__ * 100 + __clang_minor__)
+#  if CLANG_VERSION >= 300
+#    undef __GNUC_MINOR__
+#    define __GNUC_MINOR__ 3
+#  endif
+#endif
+
+#if HAVE_INFINIBAND_MLX5DV_H
+#  include <infiniband/mlx5dv.h>
+#else
+#  include <infiniband/mlx5_hw.h>
+#  include "ib_mlx5_hw.h"
+#endif
+#include "ib_mlx5_dv.h"
+
 #include <netinet/in.h>
 #include <endian.h>
 #include <string.h>
@@ -29,10 +52,17 @@
 #define UCT_IB_MLX5_CQE128_SIZE_LOG     7
 #define UCT_IB_MLX5_MAX_BB              4
 #define UCT_IB_MLX5_WORKER_BF_KEY       0x00c1b7e8u
+#define UCT_IB_MLX5_RES_DOMAIN_KEY      0x1b1bda7aU
+#define UCT_IB_MLX5_WORKER_DM_KEY       0xacdf1245u
 #define UCT_IB_MLX5_EXTENDED_UD_AV      0x80 /* htonl(0x80000000) */
+#define UCT_IB_MLX5_AV_GRH_PRESENT      0x40 /* htonl(UCS_BIT(30)) */
 #define UCT_IB_MLX5_BF_REG_SIZE         256
 #define UCT_IB_MLX5_CQE_VENDOR_SYND_ODP 0x93
 #define UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK 0x80
+#define UCT_IB_MLX5_MAX_SEND_WQE_SIZE   (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB)
+#define UCT_IB_MLX5_CQ_SET_CI           0
+#define UCT_IB_MLX5_CQ_ARM_DB           1
+#define UCT_IB_MLX5_ROCE_SRC_PORT_MIN   0xC000
 
 
 #define UCT_IB_MLX5_OPMOD_EXT_ATOMIC(_log_arg_size) \
@@ -51,9 +81,11 @@
 /* do not use direct cast from address of reserved0 to avoid compilation warnings */
 #  define mlx5_av_grh(_av)          ((struct mlx5_grh_av *)(((char*)(_av)) + \
                                      ucs_offsetof(struct mlx5_wqe_av, reserved0[0])))
-#  define UCT_IB_MLX5_AV_BASE_SIZE  sizeof(struct mlx5_wqe_av)
+#  define UCT_IB_MLX5_AV_BASE_SIZE  ucs_offsetof(struct mlx5_wqe_av, reserved0[0])
 #  define UCT_IB_MLX5_AV_FULL_SIZE  sizeof(struct mlx5_wqe_av)
 
+#  define mlx5_base_av              mlx5_wqe_av
+
 struct mlx5_grh_av {
         uint8_t         reserved0[4];
         uint8_t         rmac[6];
@@ -72,7 +104,7 @@ struct mlx5_grh_av {
 #define UCT_IB_MLX5_AM_ZCOPY_MAX_IOV  3UL
 
 #define UCT_IB_MLX5_AM_MAX_SHORT(_av_size) \
-    (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB - \
+    (UCT_IB_MLX5_MAX_SEND_WQE_SIZE - \
      (sizeof(struct mlx5_wqe_ctrl_seg) + \
       (_av_size) + \
       sizeof(struct mlx5_wqe_inl_data_seg)))
@@ -87,6 +119,28 @@ struct mlx5_grh_av {
 #define UCT_IB_MLX5_SRQ_STRIDE   (sizeof(struct mlx5_wqe_srq_next_seg) + \
                                   sizeof(struct mlx5_wqe_data_seg))
 
+typedef enum {
+    UCT_IB_MLX5_MMIO_MODE_BF_POST,    /* BF without flush, can be used only from
+                                         one thread */
+    UCT_IB_MLX5_MMIO_MODE_BF_POST_MT, /* BF with order, can be used by multiple
+                                         serialized threads */
+    UCT_IB_MLX5_MMIO_MODE_DB,         /* 8-byte doorbell (with the mandatory flush) */
+    UCT_IB_MLX5_MMIO_MODE_AUTO,       /* Auto-select according to driver/HW capabilities
+                                         and multi-thread support level */
+    UCT_IB_MLX5_MMIO_MODE_LAST
+} uct_ib_mlx5_mmio_mode_t;
+
+
+typedef struct uct_ib_mlx5_iface_config {
+#if HAVE_IBV_EXP_DM
+    struct {
+        size_t               seg_len;
+        unsigned             count;
+    } dm;
+#endif
+    uct_ib_mlx5_mmio_mode_t  mmio_mode;
+} uct_ib_mlx5_iface_config_t;
+
 
 /* Shared receive queue */
 typedef struct uct_ib_mlx5_srq {
@@ -104,30 +158,31 @@ typedef struct uct_ib_mlx5_srq {
 typedef struct uct_ib_mlx5_cq {
     void               *cq_buf;
     unsigned           cq_ci;
+    unsigned           cq_sn;
     unsigned           cq_length;
     unsigned           cqe_size_log;
-#if ENABLE_DEBUG_DATA
     unsigned           cq_num;
-#endif
+    void               *uar;
+    volatile uint32_t  *dbrec;
 } uct_ib_mlx5_cq_t;
 
 
 /* Blue flame register */
-typedef struct uct_ib_mlx5_bf {
+typedef struct uct_ib_mlx5_mmio_reg {
     uct_worker_tl_data_t        super;
     union {
         void                    *ptr;
-        uintptr_t               addr;
-    } reg;
-    unsigned                    enable_bf;       /* BF/DB method selector. DB used if zero */
-} uct_ib_mlx5_bf_t;
+        uintptr_t               uint;
+    } addr;
+    uct_ib_mlx5_mmio_mode_t     mode;
+} uct_ib_mlx5_mmio_reg_t;
 
 
 /* Send work-queue */
 typedef struct uct_ib_mlx5_txwq {
     uint16_t                    sw_pi;      /* PI for next WQE */
     uint16_t                    prev_sw_pi; /* PI where last WQE *started*  */
-    uct_ib_mlx5_bf_t            *bf;
+    uct_ib_mlx5_mmio_reg_t      *reg;
     void                        *curr;
     volatile uint32_t           *dbrec;
     void                        *qstart;
@@ -163,6 +218,21 @@ typedef struct uct_ib_mlx5_base_av {
 } UCS_S_PACKED uct_ib_mlx5_base_av_t;
 
 
+typedef struct uct_ib_mlx5_err_cqe {
+    uint8_t                     rsvd0[32];
+    uint32_t                    srqn;
+    uint8_t                     rsvd1[16];
+    uint8_t                     hw_err_synd;
+    uint8_t                     hw_synd_type;
+    uint8_t                     vendor_err_synd;
+    uint8_t                     syndrome;
+    uint32_t                    s_wqe_opcode_qpn;
+    uint16_t                    wqe_counter;
+    uint8_t                     signature;
+    uint8_t                     op_own;
+} UCS_S_PACKED uct_ib_mlx5_err_cqe_t;
+
+
 /**
  * SRQ segment
  *
@@ -208,31 +278,29 @@ struct uct_ib_mlx5_atomic_masked_cswap64_seg {
 } UCS_S_PACKED;
 
 
-/**
- * Get internal CQ information.
- */
-ucs_status_t uct_ib_mlx5_get_cq(struct ibv_cq *cq, uct_ib_mlx5_cq_t *mlx5_cq);
+struct uct_ib_mlx5_atomic_masked_fadd64_seg {
+    uint64_t           add;
+    uint64_t           filed_boundary;
+} UCS_S_PACKED;
 
-/**
- * Update CI to support req_notify_cq
- */
-void uct_ib_mlx5_update_cq_ci(struct ibv_cq *cq, unsigned cq_ci);
 
-/**
- * Retrieve CI from the driver
- */
-unsigned uct_ib_mlx5_get_cq_ci(struct ibv_cq *cq);
+extern ucs_config_field_t uct_ib_mlx5_iface_config_table[];
 
 /**
- * Get internal AV information.
+ * Get internal CQ information.
  */
-void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av);
+ucs_status_t uct_ib_mlx5_get_cq(struct ibv_cq *cq, uct_ib_mlx5_cq_t *mlx5_cq);
 
 /**
  * Get flag indicating compact AV support.
  */
 ucs_status_t uct_ib_mlx5_get_compact_av(uct_ib_iface_t *iface, int *compact_av);
 
+/**
+ * Requests completion notification.
+ */
+int uct_ib_mlx5dv_arm_cq(uct_ib_mlx5_cq_t *cq, int solicited);
+
 /**
  * Check for completion with error.
  */
@@ -242,8 +310,9 @@ void uct_ib_mlx5_check_completion(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq,
 /**
  * Initialize txwq structure.
  */
-ucs_status_t uct_ib_mlx5_txwq_init(uct_priv_worker_t *worker, uct_ib_mlx5_txwq_t *txwq,
-                                   struct ibv_qp *verbs_qp);
+ucs_status_t uct_ib_mlx5_txwq_init(uct_priv_worker_t *worker,
+                                   uct_ib_mlx5_mmio_mode_t cfg_mmio_mode,
+                                   uct_ib_mlx5_txwq_t *txwq, struct ibv_qp *verbs_qp);
 void uct_ib_mlx5_txwq_cleanup(uct_ib_mlx5_txwq_t* txwq);
 
 /**
@@ -263,5 +332,4 @@ ucs_status_t uct_ib_mlx5_srq_init(uct_ib_mlx5_srq_t *srq, struct ibv_srq *verbs_
                                   size_t sg_byte_count);
 void uct_ib_mlx5_srq_cleanup(uct_ib_mlx5_srq_t *srq, struct ibv_srq *verbs_srq);
 
-
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.inl b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.inl
index ab802015a..3efe75930 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.inl
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5.inl
@@ -13,6 +13,12 @@ uct_ib_mlx5_get_cqe(uct_ib_mlx5_cq_t *cq,  unsigned index)
     return cq->cq_buf + ((index & (cq->cq_length - 1)) << cq->cqe_size_log);
 }
 
+static UCS_F_ALWAYS_INLINE int
+uct_ib_mlx5_cqe_is_hw_owned(uint8_t op_own, unsigned index, unsigned mask)
+{
+    return (op_own & MLX5_CQE_OWNER_MASK) == !(index & mask);
+}
+
 static UCS_F_ALWAYS_INLINE struct mlx5_cqe64*
 uct_ib_mlx5_poll_cq(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq)
 {
@@ -24,13 +30,12 @@ uct_ib_mlx5_poll_cq(uct_ib_iface_t *iface, uct_ib_mlx5_cq_t *cq)
     cqe    = uct_ib_mlx5_get_cqe(cq, index);
     op_own = cqe->op_own;
 
-    if (ucs_unlikely((op_own & MLX5_CQE_OWNER_MASK) == !(index & cq->cq_length))) {
+    if (ucs_unlikely(uct_ib_mlx5_cqe_is_hw_owned(op_own, index, cq->cq_length))) {
         return NULL;
-    } else if (ucs_unlikely(op_own & 0x80)) {
+    } else if (ucs_unlikely(op_own & UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK)) {
         UCS_STATIC_ASSERT(MLX5_CQE_INVALID & (UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK >> 4));
-        if (ucs_unlikely((op_own >> 4) != MLX5_CQE_INVALID)) {
-            uct_ib_mlx5_check_completion(iface, cq, cqe);
-        }
+        ucs_assert((op_own >> 4) != MLX5_CQE_INVALID);
+        uct_ib_mlx5_check_completion(iface, cq, cqe);
         return NULL; /* No CQE */
     }
 
@@ -196,12 +201,14 @@ uct_ib_mlx5_ep_set_rdma_seg(struct mlx5_wqe_raddr_seg *raddr, uint64_t rdma_radd
 static UCS_F_ALWAYS_INLINE void
 uct_ib_mlx5_set_dgram_seg(struct mlx5_wqe_datagram_seg *seg,
                           uct_ib_mlx5_base_av_t *av, struct mlx5_grh_av *grh_av,
-                          enum ibv_qp_type qp_type)
+                          int qp_type)
 {
     if (qp_type == IBV_QPT_UD) {
         mlx5_av_base(&seg->av)->key.qkey.qkey  = htonl(UCT_IB_KEY);
-    } else if (qp_type == IBV_EXP_QPT_DC_INI) {
+#if HAVE_TL_DC
+    } else if (qp_type == UCT_IB_QPT_DCI) {
         mlx5_av_base(&seg->av)->key.dc_key     = htobe64(UCT_IB_KEY);
+#endif
     }
     mlx5_av_base(&seg->av)->dqp_dct            = av->dqp_dct;
     mlx5_av_base(&seg->av)->stat_rate_sl       = av->stat_rate_sl;
@@ -355,20 +362,36 @@ static UCS_F_ALWAYS_INLINE void uct_ib_mlx5_bf_copy_bb(void * restrict dst,
                                                        void * restrict src)
 {
 #if defined( __SSE4_2__)
-        UCS_WORD_COPY(dst, src, __m128i, MLX5_SEND_WQE_BB);
+    UCS_WORD_COPY(__m128i, dst, __m128i, src, MLX5_SEND_WQE_BB);
 #elif defined(__ARM_NEON)
-        UCS_WORD_COPY(dst, src, int16x8_t, MLX5_SEND_WQE_BB);
+    UCS_WORD_COPY(int16x8_t, dst, int16x8_t, src, MLX5_SEND_WQE_BB);
 #else /* NO SIMD support */
-        UCS_WORD_COPY(dst, src, uint64_t, MLX5_SEND_WQE_BB);
+    UCS_WORD_COPY(uint64_t, dst, uint64_t, src, MLX5_SEND_WQE_BB);
 #endif
 }
 
+static UCS_F_ALWAYS_INLINE
+void *uct_ib_mlx5_bf_copy(void *dst, void *src, uint16_t num_bb,
+                          const uct_ib_mlx5_txwq_t *wq)
+{
+    uint16_t n;
+
+    for (n = 0; n < num_bb; ++n) {
+        uct_ib_mlx5_bf_copy_bb(dst, src);
+        dst += MLX5_SEND_WQE_BB;
+        src += MLX5_SEND_WQE_BB;
+        if (ucs_unlikely(src == wq->qend)) {
+            src = wq->qstart;
+        }
+    }
+    return src;
+}
 
 static UCS_F_ALWAYS_INLINE uint16_t
 uct_ib_mlx5_post_send(uct_ib_mlx5_txwq_t *wq,
                       struct mlx5_wqe_ctrl_seg *ctrl, unsigned wqe_size)
 {
-    uint16_t n, sw_pi, num_bb;
+    uint16_t sw_pi, num_bb, res_count;
     void *src, *dst;
 
     ucs_assert(((unsigned long)ctrl % UCT_IB_MLX5_WQE_SEG_SIZE) == 0);
@@ -386,24 +409,23 @@ uct_ib_mlx5_post_send(uct_ib_mlx5_txwq_t *wq,
     ucs_memory_bus_store_fence();
 
     /* Set up copy pointers */
-    dst = wq->bf->reg.ptr;
+    dst = wq->reg->addr.ptr;
     src = ctrl;
 
     ucs_assert(wqe_size <= UCT_IB_MLX5_BF_REG_SIZE);
     ucs_assert(num_bb <= UCT_IB_MLX5_MAX_BB);
-    if (ucs_likely(wq->bf->enable_bf)) {
-        /* BF copy */
-        for (n = 0; n < num_bb; ++n) {
-            uct_ib_mlx5_bf_copy_bb(dst, src);
-            dst += MLX5_SEND_WQE_BB;
-            src += MLX5_SEND_WQE_BB;
-            if (ucs_unlikely(src == wq->qend)) {
-                src = wq->qstart;
-            }
-        }
+    if (ucs_likely(wq->reg->mode == UCT_IB_MLX5_MMIO_MODE_BF_POST)) {
+        src = uct_ib_mlx5_bf_copy(dst, src, num_bb, wq);
+        ucs_memory_bus_wc_flush();
+    } else if (wq->reg->mode == UCT_IB_MLX5_MMIO_MODE_BF_POST_MT) {
+        src = uct_ib_mlx5_bf_copy(dst, src, num_bb, wq);
+        /* Make sure that HW observes WC writes in order, in case of multiple
+         * threads which use the same BF register in a serialized way
+         */
+        ucs_memory_cpu_wc_fence();
     } else {
-        /* DB copy */
-        *(volatile uint64_t *)dst = *(volatile uint64_t *)src;
+        ucs_assert(wq->reg->mode == UCT_IB_MLX5_MMIO_MODE_DB);
+        *(volatile uint64_t*)dst = *(volatile uint64_t*)src;
         ucs_memory_bus_store_fence();
         src = uct_ib_mlx5_txwq_wrap_any(wq, src + (num_bb * MLX5_SEND_WQE_BB));
     }
@@ -411,15 +433,22 @@ uct_ib_mlx5_post_send(uct_ib_mlx5_txwq_t *wq,
     /* We don't want the compiler to reorder instructions and hurt latency */
     ucs_compiler_fence();
 
-    /* Advance queue pointer */
+    /*
+     * Advance queue pointer.
+     * We return the number of BBs the *previous* WQE has consumed, since CQEs
+     * are reporting the index of the first BB rather than the last. We have
+     * reserved QP space for at least UCT_IB_MLX5_MAX_BB to accommodate.
+     * */
     ucs_assert(ctrl == wq->curr);
-    wq->curr       = src;
-    wq->prev_sw_pi = wq->sw_pi;
-    wq->sw_pi      = sw_pi;
+    res_count       = wq->sw_pi - wq->prev_sw_pi;
+    wq->curr        = src;
+    wq->prev_sw_pi += res_count;
+    ucs_assert(wq->prev_sw_pi == wq->sw_pi);
+    wq->sw_pi       = sw_pi;
 
     /* Flip BF register */
-    wq->bf->reg.addr ^= UCT_IB_MLX5_BF_REG_SIZE;
-    return num_bb;
+    wq->reg->addr.uint ^= UCT_IB_MLX5_BF_REG_SIZE;
+    return res_count;
 }
 
 
@@ -429,3 +458,18 @@ uct_ib_mlx5_srq_get_wqe(uct_ib_mlx5_srq_t *srq, uint16_t index)
     ucs_assert(index <= srq->mask);
     return srq->buf + index * UCT_IB_MLX5_SRQ_STRIDE;
 }
+
+static inline void uct_ib_mlx5_iface_set_av_sport(uct_ib_iface_t *iface,
+                                                  uct_ib_mlx5_base_av_t *av,
+                                                  uint32_t flow_id)
+{
+    uint16_t sport;
+
+    if (!IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface)) ||
+        (ntohs(av->rlid) >= UCT_IB_MLX5_ROCE_SRC_PORT_MIN)) {
+        return;
+    }
+
+    sport    = flow_id ^ (flow_id >> 16);
+    av->rlid = htons(UCT_IB_MLX5_ROCE_SRC_PORT_MIN | sport);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.c
new file mode 100644
index 000000000..cd1652c15
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.c
@@ -0,0 +1,123 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "ib_mlx5.h"
+#include "ib_mlx5_log.h"
+
+#if HAVE_DECL_MLX5DV_INIT_OBJ
+ucs_status_t uct_ib_mlx5dv_init_obj(uct_ib_mlx5dv_t *obj, uint64_t type)
+{
+    int ret;
+
+    ret = mlx5dv_init_obj(&obj->dv, type);
+    if (ret != 0) {
+        ucs_error("DV failed to get mlx5 information. Type %lx.", type);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    return UCS_OK;
+}
+#endif
+
+#if HAVE_DC_DV
+static ucs_status_t uct_ib_mlx5_device_init(uct_ib_device_t *dev)
+{
+    struct ibv_context *ctx = dev->ibv_context;
+    struct ibv_qp_init_attr_ex qp_attr = {};
+    struct mlx5dv_qp_init_attr dv_attr = {};
+    ucs_status_t status = UCS_OK;
+    struct ibv_pd *pd;
+    struct ibv_cq *cq;
+    struct ibv_qp *qp;
+
+    if (!(uct_ib_device_spec(dev)->flags & UCT_IB_DEVICE_FLAG_MLX5_PRM)) {
+        return UCS_OK;
+    }
+
+    pd = ibv_alloc_pd(ctx);
+    if (pd == NULL) {
+        ucs_error("ibv_alloc_pd() failed: %m");
+        return UCS_ERR_IO_ERROR;
+    }
+
+    cq = ibv_create_cq(ctx, 1, NULL, NULL, 0);
+    if (cq == NULL) {
+        ucs_error("ibv_create_cq() failed: %m");
+        status = UCS_ERR_IO_ERROR;
+        goto err_cq;
+    }
+
+    qp_attr.send_cq              = cq;
+    qp_attr.recv_cq              = cq;
+    qp_attr.cap.max_send_wr      = 1;
+    qp_attr.cap.max_send_sge     = 1;
+    qp_attr.qp_type              = IBV_QPT_DRIVER;
+    qp_attr.comp_mask            = IBV_QP_INIT_ATTR_PD;
+    qp_attr.pd                   = pd;
+
+    dv_attr.comp_mask            = MLX5DV_QP_INIT_ATTR_MASK_DC;
+    dv_attr.dc_init_attr.dc_type = MLX5DV_DCTYPE_DCI;
+
+    /* create DCI qp successful means DC is supported */
+    qp = mlx5dv_create_qp(ctx, &qp_attr, &dv_attr);
+    if (qp) {
+        ibv_destroy_qp(qp);
+        dev->flags |= UCT_IB_DEVICE_FLAG_DC;
+    }
+
+    ibv_destroy_cq(cq);
+err_cq:
+    ibv_dealloc_pd(pd);
+    return status;
+}
+
+UCT_IB_DEVICE_INIT(uct_ib_mlx5_device_init);
+
+#endif
+
+int uct_ib_mlx5dv_arm_cq(uct_ib_mlx5_cq_t *cq, int solicited)
+{
+    uint64_t doorbell, sn_ci_cmd;
+    uint32_t sn, ci, cmd;
+
+    sn  = cq->cq_sn & 3;
+    ci  = cq->cq_ci & 0xffffff;
+    cmd = solicited ? MLX5_CQ_DB_REQ_NOT_SOL : MLX5_CQ_DB_REQ_NOT;
+    sn_ci_cmd = (sn << 28) | cmd | ci;
+
+    cq->dbrec[UCT_IB_MLX5_CQ_ARM_DB] = htobe32(sn_ci_cmd);
+
+    ucs_memory_cpu_fence();
+
+    doorbell = (sn_ci_cmd << 32) | cq->cq_num;
+
+    *(uint64_t *)((uint8_t *)cq->uar + MLX5_CQ_DOORBELL) = htobe64(doorbell);
+
+    ucs_memory_bus_store_fence();
+
+    return 0;
+}
+
+#if HAVE_DECL_MLX5DV_OBJ_AH
+void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av)
+{
+    struct mlx5dv_obj  dv;
+    struct mlx5dv_ah   dah;
+
+    dv.ah.in = ah;
+    dv.ah.out = &dah;
+    mlx5dv_init_obj(&dv, MLX5DV_OBJ_AH);
+
+    *av = *(dah.av);
+    av->dqp_dct |= UCT_IB_MLX5_EXTENDED_UD_AV;
+}
+#elif !HAVE_INFINIBAND_MLX5_HW_H
+void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av)
+{
+    ucs_bug("MLX5DV_OBJ_AH not supported");
+}
+#endif
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.h
new file mode 100644
index 000000000..67fbb0f2f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_dv.h
@@ -0,0 +1,61 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#ifndef UCT_IB_MLX5_DV_H_
+#define UCT_IB_MLX5_DV_H_
+
+#ifndef UCT_IB_MLX5_H_
+#  error "Never include <uct/ib/mlx5/ib_mlx5_dv.h> directly; use <uct/ib/mlx5/ib_mlx5.h> instead."
+#endif
+
+#include <ucs/type/status.h>
+#include <infiniband/verbs.h>
+
+typedef struct {
+    struct mlx5dv_obj  dv;
+} uct_ib_mlx5dv_t;
+
+typedef struct {
+    struct mlx5dv_qp   dv;
+} uct_ib_mlx5dv_qp_t;
+
+typedef struct {
+    struct mlx5dv_srq  dv;
+} uct_ib_mlx5dv_srq_t;
+
+/* Completion queue */
+typedef struct {
+    struct mlx5dv_cq   dv;
+} uct_ib_mlx5dv_cq_t;
+
+/**
+ * Get internal verbs information.
+ */
+ucs_status_t uct_ib_mlx5dv_init_obj(uct_ib_mlx5dv_t *obj, uint64_t type);
+
+/**
+ * Update CI to support req_notify_cq
+ */
+void uct_ib_mlx5_update_cq_ci(struct ibv_cq *cq, unsigned cq_ci);
+
+/**
+ * Retrieve CI from the driver
+ */
+unsigned uct_ib_mlx5_get_cq_ci(struct ibv_cq *cq);
+
+/**
+ * Get internal AV information.
+ */
+void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av);
+
+/**
+ * Backports for legacy bare-metal support
+ */
+struct ibv_qp *uct_dv_get_cmd_qp(struct ibv_srq *srq);
+
+void *uct_dv_get_info_uar0(void *uar);
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.c
new file mode 100644
index 000000000..bde4ce007
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.c
@@ -0,0 +1,239 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+#ifdef HAVE_CONFIG_H
+#  include "config.h"
+#endif
+
+#if HAVE_INFINIBAND_MLX5_HW_H
+
+#include "ib_mlx5_hw.h"
+
+#include <ucs/debug/log.h>
+#include <ucs/debug/assert.h>
+#include <ucs/arch/bitops.h>
+#include <uct/ib/base/ib_verbs.h>
+#include <infiniband/mlx5_hw.h>
+#include <string.h>
+
+/* Since this file intended to emulate DV using legacy mlx5_hw.h definitions
+ * we include DV declarations. */
+#define UCT_IB_MLX5_H_
+#include "ib_mlx5_dv.h"
+
+static void UCS_F_MAYBE_UNUSED uct_ib_mlx5_obj_error(const char *obj_name)
+{
+    ucs_error("Failed to get mlx5 %s information. Please make sure the installed "
+              "libmlx5 version matches the one UCX was compiled with (%s)",
+              obj_name, LIB_MLX5_VER);
+}
+
+#if !HAVE_DECL_MLX5DV_INIT_OBJ
+ucs_status_t uct_ib_mlx5_get_qp_info(struct ibv_qp *qp, uct_ib_mlx5dv_qp_t *qp_info)
+{
+#if HAVE_DECL_IBV_MLX5_EXP_GET_QP_INFO
+    struct ibv_mlx5_qp_info ibv_qp_info;
+    int ret;
+
+    ret = ibv_mlx5_exp_get_qp_info(qp, &ibv_qp_info);
+    if (ret != 0) {
+        uct_ib_mlx5_obj_error("qp");
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    qp_info->dv.dbrec      = ibv_qp_info.dbrec;
+    qp_info->dv.sq.buf     = ibv_qp_info.sq.buf;
+    qp_info->dv.sq.wqe_cnt = ibv_qp_info.sq.wqe_cnt;
+    qp_info->dv.sq.stride  = ibv_qp_info.sq.stride;
+    qp_info->dv.rq.buf     = ibv_qp_info.rq.buf;
+    qp_info->dv.rq.wqe_cnt = ibv_qp_info.rq.wqe_cnt;
+    qp_info->dv.rq.stride  = ibv_qp_info.rq.stride;
+    qp_info->dv.bf.reg     = ibv_qp_info.bf.reg;
+    qp_info->dv.bf.size    = ibv_qp_info.bf.size;
+#else
+    struct mlx5_qp *mqp = ucs_container_of(qp, struct mlx5_qp, verbs_qp.qp);
+
+    if ((mqp->sq.cur_post != 0) || (mqp->rq.head != 0)) {
+        ucs_warn("cur_post=%d head=%d need_lock=%d", mqp->sq.cur_post,
+                 mqp->rq.head, mqp->bf->need_lock);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    qp_info->dv.qpn        = qp->qp_num;
+    qp_info->dv.dbrec      = mqp->db;
+    qp_info->dv.sq.buf     = mqp->buf.buf + mqp->sq.offset;
+    qp_info->dv.sq.wqe_cnt = mqp->sq.wqe_cnt;
+    qp_info->dv.sq.stride  = 1 << mqp->sq.wqe_shift;
+    qp_info->dv.rq.buf     = mqp->buf.buf + mqp->rq.offset;
+    qp_info->dv.rq.wqe_cnt = mqp->rq.wqe_cnt;
+    qp_info->dv.rq.stride  = 1 << mqp->rq.wqe_shift;
+    qp_info->dv.bf.reg     = mqp->bf->reg;
+
+    if (mqp->bf->uuarn > 0) {
+        qp_info->dv.bf.size = mqp->bf->buf_size;
+    } else {
+        qp_info->dv.bf.size = 0; /* No BF */
+    }
+#endif
+    return UCS_OK;
+}
+
+ucs_status_t uct_ib_mlx5_get_srq_info(struct ibv_srq *srq,
+                                      uct_ib_mlx5dv_srq_t *srq_info)
+{
+#if HAVE_DECL_IBV_MLX5_EXP_GET_SRQ_INFO
+    struct ibv_mlx5_srq_info ibv_srq_info;
+    int ret;
+
+    ret = ibv_mlx5_exp_get_srq_info(srq, &ibv_srq_info);
+    if (ret != 0) {
+        uct_ib_mlx5_obj_error("srq");
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    srq_info->dv.buf    = ibv_srq_info.buf;
+    srq_info->dv.dbrec  = ibv_srq_info.dbrec;
+    srq_info->dv.stride = ibv_srq_info.stride;
+    srq_info->dv.head   = ibv_srq_info.head;
+    srq_info->dv.tail   = ibv_srq_info.tail;
+#else
+    struct mlx5_srq *msrq;
+
+    if (srq->handle == LEGACY_XRC_SRQ_HANDLE) {
+        srq = (struct ibv_srq *)(((struct ibv_srq_legacy *)srq)->ibv_srq);
+    }
+
+    msrq = ucs_container_of(srq, struct mlx5_srq, vsrq.srq);
+
+    if (msrq->counter != 0) {
+        ucs_error("SRQ counter is not 0 (%d)", msrq->counter);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    srq_info->dv.buf    = msrq->buf.buf;
+    srq_info->dv.dbrec  = msrq->db;
+    srq_info->dv.stride = 1 << msrq->wqe_shift;
+    srq_info->dv.head   = msrq->head;
+    srq_info->dv.tail   = msrq->tail;
+#endif
+    return UCS_OK;
+}
+
+static ucs_status_t uct_ib_mlx5_get_cq(struct ibv_cq *cq, uct_ib_mlx5dv_cq_t *mlx5_cq)
+{
+#if HAVE_DECL_IBV_MLX5_EXP_GET_CQ_INFO
+    struct ibv_mlx5_cq_info ibv_cq_info;
+    int ret;
+
+    ret = ibv_mlx5_exp_get_cq_info(cq, &ibv_cq_info);
+    if (ret != 0) {
+        uct_ib_mlx5_obj_error("cq");
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    mlx5_cq->dv.buf      = ibv_cq_info.buf;
+    mlx5_cq->dv.cqe_cnt  = ibv_cq_info.cqe_cnt;
+    mlx5_cq->dv.cqn      = ibv_cq_info.cqn;
+    mlx5_cq->dv.cqe_size = ibv_cq_info.cqe_size;
+#else
+    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
+    int ret;
+
+    if (mcq->cons_index != 0) {
+        ucs_error("CQ consumer index is not 0 (%d)", mcq->cons_index);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    mlx5_cq->dv.buf      = mcq->active_buf->buf;
+    mlx5_cq->dv.cqe_cnt  = mcq->ibv_cq.cqe + 1;
+    mlx5_cq->dv.cqn      = mcq->cqn;
+    mlx5_cq->dv.cqe_size = mcq->cqe_sz;
+#endif
+    return UCS_OK;
+}
+
+ucs_status_t uct_ib_mlx5dv_init_obj(uct_ib_mlx5dv_t *obj, uint64_t obj_type)
+{
+    ucs_status_t ret = UCS_OK;
+
+    if (obj_type & MLX5DV_OBJ_QP) {
+        ret = uct_ib_mlx5_get_qp_info(obj->dv.qp.in,
+                ucs_container_of(obj->dv.qp.out, uct_ib_mlx5dv_qp_t, dv));
+    }
+
+    if (!ret && (obj_type & MLX5DV_OBJ_CQ)) {
+        ret = uct_ib_mlx5_get_cq(obj->dv.cq.in,
+                ucs_container_of(obj->dv.cq.out, uct_ib_mlx5dv_cq_t, dv));
+    }
+
+    if (!ret && (obj_type & MLX5DV_OBJ_SRQ)) {
+        ret = uct_ib_mlx5_get_srq_info(obj->dv.srq.in,
+                ucs_container_of(obj->dv.srq.out, uct_ib_mlx5dv_srq_t, dv));
+    }
+
+    return ret;
+}
+#endif
+
+void uct_ib_mlx5_update_cq_ci(struct ibv_cq *cq, unsigned cq_ci)
+{
+#if HAVE_DECL_IBV_MLX5_EXP_UPDATE_CQ_CI
+    ibv_mlx5_exp_update_cq_ci(cq, cq_ci);
+#else
+    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
+    mcq->cons_index = cq_ci;
+#endif
+}
+
+unsigned uct_ib_mlx5_get_cq_ci(struct ibv_cq *cq)
+{
+    struct mlx5_cq *mcq = ucs_container_of(cq, struct mlx5_cq, ibv_cq);
+    return mcq->cons_index;
+}
+
+#if !HAVE_DECL_MLX5DV_OBJ_AH
+void uct_ib_mlx5_get_av(struct ibv_ah *ah, struct mlx5_wqe_av *av)
+{
+    memcpy(av, &ucs_container_of(ah, struct mlx5_ah, ibv_ah)->av, sizeof(*av));
+}
+#endif
+
+struct ibv_qp *uct_dv_get_cmd_qp(struct ibv_srq *srq)
+{
+#if HAVE_STRUCT_MLX5_SRQ_CMD_QP
+    struct mlx5_srq *msrq;
+
+    if (srq->handle == LEGACY_XRC_SRQ_HANDLE) {
+        srq = (struct ibv_srq *)(((struct ibv_srq_legacy *)srq)->ibv_srq);
+    }
+
+    msrq = ucs_container_of(srq, struct mlx5_srq, vsrq.srq);
+    if (msrq->counter != 0) {
+        ucs_error("SRQ counter is not 0 (%d)", msrq->counter);
+        return NULL;
+    }
+
+    return &msrq->cmd_qp->verbs_qp.qp;
+#else
+    return NULL;
+#endif
+}
+
+struct mlx5_uar_data {
+    enum { __DUMMY }            map_type;
+    void                        *regs;
+};
+
+void *uct_dv_get_info_uar0(void *uar)
+{
+#if HAVE_DECL_MLX5DV_INIT_OBJ
+    struct mlx5_uar_data *muar = uar;
+    return muar[0].regs;
+#else
+    return NULL;
+#endif
+}
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.h
new file mode 100644
index 000000000..320e46a05
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_hw.h
@@ -0,0 +1,79 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#ifndef UCT_IB_MLX5_HW_H_
+#define UCT_IB_MLX5_HW_H_
+
+#include <stdint.h>
+
+struct mlx5dv_qp {
+    volatile uint32_t   *dbrec;
+    struct {
+        void            *buf;
+        uint32_t        wqe_cnt;
+        uint32_t        stride;
+    } sq;
+    struct {
+        void            *buf;
+        uint32_t        wqe_cnt;
+        uint32_t        stride;
+    } rq;
+    struct {
+        void            *reg;
+        uint32_t        size;
+    } bf;
+    uint64_t            comp_mask;
+};
+
+struct mlx5dv_cq {
+    void                *buf;
+    volatile uint32_t   *dbrec;
+    uint32_t            cqe_cnt;
+    uint32_t            cqe_size;
+    void                *cq_uar;
+/* DV backport will behave as DV with fixed CQ UAR */
+#undef HAVE_STRUCT_MLX5DV_CQ_CQ_UAR
+#define HAVE_STRUCT_MLX5DV_CQ_CQ_UAR 1
+    uint32_t            cqn;
+    uint64_t            comp_mask;
+};
+
+struct mlx5dv_srq {
+    void                *buf;
+    volatile uint32_t   *dbrec;
+    uint32_t            stride;
+    uint32_t            head;
+    uint32_t            tail;
+    uint64_t            comp_mask;
+};
+
+struct mlx5dv_obj {
+    struct {
+        struct ibv_qp           *in;
+        struct mlx5dv_qp        *out;
+    } qp;
+    struct {
+        struct ibv_cq           *in;
+        struct mlx5dv_cq        *out;
+    } cq;
+    struct {
+        struct ibv_srq          *in;
+        struct mlx5dv_srq       *out;
+    } srq;
+    struct {
+        struct ibv_exp_wq       *in;
+        struct mlx5dv_rwq       *out;
+    } rwq;
+};
+
+enum mlx5dv_obj_type {
+    MLX5DV_OBJ_QP   = 1 << 0,
+    MLX5DV_OBJ_CQ   = 1 << 1,
+    MLX5DV_OBJ_SRQ  = 1 << 2,
+    MLX5DV_OBJ_RWQ  = 1 << 3,
+};
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.c
index 02eff31f3..3bb6ebd4a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.c
@@ -10,7 +10,7 @@
 #include <string.h>
 
 
-static const char *uct_ib_mlx5_cqe_err_opcode(struct mlx5_err_cqe *ecqe)
+static const char *uct_ib_mlx5_cqe_err_opcode(uct_ib_mlx5_err_cqe_t *ecqe)
 {
     uint8_t wqe_err_opcode = ntohl(ecqe->s_wqe_opcode_qpn) >> 24;
 
@@ -27,11 +27,11 @@ static const char *uct_ib_mlx5_cqe_err_opcode(struct mlx5_err_cqe *ecqe)
         case MLX5_OPCODE_RDMA_READ:
             return "RDMA_READ";
         case MLX5_OPCODE_ATOMIC_CS:
-            return "COMPARE_SWAP";
+            return "CSWAP";
         case MLX5_OPCODE_ATOMIC_FA:
             return "FETCH_ADD";
         case MLX5_OPCODE_ATOMIC_MASKED_CS:
-            return "MASKED_COMPARE_SWAP";
+            return "MASKED_CSWAP";
         case MLX5_OPCODE_ATOMIC_MASKED_FA:
             return "MASKED_FETCH_ADD";
         default:
@@ -44,7 +44,8 @@ static const char *uct_ib_mlx5_cqe_err_opcode(struct mlx5_err_cqe *ecqe)
     }
 }
 
-ucs_status_t uct_ib_mlx5_completion_with_err(struct mlx5_err_cqe *ecqe,
+ucs_status_t uct_ib_mlx5_completion_with_err(uct_ib_iface_t *iface,
+                                             uct_ib_mlx5_err_cqe_t *ecqe,
                                              ucs_log_level_t log_level)
 {
     uint16_t     wqe_counter;
@@ -108,8 +109,10 @@ ucs_status_t uct_ib_mlx5_completion_with_err(struct mlx5_err_cqe *ecqe,
         break;
     }
 
-    ucs_log(log_level, "Error on QP 0x%x wqe[%d]: %s (synd 0x%x vend 0x%x) opcode %s",
-            qp_num, wqe_counter, info, ecqe->syndrome, ecqe->vendor_err_synd,
+    ucs_log(log_level, "Error on "UCT_IB_IFACE_FMT" QP 0x%x wqe[%03d]: "
+            "%s (synd 0x%x vend 0x%x hw_synd %d/%d) opcode %s",
+            UCT_IB_IFACE_ARG(iface), qp_num, wqe_counter, info, ecqe->syndrome,
+            ecqe->vendor_err_synd, ecqe->hw_synd_type >> 4, ecqe->hw_err_synd,
             uct_ib_mlx5_cqe_err_opcode(ecqe));
     return status;
 }
@@ -174,25 +177,67 @@ static uint64_t network_to_host(void *ptr, int size)
         return *(uint64_t*)ptr;
     }
 }
-static size_t uct_ib_mlx5_dump_dgram(char *buf, size_t max, void *seg)
+static size_t uct_ib_mlx5_dump_dgram(char *buf, size_t max, void *seg, int is_eth)
 {
     struct mlx5_wqe_datagram_seg *dgseg = seg;
-
-    snprintf(buf, max-1, " [dlid %d rqpn 0x%x]",
-             ntohs(mlx5_av_base(&dgseg->av)->rlid),
-             ntohl(mlx5_av_base(&dgseg->av)->dqp_dct & ~UCT_IB_MLX5_EXTENDED_UD_AV));
+    struct mlx5_base_av *base_av;
+    struct mlx5_grh_av *grh_av;
+    char gid_buf[32];
+    int sgid_index;
+    char *p, *endp;
+
+    p       = buf;
+    endp    = buf + max - 1;
+    base_av = mlx5_av_base(&dgseg->av);
+
+    snprintf(p, endp - p, " [rqpn 0x%x",
+             ntohl(base_av->dqp_dct & ~UCT_IB_MLX5_EXTENDED_UD_AV));
+    p += strlen(p);
+
+    if (!is_eth) {
+        snprintf(p, endp - p, " rlid %d", ntohs(base_av->rlid));
+        p += strlen(p);
+    }
 
     if (mlx5_av_base(&dgseg->av)->dqp_dct & UCT_IB_MLX5_EXTENDED_UD_AV) {
+        grh_av = mlx5_av_grh(&dgseg->av);
+        if (is_eth || (grh_av->grh_gid_fl & UCT_IB_MLX5_AV_GRH_PRESENT)) {
+            if (is_eth) {
+                snprintf(p, endp - p, " rmac %02x:%02x:%02x:%02x:%02x:%02x",
+                         grh_av->rmac[0], grh_av->rmac[1], grh_av->rmac[2],
+                         grh_av->rmac[3], grh_av->rmac[4], grh_av->rmac[5]);
+                p += strlen(p);
+            }
+
+            sgid_index = (htonl(grh_av->grh_gid_fl) >> 20) & UCS_MASK(8);
+            snprintf(p, endp - p,  " sgix %d dgid %s tc %d]", sgid_index,
+                     inet_ntop(AF_INET6, grh_av->rgid, gid_buf, sizeof(gid_buf)),
+                     grh_av->tclass);
+        }
         return UCT_IB_MLX5_AV_FULL_SIZE;
     } else {
+        snprintf(p, endp - p, "]");
         return UCT_IB_MLX5_AV_BASE_SIZE;
     }
 }
 
-static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
-                                 void *wqe, void *qstart, void *qend,
+static int uct_ib_mlx5_is_qp_require_av_seg(int qp_type)
+{
+    if (qp_type == IBV_QPT_UD) {
+        return 1;
+    }
+#if HAVE_TL_DC
+    if (qp_type == UCT_IB_QPT_DCI) {
+        return 1;
+    }
+#endif
+    return 0;
+}
+
+static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, int qp_type,
+                                 void *wqe, void *qstart, void *qend, int max_sge,
                                  uct_log_data_dump_func_t packet_dump_cb,
-                                 char *buffer, size_t max)
+                                 char *buffer, size_t max, uct_ib_log_sge_t *log_sge)
 {
     static uct_ib_opcode_t opcodes[] = {
         [MLX5_OPCODE_NOP]              = { "NOP",        0 },
@@ -200,10 +245,12 @@ static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, enum ibv_qp_type qp_type
         [MLX5_OPCODE_RDMA_READ]        = { "RDMA_READ",  UCT_IB_OPCODE_FLAG_HAS_RADDR },
         [MLX5_OPCODE_SEND]             = { "SEND",       0 },
         [MLX5_OPCODE_SEND_IMM]         = { "SEND_IMM",   0 },
-        [MLX5_OPCODE_ATOMIC_CS]        = { "CS",         UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
-        [MLX5_OPCODE_ATOMIC_FA]        = { "FA",         UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
-        [MLX5_OPCODE_ATOMIC_MASKED_CS] = { "MASKED_CS",  UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
-        [MLX5_OPCODE_ATOMIC_MASKED_FA] = { "MASKED_FA",  UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
+        [MLX5_OPCODE_ATOMIC_CS]        = { "CSWAP",      UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [MLX5_OPCODE_ATOMIC_FA]        = { "FETCH_ADD",  UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_ATOMIC },
+        [MLX5_OPCODE_ATOMIC_MASKED_CS] = { "MASKED_CSWAP",
+                                           UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
+        [MLX5_OPCODE_ATOMIC_MASKED_FA] = { "MASKED_FETCH_ADD",
+                                           UCT_IB_OPCODE_FLAG_HAS_RADDR|UCT_IB_OPCODE_FLAG_HAS_EXT_ATOMIC },
    };
 
     struct mlx5_wqe_ctrl_seg *ctrl = wqe;
@@ -216,11 +263,12 @@ static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, enum ibv_qp_type qp_type
     char *ends          = buffer + max;
     struct ibv_sge sg_list[16];
     uint64_t inline_bitmap;
-    int i, is_inline;
+    int i, is_inline, is_eth;
+    size_t dg_size;
     void *seg;
 
     /* QP number and opcode name */
-    uct_ib_log_dump_opcode(qp_num, op,
+    uct_ib_log_dump_opcode(qp_num, (wqe - qstart) / MLX5_SEND_WQE_BB, op,
                            ctrl->fm_ce_se & MLX5_WQE_CTRL_CQ_UPDATE,
                            ctrl->fm_ce_se & MLX5_WQE_CTRL_FENCE,
                            ctrl->fm_ce_se & (1 << 1),
@@ -234,8 +282,9 @@ static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, enum ibv_qp_type qp_type
         seg = qstart;
     }
 
-    if ((qp_type == IBV_QPT_UD) || (qp_type == IBV_EXP_QPT_DC_INI)) {
-        size_t dg_size = uct_ib_mlx5_dump_dgram(s, ends - s, seg);
+    if (uct_ib_mlx5_is_qp_require_av_seg(qp_type)) {
+        is_eth = IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface));
+        dg_size = uct_ib_mlx5_dump_dgram(s, ends - s, seg, is_eth);
         s += strlen(s);
 
         seg = (char *)seg + dg_size;
@@ -314,29 +363,35 @@ static void uct_ib_mlx5_wqe_dump(uct_ib_iface_t *iface, enum ibv_qp_type qp_type
     }
 
     /* Data segments*/
-    i = 0;
-    inline_bitmap = 0;
-
-    while ((ds > 0) && (i < sizeof(sg_list) / sizeof(sg_list[0]))) {
-        ds -= uct_ib_mlx5_parse_dseg(&seg, qstart, qend, sg_list, &i, &is_inline);
-        if (is_inline) {
-            inline_bitmap |= UCS_BIT(i-1);
+    if (log_sge == NULL) {
+        i = 0;
+        inline_bitmap = 0;
+
+        while ((ds > 0) && (i < sizeof(sg_list) / sizeof(sg_list[0]))) {
+            ds -= uct_ib_mlx5_parse_dseg(&seg, qstart, qend, sg_list, &i, &is_inline);
+            if (is_inline) {
+                inline_bitmap |= UCS_BIT(i-1);
+            }
+            s += strlen(s);
         }
-        s += strlen(s);
     }
 
-    uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND, sg_list, i,
-                            inline_bitmap, packet_dump_cb, s, ends - s);
+    uct_ib_log_dump_sg_list(iface, UCT_AM_TRACE_TYPE_SEND,
+                            log_sge ? log_sge->sg_list : sg_list,
+                            log_sge ? log_sge->num_sge : ucs_min(i, max_sge),
+                            log_sge ? log_sge->inline_bitmap : inline_bitmap,
+                            packet_dump_cb, s, ends - s);
 }
 
 void __uct_ib_mlx5_log_tx(const char *file, int line, const char *function,
-                          uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
-                          void *wqe, void *qstart, void *qend,
+                          uct_ib_iface_t *iface, int qp_type,
+                          void *wqe, void *qstart, void *qend, int max_sge,
+                          uct_ib_log_sge_t *log_sge,
                           uct_log_data_dump_func_t packet_dump_cb)
 {
     char buf[256] = {0};
-    uct_ib_mlx5_wqe_dump(iface, qp_type, wqe, qstart, qend, packet_dump_cb,
-                         buf, sizeof(buf) - 1);
+    uct_ib_mlx5_wqe_dump(iface, qp_type, wqe, qstart, qend, max_sge, packet_dump_cb,
+                         buf, sizeof(buf) - 1, log_sge);
     uct_log_data(file, line, function, buf);
 }
 
@@ -357,7 +412,7 @@ void uct_ib_mlx5_cqe_dump(const char *file, int line, const char *function, stru
 }
 
 void __uct_ib_mlx5_log_rx(const char *file, int line, const char *function,
-                          uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+                          uct_ib_iface_t *iface, int qp_type,
                           struct mlx5_cqe64 *cqe, void *data,
                           uct_log_data_dump_func_t packet_dump_cb)
 {
@@ -365,7 +420,7 @@ void __uct_ib_mlx5_log_rx(const char *file, int line, const char *function,
     size_t length;
 
     length = ntohl(cqe->byte_cnt);
-    if ((qp_type == IBV_QPT_UD) || (qp_type == IBV_EXP_QPT_DC_INI)) {
+    if (uct_ib_mlx5_is_qp_require_av_seg(qp_type)) {
         length -= UCT_IB_GRH_LEN;
         data   += UCT_IB_GRH_LEN;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.h
index 7ed8e5d59..ebdf2d40b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/mlx5/ib_mlx5_log.h
@@ -12,27 +12,35 @@
 #include <uct/base/uct_log.h>
 
 
-ucs_status_t uct_ib_mlx5_completion_with_err(struct mlx5_err_cqe *ecqe,
+typedef struct uct_ib_log_sge {
+    int            num_sge;
+    uint64_t       inline_bitmap;
+    struct ibv_sge sg_list[2];
+} uct_ib_log_sge_t;
+
+ucs_status_t uct_ib_mlx5_completion_with_err(uct_ib_iface_t *iface,
+                                             uct_ib_mlx5_err_cqe_t *ecqe,
                                              ucs_log_level_t log_level);
 
 
 void __uct_ib_mlx5_log_tx(const char *file, int line, const char *function,
-                          uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
-                          void *wqe, void *qstart, void *qend,
+                          uct_ib_iface_t *iface, int qp_type,
+                          void *wqe, void *qstart, void *qend, int max_log_sge,
+                          uct_ib_log_sge_t *log_sge,
                           uct_log_data_dump_func_t packet_dump_cb);
 
 void __uct_ib_mlx5_log_rx(const char *file, int line, const char *function,
-                          uct_ib_iface_t *iface, enum ibv_qp_type qp_type,
+                          uct_ib_iface_t *iface, int qp_type,
                           struct mlx5_cqe64 *cqe, void *data,
                           uct_log_data_dump_func_t packet_dump_cb);
 
 void uct_ib_mlx5_cqe_dump(const char *file, int line, const char *function,
                           struct mlx5_cqe64 *cqe);
 
-#define uct_ib_mlx5_log_tx(_iface, _qpt, _wqe, _qstart, _qend, _dump_cb) \
+#define uct_ib_mlx5_log_tx(_iface, _qpt, _wqe, _qstart, _qend, _max_sge, _log_sge, _dump_cb) \
     if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) { \
         __uct_ib_mlx5_log_tx(__FILE__, __LINE__, __FUNCTION__, \
-                             _iface, _qpt, _wqe, _qstart, _qend, _dump_cb); \
+                             _iface, _qpt, _wqe, _qstart, _qend, _max_sge, _log_sge, _dump_cb); \
     }
 
 #define uct_ib_mlx5_log_rx(_iface, _qpt, _cqe, _data, _dump_cb) \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5.h
index a057456ae..4e3c87c14 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5.h
@@ -19,9 +19,10 @@
  * RC mlx5 interface configuration
  */
 typedef struct uct_rc_mlx5_iface_config {
-    uct_rc_iface_config_t  super;
-    uct_rc_fc_config_t     fc;
-    unsigned               tx_max_bb;
+    uct_rc_iface_config_t             super;
+    uct_rc_fc_config_t                fc;
+    uct_ib_mlx5_iface_config_t        mlx5_common;
+    unsigned                          tx_max_bb;
     /* TODO wc_mode, UAR mode SnB W/A... */
 } uct_rc_mlx5_iface_config_t;
 
@@ -47,7 +48,8 @@ typedef struct {
     uct_rc_iface_t              super;
     uct_rc_mlx5_iface_common_t  mlx5_common;
     struct {
-        uint16_t           bb_max;     /* limit number of outstanding WQE BBs */
+        uct_ib_mlx5_mmio_mode_t mmio_mode;
+        uint16_t                bb_max;     /* limit number of outstanding WQE BBs */
     } tx;
 } uct_rc_mlx5_iface_t;
 
@@ -88,35 +90,29 @@ ucs_status_t uct_rc_mlx5_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *hea
                                      size_t iovcnt, unsigned flags,
                                      uct_completion_t *comp);
 
-ucs_status_t uct_rc_mlx5_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey);
-
-ucs_status_t uct_rc_mlx5_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp);
-
-ucs_status_t uct_rc_mlx5_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp);
-
 ucs_status_t uct_rc_mlx5_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
                                            uint64_t remote_addr, uct_rkey_t rkey,
                                            uint64_t *result, uct_completion_t *comp);
 
-ucs_status_t uct_rc_mlx5_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_rc_mlx5_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uint32_t *result, uct_completion_t *comp);
+
+ucs_status_t uct_rc_mlx5_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey);
 
-ucs_status_t uct_rc_mlx5_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp);
+ucs_status_t uct_rc_mlx5_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey);
 
-ucs_status_t uct_rc_mlx5_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp);
+ucs_status_t uct_rc_mlx5_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint64_t value, uint64_t *result,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uct_completion_t *comp);
 
-ucs_status_t uct_rc_mlx5_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+ucs_status_t uct_rc_mlx5_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint32_t value, uint32_t *result,
                                            uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp);
+                                           uct_completion_t *comp);
 
 ucs_status_t uct_rc_mlx5_ep_flush(uct_ep_h tl_ep, unsigned flags, uct_completion_t *comp);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.c
index adfb20d25..112fb6f2d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.c
@@ -22,6 +22,24 @@ ucs_stats_class_t uct_rc_mlx5_iface_stats_class = {
 };
 #endif
 
+
+#if HAVE_IBV_EXP_DM
+/* uct_mlx5_dm_va is used to get pointer to DM mapped into process address space */
+typedef struct uct_mlx5_dm_va {
+    struct ibv_exp_dm  ibv_dm;
+    size_t             length;
+    uint64_t           *start_va;
+} uct_mlx5_dm_va_t;
+#endif
+
+
+void uct_rc_mlx5_common_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                                    void *data, size_t length, size_t valid_length,
+                                    char *buffer, size_t max)
+{
+    uct_rc_ep_packet_dump(iface, type, data, length, valid_length, buffer, max, 0);
+}
+
 unsigned uct_rc_mlx5_iface_srq_post_recv(uct_rc_iface_t *iface, uct_ib_mlx5_srq_t *srq)
 {
     uct_ib_mlx5_srq_seg_t *seg;
@@ -67,6 +85,8 @@ unsigned uct_rc_mlx5_iface_srq_post_recv(uct_rc_iface_t *iface, uct_ib_mlx5_srq_
     }
 
     count = index - srq->sw_pi;
+    ucs_assert(iface->rx.srq.available >= count);
+
     if (count > 0) {
         srq->ready_idx           = index;
         srq->sw_pi               = index;
@@ -115,13 +135,13 @@ ucs_status_t
 uct_rc_mlx5_iface_common_tag_init(uct_rc_mlx5_iface_common_t *iface,
                                   uct_rc_iface_t *rc_iface,
                                   uct_rc_iface_config_t *rc_config,
+                                  const uct_ib_mlx5_iface_config_t *mlx5_config,
                                   struct ibv_exp_create_srq_attr *srq_init_attr,
                                   unsigned rndv_hdr_len)
 {
     ucs_status_t status = UCS_OK;
 #if IBV_EXP_HW_TM
-    struct ibv_srq *srq;
-    struct mlx5_srq *msrq;
+    struct ibv_qp *cmd_qp;
     int i;
 
     if (!UCT_RC_IFACE_TM_ENABLED(rc_iface)) {
@@ -134,26 +154,20 @@ uct_rc_mlx5_iface_common_tag_init(uct_rc_mlx5_iface_common_t *iface,
         goto err;
     }
 
-    srq = rc_iface->rx.srq.srq;
-    if (srq->handle == LEGACY_XRC_SRQ_HANDLE) {
-        srq = (struct ibv_srq *)(((struct ibv_srq_legacy *)srq)->ibv_srq);
-    }
-
-    msrq = ucs_container_of(srq, struct mlx5_srq, vsrq.srq);
-    if (msrq->counter != 0) {
-        ucs_error("SRQ counter is not 0 (%d)", msrq->counter);
+    cmd_qp = uct_dv_get_cmd_qp(rc_iface->rx.srq.srq);
+    if (!cmd_qp) {
         status = UCS_ERR_NO_DEVICE;
         goto err_tag_cleanup;
     }
 
     status = uct_ib_mlx5_txwq_init(rc_iface->super.super.worker,
-                                   &iface->tm.cmd_wq.super,
-                                   &msrq->cmd_qp->verbs_qp.qp);
+                                   mlx5_config->mmio_mode,
+                                   &iface->tm.cmd_wq.super, cmd_qp);
     if (status != UCS_OK) {
         goto err_tag_cleanup;
     }
 
-    iface->tm.cmd_wq.qp_num   = msrq->cmd_qp->verbs_qp.qp.qp_num;
+    iface->tm.cmd_wq.qp_num   = cmd_qp->qp_num;
     iface->tm.cmd_wq.ops_mask = rc_iface->tm.cmd_qp_len - 1;
     iface->tm.cmd_wq.ops_head = iface->tm.cmd_wq.ops_tail = 0;
     iface->tm.cmd_wq.ops      = ucs_calloc(rc_iface->tm.cmd_qp_len,
@@ -205,18 +219,176 @@ void uct_rc_mlx5_iface_common_tag_cleanup(uct_rc_mlx5_iface_common_t *iface,
 #endif
 }
 
+
+#if HAVE_IBV_EXP_DM
+static ucs_status_t
+uct_rc_mlx5_iface_common_dm_mpool_chunk_malloc(ucs_mpool_t *mp, size_t *size_p, void **chunk_p)
+{
+    ucs_status_t status;
+
+    status = ucs_mpool_chunk_malloc(mp, size_p, chunk_p);
+    if (status == UCS_OK) {
+        memset(*chunk_p, 0, *size_p);
+    }
+
+    return status;
+}
+
+static void uct_rc_mlx5_iface_common_dm_mp_obj_init(ucs_mpool_t *mp, void *obj, void *chunk)
+{
+    uct_mlx5_dm_data_t *dm         = ucs_container_of(mp, uct_mlx5_dm_data_t, mp);
+    uct_rc_iface_send_desc_t* desc = (uct_rc_iface_send_desc_t*)obj;
+
+    ucs_assert(desc->super.buffer == NULL);
+    ucs_assert(dm->seg_attached < dm->seg_count);
+
+    desc->lkey          = dm->mr->lkey;
+    desc->super.buffer  = UCS_PTR_BYTE_OFFSET(dm->start_va, dm->seg_attached * dm->seg_len);
+    desc->super.handler = (uct_rc_send_handler_t)ucs_mpool_put;
+    dm->seg_attached++;
+}
+
+static ucs_mpool_ops_t uct_dm_iface_mpool_ops = {
+    .chunk_alloc   = uct_rc_mlx5_iface_common_dm_mpool_chunk_malloc,
+    .chunk_release = ucs_mpool_chunk_free,
+    .obj_init      = uct_rc_mlx5_iface_common_dm_mp_obj_init,
+    .obj_cleanup   = NULL
+};
+
+
+static int uct_rc_mlx5_iface_common_dm_device_cmp(uct_mlx5_dm_data_t *dm_data,
+                                                  uct_rc_iface_t *iface,
+                                                  const uct_ib_mlx5_iface_config_t *config)
+{
+    uct_ib_device_t *dev = uct_ib_iface_device(&iface->super);
+
+    return dm_data->device->ibv_context == dev->ibv_context;
+}
+
+static ucs_status_t
+uct_rc_mlx5_iface_common_dm_tl_init(uct_mlx5_dm_data_t *data,
+                                    uct_rc_iface_t *iface,
+                                    const uct_ib_mlx5_iface_config_t *config)
+{
+    ucs_status_t status;
+    struct ibv_exp_alloc_dm_attr dm_attr;
+    struct ibv_exp_reg_mr_in mr_in;
+
+    data->seg_len      = ucs_min(ucs_align_up(config->dm.seg_len,
+                                              sizeof(uct_rc_mlx5_dm_copy_data_t)),
+                                 iface->super.config.seg_size);
+    data->seg_count    = config->dm.count;
+    data->seg_attached = 0;
+    data->device       = uct_ib_iface_device(&iface->super);
+
+    dm_attr.length     = data->seg_len * data->seg_count;
+    dm_attr.comp_mask  = 0;
+    data->dm           = ibv_exp_alloc_dm(data->device->ibv_context, &dm_attr);
+
+    if (data->dm == NULL) {
+        /* TODO: prompt warning? */
+        ucs_debug("ibv_exp_alloc_dm(dev=%s length=%zu) failed: %m",
+                  uct_ib_device_name(data->device), dm_attr.length);
+        return UCS_ERR_NO_RESOURCE;
+    }
+
+    memset(&mr_in, 0, sizeof(mr_in));
+    mr_in.pd           = uct_ib_iface_md(&iface->super)->pd;
+    mr_in.comp_mask    = IBV_EXP_REG_MR_DM;
+    mr_in.dm           = data->dm;
+    mr_in.length       = dm_attr.length;
+    data->mr           = ibv_exp_reg_mr(&mr_in);
+    if (data->mr == NULL) {
+        ucs_warn("ibv_exp_reg_mr() error - On Device Memory registration failed, %d %m", errno);
+        status = UCS_ERR_NO_RESOURCE;
+        goto failed_mr;
+    }
+
+    data->start_va = ((uct_mlx5_dm_va_t*)data->dm)->start_va;
+
+    status = ucs_mpool_init(&data->mp, 0,
+                            sizeof(uct_rc_iface_send_desc_t), 0, UCS_SYS_CACHE_LINE_SIZE,
+                            data->seg_count, data->seg_count,
+                            &uct_dm_iface_mpool_ops, "mlx5_dm_desc");
+    if (status != UCS_OK) {
+        goto failed_mpool;
+    }
+
+    /* DM initialization may fail due to any reason, just
+     * free resources & continue without DM */
+    return UCS_OK;
+
+failed_mpool:
+    ibv_dereg_mr(data->mr);
+failed_mr:
+    ibv_exp_free_dm(data->dm);
+    data->dm = NULL;
+    return status;
+}
+
+static void uct_rc_mlx5_iface_common_dm_tl_cleanup(uct_mlx5_dm_data_t *data)
+{
+    ucs_assert(data->dm != NULL);
+    ucs_assert(data->mr != NULL);
+
+    ucs_mpool_cleanup(&data->mp, 1);
+    ibv_dereg_mr(data->mr);
+    ibv_exp_free_dm(data->dm);
+}
+#endif
+
+static ucs_status_t
+uct_rc_mlx5_iface_common_dm_init(uct_rc_mlx5_iface_common_t *iface,
+                                 uct_rc_iface_t *rc_iface,
+                                 const uct_ib_mlx5_iface_config_t *mlx5_config)
+{
+#if HAVE_IBV_EXP_DM
+    if ((mlx5_config->dm.seg_len * mlx5_config->dm.count) == 0) {
+        goto fallback;
+    }
+
+    iface->dm.dm = uct_worker_tl_data_get(rc_iface->super.super.worker,
+                                          UCT_IB_MLX5_WORKER_DM_KEY,
+                                          uct_mlx5_dm_data_t,
+                                          uct_rc_mlx5_iface_common_dm_device_cmp,
+                                          uct_rc_mlx5_iface_common_dm_tl_init,
+                                          rc_iface, mlx5_config);
+    if (UCS_PTR_IS_ERR(iface->dm.dm)) {
+        goto fallback;
+    }
+
+    ucs_assert(iface->dm.dm->dm != NULL);
+    iface->dm.seg_len = iface->dm.dm->seg_len;
+    return UCS_OK;
+
+fallback:
+    iface->dm.dm = NULL;
+#endif
+    return UCS_OK;
+}
+
+static void uct_rc_mlx5_iface_common_dm_cleanup(uct_rc_mlx5_iface_common_t *iface)
+{
+#if HAVE_IBV_EXP_DM
+    if (iface->dm.dm) {
+        uct_worker_tl_data_put(iface->dm.dm, uct_rc_mlx5_iface_common_dm_tl_cleanup);
+    }
+#endif
+}
+
 ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
                                            uct_rc_iface_t *rc_iface,
-                                           uct_rc_iface_config_t *config)
+                                           const uct_rc_iface_config_t *config,
+                                           const uct_ib_mlx5_iface_config_t *mlx5_config)
 {
     ucs_status_t status;
 
-    status = uct_ib_mlx5_get_cq(rc_iface->super.send_cq, &iface->tx.cq);
+    status = uct_ib_mlx5_get_cq(rc_iface->super.cq[UCT_IB_DIR_TX], &iface->cq[UCT_IB_DIR_TX]);
     if (status != UCS_OK) {
         return status;
     }
 
-    status = uct_ib_mlx5_get_cq(rc_iface->super.recv_cq, &iface->rx.cq);
+    status = uct_ib_mlx5_get_cq(rc_iface->super.cq[UCT_IB_DIR_RX], &iface->cq[UCT_IB_DIR_RX]);
     if (status != UCS_OK) {
         return status;
     }
@@ -227,6 +399,11 @@ ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
         return status;
     }
 
+    status = uct_rc_mlx5_iface_common_dm_init(iface, rc_iface, mlx5_config);
+    if (status != UCS_OK) {
+        return status;
+    }
+
     rc_iface->rx.srq.quota = iface->rx.srq.mask + 1;
 
     /* By default set to something that is always in cache */
@@ -235,13 +412,13 @@ ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
     status = UCS_STATS_NODE_ALLOC(&iface->stats, &uct_rc_mlx5_iface_stats_class,
                                   rc_iface->stats);
     if (status != UCS_OK) {
-        return status;
+        goto cleanup_dm;
     }
 
     status = uct_iface_mpool_init(&rc_iface->super.super,
                                   &iface->tx.atomic_desc_mp,
-                                  sizeof(uct_rc_iface_send_desc_t) + UCT_RC_MAX_ATOMIC_SIZE,
-                                  sizeof(uct_rc_iface_send_desc_t) + UCT_RC_MAX_ATOMIC_SIZE,
+                                  sizeof(uct_rc_iface_send_desc_t) + UCT_IB_MAX_ATOMIC_SIZE,
+                                  sizeof(uct_rc_iface_send_desc_t) + UCT_IB_MAX_ATOMIC_SIZE,
                                   UCS_SYS_CACHE_LINE_SIZE,
                                   &config->super.tx.mp,
                                   rc_iface->config.tx_qp_len,
@@ -249,6 +426,7 @@ ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
                                   "rc_mlx5_atomic_desc");
     if (status != UCS_OK) {
         UCS_STATS_NODE_FREE(iface->stats);
+        goto cleanup_dm;
     }
 
     /* For little-endian atomic reply, override the default functions, to still
@@ -264,6 +442,10 @@ ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
        rc_iface->config.atomic64_ext_handler = uct_rc_mlx5_common_atomic64_le_handler;
     }
 
+    return UCS_OK;
+
+cleanup_dm:
+    uct_rc_mlx5_iface_common_dm_cleanup(iface);
     return status;
 }
 
@@ -271,14 +453,50 @@ void uct_rc_mlx5_iface_common_cleanup(uct_rc_mlx5_iface_common_t *iface)
 {
     UCS_STATS_NODE_FREE(iface->stats);
     ucs_mpool_cleanup(&iface->tx.atomic_desc_mp, 1);
+    uct_rc_mlx5_iface_common_dm_cleanup(iface);
 }
 
-void uct_rc_mlx5_iface_common_query(uct_iface_attr_t *iface_attr)
+void uct_rc_mlx5_iface_common_query(uct_ib_iface_t *iface, uct_iface_attr_t *iface_attr)
 {
+    uct_ib_device_t *dev = uct_ib_iface_device(iface);
+
     /* Atomics */
     iface_attr->cap.flags        |= UCT_IFACE_FLAG_ERRHANDLE_ZCOPY_BUF |
                                     UCT_IFACE_FLAG_ERRHANDLE_REMOTE_MEM;
 
+    if (uct_ib_atomic_is_supported(dev, 0, sizeof(uint64_t))) {
+        iface_attr->cap.atomic64.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_ADD);
+        iface_attr->cap.atomic64.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_ADD)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_CSWAP);
+
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+    }
+
+    if (uct_ib_atomic_is_supported(dev, 1, sizeof(uint64_t))) {
+        iface_attr->cap.atomic64.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_AND)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR);
+        iface_attr->cap.atomic64.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_AND)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_SWAP);
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+    }
+
+    if (uct_ib_atomic_is_supported(dev, 1, sizeof(uint32_t))) {
+        iface_attr->cap.atomic32.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_ADD)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_AND)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR);
+        iface_attr->cap.atomic32.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_ADD)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_AND)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_SWAP) |
+                                              UCS_BIT(UCT_ATOMIC_OP_CSWAP);
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+    }
+
     /* Software overhead */
     iface_attr->overhead          = 40e-9;
 
@@ -287,61 +505,72 @@ void uct_rc_mlx5_iface_common_query(uct_iface_attr_t *iface_attr)
 void uct_rc_mlx5_iface_common_update_cqs_ci(uct_rc_mlx5_iface_common_t *iface,
                                             uct_ib_iface_t *ib_iface)
 {
-    uct_ib_mlx5_update_cq_ci(ib_iface->send_cq, iface->tx.cq.cq_ci);
-    uct_ib_mlx5_update_cq_ci(ib_iface->recv_cq, iface->rx.cq.cq_ci);
+#if !HAVE_DECL_MLX5DV_INIT_OBJ
+    uct_ib_mlx5_update_cq_ci(ib_iface->cq[UCT_IB_DIR_TX], iface->cq[UCT_IB_DIR_TX].cq_ci);
+    uct_ib_mlx5_update_cq_ci(ib_iface->cq[UCT_IB_DIR_RX], iface->cq[UCT_IB_DIR_RX].cq_ci);
+#endif
 }
 
 void uct_rc_mlx5_iface_common_sync_cqs_ci(uct_rc_mlx5_iface_common_t *iface,
                                           uct_ib_iface_t *ib_iface)
 {
-    iface->tx.cq.cq_ci = uct_ib_mlx5_get_cq_ci(ib_iface->send_cq);
-    iface->rx.cq.cq_ci = uct_ib_mlx5_get_cq_ci(ib_iface->recv_cq);
+#if !HAVE_DECL_MLX5DV_INIT_OBJ
+    iface->cq[UCT_IB_DIR_TX].cq_ci = uct_ib_mlx5_get_cq_ci(ib_iface->cq[UCT_IB_DIR_TX]);
+    iface->cq[UCT_IB_DIR_RX].cq_ci = uct_ib_mlx5_get_cq_ci(ib_iface->cq[UCT_IB_DIR_RX]);
+#endif
 }
 
-void uct_rc_mlx5_iface_commom_clean_srq(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
-                                        uct_rc_iface_t *rc_iface, uint32_t qpn)
+int uct_rc_mlx5_iface_commom_clean(uct_ib_mlx5_cq_t *mlx5_cq,
+                                   uct_ib_mlx5_srq_t *srq, uint32_t qpn)
 {
-    uct_ib_mlx5_cq_t *mlx5_cq = &mlx5_common_iface->rx.cq;
     const size_t cqe_sz       = 1ul << mlx5_cq->cqe_size_log;
     struct mlx5_cqe64 *cqe, *dest;
     uct_ib_mlx5_srq_seg_t *seg;
-    unsigned ci, pi, idx;
+    unsigned pi, idx;
     uint8_t owner_bit;
     int nfreed;
 
-    pi = ci = mlx5_cq->cq_ci;
-    while (uct_ib_mlx5_poll_cq(&rc_iface->super, mlx5_cq)) {
-        if (pi == ci + mlx5_cq->cq_length - 1) {
+    pi = mlx5_cq->cq_ci;
+    for (;;) {
+        cqe = uct_ib_mlx5_get_cqe(mlx5_cq, pi);
+        if (uct_ib_mlx5_cqe_is_hw_owned(cqe->op_own, pi, mlx5_cq->cq_length)) {
             break;
         }
+
+        ucs_assert((cqe->op_own >> 4) != MLX5_CQE_INVALID);
+
         ++pi;
+        if (pi == (mlx5_cq->cq_ci + mlx5_cq->cq_length - 1)) {
+            break;
+        }
     }
-    ucs_assert(pi == mlx5_cq->cq_ci);
 
     ucs_memory_cpu_load_fence();
 
-    /* Remove CQEs of the destroyed QP, so the drive would not see them and try
+    /* Remove CQEs of the destroyed QP, so the driver would not see them and try
      * to remove them itself, creating a mess with the free-list.
      */
     nfreed = 0;
-    while ((int)--pi - (int)ci >= 0) {
+    while ((int)--pi - (int)mlx5_cq->cq_ci >= 0) {
         cqe = uct_ib_mlx5_get_cqe(mlx5_cq, pi);
         if ((ntohl(cqe->sop_drop_qpn) & UCS_MASK(UCT_IB_QPN_ORDER)) == qpn) {
             idx = ntohs(cqe->wqe_counter);
-            seg = uct_ib_mlx5_srq_get_wqe(&mlx5_common_iface->rx.srq, idx);
-            seg->srq.free = 1;
-            ucs_trace("iface %p: freed srq seg[%d] of qpn 0x%x",
-                      mlx5_common_iface, idx, qpn);
+            if (srq) {
+                seg = uct_ib_mlx5_srq_get_wqe(srq, idx);
+                seg->srq.free = 1;
+                ucs_trace("cq %p: freed srq seg[%d] of qpn 0x%x",
+                          mlx5_cq, idx, qpn);
+            }
             ++nfreed;
         } else if (nfreed) {
-            /* push the CQEs we want to keep to cq_ci, and move cq_ci backwards */
-            dest = uct_ib_mlx5_get_cqe(mlx5_cq, mlx5_cq->cq_ci);
+            dest = uct_ib_mlx5_get_cqe(mlx5_cq, pi + nfreed);
             owner_bit = dest->op_own & MLX5_CQE_OWNER_MASK;
             memcpy((void*)(dest + 1) - cqe_sz, (void*)(cqe + 1) - cqe_sz, cqe_sz);
             dest->op_own = (dest->op_own & ~MLX5_CQE_OWNER_MASK) | owner_bit;
-            --mlx5_cq->cq_ci;
         }
     }
 
-    rc_iface->rx.srq.available += nfreed;
+    mlx5_cq->cq_ci             += nfreed;
+
+    return nfreed;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.h
index 55f28796a..cca23e2f2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_common.h
@@ -7,6 +7,7 @@
 #ifndef UCT_RC_MLX5_COMMON_H
 #define UCT_RC_MLX5_COMMON_H
 
+#include <uct/ib/base/ib_device.h>
 #include <uct/ib/rc/base/rc_iface.h>
 #include <uct/ib/rc/base/rc_ep.h>
 #include <uct/ib/mlx5/ib_mlx5.h>
@@ -32,6 +33,23 @@
 #define UCT_RC_MLX5_CHECK_PUT_SHORT(_length, _av_size) \
     UCT_CHECK_LENGTH(_length, 0, UCT_IB_MLX5_PUT_MAX_SHORT(_av_size), "put_short")
 
+#define UCT_RC_MLX5_ATOMIC_OPS (UCS_BIT(UCT_ATOMIC_OP_ADD) | \
+                                UCS_BIT(UCT_ATOMIC_OP_AND) | \
+                                UCS_BIT(UCT_ATOMIC_OP_OR)  | \
+                                UCS_BIT(UCT_ATOMIC_OP_XOR))
+
+#define UCT_RC_MLX5_ATOMIC_FOPS (UCT_RC_MLX5_ATOMIC_OPS | UCS_BIT(UCT_ATOMIC_OP_SWAP))
+
+#define UCT_RC_MLX5_CHECK_ATOMIC_OPS(_op, _size, _flags)                        \
+    if (ucs_unlikely(!(UCS_BIT(_op) & (_flags)))) {                             \
+        ucs_assertv(0, "incorrect opcode for atomic: %d", _op);                 \
+        return UCS_ERR_UNSUPPORTED;                                             \
+    } else {                                                                    \
+        ucs_assert((_size == sizeof(uint64_t)) || (_size == sizeof(uint32_t))); \
+    }
+
+#define UCT_RC_MLX5_TO_BE(_val, _size) \
+    ((_size) == sizeof(uint64_t) ? htobe64(_val) : htobe32(_val))
 
 enum {
     UCT_RC_MLX5_IFACE_STAT_RX_INL_32,
@@ -66,9 +84,15 @@ enum {
     UCT_RC_MLX5_CQE_APP_OP_TM_CONSUMED_MSG = 0xA
 };
 
+#  define UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(_av_size) \
+       (UCT_IB_MLX5_AM_MAX_SHORT(_av_size + sizeof(struct ibv_exp_tmh))/ \
+        sizeof(struct mlx5_wqe_data_seg))
+
+#  define UCT_RC_MLX5_TM_CQE_WITH_IMM(_cqe64) \
+       (((_cqe64)->op_own >> 4) == MLX5_CQE_RESP_SEND_IMM)
+
 #  define UCT_RC_MLX5_TM_IS_SW_RNDV(_cqe64, _imm_data) \
-       (ucs_unlikely((((_cqe64)->op_own >> 4) == MLX5_CQE_RESP_SEND_IMM) && \
-                     !(_imm_data)))
+       (ucs_unlikely(UCT_RC_MLX5_TM_CQE_WITH_IMM(_cqe64) && !(_imm_data)))
 
 #  define UCT_RC_MLX5_CHECK_TAG(_mlx5_common_iface) \
        if (ucs_unlikely((_mlx5_common_iface)->tm.head->next == NULL)) {  \
@@ -114,18 +138,63 @@ typedef struct uct_rc_mlx5_cmd_wq {
                                                ops array size */
 } uct_rc_mlx5_cmd_wq_t;
 
+static UCS_F_ALWAYS_INLINE void
+uct_rc_mlx5_fill_tmh(struct ibv_exp_tmh *tmh, uct_tag_t tag,
+                     uint32_t app_ctx, unsigned op)
+{
+    tmh->opcode  = op;
+    tmh->app_ctx = app_ctx;
+    tmh->tag     = tag;
+}
+
+#  define UCT_RC_MLX5_IFACE_GET_TM_BCOPY_DESC(_iface, _mp, _desc, _tag, _app_ctx, \
+                                              _pack_cb, _arg, _length) \
+       { \
+           void *hdr; \
+           UCT_RC_IFACE_GET_TX_DESC(_iface, _mp, _desc) \
+           (_desc)->super.handler = (uct_rc_send_handler_t)ucs_mpool_put; \
+           hdr = (_desc) + 1; \
+           uct_rc_mlx5_fill_tmh(hdr, _tag, _app_ctx, IBV_EXP_TMH_EAGER); \
+           hdr += sizeof(struct ibv_exp_tmh); \
+           _length = _pack_cb(hdr, _arg); \
+       }
+# else
+
+#  define UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(_av_size)   0
+
 #endif /* IBV_EXP_HW_TM  */
 
+#if HAVE_IBV_EXP_DM
+typedef struct uct_mlx5_dm_data {
+    uct_worker_tl_data_t super;
+    ucs_mpool_t          mp;
+    struct ibv_mr        *mr;
+    struct ibv_exp_dm    *dm;
+    void                 *start_va;
+    size_t               seg_len;
+    unsigned             seg_count;
+    unsigned             seg_attached;
+    uct_ib_device_t      *device;
+} uct_mlx5_dm_data_t;
+
+typedef union uct_rc_mlx5_dm_copy_data {
+    uct_rc_am_short_hdr_t am_hdr;
+#if IBV_EXP_HW_TM
+    struct ibv_exp_tmh    tm_hdr;
+#endif
+    char                  bytes[sizeof(uint64_t) * 2];
+} UCS_S_PACKED uct_rc_mlx5_dm_copy_data_t;
+#endif
+
 typedef struct uct_rc_mlx5_iface_common {
     struct {
-        uct_ib_mlx5_cq_t          cq;
         ucs_mpool_t               atomic_desc_mp;
     } tx;
     struct {
-        uct_ib_mlx5_cq_t          cq;
         uct_ib_mlx5_srq_t         srq;
         void                      *pref_ptr;
     } rx;
+    uct_ib_mlx5_cq_t              cq[UCT_IB_DIR_NUM];
 #if IBV_EXP_HW_TM
     struct {
         uct_rc_mlx5_cmd_wq_t      cmd_wq;
@@ -133,11 +202,25 @@ typedef struct uct_rc_mlx5_iface_common {
         uct_rc_mlx5_tag_entry_t   *tail;
         uct_rc_mlx5_tag_entry_t   *list;
     } tm;
+#endif
+#if HAVE_IBV_EXP_DM
+    struct {
+        uct_mlx5_dm_data_t        *dm;
+        size_t                    seg_len; /* cached value to avoid double-pointer access */
+        ucs_status_t              (*am_short)(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
+                                              const void *payload, unsigned length);
+#if IBV_EXP_HW_TM
+        ucs_status_t              (*tag_short)(uct_ep_h tl_ep, uct_tag_t tag,
+                                               const void *data, size_t length);
+#endif
+    } dm;
 #endif
     UCS_STATS_NODE_DECLARE(stats);
 } uct_rc_mlx5_iface_common_t;
 
 
+extern ucs_config_field_t uct_ib_mlx5_iface_config_table[];
+
 unsigned uct_rc_mlx5_iface_srq_post_recv(uct_rc_iface_t *iface, uct_ib_mlx5_srq_t *srq);
 
 void uct_rc_mlx5_iface_common_prepost_recvs(uct_rc_iface_t *iface,
@@ -145,11 +228,12 @@ void uct_rc_mlx5_iface_common_prepost_recvs(uct_rc_iface_t *iface,
 
 ucs_status_t uct_rc_mlx5_iface_common_init(uct_rc_mlx5_iface_common_t *iface,
                                            uct_rc_iface_t *rc_iface,
-                                           uct_rc_iface_config_t *config);
+                                           const uct_rc_iface_config_t *config,
+                                           const uct_ib_mlx5_iface_config_t *mlx5_config);
 
 void uct_rc_mlx5_iface_common_cleanup(uct_rc_mlx5_iface_common_t *iface);
 
-void uct_rc_mlx5_iface_common_query(uct_iface_attr_t *iface_attr);
+void uct_rc_mlx5_iface_common_query(uct_ib_iface_t *ib_iface, uct_iface_attr_t *iface_attr);
 
 void uct_rc_mlx5_iface_common_update_cqs_ci(uct_rc_mlx5_iface_common_t *iface,
                                             uct_ib_iface_t *ib_iface);
@@ -157,19 +241,49 @@ void uct_rc_mlx5_iface_common_update_cqs_ci(uct_rc_mlx5_iface_common_t *iface,
 void uct_rc_mlx5_iface_common_sync_cqs_ci(uct_rc_mlx5_iface_common_t *iface,
                                           uct_ib_iface_t *ib_iface);
 
-void uct_rc_mlx5_iface_commom_clean_srq(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
-                                        uct_rc_iface_t *rc_iface, uint32_t qpn);
+int uct_rc_mlx5_iface_commom_clean(uct_ib_mlx5_cq_t *mlx5_cq,
+                                   uct_ib_mlx5_srq_t *srq, uint32_t qpn);
 
 ucs_status_t
 uct_rc_mlx5_iface_common_tag_init(uct_rc_mlx5_iface_common_t *iface,
                                   uct_rc_iface_t *rc_iface,
                                   uct_rc_iface_config_t *rc_config,
+                                  const uct_ib_mlx5_iface_config_t *mlx5_config,
                                   struct ibv_exp_create_srq_attr *srq_init_attr,
                                   unsigned rndv_hdr_len);
 
 void uct_rc_mlx5_iface_common_tag_cleanup(uct_rc_mlx5_iface_common_t *iface,
                                           uct_rc_iface_t *rc_iface);
 
+void uct_rc_mlx5_common_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                                    void *data, size_t length, size_t valid_length,
+                                    char *buffer, size_t max);
+
+static UCS_F_ALWAYS_INLINE void
+uct_rc_mlx5_common_update_tx_res(uct_rc_iface_t *rc_iface, uct_ib_mlx5_txwq_t *txwq,
+                                 uct_rc_txqp_t *txqp, uint16_t hw_ci)
+{
+    uint16_t bb_num;
+
+    bb_num = uct_ib_mlx5_txwq_update_bb(txwq, hw_ci) - uct_rc_txqp_available(txqp);
+
+    /* Must always have positive number of released resources. The first completion
+     * will report bb_num=1 (because prev_sw_pi is initialized to -1) and all the rest
+     * report the amount of BBs the previous WQE has consumed.
+     */
+    ucs_assertv(bb_num > 0, "hw_ci=%d prev_sw_pi=%d available=%d bb_num=%d",
+                hw_ci, txwq->prev_sw_pi, txqp->available, bb_num);
+
+    uct_rc_txqp_available_add(txqp, bb_num);
+    ucs_assert(uct_rc_txqp_available(txqp) <= txwq->bb_max);
+
+    rc_iface->tx.cq_available += bb_num;
+    ucs_assertv(rc_iface->tx.cq_available <= rc_iface->config.tx_cq_len,
+                "cq_available=%d tx_cq_len=%d bb_num=%d txwq=%p txqp=%p",
+                rc_iface->tx.cq_available, rc_iface->config.tx_cq_len, bb_num,
+                txwq, txqp);
+}
+
 static UCS_F_ALWAYS_INLINE void
 uct_rc_mlx5_txqp_process_tx_cqe(uct_rc_txqp_t *txqp, struct mlx5_cqe64 *cqe,
                                 uint16_t hw_ci)
@@ -249,7 +363,7 @@ uct_rc_mlx5_iface_check_rx_completion(uct_rc_mlx5_iface_common_t *mlx5_common_if
                                       uct_rc_iface_t *rc_iface,
                                       struct mlx5_cqe64 *cqe)
 {
-    uct_ib_mlx5_cq_t *cq      = &mlx5_common_iface->rx.cq;
+    uct_ib_mlx5_cq_t *cq      = &mlx5_common_iface->cq[UCT_IB_DIR_RX];
     struct mlx5_err_cqe *ecqe = (void*)cqe;
     uct_ib_mlx5_srq_seg_t *seg;
     uint16_t wqe_ctr;
@@ -268,7 +382,8 @@ uct_rc_mlx5_iface_check_rx_completion(uct_rc_mlx5_iface_common_t *mlx5_common_if
                                           wqe_ctr, UCS_OK,
                                           rc_iface->super.config.rx_headroom_offset,
                                           &rc_iface->super.release_desc);
-    } else if ((ecqe->op_own >> 4) != MLX5_CQE_INVALID) {
+    } else {
+        ucs_assert((ecqe->op_own >> 4) != MLX5_CQE_INVALID);
         uct_ib_mlx5_check_completion(&rc_iface->super, cq, cqe);
     }
 }
@@ -277,7 +392,7 @@ static UCS_F_ALWAYS_INLINE struct mlx5_cqe64*
 uct_rc_mlx5_iface_poll_rx_cq(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
                              uct_rc_iface_t *rc_iface)
 {
-    uct_ib_mlx5_cq_t *cq = &mlx5_common_iface->rx.cq;
+    uct_ib_mlx5_cq_t *cq = &mlx5_common_iface->cq[UCT_IB_DIR_RX];
     struct mlx5_cqe64 *cqe;
     unsigned index;
     uint8_t op_own;
@@ -289,7 +404,7 @@ uct_rc_mlx5_iface_poll_rx_cq(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
     cqe    = uct_ib_mlx5_get_cqe(cq, index);
     op_own = cqe->op_own;
 
-    if (ucs_unlikely((op_own & MLX5_CQE_OWNER_MASK) == !(index & cq->cq_length))) {
+    if (ucs_unlikely(uct_ib_mlx5_cqe_is_hw_owned(op_own, index, cq->cq_length))) {
         return NULL;
     } else if (ucs_unlikely(op_own & UCT_IB_MLX5_CQE_OP_OWN_ERR_MASK)) {
         uct_rc_mlx5_iface_check_rx_completion(mlx5_common_iface, rc_iface, cqe);
@@ -356,7 +471,7 @@ uct_rc_mlx5_iface_common_am_handler(uct_rc_mlx5_iface_common_t *mlx5_iface,
     seg     = uct_ib_mlx5_srq_get_wqe(&mlx5_iface->rx.srq, wqe_ctr);
 
     uct_ib_mlx5_log_rx(&rc_iface->super, IBV_QPT_RC, cqe, hdr,
-                       uct_rc_ep_am_packet_dump);
+                       uct_rc_mlx5_common_packet_dump);
 
     if (ucs_unlikely(hdr->am_id & UCT_RC_EP_FC_MASK)) {
         qp_num = ntohl(cqe->sop_drop_qpn) & UCS_MASK(UCT_IB_QPN_ORDER);
@@ -376,14 +491,15 @@ uct_rc_mlx5_iface_common_am_handler(uct_rc_mlx5_iface_common_t *mlx5_iface,
 }
 
 static UCS_F_ALWAYS_INLINE void
-uct_rc_mlx5_common_post_send(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
+uct_rc_mlx5_common_post_send(uct_rc_iface_t *iface, int qp_type,
                              uct_rc_txqp_t *txqp, uct_ib_mlx5_txwq_t *txwq,
                              uint8_t opcode, uint8_t opmod, uint8_t fm_ce_se,
                              size_t wqe_size, uct_ib_mlx5_base_av_t *av,
-                             struct mlx5_grh_av *grh_av, uint32_t imm)
+                             struct mlx5_grh_av *grh_av, uint32_t imm, int max_log_sge,
+                             uct_ib_log_sge_t *log_sge)
 {
     struct mlx5_wqe_ctrl_seg *ctrl;
-    uint16_t posted;
+    uint16_t res_count;
 
     ctrl = txwq->curr;
 
@@ -396,20 +512,22 @@ uct_rc_mlx5_common_post_send(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
                                  txqp->qp->qp_num, fm_ce_se, wqe_size);
     }
 
-    if (qp_type == IBV_EXP_QPT_DC_INI) {
+#if HAVE_TL_DC
+    if (qp_type == UCT_IB_QPT_DCI) {
         uct_ib_mlx5_set_dgram_seg((void*)(ctrl + 1), av, grh_av, qp_type);
     }
+#endif
 
-    uct_ib_mlx5_log_tx(&iface->super, qp_type, ctrl, txwq->qstart, txwq->qend,
+    uct_ib_mlx5_log_tx(&iface->super, qp_type, ctrl, txwq->qstart,
+                       txwq->qend, max_log_sge, log_sge,
                        ((opcode == MLX5_OPCODE_SEND) || (opcode == MLX5_OPCODE_SEND_IMM)) ?
-                       uct_rc_ep_am_packet_dump : NULL);
+                       uct_rc_mlx5_common_packet_dump : NULL);
 
-    posted = uct_ib_mlx5_post_send(txwq, ctrl, wqe_size);
+    res_count = uct_ib_mlx5_post_send(txwq, ctrl, wqe_size);
     if (fm_ce_se & MLX5_WQE_CTRL_CQ_UPDATE) {
-        txwq->sig_pi = txwq->sw_pi - posted;
+        txwq->sig_pi = txwq->prev_sw_pi;
     }
-
-    uct_rc_txqp_posted(txqp, iface, posted, fm_ce_se & MLX5_WQE_CTRL_CQ_UPDATE);
+    uct_rc_txqp_posted(txqp, iface, res_count, fm_ce_se & MLX5_WQE_CTRL_CQ_UPDATE);
 }
 
 
@@ -430,13 +548,13 @@ uct_rc_mlx5_common_post_send(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
  * is a compile time constant
  */
 static UCS_F_ALWAYS_INLINE void
-uct_rc_mlx5_txqp_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
+uct_rc_mlx5_txqp_inline_post(uct_rc_iface_t *iface, int qp_type,
                              uct_rc_txqp_t *txqp, uct_ib_mlx5_txwq_t *txwq,
                              unsigned opcode, const void *buffer, unsigned length,
                   /* SEND */ uint8_t am_id, uint64_t am_hdr, uint32_t imm_val_be,
                   /* RDMA */ uint64_t rdma_raddr, uct_rkey_t rdma_rkey,
                   /* AV   */ uct_ib_mlx5_base_av_t *av, struct mlx5_grh_av *grh_av,
-                             size_t av_size, unsigned fm_ce_se)
+                             size_t av_size, unsigned fm_ce_se, int max_log_sge)
 {
     struct mlx5_wqe_ctrl_seg     *ctrl;
     struct mlx5_wqe_raddr_seg    *raddr;
@@ -487,7 +605,7 @@ uct_rc_mlx5_txqp_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
         inl              = uct_ib_mlx5_txwq_wrap_none(txwq, raddr + 1);
         inl->byte_count  = htonl(length | MLX5_INLINE_SEG);
         uct_ib_mlx5_inline_copy(inl + 1, buffer, length, txwq);
-        fm_ce_se        |= MLX5_WQE_CTRL_CQ_UPDATE;
+        fm_ce_se        |= uct_rc_iface_tx_moderation(iface, txqp, MLX5_WQE_CTRL_CQ_UPDATE);
         break;
 
     case MLX5_OPCODE_NOP:
@@ -503,7 +621,7 @@ uct_rc_mlx5_txqp_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
     }
 
     uct_rc_mlx5_common_post_send(iface, qp_type, txqp, txwq, opcode, 0, fm_ce_se,
-                                 wqe_size, av, grh_av, imm_val_be);
+                                 wqe_size, av, grh_av, imm_val_be, max_log_sge, NULL);
 }
 
 /*
@@ -525,14 +643,16 @@ uct_rc_mlx5_txqp_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
  * is a compile time constant
  */
 static UCS_F_ALWAYS_INLINE void
-uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
+uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, int qp_type,
                            uct_rc_txqp_t *txqp, uct_ib_mlx5_txwq_t *txwq,
                            unsigned opcode_flags, const void *buffer,
                            unsigned length, uint32_t *lkey_p,
          /* RDMA/ATOMIC */ uint64_t remote_addr, uct_rkey_t rkey,
-         /* ATOMIC      */ uint64_t compare_mask, uint64_t compare, uint64_t swap_add,
+         /* ATOMIC      */ uint64_t compare_mask, uint64_t compare,
+         /* ATOMIC      */ uint64_t swap_mask, uint64_t swap_add,
          /* AV          */ uct_ib_mlx5_base_av_t *av, struct mlx5_grh_av *grh_av,
-                           size_t av_size, uint8_t fm_ce_se, uint32_t imm_val_be)
+                           size_t av_size, uint8_t fm_ce_se, uint32_t imm_val_be,
+                           int max_log_sge, uct_ib_log_sge_t *log_sge)
 {
     struct mlx5_wqe_ctrl_seg                     *ctrl;
     struct mlx5_wqe_raddr_seg                    *raddr;
@@ -541,6 +661,7 @@ uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
     struct uct_ib_mlx5_atomic_masked_cswap32_seg *masked_cswap32;
     struct uct_ib_mlx5_atomic_masked_fadd32_seg  *masked_fadd32;
     struct uct_ib_mlx5_atomic_masked_cswap64_seg *masked_cswap64;
+    struct uct_ib_mlx5_atomic_masked_fadd64_seg  *masked_fadd64;
     size_t  wqe_size, ctrl_av_size;
     uint8_t opmod;
     void *next_seg;
@@ -613,7 +734,7 @@ uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
             masked_cswap32               = uct_ib_mlx5_txwq_wrap_none(txwq, raddr + 1);
             masked_cswap32->swap         = swap_add;
             masked_cswap32->compare      = compare;
-            masked_cswap32->swap_mask    = (uint32_t)-1;
+            masked_cswap32->swap_mask    = swap_mask;
             masked_cswap32->compare_mask = compare_mask;
             dptr                         = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_cswap32 + 1);
             wqe_size                     = ctrl_av_size + sizeof(*raddr) +
@@ -627,7 +748,7 @@ uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
 
             /* 2nd half of masked_cswap64 can wrap */
             masked_cswap64               = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_cswap64 + 1);
-            masked_cswap64->swap         = (uint64_t)-1;
+            masked_cswap64->swap         = swap_mask;
             masked_cswap64->compare      = compare_mask;
 
             dptr                         = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_cswap64 + 1);
@@ -641,18 +762,33 @@ uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
         break;
 
      case MLX5_OPCODE_ATOMIC_MASKED_FA:
-        ucs_assert(length == sizeof(uint32_t));
-        raddr = uct_ib_mlx5_txwq_wrap_exact(txwq, (void*)ctrl + ctrl_av_size);
+        raddr = next_seg;
         uct_ib_mlx5_ep_set_rdma_seg(raddr, remote_addr, rkey);
 
-        opmod                         = UCT_IB_MLX5_OPMOD_EXT_ATOMIC(2);
-        masked_fadd32                 = uct_ib_mlx5_txwq_wrap_none(txwq, raddr + 1);
-        masked_fadd32->add            = swap_add;
-        masked_fadd32->filed_boundary = 0;
-
-        dptr                          = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_fadd32 + 1);
-        wqe_size                      = ctrl_av_size + sizeof(*raddr) +
-                                        sizeof(*masked_fadd32) + sizeof(*dptr);
+        switch (length) {
+        case sizeof(uint32_t):
+            opmod                         = UCT_IB_MLX5_OPMOD_EXT_ATOMIC(2);
+            masked_fadd32                 = uct_ib_mlx5_txwq_wrap_none(txwq, raddr + 1);
+            masked_fadd32->add            = swap_add;
+            masked_fadd32->filed_boundary = compare;
+
+            dptr                          = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_fadd32 + 1);
+            wqe_size                      = ctrl_av_size + sizeof(*raddr) +
+                                            sizeof(*masked_fadd32) + sizeof(*dptr);
+            break;
+        case sizeof(uint64_t):
+            opmod                         = UCT_IB_MLX5_OPMOD_EXT_ATOMIC(3); /* Ext. atomic, size 2**3 */
+            masked_fadd64                 = uct_ib_mlx5_txwq_wrap_none(txwq, raddr + 1);
+            masked_fadd64->add            = swap_add;
+            masked_fadd64->filed_boundary = compare;
+
+            dptr                          = uct_ib_mlx5_txwq_wrap_exact(txwq, masked_fadd64 + 1);
+            wqe_size                      = ctrl_av_size + sizeof(*raddr) +
+                                            sizeof(*masked_fadd64) + sizeof(*dptr);
+            break;
+        default:
+            ucs_fatal("invalid atomic type length %d", length);
+        }
         uct_ib_mlx5_set_data_seg(dptr, buffer, length, *lkey_p);
         break;
 
@@ -661,12 +797,13 @@ uct_rc_mlx5_txqp_dptr_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
     }
 
     uct_rc_mlx5_common_post_send(iface, qp_type, txqp, txwq,
-                                 (opcode_flags & UCT_RC_MLX5_OPCODE_MASK),
-                                 opmod, fm_ce_se, wqe_size, av, grh_av, imm_val_be);
+                                 (opcode_flags & UCT_RC_MLX5_OPCODE_MASK), opmod,
+                                 fm_ce_se, wqe_size, av, grh_av, imm_val_be,
+                                 max_log_sge, log_sge);
 }
 
 static UCS_F_ALWAYS_INLINE
-void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
+void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, int qp_type,
                                     uct_rc_txqp_t *txqp, uct_ib_mlx5_txwq_t *txwq,
                                     unsigned opcode_flags,
                          /* IOV  */ const uct_iov_t *iov, size_t iovcnt,
@@ -674,7 +811,7 @@ void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, enum ibv_qp_type qp_t
                          /* RDMA */ uint64_t remote_addr, uct_rkey_t rkey,
                          /* TAG  */ uct_tag_t tag, uint32_t app_ctx, uint32_t ib_imm_be,
                          /* AV   */ uct_ib_mlx5_base_av_t *av, struct mlx5_grh_av *grh_av,
-                                    size_t av_size, uint8_t fm_ce_se)
+                                    size_t av_size, uint8_t fm_ce_se, int max_log_sge)
 {
     struct mlx5_wqe_ctrl_seg     *ctrl;
     struct mlx5_wqe_raddr_seg    *raddr;
@@ -713,7 +850,7 @@ void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, enum ibv_qp_type qp_t
         wqe_size         = ctrl_av_size + inl_seg_size +
                            uct_ib_mlx5_set_data_seg_iov(txwq, dptr, iov, iovcnt);
 
-        ucs_assert(wqe_size <= (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB));
+        ucs_assert(wqe_size <= UCT_IB_MLX5_MAX_SEND_WQE_SIZE);
         break;
 
 #if IBV_EXP_HW_TM
@@ -727,9 +864,9 @@ void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, enum ibv_qp_type qp_t
         wqe_size         = ctrl_av_size + inl_seg_size +
                            uct_ib_mlx5_set_data_seg_iov(txwq, dptr, iov, iovcnt);
 
-        uct_rc_iface_fill_tmh((struct ibv_exp_tmh*)(inl + 1), tag, app_ctx,
-                              IBV_EXP_TMH_EAGER);
-        ucs_assert(wqe_size <= (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB));
+        uct_rc_mlx5_fill_tmh((struct ibv_exp_tmh*)(inl + 1), tag, app_ctx,
+                             IBV_EXP_TMH_EAGER);
+        ucs_assert(wqe_size <= UCT_IB_MLX5_MAX_SEND_WQE_SIZE);
         break;
 #endif
 
@@ -753,7 +890,8 @@ void uct_rc_mlx5_txqp_dptr_post_iov(uct_rc_iface_t *iface, enum ibv_qp_type qp_t
 
     uct_rc_mlx5_common_post_send(iface, qp_type, txqp, txwq,
                                  opcode_flags & UCT_RC_MLX5_OPCODE_MASK,
-                                 0, fm_ce_se, wqe_size, av, grh_av, ib_imm_be);
+                                 0, fm_ce_se, wqe_size, av, grh_av, ib_imm_be,
+                                 max_log_sge, NULL);
 }
 
 #if IBV_EXP_HW_TM
@@ -778,8 +916,8 @@ uct_rc_mlx5_set_tm_seg(uct_ib_mlx5_txwq_t *txwq,
         return;
     }
 
-    tmseg->append_tag  = htobe64(tag);
-    tmseg->append_mask = htobe64(mask);
+    tmseg->append_tag  = tag;
+    tmseg->append_mask = mask;
 }
 
 static UCS_F_ALWAYS_INLINE void
@@ -805,7 +943,7 @@ uct_rc_mlx5_add_cmd_qp_op(uct_rc_mlx5_iface_common_t *iface,
 }
 
 static UCS_F_ALWAYS_INLINE void
-uct_rc_mlx5_txqp_tag_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type,
+uct_rc_mlx5_txqp_tag_inline_post(uct_rc_iface_t *iface, int qp_type,
                                  uct_rc_txqp_t *txqp, uct_ib_mlx5_txwq_t *txwq,
                                  unsigned opcode, const void *buffer, unsigned length,
                                  const uct_iov_t *iov, /* relevant for RNDV */
@@ -845,7 +983,7 @@ uct_rc_mlx5_txqp_tag_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type
                               ((uct_ib_mem_t*)iov->memh)->mr->rkey, iov->length);
         uct_ib_mlx5_inline_copy(tmh + 1, &rvh, sizeof(rvh), txwq);
 
-        if (qp_type == IBV_EXP_QPT_DC_INI) {
+        if (qp_type == UCT_IB_QPT_DCI) {
             /* RAVH can be wrapped as well */
             ravh_ptr = uct_ib_mlx5_txwq_wrap_data(txwq, (char*)tmh +
                                                   sizeof(*tmh) + sizeof(rvh));
@@ -870,16 +1008,16 @@ uct_rc_mlx5_txqp_tag_inline_post(uct_rc_iface_t *iface, enum ibv_qp_type qp_type
         break;
     }
 
-    ucs_assert(wqe_size <= (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB));
+    ucs_assert(wqe_size <= UCT_IB_MLX5_MAX_SEND_WQE_SIZE);
 
-    uct_rc_iface_fill_tmh(tmh, tag, app_ctx, tm_op);
+    uct_rc_mlx5_fill_tmh(tmh, tag, app_ctx, tm_op);
 
     /* In case of RNDV first bytes of data could be stored in TMH */
     uct_ib_mlx5_inline_copy(data, (char*)buffer + tmh_data_len, length, txwq);
     fm_ce_se |= uct_rc_iface_tx_moderation(iface, txqp, MLX5_WQE_CTRL_CQ_UPDATE);
 
     uct_rc_mlx5_common_post_send(iface, qp_type, txqp, txwq, opcode, 0, fm_ce_se,
-                                 wqe_size, av, grh_av, imm_val_be);
+                                 wqe_size, av, grh_av, imm_val_be, INT_MAX, NULL);
 }
 
 static UCS_F_ALWAYS_INLINE void
@@ -951,6 +1089,9 @@ uct_rc_mlx5_iface_common_tag_recv(uct_rc_mlx5_iface_common_t *iface,
                                          tag_mask,
                                          UCT_RC_MLX5_SRQ_FLAG_TM_CQE_REQ |
                                          UCT_RC_MLX5_SRQ_FLAG_TM_SW_CNT);
+
+    UCT_RC_IFACE_TM_STAT(rc_iface, LIST_ADD);
+
     return UCS_OK;
 }
 
@@ -978,6 +1119,9 @@ uct_rc_mlx5_iface_common_tag_recv_cancel(uct_rc_mlx5_iface_common_t *iface,
                                          UCT_RC_MLX5_TM_OPCODE_REMOVE, index,
                                          rc_iface->tm.unexpected_cnt, 0ul, 0ul,
                                          flags);
+
+    UCT_RC_IFACE_TM_STAT(rc_iface, LIST_DEL);
+
     return UCS_OK;
 }
 
@@ -1018,8 +1162,8 @@ uct_rc_mlx5_iface_tag_consumed(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
         /* Need to save TMH info, which will be used when
          * UCT_RC_MLX5_CQE_APP_OP_TM_EXPECTED CQE is received */
         priv           = uct_rc_iface_ctx_priv(ctx);
-        priv->tag      = be64toh(tmh->tag);
-        priv->app_ctx  = ntohl(tmh->app_ctx);
+        priv->tag      = tmh->tag;
+        priv->app_ctx  = tmh->app_ctx;
     }
 }
 
@@ -1052,8 +1196,10 @@ uct_rc_mlx5_iface_handle_expected(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
 
     if (UCT_RC_MLX5_TM_IS_SW_RNDV(cqe, imm_data)) {
         ctx->rndv_cb(ctx, tag, priv->buffer, byte_len, UCS_OK);
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_REQ_EXP);
     } else {
         ctx->completed_cb(ctx, tag, imm_data, byte_len, UCS_OK);
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_EXP);
     }
 }
 
@@ -1075,6 +1221,8 @@ uct_rc_mlx5_iface_unexp_consumed(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
                                              UCT_RC_MLX5_TM_OPCODE_NOP, 0,
                                              rc_iface->tm.unexpected_cnt, 0ul, 0ul,
                                              UCT_RC_MLX5_SRQ_FLAG_TM_SW_CNT);
+
+        UCT_RC_IFACE_TM_STAT(rc_iface, LIST_SYNC);
     }
 }
 
@@ -1091,41 +1239,51 @@ uct_rc_mlx5_iface_tag_handle_unexp(uct_rc_mlx5_iface_common_t *mlx5_common_iface
     tmh = uct_rc_mlx5_iface_common_data(mlx5_common_iface, rc_iface, cqe,
                                         byte_len, &flags);
 
-    switch (tmh->opcode) {
-    case IBV_EXP_TMH_EAGER:
+    if (ucs_likely((tmh->opcode == IBV_EXP_TMH_EAGER) &&
+                   !UCT_RC_MLX5_TM_CQE_WITH_IMM(cqe))) {
+        status = rc_iface->tm.eager_unexp.cb(rc_iface->tm.eager_unexp.arg,
+                                             tmh + 1, byte_len - sizeof(*tmh),
+                                             flags, tmh->tag, 0);
+
+        uct_rc_mlx5_iface_unexp_consumed(mlx5_common_iface, rc_iface,
+                                         &rc_iface->tm.eager_desc, status,
+                                         ntohs(cqe->wqe_counter));
+
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_EAGER_UNEXP);
+
+    } else if (tmh->opcode == IBV_EXP_TMH_EAGER) {
         imm_data = uct_rc_iface_tag_imm_data_unpack(cqe->imm_inval_pkey,
-                                                    ntohl(tmh->app_ctx),
-                                                    (cqe->op_own >> 4) ==
-                                                    MLX5_CQE_RESP_SEND_IMM);
+                                                    tmh->app_ctx, 1);
 
-        if (UCT_RC_MLX5_TM_IS_SW_RNDV(cqe, cqe->imm_inval_pkey)) {
+        if (ucs_unlikely(!imm_data)) {
+            /* Opcode is WITH_IMM, but imm_data is 0 - this must be SW RNDV */
             status = rc_iface->tm.rndv_unexp.cb(rc_iface->tm.rndv_unexp.arg,
-                                                flags, be64toh(tmh->tag), tmh + 1,
+                                                flags, tmh->tag, tmh + 1,
                                                 byte_len - sizeof(*tmh),
                                                 0ul, 0, NULL);
+
+            UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_REQ_UNEXP);
         } else {
             status = rc_iface->tm.eager_unexp.cb(rc_iface->tm.eager_unexp.arg,
                                                  tmh + 1, byte_len - sizeof(*tmh),
-                                                 flags, be64toh(tmh->tag),
-                                                 imm_data);
+                                                 flags, tmh->tag, imm_data);
+
+            UCT_RC_IFACE_TM_STAT(rc_iface, RX_EAGER_UNEXP);
         }
 
         uct_rc_mlx5_iface_unexp_consumed(mlx5_common_iface, rc_iface,
                                          &rc_iface->tm.eager_desc, status,
                                          ntohs(cqe->wqe_counter));
-        break;
-
-    case IBV_EXP_TMH_RNDV:
-        status = uct_rc_iface_handle_rndv(rc_iface, tmh, byte_len);
+    } else {
+        ucs_assertv_always(tmh->opcode == IBV_EXP_TMH_RNDV,
+                           "Unsupported packet arrived %d", tmh->opcode);
+        status = uct_rc_iface_handle_rndv(rc_iface, tmh, tmh->tag, byte_len);
 
         uct_rc_mlx5_iface_unexp_consumed(mlx5_common_iface, rc_iface,
                                          &rc_iface->tm.rndv_desc, status,
                                          ntohs(cqe->wqe_counter));
-        break;
 
-    default:
-        ucs_fatal("Unsupported packet arrived %d", tmh->opcode);
-        break;
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_UNEXP);
     }
 }
 
@@ -1176,6 +1334,14 @@ uct_rc_mlx5_iface_common_poll_rx(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
 #if IBV_EXP_HW_TM
     ucs_assert(cqe->app == UCT_RC_MLX5_CQE_APP_TAG_MATCHING);
 
+    /* Should be a fast path, because small (latency-critical) messages
+     * are not supposed to be offloaded to the HW.  */
+    if (ucs_likely(cqe->app_op == UCT_RC_MLX5_CQE_APP_OP_TM_UNEXPECTED)) {
+        uct_rc_mlx5_iface_tag_handle_unexp(mlx5_common_iface, rc_iface, cqe,
+                                           byte_len);
+        goto done;
+    }
+
     switch (cqe->app_op) {
     case UCT_RC_MLX5_CQE_APP_OP_TM_APPEND:
         uct_rc_mlx5_iface_handle_tm_list_op(mlx5_common_iface,
@@ -1196,19 +1362,16 @@ uct_rc_mlx5_iface_common_poll_rx(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
                                                 byte_len);
         } else {
             ucs_assert(tmh->opcode == IBV_EXP_TMH_FIN);
-            uct_rc_iface_handle_rndv_fin(rc_iface, tmh);
+            uct_rc_iface_handle_rndv_fin(rc_iface, tmh->app_ctx);
             seg = uct_ib_mlx5_srq_get_wqe(&mlx5_common_iface->rx.srq,
                                           ntohs(cqe->wqe_counter));
 
             uct_rc_mlx5_iface_release_srq_seg(mlx5_common_iface, rc_iface, seg,
                                               ntohs(cqe->wqe_counter), UCS_OK, 0,
                                               NULL);
-        }
-        break;
 
-    case UCT_RC_MLX5_CQE_APP_OP_TM_UNEXPECTED:
-        uct_rc_mlx5_iface_tag_handle_unexp(mlx5_common_iface, rc_iface, cqe,
-                                           byte_len);
+            UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_FIN);
+        }
         break;
 
     case UCT_RC_MLX5_CQE_APP_OP_TM_CONSUMED:
@@ -1223,8 +1386,7 @@ uct_rc_mlx5_iface_common_poll_rx(uct_rc_mlx5_iface_common_t *mlx5_common_iface,
                                        UCT_RC_MLX5_CQE_APP_OP_TM_CONSUMED_MSG);
 
         uct_rc_mlx5_iface_handle_expected(mlx5_common_iface, rc_iface, cqe,
-                                          byte_len, be64toh(tmh->tag),
-                                          ntohl(tmh->app_ctx));
+                                          byte_len, tmh->tag, tmh->app_ctx);
         break;
 
     case UCT_RC_MLX5_CQE_APP_OP_TM_EXPECTED:
@@ -1249,4 +1411,179 @@ done:
     return count;
 }
 
+#if HAVE_IBV_EXP_DM
+/* DM memory should be written by 8 bytes to eliminate
+ * processor cache issues. To make this used uct_rc_mlx5_dm_copy_data_t
+ * datatype where first hdr_len bytes are filled by message header
+ * and tail is filled by head of message. */
+static void UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_iface_common_copy_to_dm(uct_rc_mlx5_dm_copy_data_t *cache, size_t hdr_len,
+                                    const void *payload, size_t length, void *dm,
+                                    uct_ib_log_sge_t *log_sge)
+{
+    typedef uint64_t misaligned_t UCS_V_ALIGNED(1);
+
+    uint64_t padding = 0; /* init by 0 to suppress valgrind error */
+    size_t head      = (cache && hdr_len) ? ucs_min(length, sizeof(*cache) - hdr_len) : 0;
+    size_t body      = ucs_align_down(length - head, sizeof(padding));
+    size_t tail      = length - (head + body);
+    char   *dst      = dm;
+    int i            = 0;
+
+    ucs_assert(sizeof(*cache) >= hdr_len);
+    ucs_assert(head + body + tail == length);
+    ucs_assert(tail < sizeof(padding));
+
+    /* copy head of payload to tail of cache */
+    memcpy(cache->bytes + hdr_len, payload, head);
+
+    UCS_STATIC_ASSERT(sizeof(*cache) == sizeof(cache->bytes));
+    UCS_STATIC_ASSERT(sizeof(log_sge->sg_list) / sizeof(log_sge->sg_list[0]) >= 2);
+
+    /* condition is static-evaluated */
+    if (cache && hdr_len) {
+        /* atomically by 8 bytes copy data to DM */
+        /* cache buffer must be aligned, so, source data type is aligned */
+        UCS_WORD_COPY(volatile uint64_t, dst, uint64_t, cache->bytes, sizeof(cache->bytes));
+        dst += sizeof(cache->bytes);
+        if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) {
+            log_sge->sg_list[0].addr   = (uint64_t)cache;
+            log_sge->sg_list[0].length = (uint32_t)hdr_len;
+            i++;
+        }
+    }
+    if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) {
+        log_sge->sg_list[i].addr   = (uint64_t)payload;
+        log_sge->sg_list[i].length = (uint32_t)length;
+        i++;
+    }
+    log_sge->num_sge = i;
+
+    /* copy payload to DM */
+    UCS_WORD_COPY(volatile uint64_t, dst, misaligned_t, payload + head, body);
+    if (tail) {
+        dst += body;
+        memcpy(&padding, payload + head + body, tail);
+        /* use uint64_t for source datatype because it is aligned buffer on stack */
+        UCS_WORD_COPY(volatile uint64_t, dst, uint64_t, &padding, sizeof(padding));
+    }
+}
+
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_common_dm_make_data(uct_rc_mlx5_iface_common_t *iface,
+                                uct_rc_iface_t *rc_iface,
+                                uct_rc_mlx5_dm_copy_data_t *cache,
+                                size_t hdr_len, const void *payload,
+                                unsigned length,
+                                uct_rc_iface_send_desc_t **desc_p,
+                                void **buffer_p, uct_ib_log_sge_t *log_sge)
+{
+    uct_rc_iface_send_desc_t *desc;
+    void *buffer;
+
+    ucs_assert(iface->dm.dm != NULL);
+    ucs_assert(log_sge != NULL);
+
+    desc = ucs_mpool_get_inline(&iface->dm.dm->mp);
+    if (ucs_unlikely(desc == NULL)) {
+        /* in case if no resources available - fallback to bcopy */
+        UCT_RC_IFACE_GET_TX_DESC(rc_iface, &rc_iface->tx.mp, desc);
+        desc->super.handler = (uct_rc_send_handler_t)ucs_mpool_put;
+        buffer = desc + 1;
+
+        /* condition is static-evaluated, no performance penalty */
+        if (cache && hdr_len) {
+            memcpy(buffer, cache->bytes, hdr_len);
+        }
+        memcpy(UCS_PTR_BYTE_OFFSET(buffer, hdr_len), payload, length);
+        log_sge->num_sge = 0;
+    } else {
+        /* desc must be partially initialized by mpool.
+         * hint to valgrind to make it defined */
+        VALGRIND_MAKE_MEM_DEFINED(desc, sizeof(*desc));
+        ucs_assert(desc->super.buffer != NULL);
+        buffer = (void*)(desc->super.buffer - iface->dm.dm->start_va);
+
+        uct_rc_mlx5_iface_common_copy_to_dm(cache, hdr_len, payload,
+                                            length, desc->super.buffer, log_sge);
+        if (ucs_log_is_enabled(UCS_LOG_LEVEL_TRACE_DATA)) {
+            log_sge->sg_list[0].lkey = log_sge->sg_list[1].lkey = desc->lkey;
+            log_sge->inline_bitmap = 0;
+        }
+    }
+
+    *desc_p   = desc;
+    *buffer_p = buffer;
+    return UCS_OK;
+}
+#endif
+
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_iface_common_atomic_data(unsigned opcode, unsigned size, uint64_t value,
+                                     int *op, uint64_t *compare_mask, uint64_t *compare,
+                                     uint64_t *swap_mask, uint64_t *swap, int *ext)
+{
+    ucs_assert((size == sizeof(uint64_t)) || (size == sizeof(uint32_t)));
+
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        switch (size) {
+        case sizeof(uint64_t):
+            *op       = MLX5_OPCODE_ATOMIC_FA;
+            *ext      = 0;
+            break;
+        case sizeof(uint32_t):
+            *op       = MLX5_OPCODE_ATOMIC_MASKED_FA;
+            *ext      = 1;
+            break;
+        default:
+            ucs_assertv(0, "incorrect atomic size: %d", size);
+            return UCS_ERR_INVALID_PARAM;
+        }
+        *compare_mask = 0;
+        *compare      = 0;
+        *swap_mask    = 0;
+        *swap         = UCT_RC_MLX5_TO_BE(value, size);
+        break;
+    case UCT_ATOMIC_OP_AND:
+        *op           = MLX5_OPCODE_ATOMIC_MASKED_CS;
+        *compare_mask = 0;
+        *compare      = 0;
+        *swap_mask    = UCT_RC_MLX5_TO_BE(~value, size);
+        *swap         = UCT_RC_MLX5_TO_BE(value, size);
+        *ext          = 1;
+        break;
+    case UCT_ATOMIC_OP_OR:
+        *op           = MLX5_OPCODE_ATOMIC_MASKED_CS;
+        *compare_mask = 0;
+        *compare      = 0;
+        *swap_mask    = UCT_RC_MLX5_TO_BE(value, size);
+        *swap         = UCT_RC_MLX5_TO_BE(value, size);
+        *ext          = 1;
+        break;
+    case UCT_ATOMIC_OP_XOR:
+        *op           = MLX5_OPCODE_ATOMIC_MASKED_FA;
+        *compare_mask = 0;
+        *compare      = -1;
+        *swap_mask    = 0;
+        *swap         = UCT_RC_MLX5_TO_BE(value, size);
+        *ext          = 1;
+        break;
+    case UCT_ATOMIC_OP_SWAP:
+        *op           = MLX5_OPCODE_ATOMIC_MASKED_CS;
+        *compare_mask = 0;
+        *compare      = 0;
+        *swap_mask    = -1;
+        *swap         = UCT_RC_MLX5_TO_BE(value, size);
+        *ext          = 1;
+        break;
+    default:
+        ucs_assertv(0, "incorrect atomic opcode: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+    return UCS_OK;
+}
+
+
+
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_ep.c
index e839b058c..5c6599cb4 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_ep.c
@@ -24,14 +24,15 @@ uct_rc_mlx5_txqp_bcopy_post(uct_rc_iface_t *iface, uct_rc_txqp_t *txqp, uct_ib_m
                             unsigned opcode, unsigned length,
                             /* RDMA */ uint64_t rdma_raddr, uct_rkey_t rdma_rkey,
                             uint8_t fm_ce_se, uint32_t imm_val_be,
-                            uct_rc_iface_send_desc_t *desc)
+                            uct_rc_iface_send_desc_t *desc, const void *buffer,
+                            uct_ib_log_sge_t *log_sge)
 {
     desc->super.sn = txwq->sw_pi;
     uct_rc_mlx5_txqp_dptr_post(iface, IBV_QPT_RC, txqp, txwq,
-                               opcode, desc + 1, length, &desc->lkey,
+                               opcode, buffer, length, &desc->lkey,
                                rdma_raddr, uct_ib_md_direct_rkey(rdma_rkey),
-                               0, 0, 0,
-                               NULL, NULL, 0, fm_ce_se, imm_val_be);
+                               0, 0, 0, 0,
+                               NULL, NULL, 0, fm_ce_se, imm_val_be, INT_MAX, log_sge);
     uct_rc_txqp_add_send_op(txqp, &desc->super);
 }
 
@@ -61,89 +62,119 @@ uct_rc_mlx5_ep_zcopy_post(uct_rc_mlx5_ep_t *ep,
                                    rdma_raddr, uct_ib_md_direct_rkey(rdma_rkey),
                                    tag, app_ctx, ib_imm_be,
                                    NULL, NULL, 0,
-                                   (comp == NULL) ? force_sig : MLX5_WQE_CTRL_CQ_UPDATE);
+                                   (comp == NULL) ? force_sig : MLX5_WQE_CTRL_CQ_UPDATE,
+                                   UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super));
 
     uct_rc_txqp_add_send_comp(iface, &ep->super.txqp, comp, sn);
     return UCS_INPROGRESS;
 }
 
-static UCS_F_ALWAYS_INLINE void
-uct_rc_mlx5_ep_atomic_post(uct_rc_mlx5_ep_t *ep, unsigned opcode,
-                           uct_rc_iface_send_desc_t *desc, unsigned length,
-                           uint64_t remote_addr, uct_rkey_t rkey,
-                           uint64_t compare_mask, uint64_t compare,
-                           uint64_t swap_add, int signal)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_put_short_inline(uct_ep_h tl_ep, const void *buffer, unsigned length,
+                                uint64_t remote_addr, uct_rkey_t rkey)
 {
-    uct_rc_iface_t *iface  = ucs_derived_of(ep->super.super.super.iface,
-                                            uct_rc_iface_t);
-    uint32_t ib_rkey = uct_ib_resolve_atomic_rkey(rkey, ep->super.atomic_mr_offset,
-                                                  &remote_addr);
+    uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
+    uct_rc_mlx5_ep_t *ep  = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
 
-    desc->super.sn = ep->tx.wq.sw_pi;
-    uct_rc_mlx5_txqp_dptr_post(iface, IBV_QPT_RC,
-                               &ep->super.txqp, &ep->tx.wq,
-                               opcode, desc + 1, length, &desc->lkey,
-                               remote_addr, ib_rkey,
-                               compare_mask, compare, swap_add,
-                               NULL, NULL, 0, signal, 0);
+    UCT_RC_MLX5_CHECK_PUT_SHORT(length, 0);
+    UCT_RC_CHECK_RES(iface, &ep->super);
 
-    UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
-    uct_rc_txqp_add_send_op(&ep->super.txqp, &desc->super);
+    uct_rc_mlx5_txqp_inline_post(iface, IBV_QPT_RC,
+                                 &ep->super.txqp, &ep->tx.wq,
+                                 MLX5_OPCODE_RDMA_WRITE,
+                                 buffer, length, 0, 0, 0,
+                                 remote_addr, uct_ib_md_direct_rkey(rkey),
+                                 NULL, NULL, 0, 0, INT_MAX);
+    UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
+    return UCS_OK;
 }
 
-static UCS_F_ALWAYS_INLINE ucs_status_t
-uct_rc_mlx5_ep_atomic(uct_rc_mlx5_ep_t *ep, int opcode, void *result, int ext,
-                      unsigned length, uint64_t remote_addr, uct_rkey_t rkey,
-                      uint64_t compare_mask, uint64_t compare,
-                      uint64_t swap_add, uct_completion_t *comp)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_am_short_inline(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
+                               const void *payload, unsigned length)
 {
-    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface,
-                                                uct_rc_mlx5_iface_t);
-    uct_rc_iface_send_desc_t *desc;
+    uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
+    uct_rc_mlx5_ep_t *ep  = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
 
-    UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->mlx5_common.tx.atomic_desc_mp, desc,
-                                    uct_rc_iface_atomic_handler(&iface->super, ext, length),
-                                    result, comp);
-    uct_rc_mlx5_ep_atomic_post(ep, opcode, desc, length, remote_addr, rkey,
-                               compare_mask, compare, swap_add,
-                               MLX5_WQE_CTRL_CQ_UPDATE);
-    return UCS_INPROGRESS;
+    UCT_RC_MLX5_CHECK_AM_SHORT(id, length, 0);
+
+    UCT_RC_CHECK_RES(iface, &ep->super);
+    UCT_RC_CHECK_FC(iface, &ep->super, id);
+
+    uct_rc_mlx5_txqp_inline_post(iface, IBV_QPT_RC,
+                                 &ep->super.txqp, &ep->tx.wq,
+                                 MLX5_OPCODE_SEND,
+                                 payload, length,
+                                 id, hdr, 0,
+                                 0, 0,
+                                 NULL, NULL, 0,
+                                 MLX5_WQE_CTRL_SOLICITED,
+                                 INT_MAX);
+    UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(hdr) + length);
+    UCT_RC_UPDATE_FC(iface, &ep->super, id);
+    return UCS_OK;
 }
 
-static UCS_F_ALWAYS_INLINE ucs_status_t
-uct_rc_mlx5_ep_atomic_add(uct_ep_h tl_ep, int opcode, unsigned length,
-                          uint64_t add, uint64_t remote_addr, uct_rkey_t rkey)
+#if HAVE_IBV_EXP_DM
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_short_dm(uct_rc_mlx5_ep_t *ep, uct_rc_mlx5_dm_copy_data_t *cache,
+                        size_t hdr_len, const void *payload, unsigned length,
+                        unsigned opcode, uint8_t fm_ce_se,
+                        uint64_t rdma_raddr, uct_rkey_t rdma_rkey)
 {
-    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_mlx5_iface_t);
-    uct_rc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface, uct_rc_mlx5_iface_t);
+    uct_rc_iface_t *rc_iface   = &iface->super;
     uct_rc_iface_send_desc_t *desc;
+    void *buffer;
+    ucs_status_t status;
+    uct_ib_log_sge_t log_sge;
 
-    UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super, &iface->mlx5_common.tx.atomic_desc_mp, desc);
+    status = uct_rc_mlx5_common_dm_make_data(&iface->mlx5_common, &iface->super,
+                                             cache, hdr_len, payload, length, &desc,
+                                             &buffer, &log_sge);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
 
-    uct_rc_mlx5_ep_atomic_post(ep, opcode, desc, length, remote_addr, rkey, 0,
-                               0, add, 0);
+    uct_rc_mlx5_txqp_bcopy_post(rc_iface, &ep->super.txqp, &ep->tx.wq,
+                                opcode, hdr_len + length,
+                                rdma_raddr, rdma_rkey, fm_ce_se,
+                                0, desc, buffer,
+                                log_sge.num_sge ? &log_sge : NULL);
     return UCS_OK;
 }
+#endif
 
-ucs_status_t uct_rc_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer, unsigned length,
-                                      uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t
+uct_rc_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer, unsigned length,
+                         uint64_t remote_addr, uct_rkey_t rkey)
 {
-    uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
-    uct_rc_mlx5_ep_t *ep  = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+#if HAVE_IBV_EXP_DM
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_mlx5_iface_t);
+    uct_rc_iface_t *rc_iface   = &iface->super;
+    uct_rc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    ucs_status_t status;
 
-    UCT_RC_MLX5_CHECK_PUT_SHORT(length, 0);
-    UCT_RC_CHECK_RES(iface, &ep->super);
+    if (ucs_likely((length <= UCT_IB_MLX5_PUT_MAX_SHORT(0)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_rc_mlx5_ep_put_short_inline(tl_ep, buffer, length, remote_addr, rkey);
+#if HAVE_IBV_EXP_DM
+    }
+
+    UCT_CHECK_LENGTH(length, 0, iface->mlx5_common.dm.seg_len, "put_short");
+    UCT_RC_CHECK_RES(rc_iface, &ep->super);
+    status =  uct_rc_mlx5_ep_short_dm(ep, NULL, 0, buffer, length,
+                                      MLX5_OPCODE_RDMA_WRITE,
+                                      MLX5_WQE_CTRL_CQ_UPDATE,
+                                      remote_addr, rkey);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
 
-    uct_rc_mlx5_txqp_inline_post(iface, IBV_QPT_RC,
-                                 &ep->super.txqp, &ep->tx.wq,
-                                 MLX5_OPCODE_RDMA_WRITE,
-                                 buffer, length, 0, 0, 0,
-                                 remote_addr, uct_ib_md_direct_rkey(rkey),
-                                 NULL, NULL, 0, 0);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
     return UCS_OK;
+#endif
 }
 
 ssize_t uct_rc_mlx5_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
@@ -160,7 +191,8 @@ ssize_t uct_rc_mlx5_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
 
     uct_rc_mlx5_txqp_bcopy_post(iface, &ep->super.txqp, &ep->tx.wq,
                                 MLX5_OPCODE_RDMA_WRITE, length, remote_addr,
-                                rkey, MLX5_WQE_CTRL_CQ_UPDATE, 0, desc);
+                                rkey, MLX5_WQE_CTRL_CQ_UPDATE, 0, desc, desc + 1,
+                                NULL);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, BCOPY, length);
     return length;
 }
@@ -203,7 +235,8 @@ ucs_status_t uct_rc_mlx5_ep_get_bcopy(uct_ep_h tl_ep,
 
     uct_rc_mlx5_txqp_bcopy_post(iface, &ep->super.txqp, &ep->tx.wq,
                                 MLX5_OPCODE_RDMA_READ, length, remote_addr,
-                                rkey, MLX5_WQE_CTRL_CQ_UPDATE, 0, desc);
+                                rkey, MLX5_WQE_CTRL_CQ_UPDATE, 0, desc, desc + 1,
+                                NULL);
     UCT_TL_EP_STAT_OP(&ep->super.super, GET, BCOPY, length);
     return UCS_INPROGRESS;
 }
@@ -230,28 +263,45 @@ ucs_status_t uct_rc_mlx5_ep_get_zcopy(uct_ep_h tl_ep, const uct_iov_t *iov, size
     return status;
 }
 
-ucs_status_t uct_rc_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
-                                     const void *payload, unsigned length)
+ucs_status_t
+uct_rc_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
+                        const void *payload, unsigned length)
 {
-    uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
-    uct_rc_mlx5_ep_t *ep  = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+#if HAVE_IBV_EXP_DM
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_mlx5_iface_t);
+    uct_rc_iface_t *rc_iface   = &iface->super;
+    uct_rc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    ucs_status_t status;
+    uct_rc_mlx5_dm_copy_data_t cache;
 
-    UCT_RC_MLX5_CHECK_AM_SHORT(id, length, 0);
+    if (ucs_likely((sizeof(uct_rc_am_short_hdr_t) + length <= UCT_IB_MLX5_AM_MAX_SHORT(0)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_rc_mlx5_ep_am_short_inline(tl_ep, id, hdr, payload, length);
+#if HAVE_IBV_EXP_DM
+    }
 
-    UCT_RC_CHECK_RES(iface, &ep->super);
-    UCT_RC_CHECK_FC(iface, &ep->super, id);
+    UCT_CHECK_LENGTH(length + sizeof(uct_rc_am_short_hdr_t), 0,
+                     iface->mlx5_common.dm.seg_len, "am_short");
+    UCT_CHECK_AM_ID(id);
+    UCT_RC_CHECK_RES(rc_iface, &ep->super);
+    UCT_RC_CHECK_FC(rc_iface, &ep->super, id);
 
-    uct_rc_mlx5_txqp_inline_post(iface, IBV_QPT_RC,
-                                 &ep->super.txqp, &ep->tx.wq,
-                                 MLX5_OPCODE_SEND,
-                                 payload, length,
-                                 id, hdr, 0,
-                                 0, 0,
-                                 NULL, NULL, 0,
-                                 MLX5_WQE_CTRL_SOLICITED);
-    UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(hdr) + length);
-    UCT_RC_UPDATE_FC(iface, &ep->super, id);
+    uct_rc_am_hdr_fill(&cache.am_hdr.rc_hdr, id);
+    cache.am_hdr.am_hdr = hdr;
+
+    status = uct_rc_mlx5_ep_short_dm(ep, &cache, sizeof(cache.am_hdr), payload, length,
+                                     MLX5_OPCODE_SEND,
+                                     MLX5_WQE_CTRL_SOLICITED | MLX5_WQE_CTRL_CQ_UPDATE,
+                                     0, 0);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(cache.am_hdr) + length);
+    UCT_RC_UPDATE_FC(rc_iface, &ep->super, id);
     return UCS_OK;
+#endif
 }
 
 ssize_t uct_rc_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
@@ -271,7 +321,8 @@ ssize_t uct_rc_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
 
     uct_rc_mlx5_txqp_bcopy_post(iface, &ep->super.txqp, &ep->tx.wq,
                                 MLX5_OPCODE_SEND, sizeof(uct_rc_hdr_t) + length,
-                                0, 0, MLX5_WQE_CTRL_SOLICITED, 0, desc);
+                                0, 0, MLX5_WQE_CTRL_SOLICITED, 0, desc, desc + 1,
+                                NULL);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, BCOPY, length);
     UCT_RC_UPDATE_FC(iface, &ep->super, id);
     return length;
@@ -303,78 +354,157 @@ ucs_status_t uct_rc_mlx5_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *hea
     return status;
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey)
+static UCS_F_ALWAYS_INLINE void
+uct_rc_mlx5_ep_atomic_post(uct_rc_mlx5_ep_t *ep, unsigned opcode,
+                           uct_rc_iface_send_desc_t *desc, unsigned length,
+                           uint64_t remote_addr, uct_rkey_t rkey,
+                           uint64_t compare_mask, uint64_t compare,
+                           uint64_t swap_mask, uint64_t swap_add, int signal)
+{
+    uct_rc_iface_t *iface  = ucs_derived_of(ep->super.super.super.iface,
+                                            uct_rc_iface_t);
+    uint32_t ib_rkey       = uct_ib_resolve_atomic_rkey(rkey, ep->super.atomic_mr_offset,
+                                                        &remote_addr);
+
+    desc->super.sn = ep->tx.wq.sw_pi;
+    uct_rc_mlx5_txqp_dptr_post(iface, IBV_QPT_RC,
+                               &ep->super.txqp, &ep->tx.wq,
+                               opcode, desc + 1, length, &desc->lkey,
+                               remote_addr, ib_rkey,
+                               compare_mask, compare, swap_mask, swap_add,
+                               NULL, NULL, 0, signal, 0, INT_MAX, NULL);
+
+    UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
+    uct_rc_txqp_add_send_op(&ep->super.txqp, &desc->super);
+}
+
+static UCS_F_ALWAYS_INLINE ucs_status_t
+uct_rc_mlx5_ep_atomic_fop(uct_rc_mlx5_ep_t *ep, int opcode, void *result, int ext,
+                          unsigned length, uint64_t remote_addr, uct_rkey_t rkey,
+                          uint64_t compare_mask, uint64_t compare,
+                          uint64_t swap_mask, uint64_t swap_add, uct_completion_t *comp)
 {
-    return uct_rc_mlx5_ep_atomic_add(tl_ep, MLX5_OPCODE_ATOMIC_FA, sizeof(uint64_t),
-                                     htobe64(add), remote_addr, rkey);
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface,
+                                                uct_rc_mlx5_iface_t);
+    uct_rc_iface_send_desc_t *desc;
+
+    UCT_RC_CHECK_RES(&iface->super, &ep->super);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super, &iface->mlx5_common.tx.atomic_desc_mp,
+                                          desc, uct_rc_iface_atomic_handler(&iface->super, ext,
+                                                                            length),
+                                          result, comp);
+    uct_rc_mlx5_ep_atomic_post(ep, opcode, desc, length, remote_addr, rkey,
+                               compare_mask, compare, swap_mask, swap_add,
+                               MLX5_WQE_CTRL_CQ_UPDATE);
+    return UCS_INPROGRESS;
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_atomic_op_post(uct_ep_h tl_ep, unsigned opcode, unsigned size,
+                              uint64_t value, uint64_t remote_addr, uct_rkey_t rkey)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_FA, result, 0, sizeof(uint64_t),
-                                 remote_addr, rkey, 0, 0, htobe64(add), comp);
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_mlx5_iface_t);
+    uct_rc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    uct_rc_iface_send_desc_t *desc;
+    int op;
+    uint64_t compare_mask;
+    uint64_t compare;
+    uint64_t swap_mask;
+    uint64_t swap;
+    int      ext; /* not used here */
+    ucs_status_t status;
+
+    UCT_RC_CHECK_RES(&iface->super, &ep->super);
+    UCT_RC_MLX5_CHECK_ATOMIC_OPS(opcode, size, UCT_RC_MLX5_ATOMIC_OPS);
+
+    status = uct_rc_mlx5_iface_common_atomic_data(opcode, size, value, &op, &compare_mask,
+                                                  &compare, &swap_mask, &swap, &ext);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
+
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->mlx5_common.tx.atomic_desc_mp, desc);
+
+    uct_rc_mlx5_ep_atomic_post(ep, op, desc, size, remote_addr, rkey,
+                               compare_mask, compare, swap_mask, swap, 0);
+    return UCS_OK;
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint64_t *result, uct_completion_t *comp)
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_atomic_fop_post(uct_ep_h tl_ep, unsigned opcode, unsigned size,
+                               uint64_t value, void *result,
+                               uint64_t remote_addr, uct_rkey_t rkey,
+                               uct_completion_t *comp)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint64_t), remote_addr, rkey, 0, 0,
-                                 htobe64(swap), comp);
+    uct_rc_mlx5_ep_t *ep = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    int op;
+    uint64_t compare_mask;
+    uint64_t compare;
+    uint64_t swap_mask;
+    uint64_t swap;
+    int      ext;
+    ucs_status_t status;
+
+    UCT_RC_MLX5_CHECK_ATOMIC_OPS(opcode, size, UCT_RC_MLX5_ATOMIC_FOPS);
+
+    status = uct_rc_mlx5_iface_common_atomic_data(opcode, size, value, &op, &compare_mask,
+                                                  &compare, &swap_mask, &swap, &ext);
+    if (ucs_unlikely(UCS_STATUS_IS_ERR(status))) {
+        return status;
+    }
+
+    return uct_rc_mlx5_ep_atomic_fop(ep, op, result, ext, size, remote_addr, rkey,
+                                     compare_mask, compare, swap_mask, swap, comp);
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp)
+ucs_status_t uct_rc_mlx5_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_CS, result, 0, sizeof(uint64_t),
-                                 remote_addr, rkey, 0, htobe64(compare), htobe64(swap),
-                                 comp);
+    return uct_rc_mlx5_ep_atomic_op_post(ep, opcode, sizeof(value), value, remote_addr, rkey);
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                         uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_rc_mlx5_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                          uint64_t remote_addr, uct_rkey_t rkey)
 {
-    return uct_rc_mlx5_ep_atomic_add(tl_ep, MLX5_OPCODE_ATOMIC_MASKED_FA,
-                                     sizeof(uint32_t), htonl(add), remote_addr,
-                                     rkey);
+    return uct_rc_mlx5_ep_atomic_op_post(ep, opcode, sizeof(value), value, remote_addr, rkey);
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_rc_mlx5_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint64_t value, uint64_t *result,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uct_completion_t *comp)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_FA, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, 0, 0,
-                                 htonl(add), comp);
+    return uct_rc_mlx5_ep_atomic_fop_post(ep, opcode, sizeof(value), value, result,
+                                          remote_addr, rkey, comp);
 }
 
-ucs_status_t uct_rc_mlx5_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                          uint64_t remote_addr, uct_rkey_t rkey,
-                                          uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_rc_mlx5_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                           uint32_t value, uint32_t *result,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uct_completion_t *comp)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, 0, 0,
-                                 htonl(swap), comp);
+    return uct_rc_mlx5_ep_atomic_fop_post(ep, opcode, sizeof(value), value, result,
+                                          remote_addr, rkey, comp);
+}
+
+ucs_status_t uct_rc_mlx5_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
+                                           uint64_t remote_addr, uct_rkey_t rkey,
+                                           uint64_t *result, uct_completion_t *comp)
+{
+    return uct_rc_mlx5_ep_atomic_fop(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
+                                     MLX5_OPCODE_ATOMIC_CS, result, 0, sizeof(uint64_t),
+                                     remote_addr, rkey, 0, htobe64(compare), -1, htobe64(swap),
+                                     comp);
 }
 
 ucs_status_t uct_rc_mlx5_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
                                            uint64_t remote_addr, uct_rkey_t rkey,
                                            uint32_t *result, uct_completion_t *comp)
 {
-    return uct_rc_mlx5_ep_atomic(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
-                                 MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
-                                 sizeof(uint32_t), remote_addr, rkey, UCS_MASK(32),
-                                 htonl(compare), htonl(swap), comp);
+    return uct_rc_mlx5_ep_atomic_fop(ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t),
+                                     MLX5_OPCODE_ATOMIC_MASKED_CS, result, 1,
+                                     sizeof(uint32_t), remote_addr, rkey, UCS_MASK(32),
+                                     htonl(compare), -1, htonl(swap), comp);
 }
 
 ucs_status_t uct_rc_mlx5_ep_flush(uct_ep_h tl_ep, unsigned flags,
@@ -398,14 +528,18 @@ ucs_status_t uct_rc_mlx5_ep_flush(uct_ep_h tl_ep, unsigned flags,
                                      MLX5_OPCODE_NOP, NULL, 0,
                                      0, 0, 0,
                                      0, 0,
-                                     NULL, NULL, 0, 0);
+                                     NULL, NULL, 0, 0,
+                                     INT_MAX);
     } else {
         sn = ep->tx.wq.sig_pi;
     }
 
-    uct_rc_txqp_add_send_comp(&iface->super, &ep->super.txqp, comp, sn);
-    UCT_TL_EP_STAT_FLUSH_WAIT(&ep->super.super);
-    return UCS_INPROGRESS;
+    status = uct_rc_txqp_add_flush_comp(&iface->super, &ep->super.txqp, comp, sn);
+    if (status == UCS_INPROGRESS) {
+        UCT_TL_EP_STAT_FLUSH_WAIT(&ep->super.super);
+    }
+
+    return status;
 }
 
 ucs_status_t uct_rc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
@@ -425,13 +559,16 @@ ucs_status_t uct_rc_mlx5_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
                                  NULL, 0,
                                  UCT_RC_EP_FC_PURE_GRANT, 0, 0,
                                  0, 0,
-                                 NULL, NULL, 0, 0);
+                                 NULL, NULL, 0, 0,
+                                 INT_MAX);
     return UCS_OK;
 }
 
 #if IBV_EXP_HW_TM
-ucs_status_t uct_rc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
-                                            const void *data, size_t length)
+
+static ucs_status_t UCS_F_ALWAYS_INLINE
+uct_rc_mlx5_ep_tag_eager_short_inline(uct_ep_h tl_ep, uct_tag_t tag,
+                                      const void *data, size_t length)
 {
     uct_rc_mlx5_ep_t *ep  = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
     uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
@@ -444,9 +581,47 @@ ucs_status_t uct_rc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
                                      &ep->tx.wq, MLX5_OPCODE_SEND, data, length,
                                      NULL, tag, 0, IBV_EXP_TMH_EAGER, 0, NULL,
                                      NULL, 0, NULL, 0, MLX5_WQE_CTRL_SOLICITED);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
+
     return UCS_OK;
 }
 
+ucs_status_t uct_rc_mlx5_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
+                                            const void *data, size_t length)
+{
+#if HAVE_IBV_EXP_DM
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_mlx5_iface_t);
+    uct_rc_iface_t *rc_iface   = &iface->super;
+    uct_rc_mlx5_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_mlx5_ep_t);
+    uct_rc_mlx5_dm_copy_data_t cache;
+    ucs_status_t status;
+
+    if (ucs_likely((sizeof(struct ibv_exp_tmh) + length <= UCT_IB_MLX5_AM_MAX_SHORT(0)) ||
+                   !iface->mlx5_common.dm.dm)) {
+#endif
+        return uct_rc_mlx5_ep_tag_eager_short_inline(tl_ep, tag, data, length);
+#if HAVE_IBV_EXP_DM
+    }
+
+    UCT_CHECK_LENGTH(length + sizeof(struct ibv_exp_tmh), 0,
+                     iface->mlx5_common.dm.seg_len, "tag_short");
+    UCT_RC_CHECK_RES(rc_iface, &ep->super);
+
+    uct_rc_mlx5_fill_tmh(ucs_unaligned_ptr(&cache.tm_hdr), tag, 0, IBV_EXP_TMH_EAGER);
+
+    status = uct_rc_mlx5_ep_short_dm(ep, &cache, sizeof(cache.tm_hdr), data, length,
+                                   MLX5_OPCODE_SEND,
+                                   MLX5_WQE_CTRL_SOLICITED | MLX5_WQE_CTRL_CQ_UPDATE,
+                                   0, 0);
+    if (!UCS_STATUS_IS_ERR(status)) {
+        UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
+    }
+
+    return status;
+#endif
+}
+
 ssize_t uct_rc_mlx5_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                        uint64_t imm,
                                        uct_pack_callback_t pack_cb,
@@ -464,12 +639,16 @@ ssize_t uct_rc_mlx5_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
     UCT_RC_IFACE_FILL_TM_IMM(imm, app_ctx, ib_imm, opcode, MLX5_OPCODE_SEND,
                              _IMM);
 
-    UCT_RC_IFACE_GET_TM_BCOPY_DESC(iface, &iface->tx.mp, desc,
-                                   tag, app_ctx, pack_cb, arg, length);
+    UCT_RC_MLX5_IFACE_GET_TM_BCOPY_DESC(iface, &iface->tx.mp, desc, tag,
+                                        app_ctx, pack_cb, arg, length);
 
     uct_rc_mlx5_txqp_bcopy_post(iface, &ep->super.txqp, &ep->tx.wq,
                                 opcode, sizeof(struct ibv_exp_tmh) + length,
-                                0, 0, MLX5_WQE_CTRL_SOLICITED, ib_imm, desc);
+                                0, 0, MLX5_WQE_CTRL_SOLICITED, ib_imm,
+                                desc, desc + 1, NULL);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, BCOPY, length);
+
     return length;
 }
 
@@ -483,7 +662,8 @@ ucs_status_t uct_rc_mlx5_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     uint32_t app_ctx, ib_imm;
     int opcode;
 
-    UCT_CHECK_IOV_SIZE(iovcnt, 1ul, "uct_rc_mlx5_ep_tag_eager_zcopy");
+    UCT_CHECK_IOV_SIZE(iovcnt, UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(0),
+                       "uct_rc_mlx5_ep_tag_eager_zcopy");
     UCT_RC_CHECK_ZCOPY_DATA(sizeof(struct ibv_exp_tmh),
                             uct_iov_total_length(iov, iovcnt),
                             iface->super.config.seg_size);
@@ -491,6 +671,9 @@ ucs_status_t uct_rc_mlx5_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     UCT_RC_IFACE_FILL_TM_IMM(imm, app_ctx, ib_imm, opcode, MLX5_OPCODE_SEND,
                              _IMM);
 
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
+
     return uct_rc_mlx5_ep_zcopy_post(ep, opcode|UCT_RC_MLX5_OPCODE_FLAG_TM,
                                      iov, iovcnt, 0, "", 0, 0, 0,
                                      tag, app_ctx, ib_imm,
@@ -557,7 +740,8 @@ UCS_CLASS_INIT_FUNC(uct_rc_mlx5_ep_t, uct_iface_h tl_iface)
 
     UCS_CLASS_CALL_SUPER_INIT(uct_rc_ep_t, &iface->super);
 
-    status = uct_ib_mlx5_txwq_init(iface->super.super.super.worker, &self->tx.wq,
+    status = uct_ib_mlx5_txwq_init(iface->super.super.super.worker,
+                                   iface->tx.mmio_mode, &self->tx.wq,
                                    self->super.txqp.qp);
     if (status != UCS_OK) {
         ucs_error("Failed to get mlx5 QP information");
@@ -570,9 +754,10 @@ UCS_CLASS_INIT_FUNC(uct_rc_mlx5_ep_t, uct_iface_h tl_iface)
     return UCS_OK;
 }
 
-static void uct_rc_mlx5_ep_reset_qp(uct_rc_mlx5_ep_t *ep)
+static void uct_rc_mlx5_ep_clean_qp(uct_rc_mlx5_ep_t *ep, struct ibv_qp *qp)
 {
-    uct_rc_txqp_t *txqp = &ep->super.txqp;
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ep->super.super.super.iface,
+                                                uct_rc_mlx5_iface_t);
 
     /* Make the HW generate CQEs for all in-progress SRQ receives from the QP,
      * so we clean them all before ibv_modify_qp() can see them.
@@ -587,13 +772,24 @@ static void uct_rc_mlx5_ep_reset_qp(uct_rc_mlx5_ep_t *ep)
      */
     memset(&qp_attr, 0, sizeof(qp_attr));
     qp_attr.qp_state = IBV_QPS_RESET;
-    ret = ibv_cmd_modify_qp(txqp->qp, &qp_attr, IBV_QP_STATE, &cmd, sizeof(cmd));
+    ret = ibv_cmd_modify_qp(qp, &qp_attr, IBV_QP_STATE, &cmd, sizeof(cmd));
     if (ret) {
-        ucs_warn("modify qp 0x%x to RESET failed: %m", txqp->qp->qp_num);
+        ucs_warn("modify qp 0x%x to RESET failed: %m", qp->qp_num);
     }
 #else
-    (void)uct_rc_modify_qp(txqp, IBV_QPS_ERR);
+    (void)uct_ib_modify_qp(qp, IBV_QPS_ERR);
 #endif
+
+    iface->super.rx.srq.available += uct_rc_mlx5_iface_commom_clean(
+            &iface->mlx5_common.cq[UCT_IB_DIR_RX],
+            &iface->mlx5_common.rx.srq, qp->qp_num);
+
+    /* Synchronize CQ index with the driver, since it would remove pending
+     * completions for this QP (both send and receive) during ibv_destroy_qp().
+     */
+    uct_rc_mlx5_iface_common_update_cqs_ci(&iface->mlx5_common, &iface->super.super);
+    (void)uct_ib_modify_qp(qp, IBV_QPS_RESET);
+    uct_rc_mlx5_iface_common_sync_cqs_ci(&iface->mlx5_common, &iface->super.super);
 }
 
 static UCS_CLASS_CLEANUP_FUNC(uct_rc_mlx5_ep_t)
@@ -602,17 +798,12 @@ static UCS_CLASS_CLEANUP_FUNC(uct_rc_mlx5_ep_t)
                                                 uct_rc_mlx5_iface_t);
 
     uct_ib_mlx5_txwq_cleanup(&self->tx.wq);
-
-    uct_rc_mlx5_ep_reset_qp(self);
-    uct_rc_mlx5_iface_commom_clean_srq(&iface->mlx5_common, &iface->super,
-                                       self->qp_num);
-
-    /* Synchronize CQ index with the driver, since it would remove pending
-     * completions for this QP (both send and receive) during ibv_destroy_qp().
-     */
-    uct_rc_mlx5_iface_common_update_cqs_ci(&iface->mlx5_common, &iface->super.super);
-    (void)uct_rc_modify_qp(&self->super.txqp, IBV_QPS_RESET);
-    uct_rc_mlx5_iface_common_sync_cqs_ci(&iface->mlx5_common, &iface->super.super);
+    uct_rc_mlx5_ep_clean_qp(self, self->super.txqp.qp);
+#if IBV_EXP_HW_TM
+    if (UCT_RC_IFACE_TM_ENABLED(&iface->super)) {
+        uct_rc_mlx5_ep_clean_qp(self, self->super.tm_qp);
+    }
+#endif
 
     /* Return all credits if user do flush(UCT_FLUSH_FLAG_CANCEL) before
      * ep_destroy.
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_iface.c
index fce6f8d48..1d4eaa24c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/accel/rc_mlx5_iface.c
@@ -7,6 +7,7 @@
 
 #include <uct/ib/mlx5/ib_mlx5.h>
 #include <uct/ib/mlx5/ib_mlx5_log.h>
+#include <uct/ib/mlx5/ib_mlx5_dv.h>
 #include <uct/ib/base/ib_device.h>
 #include <uct/base/uct_md.h>
 #include <ucs/arch/cpu.h>
@@ -23,6 +24,10 @@ ucs_config_field_t uct_rc_mlx5_iface_config_table[] = {
    ucs_offsetof(uct_rc_mlx5_iface_config_t, fc),
    UCS_CONFIG_TYPE_TABLE(uct_rc_fc_config_table)},
 
+  {"", "", NULL,
+   ucs_offsetof(uct_rc_mlx5_iface_config_t, mlx5_common),
+   UCS_CONFIG_TYPE_TABLE(uct_ib_mlx5_iface_config_table)},
+
   {"TX_MAX_BB", "-1",
    "Limits the number of outstanding WQE building blocks. The actual limit is\n"
    "a minimum between this value and the number of building blocks in the TX QP.\n"
@@ -40,9 +45,9 @@ uct_rc_mlx5_iface_poll_tx(uct_rc_mlx5_iface_t *iface)
     struct mlx5_cqe64 *cqe;
     uct_rc_mlx5_ep_t *ep;
     unsigned qp_num;
-    uint16_t hw_ci, bb_num;
+    uint16_t hw_ci;
 
-    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->mlx5_common.tx.cq);
+    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->mlx5_common.cq[UCT_IB_DIR_TX]);
     if (cqe == NULL) {
         return 0;
     }
@@ -58,11 +63,8 @@ uct_rc_mlx5_iface_poll_tx(uct_rc_mlx5_iface_t *iface)
     ucs_trace_poll("rc_mlx5 iface %p tx_cqe: ep %p qpn 0x%x hw_ci %d", iface, ep,
                    qp_num, hw_ci);
 
-    bb_num = uct_ib_mlx5_txwq_update_bb(&ep->tx.wq, hw_ci) -
-             uct_rc_txqp_available(&ep->super.txqp);
-    uct_rc_txqp_available_add(&ep->super.txqp, bb_num);
-    iface->super.tx.cq_available += bb_num;
-
+    uct_rc_mlx5_common_update_tx_res(&iface->super, &ep->tx.wq, &ep->super.txqp,
+                                     hw_ci);
     uct_rc_mlx5_txqp_process_tx_cqe(&ep->super.txqp, cqe, hw_ci);
 
     ucs_arbiter_group_schedule(&iface->super.tx.arbiter, &ep->super.arb_group);
@@ -86,36 +88,46 @@ unsigned uct_rc_mlx5_iface_progress(void *arg)
 
 static ucs_status_t uct_rc_mlx5_iface_query(uct_iface_h tl_iface, uct_iface_attr_t *iface_attr)
 {
-    uct_rc_iface_t *iface = ucs_derived_of(tl_iface, uct_rc_iface_t);
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(tl_iface, uct_rc_mlx5_iface_t);
+    uct_rc_iface_t *rc_iface   = &iface->super;
+    size_t max_am_inline       = UCT_IB_MLX5_AM_MAX_SHORT(0);
+    size_t max_put_inline      = UCT_IB_MLX5_PUT_MAX_SHORT(0);
     ucs_status_t status;
 
-    status = uct_rc_iface_query(iface, iface_attr,
-                                UCT_IB_MLX5_PUT_MAX_SHORT(0),
-                                UCT_IB_MLX5_AM_MAX_SHORT(0),
+#if HAVE_IBV_EXP_DM
+    if (iface->mlx5_common.dm.dm != NULL) {
+        max_am_inline  = ucs_max(iface->mlx5_common.dm.dm->seg_len, UCT_IB_MLX5_AM_MAX_SHORT(0));
+        max_put_inline = ucs_max(iface->mlx5_common.dm.dm->seg_len, UCT_IB_MLX5_PUT_MAX_SHORT(0));
+    }
+#endif
+
+    status = uct_rc_iface_query(rc_iface, iface_attr,
+                                max_put_inline,
+                                max_am_inline,
                                 UCT_IB_MLX5_AM_ZCOPY_MAX_HDR(0),
-                                UCT_IB_MLX5_AM_ZCOPY_MAX_IOV);
+                                UCT_IB_MLX5_AM_ZCOPY_MAX_IOV,
+                                UCT_RC_MLX5_TM_EAGER_ZCOPY_MAX_IOV(0));
     if (status != UCS_OK) {
         return status;
     }
 
-    uct_rc_mlx5_iface_common_query(iface_attr);
+    uct_rc_mlx5_iface_common_query(&rc_iface->super, iface_attr);
     iface_attr->latency.growth += 1e-9; /* 1 ns per each extra QP */
     return UCS_OK;
 }
 
-static ucs_status_t uct_rc_mlx5_iface_arm_tx_cq(uct_ib_iface_t *ib_iface)
-{
-    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_rc_mlx5_iface_t);
-    uct_ib_mlx5_update_cq_ci(iface->super.super.send_cq, iface->mlx5_common.tx.cq.cq_ci);
-    return uct_ib_iface_arm_tx_cq(ib_iface);
-}
-
-static ucs_status_t uct_rc_mlx5_iface_arm_rx_cq(uct_ib_iface_t *ib_iface,
-                                                int solicited_only)
+static ucs_status_t uct_rc_mlx5_iface_arm_cq(uct_ib_iface_t *ib_iface,
+                                             uct_ib_dir_t dir,
+                                             int solicited_only)
 {
     uct_rc_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_rc_mlx5_iface_t);
-    uct_ib_mlx5_update_cq_ci(iface->super.super.recv_cq, iface->mlx5_common.rx.cq.cq_ci);
-    return uct_ib_iface_arm_rx_cq(ib_iface, solicited_only);
+#if HAVE_DECL_MLX5DV_INIT_OBJ
+    return uct_ib_mlx5dv_arm_cq(&iface->mlx5_common.cq[dir], solicited_only);
+#else
+    uct_ib_mlx5_update_cq_ci(iface->super.super.cq[dir],
+                             iface->mlx5_common.cq[dir].cq_ci);
+    return uct_ib_iface_arm_cq(ib_iface, dir, solicited_only);
+#endif
 }
 
 static void
@@ -145,7 +157,7 @@ uct_rc_mlx5_iface_handle_failure(uct_ib_iface_t *ib_iface, void *arg,
         log_lvl = ib_iface->super.config.failure_level;
     }
 
-    uct_ib_mlx5_completion_with_err(arg, log_lvl);
+    uct_ib_mlx5_completion_with_err(ib_iface, arg, log_lvl);
 }
 
 static ucs_status_t uct_rc_mlx5_ep_set_failed(uct_ib_iface_t *iface,
@@ -207,7 +219,7 @@ static ucs_status_t uct_rc_mlx5_iface_tag_recv_cancel(uct_iface_h tl_iface,
 
 static ucs_status_t
 uct_rc_mlx5_iface_tag_init(uct_rc_mlx5_iface_t *iface,
-                           uct_rc_iface_config_t *rc_config)
+                           uct_rc_mlx5_iface_config_t *config)
 {
 #if IBV_EXP_HW_TM
     if (UCT_RC_IFACE_TM_ENABLED(&iface->super)) {
@@ -216,7 +228,8 @@ uct_rc_mlx5_iface_tag_init(uct_rc_mlx5_iface_t *iface,
         iface->super.progress = uct_rc_mlx5_iface_progress_tm;
 
         return uct_rc_mlx5_iface_common_tag_init(&iface->mlx5_common,
-                                                 &iface->super, rc_config,
+                                                 &iface->super, &config->super,
+                                                 &config->mlx5_common,
                                                  &srq_init_attr,
                                                  sizeof(struct ibv_exp_tmh_rvh));
     }
@@ -225,6 +238,14 @@ uct_rc_mlx5_iface_tag_init(uct_rc_mlx5_iface_t *iface,
     return UCS_OK;
 }
 
+static void uct_rc_mlx5_iface_event_cq(uct_ib_iface_t *ib_iface,
+                                       uct_ib_dir_t dir)
+{
+    uct_rc_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_rc_mlx5_iface_t);
+
+    iface->mlx5_common.cq[dir].cq_sn++;
+}
+
 
 static UCS_CLASS_INIT_FUNC(uct_rc_mlx5_iface_t, uct_md_h md, uct_worker_h worker,
                            const uct_iface_params_t *params,
@@ -232,13 +253,19 @@ static UCS_CLASS_INIT_FUNC(uct_rc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
 {
     uct_rc_mlx5_iface_config_t *config = ucs_derived_of(tl_config,
                                                         uct_rc_mlx5_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     ucs_status_t status;
 
+    init_attr.res_domain_key = UCT_IB_MLX5_RES_DOMAIN_KEY;
+    init_attr.tm_cap_bit     = IBV_EXP_TM_CAP_RC;
+    init_attr.fc_req_size    = sizeof(uct_rc_fc_request_t);
+    init_attr.flags          = UCT_IB_CQ_IGNORE_OVERRUN;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_rc_iface_t, &uct_rc_mlx5_iface_ops, md, worker,
-                              params, &config->super, 0,
-                              sizeof(uct_rc_fc_request_t), IBV_EXP_TM_CAP_RC);
+                              params, &config->super, &init_attr);
 
 
+    self->tx.mmio_mode               = config->mlx5_common.mmio_mode;
     self->tx.bb_max                  = ucs_min(config->tx_max_bb, UINT16_MAX);
     self->super.config.tx_moderation = ucs_min(self->super.config.tx_moderation,
                                                self->tx.bb_max / 4);
@@ -248,12 +275,13 @@ static UCS_CLASS_INIT_FUNC(uct_rc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
         return status;
     }
 
-    status = uct_rc_mlx5_iface_tag_init(self, &config->super);
+    status = uct_rc_mlx5_iface_tag_init(self, config);
     if (status != UCS_OK) {
         return status;
     }
 
-    status = uct_rc_mlx5_iface_common_init(&self->mlx5_common, &self->super, &config->super);
+    status = uct_rc_mlx5_iface_common_init(&self->mlx5_common, &self->super,
+                                           &config->super, &config->mlx5_common);
     if (status != UCS_OK) {
         uct_rc_mlx5_iface_common_tag_cleanup(&self->mlx5_common, &self->super);
         return status;
@@ -261,7 +289,7 @@ static UCS_CLASS_INIT_FUNC(uct_rc_mlx5_iface_t, uct_md_h md, uct_worker_h worker
 
     /* Set max_iov for put_zcopy and get_zcopy */
     uct_ib_iface_set_max_iov(&self->super.super,
-                             ((UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB) -
+                             (UCT_IB_MLX5_MAX_SEND_WQE_SIZE -
                              sizeof(struct mlx5_wqe_raddr_seg) -
                              sizeof(struct mlx5_wqe_ctrl_seg)) /
                              sizeof(struct mlx5_wqe_data_seg));
@@ -296,14 +324,12 @@ static uct_rc_iface_ops_t uct_rc_mlx5_iface_ops = {
     .ep_am_short              = uct_rc_mlx5_ep_am_short,
     .ep_am_bcopy              = uct_rc_mlx5_ep_am_bcopy,
     .ep_am_zcopy              = uct_rc_mlx5_ep_am_zcopy,
-    .ep_atomic_add64          = uct_rc_mlx5_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_rc_mlx5_ep_atomic_fadd64,
-    .ep_atomic_swap64         = uct_rc_mlx5_ep_atomic_swap64,
     .ep_atomic_cswap64        = uct_rc_mlx5_ep_atomic_cswap64,
-    .ep_atomic_add32          = uct_rc_mlx5_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_rc_mlx5_ep_atomic_fadd32,
-    .ep_atomic_swap32         = uct_rc_mlx5_ep_atomic_swap32,
     .ep_atomic_cswap32        = uct_rc_mlx5_ep_atomic_cswap32,
+    .ep_atomic64_post         = uct_rc_mlx5_ep_atomic64_post,
+    .ep_atomic32_post         = uct_rc_mlx5_ep_atomic32_post,
+    .ep_atomic64_fetch        = uct_rc_mlx5_ep_atomic64_fetch,
+    .ep_atomic32_fetch        = uct_rc_mlx5_ep_atomic32_fetch,
     .ep_pending_add           = uct_rc_ep_pending_add,
     .ep_pending_purge         = uct_rc_ep_pending_purge,
     .ep_flush                 = uct_rc_mlx5_ep_flush,
@@ -335,10 +361,11 @@ static uct_rc_iface_ops_t uct_rc_mlx5_iface_ops = {
     .iface_get_device_address = uct_ib_iface_get_device_address,
     .iface_is_reachable       = uct_rc_iface_is_reachable
     },
-    .arm_tx_cq                = uct_rc_mlx5_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_rc_mlx5_iface_arm_rx_cq,
+    .arm_cq                   = uct_rc_mlx5_iface_arm_cq,
+    .event_cq                 = uct_rc_mlx5_iface_event_cq,
     .handle_failure           = uct_rc_mlx5_iface_handle_failure,
-    .set_ep_failed            = uct_rc_mlx5_ep_set_failed
+    .set_ep_failed            = uct_rc_mlx5_ep_set_failed,
+    .create_qp                = uct_ib_iface_create_qp
     },
     .fc_ctrl                  = uct_rc_mlx5_ep_fc_ctrl,
     .fc_handler               = uct_rc_iface_fc_handler
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.c
index 72c34aa1a..71f09e02e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.c
@@ -125,8 +125,7 @@ static void uct_rc_ep_tag_qp_destroy(uct_rc_ep_t *ep)
 #endif
 }
 
-static ucs_status_t uct_rc_ep_tag_qp_create(uct_rc_iface_t *iface,
-                                            uct_rc_ep_t *ep)
+static ucs_status_t uct_rc_ep_tag_qp_create(uct_rc_iface_t *iface, uct_rc_ep_t *ep)
 {
 #if IBV_EXP_HW_TM
     struct ibv_qp_cap cap;
@@ -283,46 +282,54 @@ ucs_status_t uct_rc_ep_connect_to_ep(uct_ep_h tl_ep, const uct_device_addr_t *de
     return UCS_OK;
 }
 
-ucs_status_t uct_rc_modify_qp(uct_rc_txqp_t *txqp, enum ibv_qp_state state)
-{
-    struct ibv_qp_attr qp_attr;
-
-    ucs_debug("modify QP 0x%x to state %d", txqp->qp->qp_num, state);
-    memset(&qp_attr, 0, sizeof(qp_attr));
-    qp_attr.qp_state = state;
-    if (ibv_modify_qp(txqp->qp, &qp_attr, IBV_QP_STATE)) {
-        ucs_warn("modify qp 0x%x to state %d failed: %m", state,
-                 txqp->qp->qp_num);
-        return UCS_ERR_IO_ERROR;
-    }
-
-    return UCS_OK;
-}
-
-void uct_rc_ep_am_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
-                              void *data, size_t length, size_t valid_length,
-                              char *buffer, size_t max)
+void uct_rc_ep_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                           void *data, size_t length, size_t valid_length,
+                           char *buffer, size_t max, int is_tmh_be)
 {
     uct_rc_hdr_t *rch = data;
     uint8_t fc_hdr    = uct_rc_fc_get_fc_hdr(rch->am_id);
     uint8_t am_wo_fc;
 
+#if IBV_EXP_HW_TM
+    if (rch->tmh_opcode != IBV_EXP_TMH_NO_TAG) {
+        struct ibv_exp_tmh *tmh = (void*)rch;
+        struct ibv_exp_tmh_rvh *rvh = (void*)(tmh + 1);
+        uct_tag_t tag;
+        uint32_t app_ctx;
+
+        if (is_tmh_be) {
+            tag     = be64toh(tmh->tag);
+            app_ctx = ntohl(tmh->app_ctx);
+        } else {
+            tag     = tmh->tag;
+            app_ctx = tmh->app_ctx;
+        }
+
+        switch (rch->tmh_opcode) {
+        case IBV_EXP_TMH_EAGER:
+            snprintf(buffer, max, " EAGER tag %lx app_ctx %d", tag, app_ctx);
+            return;
+        case IBV_EXP_TMH_RNDV:
+            snprintf(buffer, max, " RNDV tag %lx app_ctx %d va 0x%lx len %d rkey %x",
+                     tag, app_ctx, be64toh(rvh->va), ntohl(rvh->len), ntohl(rvh->rkey));
+            return;
+        case IBV_EXP_TMH_FIN:
+            snprintf(buffer, max, " FIN tag %lx app_ctx %d", tag, app_ctx);
+            return;
+        default:
+            break;
+        }
+    }
+#endif
+
     /* Do not invoke AM tracer for auxiliary pure FC_GRANT message */
     if (fc_hdr != UCT_RC_EP_FC_PURE_GRANT) {
         am_wo_fc = rch->am_id & ~UCT_RC_EP_FC_MASK; /* mask out FC bits*/
-        snprintf(buffer, max, " %c%c am %d "
-#if IBV_EXP_HW_TM
-                 "tm_op %d "
-#endif
-                 ,
+        snprintf(buffer, max, " %c%c am %d ",
                  fc_hdr & UCT_RC_EP_FC_FLAG_SOFT_REQ ? 's' :
                  fc_hdr & UCT_RC_EP_FC_FLAG_HARD_REQ ? 'h' : '-',
                  fc_hdr & UCT_RC_EP_FC_FLAG_GRANT    ? 'g' : '-',
-                 am_wo_fc
-#if IBV_EXP_HW_TM
-                 , rch->tmh_opcode
-#endif
-                 );
+                 am_wo_fc);
         uct_iface_dump_am(iface, type, am_wo_fc, rch + 1, length - sizeof(*rch),
                           buffer + strlen(buffer), max - strlen(buffer));
     } else {
@@ -362,7 +369,15 @@ void uct_rc_ep_send_op_completion_handler(uct_rc_iface_send_op_t *op,
     uct_rc_iface_put_send_op(op);
 }
 
-ucs_status_t uct_rc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n)
+void uct_rc_ep_flush_op_completion_handler(uct_rc_iface_send_op_t *op,
+                                           const void *resp)
+{
+    uct_invoke_completion(op->user_comp, UCS_OK);
+    ucs_mpool_put(op);
+}
+
+ucs_status_t uct_rc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                   unsigned flags)
 {
     uct_rc_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_iface_t);
     uct_rc_ep_t *ep = ucs_derived_of(tl_ep, uct_rc_ep_t);
@@ -493,6 +508,8 @@ void uct_rc_txqp_purge_outstanding(uct_rc_txqp_t *txqp, ucs_status_t status,
         op->flags &= ~UCT_RC_IFACE_SEND_OP_FLAG_INUSE;
         if (op->handler == uct_rc_ep_send_op_completion_handler) {
             uct_rc_iface_put_send_op(op);
+        } else if (op->handler == uct_rc_ep_flush_op_completion_handler) {
+            ucs_mpool_put(op);
         } else {
             desc = ucs_derived_of(op, uct_rc_iface_send_desc_t);
             ucs_mpool_put(desc);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.h
index a8dff96a9..2e303ddb4 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_ep.h
@@ -230,11 +230,9 @@ ucs_status_t uct_rc_ep_get_address(uct_ep_h tl_ep, uct_ep_addr_t *addr);
 ucs_status_t uct_rc_ep_connect_to_ep(uct_ep_h tl_ep, const uct_device_addr_t *dev_addr,
                                      const uct_ep_addr_t *ep_addr);
 
-ucs_status_t uct_rc_modify_qp(uct_rc_txqp_t *txqp, enum ibv_qp_state state);
-
-void uct_rc_ep_am_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
-                              void *data, size_t length, size_t valid_length,
-                              char *buffer, size_t max);
+void uct_rc_ep_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                           void *data, size_t length, size_t valid_length,
+                           char *buffer, size_t max, int is_tmh_be);
 
 void uct_rc_ep_get_bcopy_handler(uct_rc_iface_send_op_t *op, const void *resp);
 
@@ -244,7 +242,11 @@ void uct_rc_ep_get_bcopy_handler_no_completion(uct_rc_iface_send_op_t *op,
 void uct_rc_ep_send_op_completion_handler(uct_rc_iface_send_op_t *op,
                                              const void *resp);
 
-ucs_status_t uct_rc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n);
+void uct_rc_ep_flush_op_completion_handler(uct_rc_iface_send_op_t *op,
+                                           const void *resp);
+
+ucs_status_t uct_rc_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                   unsigned flags);
 
 void uct_rc_ep_pending_purge(uct_ep_h ep, uct_pending_purge_callback_t cb,
                              void*arg);
@@ -358,6 +360,28 @@ uct_rc_txqp_add_send_comp(uct_rc_iface_t *iface, uct_rc_txqp_t *txqp,
     uct_rc_txqp_add_send_op_sn(txqp, op, sn);
 }
 
+static UCS_F_ALWAYS_INLINE ucs_status_t
+uct_rc_txqp_add_flush_comp(uct_rc_iface_t *iface, uct_rc_txqp_t *txqp,
+                           uct_completion_t *comp, uint16_t sn)
+{
+    uct_rc_iface_send_op_t *op;
+
+    if (comp != NULL) {
+        op = (uct_rc_iface_send_op_t*)ucs_mpool_get(&iface->tx.flush_mp);
+        if (ucs_unlikely(op == NULL)) {
+            ucs_error("Failed to allocate flush completion");
+            return UCS_ERR_NO_MEMORY;
+        }
+
+        op->flags     = 0;
+        op->user_comp = comp;
+        uct_rc_txqp_add_send_op_sn(txqp, op, sn);
+        VALGRIND_MAKE_MEM_DEFINED(op, sizeof(*op)); /* handler set by mpool init */
+    }
+
+    return UCS_INPROGRESS;
+}
+
 static UCS_F_ALWAYS_INLINE void
 uct_rc_txqp_completion_op(uct_rc_iface_send_op_t *op, const void *resp)
 {
@@ -399,7 +423,8 @@ uct_rc_iface_tx_moderation(uct_rc_iface_t *iface, uct_rc_txqp_t *txqp, uint8_t f
 }
 
 static UCS_F_ALWAYS_INLINE void
-uct_rc_txqp_posted(uct_rc_txqp_t *txqp, uct_rc_iface_t *iface, uint16_t res_count, int signaled)
+uct_rc_txqp_posted(uct_rc_txqp_t *txqp, uct_rc_iface_t *iface, uint16_t res_count,
+                   int signaled)
 {
     if (signaled) {
         ucs_assert(uct_rc_iface_have_tx_cqe_avail(iface));
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.c
index 270be52f0..bc2f44930 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.c
@@ -76,6 +76,10 @@ ucs_config_field_t uct_rc_iface_config_table[] = {
    "is a minimum between this value and the maximum value supported by the HW. \n"
    "-1 means no limit.",
    ucs_offsetof(uct_rc_iface_config_t, tm.list_size), UCS_CONFIG_TYPE_UINT},
+
+  {"TM_MAX_BCOPY", "48k",
+   "Maximal size of copy-out sends when tag-matching offload is enabled",
+   ucs_offsetof(uct_rc_iface_config_t, tm.max_bcopy), UCS_CONFIG_TYPE_MEMUNITS},
 #endif
 
   {NULL}
@@ -99,11 +103,30 @@ static ucs_stats_class_t uct_rc_iface_stats_class = {
     .counter_names = {
         [UCT_RC_IFACE_STAT_RX_COMPLETION] = "rx_completion",
         [UCT_RC_IFACE_STAT_TX_COMPLETION] = "tx_completion",
-        [UCT_RC_IFACE_STAT_NO_CQE]        = "no_cqe",
+        [UCT_RC_IFACE_STAT_NO_CQE]        = "no_cqe"
+    }
+};
+
+#if IBV_EXP_HW_TM
+static ucs_stats_class_t uct_rc_iface_tag_stats_class = {
+    .name = "tag",
+    .num_counters = UCT_RC_IFACE_STAT_TAG_LAST,
+    .counter_names = {
+        [UCT_RC_IFACE_STAT_TAG_RX_EXP]            = "rx_exp",
+        [UCT_RC_IFACE_STAT_TAG_RX_EAGER_UNEXP]    = "rx_unexp_eager",
+        [UCT_RC_IFACE_STAT_TAG_RX_RNDV_UNEXP]     = "rx_unexp_rndv",
+        [UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_EXP]   = "rx_exp_rndv_req",
+        [UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_UNEXP] = "rx_unexp_rndv_req",
+        [UCT_RC_IFACE_STAT_TAG_RX_RNDV_FIN]       = "rx_rndv_fin",
+        [UCT_RC_IFACE_STAT_TAG_LIST_ADD]          = "tx_add_op",
+        [UCT_RC_IFACE_STAT_TAG_LIST_DEL]          = "tx_del_op",
+        [UCT_RC_IFACE_STAT_TAG_LIST_SYNC]         = "tx_sync_op"
     }
 };
 #endif
 
+#endif /* ENABLE_STATS */
+
 
 static ucs_mpool_ops_t uct_rc_fc_pending_mpool_ops = {
     .chunk_alloc   = ucs_mpool_chunk_malloc,
@@ -112,9 +135,27 @@ static ucs_mpool_ops_t uct_rc_fc_pending_mpool_ops = {
     .obj_cleanup   = NULL
 };
 
+static void
+uct_rc_iface_flush_comp_init(ucs_mpool_t *mp, void *obj, void *chunk)
+{
+    uct_rc_iface_t *iface      = ucs_container_of(mp, uct_rc_iface_t, tx.flush_mp);
+    uct_rc_iface_send_op_t *op = obj;
+
+    op->handler = uct_rc_ep_flush_op_completion_handler;
+    op->flags   = 0;
+    op->iface   = iface;
+}
+
+static ucs_mpool_ops_t uct_rc_flush_comp_mpool_ops = {
+    .chunk_alloc   = ucs_mpool_chunk_malloc,
+    .chunk_release = ucs_mpool_chunk_free,
+    .obj_init      = uct_rc_iface_flush_comp_init,
+    .obj_cleanup   = NULL
+};
+
 static void uct_rc_iface_tag_query(uct_rc_iface_t *iface,
                                    uct_iface_attr_t *iface_attr,
-                                   size_t max_inline)
+                                   size_t max_inline, size_t max_iov)
 {
 #if IBV_EXP_HW_TM
     unsigned eager_hdr_size = sizeof(struct ibv_exp_tmh);
@@ -137,13 +178,14 @@ static void uct_rc_iface_tag_query(uct_rc_iface_t *iface,
                                           eager_hdr_size;
     iface_attr->cap.tag.eager.max_zcopy = iface->super.config.seg_size -
                                           eager_hdr_size;
-    iface_attr->cap.tag.eager.max_iov   = 1;
+    iface_attr->cap.tag.eager.max_iov   = max_iov;
 
     port_attr = uct_ib_iface_port_attr(&iface->super);
     iface_attr->cap.tag.rndv.max_zcopy  = port_attr->max_msg_sz;
 
     /* TMH can carry 2 additional bytes of private data */
-    iface_attr->cap.tag.rndv.max_hdr    = iface->tm.max_rndv_data + 2;
+    iface_attr->cap.tag.rndv.max_hdr    = iface->tm.max_rndv_data +
+                                          UCT_RC_IFACE_TMH_PRIV_LEN;
     iface_attr->cap.tag.rndv.max_iov    = 1;
 
     iface_attr->cap.tag.recv.max_zcopy       = port_attr->max_msg_sz;
@@ -156,7 +198,8 @@ static void uct_rc_iface_tag_query(uct_rc_iface_t *iface,
 ucs_status_t uct_rc_iface_query(uct_rc_iface_t *iface,
                                 uct_iface_attr_t *iface_attr,
                                 size_t put_max_short, size_t max_inline,
-                                size_t am_max_hdr, size_t am_max_iov)
+                                size_t am_max_hdr, size_t am_max_iov,
+                                size_t tag_max_iov)
 {
     uct_ib_device_t *dev = uct_ib_iface_device(&iface->super);
     ucs_status_t status;
@@ -186,24 +229,32 @@ ucs_status_t uct_rc_iface_query(uct_rc_iface_t *iface,
                                   UCT_IFACE_FLAG_EVENT_RECV;
 
     if (uct_ib_atomic_is_supported(dev, 0, sizeof(uint64_t))) {
-        iface_attr->cap.flags  |= UCT_IFACE_FLAG_ATOMIC_ADD64 |
-                                  UCT_IFACE_FLAG_ATOMIC_FADD64 |
-                                  UCT_IFACE_FLAG_ATOMIC_CSWAP64 |
-                                  UCT_IFACE_FLAG_ATOMIC_DEVICE;
+        /* TODO: remove deprecated flags */
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+
+        iface_attr->cap.atomic64.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_ADD);
+        iface_attr->cap.atomic64.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_ADD)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_CSWAP);
     }
 
+#if HAVE_IB_EXT_ATOMICS
     if (uct_ib_atomic_is_supported(dev, 1, sizeof(uint64_t))) {
-        iface_attr->cap.flags |= UCT_IFACE_FLAG_ATOMIC_SWAP64 |
-                                 UCT_IFACE_FLAG_ATOMIC_DEVICE;
+        /* TODO: remove deprecated flags */
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+
+        iface_attr->cap.atomic64.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_SWAP);
     }
 
     if (uct_ib_atomic_is_supported(dev, 1, sizeof(uint32_t))) {
-        iface_attr->cap.flags |= UCT_IFACE_FLAG_ATOMIC_ADD32 |
-                                 UCT_IFACE_FLAG_ATOMIC_FADD32 |
-                                 UCT_IFACE_FLAG_ATOMIC_SWAP32 |
-                                 UCT_IFACE_FLAG_ATOMIC_CSWAP32 |
-                                 UCT_IFACE_FLAG_ATOMIC_DEVICE;
+        /* TODO: remove deprecated flags */
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_ATOMIC_DEVICE;
+
+        iface_attr->cap.atomic32.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_ADD);
+        iface_attr->cap.atomic32.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_ADD)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_SWAP) |
+                                              UCS_BIT(UCT_ATOMIC_OP_CSWAP);
     }
+#endif
 
     iface_attr->cap.put.opt_zcopy_align = UCS_SYS_PCI_MAX_PAYLOAD;
     iface_attr->cap.get.opt_zcopy_align = UCS_SYS_PCI_MAX_PAYLOAD;
@@ -238,7 +289,7 @@ ucs_status_t uct_rc_iface_query(uct_rc_iface_t *iface,
     iface_attr->cap.flags        |= UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE;
 
     /* Tag Offload */
-    uct_rc_iface_tag_query(iface, iface_attr, max_inline);
+    uct_rc_iface_tag_query(iface, iface_attr, max_inline, tag_max_iov);
 
     return UCS_OK;
 }
@@ -440,7 +491,7 @@ ucs_status_t uct_rc_iface_fc_handler(uct_rc_iface_t *iface, unsigned qp_num,
         status = uct_rc_ep_fc_grant(&fc_req->super);
 
         if (status == UCS_ERR_NO_RESOURCE){
-            status = uct_ep_pending_add(&ep->super.super, &fc_req->super);
+            status = uct_ep_pending_add(&ep->super.super, &fc_req->super, 0);
         }
         ucs_assertv_always(status == UCS_OK, "Failed to send FC grant msg: %s",
                            ucs_status_string(status));
@@ -455,6 +506,7 @@ static ucs_status_t uct_rc_iface_tx_ops_init(uct_rc_iface_t *iface)
 {
     const unsigned count = iface->config.tx_ops_count;
     uct_rc_iface_send_op_t *op;
+    ucs_status_t status;
 
     iface->tx.ops_buffer = ucs_calloc(count, sizeof(*iface->tx.ops_buffer),
                                       "rc_tx_ops");
@@ -463,14 +515,22 @@ static ucs_status_t uct_rc_iface_tx_ops_init(uct_rc_iface_t *iface)
     }
 
     iface->tx.free_ops = &iface->tx.ops_buffer[0];
-    for (op = iface->tx.ops_buffer; op < iface->tx.ops_buffer + count - 1; ++op) {
+    for (op = iface->tx.ops_buffer; op < iface->tx.ops_buffer + count; ++op) {
         op->handler = uct_rc_ep_send_op_completion_handler;
         op->flags   = UCT_RC_IFACE_SEND_OP_FLAG_IFACE;
         op->iface   = iface;
-        op->next    = op + 1;
+        op->next    = (op == (iface->tx.ops_buffer + count - 1)) ? NULL : (op + 1);
     }
-    iface->tx.ops_buffer[count - 1].next = NULL;
-    return UCS_OK;
+
+    /* Create memory pool for flush completions. Can't just alloc a certain
+     * size buffer, because number of simultaneous flushes is not limited by
+     * CQ or QP resources. */
+    status = ucs_mpool_init(&iface->tx.flush_mp, 0, sizeof(*op), 0,
+                            UCS_SYS_CACHE_LINE_SIZE, 256,
+                            UINT_MAX, &uct_rc_flush_comp_mpool_ops,
+                            "flush-comps-only");
+
+    return status;
 }
 
 static void uct_rc_iface_tx_ops_cleanup(uct_rc_iface_t *iface)
@@ -489,6 +549,8 @@ static void uct_rc_iface_tx_ops_cleanup(uct_rc_iface_t *iface)
                  total_count- free_count, total_count);
     }
     ucs_free(iface->tx.ops_buffer);
+
+    ucs_mpool_cleanup(&iface->tx.flush_mp, 1);
 }
 
 #if IBV_EXP_HW_TM
@@ -500,8 +562,10 @@ static void uct_rc_iface_release_desc(uct_recv_desc_t *self, void *desc)
     ucs_mpool_put_inline(ib_desc);
 }
 
+/* tag is passed as parameter, because some (but not all!) transports may need
+ * to translate TMH to LE */
 ucs_status_t uct_rc_iface_handle_rndv(uct_rc_iface_t *iface,
-                                      struct ibv_exp_tmh *tmh,
+                                      struct ibv_exp_tmh *tmh, uct_tag_t tag,
                                       unsigned byte_len)
 {
     uct_rc_iface_tmh_priv_data_t *priv = (uct_rc_iface_tmh_priv_data_t*)tmh->reserved;
@@ -537,8 +601,7 @@ ucs_status_t uct_rc_iface_handle_rndv(uct_rc_iface_t *iface,
     uct_ib_md_pack_rkey(ntohl(rvh->rkey), UCT_IB_INVALID_RKEY, rb);
 
     /* Do not pass flags to cb, because rkey is allocated on stack */
-    return iface->tm.rndv_unexp.cb(iface->tm.rndv_unexp.arg, 0,
-                                   be64toh(tmh->tag),
+    return iface->tm.rndv_unexp.cb(iface->tm.rndv_unexp.arg, 0, tag,
                                    (char *)rndv_usr_hdr - priv->length,
                                    rndv_usr_hdr_len + priv->length,
                                    be64toh(rvh->va), rndv_data_len,
@@ -549,14 +612,15 @@ ucs_status_t uct_rc_iface_handle_rndv(uct_rc_iface_t *iface,
 static void uct_rc_iface_preinit(uct_rc_iface_t *iface, uct_md_h md,
                                  const uct_rc_iface_config_t *config,
                                  const uct_iface_params_t *params,
-                                 int tm_cap_flag, unsigned *rx_cq_len)
+                                 uct_ib_iface_init_attr_t *init_attr)
 {
 #if IBV_EXP_HW_TM
-    struct ibv_exp_tmh tmh;
     uct_ib_device_t *dev = &ucs_derived_of(md, uct_ib_md_t)->dev;
     uint32_t cap_flags   = IBV_DEVICE_TM_CAPS(dev, capability_flags);
+    struct ibv_exp_tmh tmh;
 
-    iface->tm.enabled = (config->tm.enable && (cap_flags & tm_cap_flag));
+    iface->tm.enabled = config->tm.enable &&
+                        (cap_flags & init_attr->tm_cap_bit);
 
     if (!iface->tm.enabled) {
         goto out_tm_disabled;
@@ -585,13 +649,17 @@ static void uct_rc_iface_preinit(uct_rc_iface_t *iface, uct_md_h md,
      * - up to 3 CQEs for every posted tag: ADD, TM_CONSUMED and MSG_ARRIVED
      * - one SYNC CQE per every IBV_DEVICE_MAX_UNEXP_COUNT unexpected receives */
     UCS_STATIC_ASSERT(IBV_DEVICE_MAX_UNEXP_COUNT);
-    *rx_cq_len = config->super.rx.queue_len + iface->tm.num_tags * 3  +
-                 config->super.rx.queue_len / IBV_DEVICE_MAX_UNEXP_COUNT;
+    init_attr->rx_cq_len = config->super.rx.queue_len + iface->tm.num_tags * 3 +
+                           config->super.rx.queue_len /
+                           IBV_DEVICE_MAX_UNEXP_COUNT;
+    init_attr->seg_size  = ucs_max(config->tm.max_bcopy,
+                                   config->super.super.max_bcopy);
     return;
 
 out_tm_disabled:
 #endif
-    *rx_cq_len  = config->super.rx.queue_len;
+    init_attr->rx_cq_len = config->super.rx.queue_len;
+    init_attr->seg_size  = config->super.super.max_bcopy;
 }
 
 ucs_status_t uct_rc_iface_tag_init(uct_rc_iface_t *iface,
@@ -604,6 +672,7 @@ ucs_status_t uct_rc_iface_tag_init(uct_rc_iface_t *iface,
 #if IBV_EXP_HW_TM
     uct_ib_md_t *md       = uct_ib_iface_md(&iface->super);
     unsigned tmh_hdrs_len = sizeof(struct ibv_exp_tmh) + rndv_hdr_len;
+    ucs_status_t status;
 
     if (!UCT_RC_IFACE_TM_ENABLED(iface)) {
         goto out_tm_disabled;
@@ -634,7 +703,7 @@ ucs_status_t uct_rc_iface_tag_init(uct_rc_iface_t *iface,
     srq_init_attr->base.srq_context    = iface;
     srq_init_attr->srq_type            = IBV_EXP_SRQT_TAG_MATCHING;
     srq_init_attr->pd                  = md->pd;
-    srq_init_attr->cq                  = iface->super.recv_cq;
+    srq_init_attr->cq                  = iface->super.cq[UCT_IB_DIR_RX];
     srq_init_attr->tm_cap.max_num_tags = iface->tm.num_tags;
 
     /* 2 ops for each tag (ADD + DEL) and extra ops for SYNC.
@@ -648,11 +717,18 @@ ucs_status_t uct_rc_iface_tag_init(uct_rc_iface_t *iface,
 
     iface->rx.srq.srq = ibv_exp_create_srq(md->dev.ibv_context, srq_init_attr);
     if (iface->rx.srq.srq == NULL) {
-        ucs_error("Failed to create TM XRQ: %m");
+        ucs_error("ibv_exp_create_srq(device=%s) failed: %m",
+                  uct_ib_device_name(&md->dev));
         return UCS_ERR_IO_ERROR;
     }
 
-    iface->rx.srq.available   = srq_init_attr->base.attr.max_wr;
+    iface->rx.srq.quota = srq_init_attr->base.attr.max_wr;
+
+    status = UCS_STATS_NODE_ALLOC(&iface->tm.stats, &uct_rc_iface_tag_stats_class,
+                                  iface->stats);
+    if (status != UCS_OK) {
+        ucs_bug("Failed to allocate tag stats: %s", ucs_status_string(status));
+    }
 
     ucs_debug("Tag Matching enabled: tag list size %d", iface->tm.num_tags);
 
@@ -667,6 +743,7 @@ void uct_rc_iface_tag_cleanup(uct_rc_iface_t *iface)
 #if IBV_EXP_HW_TM
     if (UCT_RC_IFACE_TM_ENABLED(iface)) {
         ucs_ptr_array_cleanup(&iface->tm.rndv_comps);
+        UCS_STATS_NODE_FREE(iface->tm.stats);
     }
 #endif
 }
@@ -679,22 +756,21 @@ unsigned uct_rc_iface_do_progress(uct_iface_h tl_iface)
 
 UCS_CLASS_INIT_FUNC(uct_rc_iface_t, uct_rc_iface_ops_t *ops, uct_md_h md,
                     uct_worker_h worker, const uct_iface_params_t *params,
-                    const uct_rc_iface_config_t *config, unsigned rx_priv_len,
-                    unsigned fc_req_size, int tm_cap_flag)
+                    const uct_rc_iface_config_t *config,
+                    uct_ib_iface_init_attr_t *init_attr)
 {
     uct_ib_device_t *dev = &ucs_derived_of(md, uct_ib_md_t)->dev;
-    unsigned tx_cq_len   = config->tx.cq_len;
     struct ibv_srq_init_attr srq_init_attr;
     ucs_status_t status;
-    unsigned rx_cq_len;
 
-    uct_rc_iface_preinit(self, md, config, params, tm_cap_flag, &rx_cq_len);
+    uct_rc_iface_preinit(self, md, config, params, init_attr);
+    init_attr->rx_hdr_len = sizeof(uct_rc_hdr_t);
+    init_attr->tx_cq_len  = config->tx.cq_len;
 
     UCS_CLASS_CALL_SUPER_INIT(uct_ib_iface_t, &ops->super, md, worker, params,
-                              rx_priv_len, sizeof(uct_rc_hdr_t), tx_cq_len,
-                              rx_cq_len, SIZE_MAX, &config->super);
+                              &config->super, init_attr);
 
-    self->tx.cq_available           = tx_cq_len - 1;
+    self->tx.cq_available           = init_attr->tx_cq_len - 1;
     self->rx.srq.available          = 0;
     self->rx.srq.quota              = 0;
     self->config.tx_qp_len          = config->super.tx.queue_len;
@@ -702,7 +778,7 @@ UCS_CLASS_INIT_FUNC(uct_rc_iface_t, uct_rc_iface_ops_t *ops, uct_md_h md,
     self->config.tx_min_inline      = config->super.tx.min_inline;
     self->config.tx_moderation      = ucs_min(config->super.tx.cq_moderation,
                                               config->super.tx.queue_len / 4);
-    self->config.tx_ops_count       = tx_cq_len;
+    self->config.tx_ops_count       = init_attr->tx_cq_len;
     self->config.rx_inline          = config->super.rx.inl;
     self->config.min_rnr_timer      = uct_ib_to_fabric_time(config->tx.rnr_timeout);
     self->config.timeout            = uct_ib_to_fabric_time(config->tx.timeout);
@@ -712,6 +788,9 @@ UCS_CLASS_INIT_FUNC(uct_rc_iface_t, uct_rc_iface_ops_t *ops, uct_md_h md,
                                               UCR_RC_QP_MAX_RETRY_COUNT);
     self->config.max_rd_atomic      = config->max_rd_atomic;
     self->config.ooo_rw             = config->ooo_rw;
+#if ENABLE_ASSERT
+    self->config.tx_cq_len          = init_attr->tx_cq_len;
+#endif
 
     uct_rc_iface_set_path_mtu(self, config);
     memset(self->eps, 0, sizeof(self->eps));
@@ -802,7 +881,7 @@ UCS_CLASS_INIT_FUNC(uct_rc_iface_t, uct_rc_iface_ops_t *ops, uct_md_h md,
         /* Create mempool for pending requests for FC grant */
         status = ucs_mpool_init(&self->tx.fc_mp,
                                 0,
-                                fc_req_size,
+                                init_attr->fc_req_size,
                                 0,
                                 1,
                                 128,
@@ -875,27 +954,9 @@ ucs_status_t uct_rc_iface_qp_create(uct_rc_iface_t *iface, int qp_type,
                                     struct ibv_qp **qp_p, struct ibv_qp_cap *cap,
                                     unsigned max_send_wr)
 {
-    uct_ib_device_t *dev UCS_V_UNUSED = uct_ib_iface_device(&iface->super);
-    struct ibv_exp_qp_init_attr qp_init_attr;
-    const char *qp_type_str;
-    struct ibv_qp *qp;
-    int inline_recv = 0;
+    uct_ib_qp_attr_t qp_init_attr    = {};
+    static ucs_status_t status;
 
-    /* Check QP type */
-    if (qp_type == IBV_QPT_RC) {
-        qp_type_str = "rc";
-#if HAVE_DECL_IBV_EXP_QPT_DC_INI
-    } else if (qp_type == IBV_EXP_QPT_DC_INI) {
-        qp_type_str = "dci";
-#endif
-    } else {
-        ucs_bug("invalid qp type: %d", qp_type);
-    }
-
-    memset(&qp_init_attr, 0, sizeof(qp_init_attr));
-    qp_init_attr.qp_context          = NULL;
-    qp_init_attr.send_cq             = iface->super.send_cq;
-    qp_init_attr.recv_cq             = iface->super.recv_cq;
     if (qp_type == IBV_QPT_RC) {
         qp_init_attr.srq             = iface->rx.srq.srq;
     }
@@ -906,50 +967,14 @@ ucs_status_t uct_rc_iface_qp_create(uct_rc_iface_t *iface, int qp_type,
     qp_init_attr.cap.max_inline_data = iface->config.tx_min_inline;
     qp_init_attr.qp_type             = qp_type;
     qp_init_attr.sq_sig_all          = !iface->config.tx_moderation;
+    qp_init_attr.max_inl_recv        = iface->config.rx_inline;
 
-#if HAVE_DECL_IBV_EXP_CREATE_QP
-    qp_init_attr.comp_mask           = IBV_EXP_QP_INIT_ATTR_PD;
-    qp_init_attr.pd                  = uct_ib_iface_md(&iface->super)->pd;
-
-#  if HAVE_IB_EXT_ATOMICS
-    qp_init_attr.comp_mask          |= IBV_EXP_QP_INIT_ATTR_ATOMICS_ARG;
-    qp_init_attr.max_atomic_arg      = UCT_RC_MAX_ATOMIC_SIZE;
-#  endif
-
-#  if HAVE_DECL_IBV_EXP_ATOMIC_HCA_REPLY_BE
-    if (dev->dev_attr.exp_atomic_cap == IBV_EXP_ATOMIC_HCA_REPLY_BE) {
-        qp_init_attr.comp_mask       |= IBV_EXP_QP_INIT_ATTR_CREATE_FLAGS;
-        qp_init_attr.exp_create_flags = IBV_EXP_QP_CREATE_ATOMIC_BE_REPLY;
-    }
-#  endif
-
-#  if HAVE_STRUCT_IBV_EXP_QP_INIT_ATTR_MAX_INL_RECV
-    qp_init_attr.comp_mask           |= IBV_EXP_QP_INIT_ATTR_INL_RECV;
-    qp_init_attr.max_inl_recv         = iface->config.rx_inline;
-#  endif
-
-    qp = ibv_exp_create_qp(dev->ibv_context, &qp_init_attr);
-#else
-    qp = ibv_create_qp(uct_ib_iface_md(&iface->super)->pd, &qp_init_attr);
-#endif
-    if (qp == NULL) {
-        ucs_error("failed to create qp: %m");
-        return UCS_ERR_IO_ERROR;
+    status = iface->super.ops->create_qp(&iface->super, &qp_init_attr, qp_p);
+    if (status == UCS_OK) {
+        *cap = qp_init_attr.cap;
     }
 
-#if HAVE_STRUCT_IBV_EXP_QP_INIT_ATTR_MAX_INL_RECV
-    qp_init_attr.max_inl_recv = qp_init_attr.max_inl_recv / 2; /* Driver bug W/A */
-    inline_recv = qp_init_attr.max_inl_recv;
-#endif
-
-    ucs_debug("created %s qp 0x%x tx %d rx %d tx_inline %d rx_inline %d",
-              qp_type_str, qp->qp_num, qp_init_attr.cap.max_send_wr,
-              qp_init_attr.cap.max_recv_wr, qp_init_attr.cap.max_inline_data,
-              inline_recv);
-
-    *qp_p = qp;
-    *cap  = qp_init_attr.cap;
-    return UCS_OK;
+    return status;
 }
 
 ucs_status_t uct_rc_iface_qp_init(uct_rc_iface_t *iface, struct ibv_qp *qp)
@@ -1072,7 +1097,7 @@ ucs_status_t uct_rc_iface_common_event_arm(uct_iface_h tl_iface,
     }
 
     if (events & UCT_EVENT_SEND_COMP) {
-        status = iface->super.ops->arm_tx_cq(&iface->super);
+        status = iface->super.ops->arm_cq(&iface->super, UCT_IB_DIR_TX, 0);
         if (status != UCS_OK) {
             return status;
         }
@@ -1089,8 +1114,8 @@ ucs_status_t uct_rc_iface_common_event_arm(uct_iface_h tl_iface,
     }
 
     if (arm_rx_solicited || arm_rx_all) {
-        status = iface->super.ops->arm_rx_cq(&iface->super,
-                                             arm_rx_solicited && !arm_rx_all);
+        status = iface->super.ops->arm_cq(&iface->super, UCT_IB_DIR_RX,
+                                          arm_rx_solicited && !arm_rx_all);
         if (status != UCS_OK) {
             return status;
         }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.h
index 859248f3f..cd8dbc9c5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/base/rc_iface.h
@@ -21,7 +21,6 @@
 #define UCT_RC_QP_TABLE_ORDER       12
 #define UCT_RC_QP_TABLE_SIZE        UCS_BIT(UCT_RC_QP_TABLE_ORDER)
 #define UCT_RC_QP_TABLE_MEMB_ORDER  (UCT_IB_QPN_ORDER - UCT_RC_QP_TABLE_ORDER)
-#define UCT_RC_MAX_ATOMIC_SIZE      sizeof(uint64_t)
 #define UCR_RC_QP_MAX_RETRY_COUNT   7
 
 #define UCT_RC_CHECK_AM_SHORT(_am_id, _length, _max_inline) \
@@ -55,8 +54,8 @@
 
 #define UCT_RC_IFACE_GET_TX_PUT_BCOPY_DESC(_iface, _mp, _desc, _pack_cb, _arg, _length) \
     UCT_RC_IFACE_GET_TX_DESC(_iface, _mp, _desc) \
-    desc->super.handler = (uct_rc_send_handler_t)ucs_mpool_put; \
-    _length = pack_cb(_desc + 1, _arg); \
+    (_desc)->super.handler = (uct_rc_send_handler_t)ucs_mpool_put; \
+    _length = _pack_cb(_desc + 1, _arg); \
     UCT_SKIP_ZERO_LENGTH(_length, _desc);
 
 #define UCT_RC_IFACE_GET_TX_GET_BCOPY_DESC(_iface, _mp, _desc, _unpack_cb, _comp, _arg, _length) \
@@ -71,11 +70,11 @@
     _desc->unpack_cb         = _unpack_cb;
 
 
-#define UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(_iface, _mp, _desc) \
+#define UCT_RC_IFACE_GET_TX_ATOMIC_DESC(_iface, _mp, _desc) \
     UCT_RC_IFACE_GET_TX_DESC(_iface, _mp, _desc) \
     _desc->super.handler = (uct_rc_send_handler_t)ucs_mpool_put;
 
-#define UCT_RC_IFACE_GET_TX_ATOMIC_DESC(_iface, _mp, _desc, _handler, _result, _comp) \
+#define UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(_iface, _mp, _desc, _handler, _result, _comp) \
     UCT_CHECK_PARAM(_comp != NULL, "completion must be non-NULL"); \
     UCT_RC_IFACE_GET_TX_DESC(_iface, _mp, _desc) \
     _desc->super.handler   = _handler; \
@@ -162,6 +161,7 @@ struct uct_rc_iface_config {
     struct {
         int                  enable;
         unsigned             list_size;
+        size_t               max_bcopy;
     } tm;
 #endif
 
@@ -188,6 +188,19 @@ typedef struct uct_rc_srq {
 
 #if IBV_EXP_HW_TM
 
+enum {
+    UCT_RC_IFACE_STAT_TAG_RX_EXP,
+    UCT_RC_IFACE_STAT_TAG_RX_EAGER_UNEXP,
+    UCT_RC_IFACE_STAT_TAG_RX_RNDV_UNEXP,
+    UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_EXP,
+    UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_UNEXP,
+    UCT_RC_IFACE_STAT_TAG_RX_RNDV_FIN,
+    UCT_RC_IFACE_STAT_TAG_LIST_ADD,
+    UCT_RC_IFACE_STAT_TAG_LIST_DEL,
+    UCT_RC_IFACE_STAT_TAG_LIST_SYNC,
+    UCT_RC_IFACE_STAT_TAG_LAST
+};
+
 typedef struct uct_rc_iface_tmh_priv_data {
     uint8_t                     length;
     uint16_t                    data;
@@ -215,8 +228,9 @@ struct uct_rc_iface {
     uct_ib_iface_t              super;
 
     struct {
-        ucs_mpool_t             mp;      /* pool for send descriptors */
-        ucs_mpool_t             fc_mp;   /* pool for FC grant pending requests */
+        ucs_mpool_t             mp;       /* pool for send descriptors */
+        ucs_mpool_t             fc_mp;    /* pool for FC grant pending requests */
+        ucs_mpool_t             flush_mp; /* pool for flush completions */
         /* Credits for completions.
          * May be negative in case mlx5 because we take "num_bb" credits per
          * post to be able to calculate credits of outstanding ops on failure.
@@ -253,7 +267,7 @@ struct uct_rc_iface {
         } rndv_unexp;
         uct_rc_iface_release_desc_t  eager_desc;
         uct_rc_iface_release_desc_t  rndv_desc;
-
+        UCS_STATS_NODE_DECLARE(stats);
     } tm;
 #endif
 
@@ -284,6 +298,9 @@ struct uct_rc_iface {
         enum ibv_mtu         path_mtu;
         /* Enable out-of-order RDMA data placement */
         uint8_t              ooo_rw;
+#if ENABLE_ASSERT
+        int                  tx_cq_len;
+#endif
 
         /* Atomic callbacks */
         uct_rc_send_handler_t  atomic64_handler;      /* 64bit ib-spec */
@@ -299,9 +316,9 @@ struct uct_rc_iface {
     /* Progress function (either regular or TM aware) */
     ucs_callback_t           progress;
 };
-UCS_CLASS_DECLARE(uct_rc_iface_t, uct_rc_iface_ops_t*, uct_md_h,
-                  uct_worker_h, const uct_iface_params_t*,
-                  const uct_rc_iface_config_t*, unsigned, unsigned, int)
+UCS_CLASS_DECLARE(uct_rc_iface_t, uct_rc_iface_ops_t*, uct_md_h, uct_worker_h,
+                  const uct_iface_params_t*, const uct_rc_iface_config_t*,
+                  uct_ib_iface_init_attr_t*);
 
 
 struct uct_rc_iface_send_op {
@@ -340,8 +357,13 @@ typedef struct uct_rc_am_short_hdr {
 
 #if IBV_EXP_HW_TM
 
+#  define UCT_RC_IFACE_TM_STAT(_iface, _op) \
+       UCS_STATS_UPDATE_COUNTER((_iface)->tm.stats, UCT_RC_IFACE_STAT_TAG_##_op, 1)
+
 #  define UCT_RC_IFACE_TM_ENABLED(_iface) (_iface)->tm.enabled
 
+
+
 /* TMH can carry 2 bytes of data in its reserved filed */
 #  define UCT_RC_IFACE_TMH_PRIV_LEN       ucs_field_sizeof(uct_rc_iface_tmh_priv_data_t, \
                                                            data)
@@ -377,22 +399,22 @@ typedef struct uct_rc_am_short_hdr {
 #  define UCT_RC_IFACE_GET_TX_TM_DESC(_iface, _mp, _desc, _tag, _app_ctx, _hdr) \
       { \
           UCT_RC_IFACE_GET_TX_DESC(_iface, _mp, _desc) \
-          hdr = _desc + 1; \
+          _hdr = _desc + 1; \
           uct_rc_iface_fill_tmh(_hdr, _tag, _app_ctx, IBV_EXP_TMH_EAGER); \
-          hdr += sizeof(struct ibv_exp_tmh); \
+          _hdr += sizeof(struct ibv_exp_tmh); \
       }
 
 #  define UCT_RC_IFACE_GET_TM_BCOPY_DESC(_iface, _mp, _desc, _tag, _app_ctx, \
-                                       _pack_cb, _arg, _length) \
+                                         _pack_cb, _arg, _length) \
        { \
            void *hdr; \
            UCT_RC_IFACE_GET_TX_TM_DESC(_iface, _mp, _desc, _tag, _app_ctx, hdr) \
            (_desc)->super.handler = (uct_rc_send_handler_t)ucs_mpool_put; \
-           _length = pack_cb(hdr, arg); \
+           _length = _pack_cb(hdr, _arg); \
        }
 
 ucs_status_t uct_rc_iface_handle_rndv(uct_rc_iface_t *iface,
-                                      struct ibv_exp_tmh *tmh,
+                                      struct ibv_exp_tmh *tmh, uct_tag_t tag,
                                       unsigned byte_len);
 
 
@@ -462,18 +484,15 @@ uct_rc_iface_ctx_priv(uct_tag_context_t *ctx)
 }
 
 static UCS_F_ALWAYS_INLINE void
-uct_rc_iface_handle_rndv_fin(uct_rc_iface_t *iface, struct ibv_exp_tmh *tmh)
+uct_rc_iface_handle_rndv_fin(uct_rc_iface_t *iface, uint32_t app_ctx)
 {
     int found;
     void *rndv_comp;
 
-    ucs_assert(tmh->opcode == IBV_EXP_TMH_FIN);
-
-    found = ucs_ptr_array_lookup(&iface->tm.rndv_comps, ntohl(tmh->app_ctx),
-                                 rndv_comp);
+    found = ucs_ptr_array_lookup(&iface->tm.rndv_comps, app_ctx, rndv_comp);
     ucs_assert_always(found > 0);
     uct_invoke_completion((uct_completion_t*)rndv_comp, UCS_OK);
-    ucs_ptr_array_remove(&iface->tm.rndv_comps, ntohl(tmh->app_ctx), 0);
+    ucs_ptr_array_remove(&iface->tm.rndv_comps, app_ctx, 0);
 }
 
 #else
@@ -491,7 +510,8 @@ unsigned uct_rc_iface_do_progress(uct_iface_h tl_iface);
 ucs_status_t uct_rc_iface_query(uct_rc_iface_t *iface,
                                 uct_iface_attr_t *iface_attr,
                                 size_t put_max_short, size_t max_inline,
-                                size_t am_max_hdr, size_t am_max_iov);
+                                size_t am_max_hdr, size_t am_max_iov,
+                                size_t tag_max_iov);
 
 ucs_status_t uct_rc_iface_get_address(uct_iface_h tl_iface,
                                       uct_iface_addr_t *addr);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs.h
index a13e0671b..8fd83db20 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs.h
@@ -117,35 +117,29 @@ ucs_status_t uct_rc_verbs_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *he
                                       size_t iovcnt, unsigned flags,
                                       uct_completion_t *comp);
 
-ucs_status_t uct_rc_verbs_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey);
-
-ucs_status_t uct_rc_verbs_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp);
-
-ucs_status_t uct_rc_verbs_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp);
-
 ucs_status_t uct_rc_verbs_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
                                             uint64_t remote_addr, uct_rkey_t rkey,
                                             uint64_t *result, uct_completion_t *comp);
 
-ucs_status_t uct_rc_verbs_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_rc_verbs_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uint32_t *result, uct_completion_t *comp);
+
+ucs_status_t uct_rc_verbs_ep_atomic64_post(uct_ep_h tl_ep, unsigned opcode, uint64_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey);
 
-ucs_status_t uct_rc_verbs_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp);
+ucs_status_t uct_rc_verbs_ep_atomic32_post(uct_ep_h tl_ep, unsigned opcode, uint32_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey);
 
-ucs_status_t uct_rc_verbs_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp);
+ucs_status_t uct_rc_verbs_ep_atomic64_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint64_t value, uint64_t *result,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uct_completion_t *comp);
 
-ucs_status_t uct_rc_verbs_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+ucs_status_t uct_rc_verbs_ep_atomic32_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint32_t value, uint32_t *result,
                                             uint64_t remote_addr, uct_rkey_t rkey,
-                                            uint32_t *result, uct_completion_t *comp);
+                                            uct_completion_t *comp);
 
 ucs_status_t uct_rc_verbs_ep_flush(uct_ep_h tl_ep, unsigned flags,
                                    uct_completion_t *comp);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.c
index 7a39ad959..4e32a80ff 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.c
@@ -142,7 +142,7 @@ ucs_status_t uct_rc_verbs_iface_common_init(uct_rc_verbs_iface_common_t *iface,
     /* Configuration */
     iface->config.short_desc_size = ucs_max(sizeof(uct_rc_hdr_t),
                                             config->max_am_hdr);
-    iface->config.short_desc_size = ucs_max(UCT_RC_MAX_ATOMIC_SIZE,
+    iface->config.short_desc_size = ucs_max(UCT_IB_MAX_ATOMIC_SIZE,
                                             iface->config.short_desc_size);
 
     /* Create AM headers and Atomic mempool */
@@ -189,3 +189,11 @@ ucs_status_t uct_rc_verbs_wc_to_ucs_status(enum ibv_wc_status status)
         return UCS_ERR_IO_ERROR;
     }
 }
+
+void uct_rc_verbs_common_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                                     void *data, size_t length, size_t valid_length,
+                                     char *buffer, size_t max)
+{
+    uct_rc_ep_packet_dump(iface, type, data, length, valid_length, buffer, max, 1);
+}
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.h
index 7461374ef..221947c6d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_common.h
@@ -17,7 +17,7 @@
 
 
 #define UCT_RC_VERBS_IFACE_FOREACH_TXWQE(_iface, _i, _wc, _num_wcs) \
-      status = uct_ib_poll_cq((_iface)->super.send_cq, &_num_wcs, _wc); \
+      status = uct_ib_poll_cq((_iface)->super.cq[UCT_IB_DIR_TX], &_num_wcs, _wc); \
       if (status != UCS_OK) { \
           return 0; \
       } \
@@ -69,6 +69,10 @@ void uct_rc_verbs_txcnt_init(uct_rc_verbs_txcnt_t *txcnt);
 
 ucs_status_t uct_rc_verbs_wc_to_ucs_status(enum ibv_wc_status status);
 
+void uct_rc_verbs_common_packet_dump(uct_base_iface_t *iface, uct_am_trace_type_t type,
+                                     void *data, size_t length, size_t valid_length,
+                                     char *buffer, size_t max);
+
 static inline void
 uct_rc_verbs_txqp_posted(uct_rc_txqp_t *txqp, uct_rc_verbs_txcnt_t *txcnt,
                          uct_rc_iface_t *iface, int signaled)
@@ -175,7 +179,7 @@ uct_rc_verbs_iface_poll_rx_common(uct_rc_iface_t *iface)
     unsigned num_wcs = iface->super.config.rx_max_poll;
     struct ibv_wc wc[num_wcs];
 
-    status = uct_ib_poll_cq(iface->super.recv_cq, &num_wcs, wc);
+    status = uct_ib_poll_cq(iface->super.cq[UCT_IB_DIR_RX], &num_wcs, wc);
     if (status != UCS_OK) {
         num_wcs = 0;
         goto out;
@@ -183,7 +187,7 @@ uct_rc_verbs_iface_poll_rx_common(uct_rc_iface_t *iface)
 
     UCT_IB_IFACE_VERBS_FOREACH_RXWQE(&iface->super, i, hdr, wc, num_wcs) {
         uct_ib_log_recv_completion(&iface->super, IBV_QPT_RC, &wc[i], hdr,
-                                   wc[i].byte_len, uct_rc_ep_am_packet_dump);
+                                   wc[i].byte_len, uct_rc_verbs_common_packet_dump);
         uct_rc_verbs_iface_handle_am(iface, hdr, wc[i].wr_id, wc[i].qp_num,
                                      wc[i].byte_len, wc[i].imm_data, wc[i].slid);
     }
@@ -331,6 +335,9 @@ uct_rc_verbs_iface_common_tag_recv(uct_rc_verbs_iface_common_t *iface,
     priv->tag        = tag;
     priv->buffer     = iov->buffer; /* Only one iov is supported so far */
     priv->length     = uct_iov_total_length(iov, iovcnt);
+
+    UCT_RC_IFACE_TM_STAT(rc_iface, LIST_ADD);
+
     return UCS_OK;
 }
 
@@ -367,6 +374,8 @@ uct_rc_verbs_iface_common_tag_recv_cancel(uct_rc_verbs_iface_common_t *iface,
        }
    }
 
+   UCT_RC_IFACE_TM_STAT(rc_iface, LIST_DEL);
+
    return UCS_OK;
 }
 
@@ -404,7 +413,7 @@ uct_rc_verbs_iface_wc_error(enum ibv_wc_status status)
 {
     /* TODO: handle MSG TRUNCATED error */
     ucs_fatal("Receive completion with error on XRQ: %s",
-              ibv_wc_status_str(status));
+              uct_ib_wc_status_str(status));
 }
 
 static UCS_F_ALWAYS_INLINE void
@@ -429,8 +438,10 @@ uct_rc_verbs_iface_tag_handle_exp(uct_rc_iface_t *iface, struct ibv_exp_wc *wc)
         VALGRIND_MAKE_MEM_DEFINED(priv->buffer, wc->byte_len);
         if (UCT_RC_VERBS_TM_IS_SW_RNDV(wc->exp_wc_flags, imm_data)) {
             ctx->rndv_cb(ctx, priv->tag, priv->buffer, wc->byte_len, UCS_OK);
+            UCT_RC_IFACE_TM_STAT(iface, RX_RNDV_REQ_EXP);
         } else {
             ctx->completed_cb(ctx, priv->tag, imm_data, wc->byte_len, UCS_OK);
+            UCT_RC_IFACE_TM_STAT(iface, RX_EXP);
         }
         ++iface->tm.num_tags;
     }
@@ -456,6 +467,7 @@ uct_rc_verbs_iface_unexp_consumed(uct_rc_verbs_iface_common_t *iface,
     if (ucs_unlikely(!(++rc_iface->tm.unexpected_cnt % IBV_DEVICE_MAX_UNEXP_COUNT))) {
         uct_rc_verbs_iface_post_signaled_op(iface, rc_iface, &wr,
                                             IBV_EXP_WR_TAG_SYNC);
+        UCT_RC_IFACE_TM_STAT(rc_iface, LIST_SYNC);
     }
 }
 
@@ -486,11 +498,15 @@ uct_rc_verbs_iface_tag_handle_unexp(uct_rc_verbs_iface_common_t *iface,
                                                 be64toh(tmh->tag), tmh + 1,
                                                 wc->byte_len - sizeof(*tmh),
                                                 0ul, 0, NULL);
+
+            UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_REQ_UNEXP);
         } else {
             status = rc_iface->tm.eager_unexp.cb(rc_iface->tm.eager_unexp.arg,
                                                  tmh + 1, wc->byte_len - sizeof(*tmh),
                                                  UCT_CB_PARAM_FLAG_DESC,
                                                  be64toh(tmh->tag), imm_data);
+
+            UCT_RC_IFACE_TM_STAT(rc_iface, RX_EAGER_UNEXP);
         }
         uct_rc_verbs_iface_unexp_consumed(iface, rc_iface, ib_desc,
                                           &rc_iface->tm.eager_desc, status);
@@ -499,21 +515,25 @@ uct_rc_verbs_iface_tag_handle_unexp(uct_rc_verbs_iface_common_t *iface,
     case IBV_EXP_TMH_NO_TAG:
         rc_hdr = (uct_rc_hdr_t*)tmh;
         uct_ib_log_recv_completion(&rc_iface->super, IBV_QPT_RC, wc, rc_hdr,
-                                   wc->byte_len, uct_rc_ep_am_packet_dump);
+                                   wc->byte_len, uct_rc_verbs_common_packet_dump);
         uct_rc_verbs_iface_handle_am(rc_iface, rc_hdr, wc->wr_id, wc->qp_num,
                                      wc->byte_len, wc->imm_data, wc->slid);
         break;
 
     case IBV_EXP_TMH_RNDV:
-        status = uct_rc_iface_handle_rndv(rc_iface, tmh, wc->byte_len);
+        status = uct_rc_iface_handle_rndv(rc_iface, tmh, be64toh(tmh->tag),
+                                          wc->byte_len);
 
         uct_rc_verbs_iface_unexp_consumed(iface, rc_iface, ib_desc,
                                           &rc_iface->tm.rndv_desc, status);
+
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_UNEXP);
         break;
 
     case IBV_EXP_TMH_FIN:
-        uct_rc_iface_handle_rndv_fin(rc_iface, tmh);
+        uct_rc_iface_handle_rndv_fin(rc_iface, ntohl(tmh->app_ctx));
         ucs_mpool_put_inline(ib_desc);
+        UCT_RC_IFACE_TM_STAT(rc_iface, RX_RNDV_FIN);
         break;
 
     default:
@@ -534,7 +554,7 @@ uct_rc_verbs_iface_poll_rx_tm(uct_rc_verbs_iface_common_t *iface,
     uct_rc_iface_ctx_priv_t *priv;
     int num_wcs, i;
 
-    num_wcs = ibv_exp_poll_cq(rc_iface->super.recv_cq, max_wcs, wc,
+    num_wcs = ibv_exp_poll_cq(rc_iface->super.cq[UCT_IB_DIR_RX], max_wcs, wc,
                               sizeof(wc[0]));
     if (num_wcs <= 0) {
         if (ucs_unlikely(num_wcs < 0)) {
@@ -684,6 +704,26 @@ uct_rc_verbs_fill_ext_atomic_wr(struct ibv_exp_send_wr *wr, struct ibv_sge *sge,
         break;
     }
 }
+
+static UCS_F_ALWAYS_INLINE
+ucs_status_t uct_rc_verbs_ep_atomic32_data(uct_atomic_op_t opcode, uint32_t value,
+                                           int *op, uint32_t *add, uint32_t *swap)
+{
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        *op   = IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD;
+        *add  = value;
+        *swap = 0;
+        return UCS_OK;
+    case UCT_ATOMIC_OP_SWAP:
+        *op   = IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP;
+        *add  = 0;
+        *swap = value;
+        return UCS_OK;
+    default:
+        return UCS_ERR_UNSUPPORTED;
+    }
+}
 #endif
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_ep.c
index 1cc8743fa..d8abee6fe 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_ep.c
@@ -12,7 +12,7 @@
 
 static UCS_F_ALWAYS_INLINE void
 uct_rc_verbs_ep_post_send(uct_rc_verbs_iface_t* iface, uct_rc_verbs_ep_t* ep,
-                          struct ibv_send_wr *wr, int send_flags)
+                          struct ibv_send_wr *wr, int send_flags, int max_log_sge)
 {
     struct ibv_send_wr *bad_wr;
     int ret;
@@ -26,8 +26,8 @@ uct_rc_verbs_ep_post_send(uct_rc_verbs_iface_t* iface, uct_rc_verbs_ep_t* ep,
     wr->send_flags = send_flags;
     wr->wr_id      = uct_rc_txqp_unsignaled(&ep->super.txqp);
 
-    uct_ib_log_post_send(&iface->super.super, ep->super.txqp.qp, wr,
-                         (wr->opcode == IBV_WR_SEND) ? uct_rc_ep_am_packet_dump : NULL);
+    uct_ib_log_post_send(&iface->super.super, ep->super.txqp.qp, wr, max_log_sge,
+                         (wr->opcode == IBV_WR_SEND) ? uct_rc_verbs_common_packet_dump : NULL);
 
     ret = ibv_post_send(ep->super.txqp.qp, wr, &bad_wr);
     if (ret != 0) {
@@ -40,7 +40,7 @@ uct_rc_verbs_ep_post_send(uct_rc_verbs_iface_t* iface, uct_rc_verbs_ep_t* ep,
 #if HAVE_DECL_IBV_EXP_POST_SEND && (HAVE_DECL_IBV_EXP_WR_NOP || HAVE_IB_EXT_ATOMICS)
 static UCS_F_ALWAYS_INLINE void
 uct_rc_verbs_exp_post_send(uct_rc_verbs_ep_t *ep, struct ibv_exp_send_wr *wr,
-                           uint64_t signal)
+                           uint64_t signal, int max_log_sge)
 {
     uct_rc_verbs_iface_t *iface = ucs_derived_of(ep->super.super.super.iface,
                                                  uct_rc_verbs_iface_t);
@@ -54,9 +54,9 @@ uct_rc_verbs_exp_post_send(uct_rc_verbs_ep_t *ep, struct ibv_exp_send_wr *wr,
     wr->exp_send_flags = signal;
     wr->wr_id          = uct_rc_txqp_unsignaled(&ep->super.txqp);
 
-    uct_ib_log_exp_post_send(&iface->super.super, ep->super.txqp.qp, wr,
+    uct_ib_log_exp_post_send(&iface->super.super, ep->super.txqp.qp, wr, max_log_sge,
                              (wr->exp_opcode == IBV_EXP_WR_SEND) ?
-                             uct_rc_ep_am_packet_dump : NULL);
+                             uct_rc_verbs_common_packet_dump : NULL);
 
     ret = ibv_exp_post_send(ep->super.txqp.qp, wr, &bad_wr);
     if (ret != 0) {
@@ -74,12 +74,13 @@ uct_rc_verbs_exp_post_send(uct_rc_verbs_ep_t *ep, struct ibv_exp_send_wr *wr,
  */
 static UCS_F_ALWAYS_INLINE void
 uct_rc_verbs_ep_post_send_desc(uct_rc_verbs_ep_t* ep, struct ibv_send_wr *wr,
-                               uct_rc_iface_send_desc_t *desc, int send_flags)
+                               uct_rc_iface_send_desc_t *desc, int send_flags,
+                               int max_log_sge)
 {
     uct_rc_verbs_iface_t *iface = ucs_derived_of(ep->super.super.super.iface,
                                                  uct_rc_verbs_iface_t);
     UCT_RC_VERBS_FILL_DESC_WR(wr, desc);
-    uct_rc_verbs_ep_post_send(iface, ep, wr, send_flags);
+    uct_rc_verbs_ep_post_send(iface, ep, wr, send_flags, max_log_sge);
     uct_rc_txqp_add_send_op_sn(&ep->super.txqp, &desc->super, ep->txcnt.pi);
 }
 
@@ -100,7 +101,7 @@ uct_rc_verbs_ep_rdma_zcopy(uct_rc_verbs_ep_t *ep, const uct_iov_t *iov,
     UCT_RC_VERBS_FILL_RDMA_WR_IOV(wr, wr.opcode, opcode, sge, sge_cnt, remote_addr, rkey);
     wr.next = NULL;
 
-    uct_rc_verbs_ep_post_send(iface, ep, &wr, IBV_SEND_SIGNALED);
+    uct_rc_verbs_ep_post_send(iface, ep, &wr, IBV_SEND_SIGNALED, INT_MAX);
     uct_rc_txqp_add_send_comp(&iface->super, &ep->super.txqp, comp, ep->txcnt.pi);
     return UCS_INPROGRESS;
 }
@@ -118,7 +119,7 @@ uct_rc_verbs_ep_atomic_post(uct_rc_verbs_ep_t *ep, int opcode, uint64_t compare_
     UCT_RC_VERBS_FILL_ATOMIC_WR(wr, wr.opcode, sge, opcode, compare_add, swap,
                                 remote_addr, ib_rkey);
     UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, force_sig);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, force_sig, INT_MAX);
 }
 
 static UCS_F_ALWAYS_INLINE ucs_status_t
@@ -131,9 +132,9 @@ uct_rc_verbs_ep_atomic(uct_rc_verbs_ep_t *ep, int opcode, void *result,
     uct_rc_iface_send_desc_t *desc;
 
     UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc,
-                                    iface->super.config.atomic64_handler,
-                                    result, comp);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super, &iface->verbs_common.short_desc_mp,
+                                          desc, iface->super.config.atomic64_handler,
+                                          result, comp);
     uct_rc_verbs_ep_atomic_post(ep, opcode, compare_add, swap, remote_addr,
                                 rkey, desc, IBV_SEND_SIGNALED);
     return UCS_INPROGRESS;
@@ -153,7 +154,7 @@ uct_rc_verbs_ep_ext_atomic_post(uct_rc_verbs_ep_t *ep, int opcode, uint32_t leng
                                     compare_add, swap, remote_addr, rkey, ep->super.atomic_mr_offset);
     UCT_RC_VERBS_FILL_DESC_WR(&wr, desc);
     UCT_TL_EP_STAT_ATOMIC(&ep->super.super);
-    uct_rc_verbs_exp_post_send(ep, &wr, force_sig|IBV_EXP_SEND_EXT_ATOMIC_INLINE);
+    uct_rc_verbs_exp_post_send(ep, &wr, force_sig|IBV_EXP_SEND_EXT_ATOMIC_INLINE, INT_MAX);
     uct_rc_txqp_add_send_op_sn(&ep->super.txqp, &desc->super, ep->txcnt.pi);
 }
 
@@ -170,8 +171,8 @@ uct_rc_verbs_ep_ext_atomic(uct_rc_verbs_ep_t *ep, int opcode, void *result,
     uct_rc_iface_send_desc_t *desc;
 
     UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc,
-                                    handler, result, comp);
+    UCT_RC_IFACE_GET_TX_ATOMIC_FETCH_DESC(&iface->super, &iface->verbs_common.short_desc_mp,
+                                          desc, handler, result, comp);
     uct_rc_verbs_ep_ext_atomic_post(ep, opcode, length, compare_mask, compare_add,
                                     swap, remote_addr, rkey, desc,
                                     IBV_EXP_SEND_SIGNALED);
@@ -192,7 +193,7 @@ ucs_status_t uct_rc_verbs_ep_put_short(uct_ep_h tl_ep, const void *buffer,
     UCT_RC_VERBS_FILL_INL_PUT_WR(iface, remote_addr, rkey, buffer, length);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, SHORT, length);
     uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_rwrite_wr,
-                              IBV_SEND_INLINE | IBV_SEND_SIGNALED);
+                              IBV_SEND_INLINE | IBV_SEND_SIGNALED, INT_MAX);
     return UCS_OK;
 }
 
@@ -212,7 +213,7 @@ ssize_t uct_rc_verbs_ep_put_bcopy(uct_ep_h tl_ep, uct_pack_callback_t pack_cb,
     UCT_RC_VERBS_FILL_RDMA_WR(wr, wr.opcode, IBV_WR_RDMA_WRITE, sge,
                               length, remote_addr, rkey);
     UCT_TL_EP_STAT_OP(&ep->super.super, PUT, BCOPY, length);
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SIGNALED);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SIGNALED, INT_MAX);
     return length;
 }
 
@@ -254,7 +255,7 @@ ucs_status_t uct_rc_verbs_ep_get_bcopy(uct_ep_h tl_ep,
                               rkey);
 
     UCT_TL_EP_STAT_OP(&ep->super.super, GET, BCOPY, length);
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SIGNALED);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SIGNALED, INT_MAX);
     return UCS_INPROGRESS;
 }
 
@@ -290,7 +291,7 @@ ucs_status_t uct_rc_verbs_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
     uct_rc_verbs_iface_fill_inl_am_sge(verbs_common, id, hdr, buffer, length);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, SHORT, sizeof(hdr) + length);
     uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_am_wr,
-                              IBV_SEND_INLINE | IBV_SEND_SOLICITED);
+                              IBV_SEND_INLINE | IBV_SEND_SOLICITED, INT_MAX);
     UCT_RC_UPDATE_FC(&iface->super, &ep->super, id);
 
     return UCS_OK;
@@ -316,7 +317,7 @@ ssize_t uct_rc_verbs_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
     UCT_RC_VERBS_FILL_AM_BCOPY_WR(wr, sge, length + sizeof(uct_rc_hdr_t),
                                   wr.opcode);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, BCOPY, length);
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SOLICITED);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, IBV_SEND_SOLICITED, INT_MAX);
     UCT_RC_UPDATE_FC(&iface->super, &ep->super, id);
 
     return length;
@@ -353,76 +354,52 @@ ucs_status_t uct_rc_verbs_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *he
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, ZCOPY,
                       (header_length + uct_iov_total_length(iov, iovcnt)));
 
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, send_flags | IBV_SEND_SOLICITED);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, send_flags | IBV_SEND_SOLICITED,
+                                   UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super));
     UCT_RC_UPDATE_FC(&iface->super, &ep->super, id);
 
     return UCS_INPROGRESS;
 }
 
-ucs_status_t uct_rc_verbs_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_rc_verbs_ep_atomic64_post(uct_ep_h tl_ep, unsigned opcode, uint64_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey)
 {
     uct_rc_verbs_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_verbs_iface_t);
-    uct_rc_verbs_ep_t *ep = ucs_derived_of(tl_ep, uct_rc_verbs_ep_t);
+    uct_rc_verbs_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_verbs_ep_t);
     uct_rc_iface_send_desc_t *desc;
 
+    if (opcode != UCT_ATOMIC_OP_ADD) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
     /* TODO don't allocate descriptor - have dummy buffer */
     UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc);
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc);
 
     uct_rc_verbs_ep_atomic_post(ep,
-                                IBV_WR_ATOMIC_FETCH_AND_ADD, add, 0,
+                                IBV_WR_ATOMIC_FETCH_AND_ADD, value, 0,
                                 remote_addr, rkey, desc,
                                 IBV_SEND_SIGNALED);
     return UCS_OK;
 }
 
-ucs_status_t uct_rc_verbs_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp)
-{
-    return uct_rc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
-                                  IBV_WR_ATOMIC_FETCH_AND_ADD, result, add, 0,
-                                  remote_addr, rkey, comp);
-}
-
-ucs_status_t uct_rc_verbs_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint64_t *result, uct_completion_t *comp)
-{
-#if HAVE_IB_EXT_ATOMICS
-    return uct_rc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
-                                      result, sizeof(uint64_t), 0, 0, swap, remote_addr,
-                                      rkey, comp);
-#else
-    return UCS_ERR_UNSUPPORTED;
-#endif
-}
-
-ucs_status_t uct_rc_verbs_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
-                                            uint64_t remote_addr, uct_rkey_t rkey,
-                                            uint64_t *result, uct_completion_t *comp)
-{
-    return uct_rc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
-                                  IBV_WR_ATOMIC_CMP_AND_SWP, result, compare, swap,
-                                  remote_addr, rkey, comp);
-}
-
-ucs_status_t uct_rc_verbs_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                          uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_rc_verbs_ep_atomic32_post(uct_ep_h tl_ep, unsigned opcode, uint32_t value,
+                                           uint64_t remote_addr, uct_rkey_t rkey)
 {
 #if HAVE_IB_EXT_ATOMICS
     uct_rc_verbs_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_rc_verbs_iface_t);
-    uct_rc_verbs_ep_t *ep = ucs_derived_of(tl_ep, uct_rc_verbs_ep_t);
+    uct_rc_verbs_ep_t *ep       = ucs_derived_of(tl_ep, uct_rc_verbs_ep_t);
     uct_rc_iface_send_desc_t *desc;
 
+    if (opcode != UCT_ATOMIC_OP_ADD) {
+        return UCS_ERR_UNSUPPORTED;
+    }
+
     UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    UCT_RC_IFACE_GET_TX_ATOMIC_ADD_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc);
+    UCT_RC_IFACE_GET_TX_ATOMIC_DESC(&iface->super, &iface->verbs_common.short_desc_mp, desc);
 
-    /* TODO don't allocate descriptor - have dummy buffer */
     uct_rc_verbs_ep_ext_atomic_post(ep, IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD,
-                                    sizeof(uint32_t), 0, add, 0, remote_addr,
+                                    sizeof(uint32_t), 0, value, 0, remote_addr,
                                     rkey, desc, IBV_EXP_SEND_SIGNALED);
     return UCS_OK;
 #else
@@ -430,34 +407,63 @@ ucs_status_t uct_rc_verbs_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
 #endif
 }
 
-ucs_status_t uct_rc_verbs_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_rc_verbs_ep_atomic64_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint64_t value, uint64_t *result,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uct_completion_t *comp)
 {
-#if HAVE_IB_EXT_ATOMICS
-    return uct_rc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_FETCH_AND_ADD,
-                                      result, sizeof(uint32_t), 0, add, 0,
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_rc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
+                                      IBV_WR_ATOMIC_FETCH_AND_ADD, result, value, 0,
                                       remote_addr, rkey, comp);
-#else
-    return UCS_ERR_UNSUPPORTED;
+#if HAVE_IB_EXT_ATOMICS
+    case UCT_ATOMIC_OP_SWAP:
+        return uct_rc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
+                                          IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
+                                          result, sizeof(uint64_t), 0, 0, value, remote_addr,
+                                          rkey, comp);
 #endif
+    default:
+        break;
+    }
+
+    return UCS_ERR_UNSUPPORTED;
 }
 
-ucs_status_t uct_rc_verbs_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                           uint64_t remote_addr, uct_rkey_t rkey,
-                                           uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_rc_verbs_ep_atomic32_fetch(uct_ep_h tl_ep, uct_atomic_op_t opcode,
+                                            uint32_t value, uint32_t *result,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uct_completion_t *comp)
 {
 #if HAVE_IB_EXT_ATOMICS
-    return uct_rc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
-                                      IBV_EXP_WR_EXT_MASKED_ATOMIC_CMP_AND_SWP,
-                                   result, sizeof(uint32_t), 0, 0, swap,
-                                   remote_addr, rkey, comp);
+    int op;
+    uint32_t add;
+    uint32_t swap;
+    ucs_status_t status;
+
+    status = uct_rc_verbs_ep_atomic32_data(opcode, value, &op, &add, &swap);
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    return uct_rc_verbs_ep_ext_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t), op,
+                                      result, sizeof(uint32_t), 0, add, swap,
+                                      remote_addr, rkey, comp);
 #else
     return UCS_ERR_UNSUPPORTED;
 #endif
 }
 
+ucs_status_t uct_rc_verbs_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
+                                            uint64_t remote_addr, uct_rkey_t rkey,
+                                            uint64_t *result, uct_completion_t *comp)
+{
+    return uct_rc_verbs_ep_atomic(ucs_derived_of(tl_ep, uct_rc_verbs_ep_t),
+                                  IBV_WR_ATOMIC_CMP_AND_SWP, result, compare, swap,
+                                  remote_addr, rkey, comp);
+}
+
 ucs_status_t uct_rc_verbs_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
                                             uint64_t remote_addr, uct_rkey_t rkey,
                                             uint32_t *result, uct_completion_t *comp)
@@ -485,7 +491,7 @@ static ucs_status_t uct_rc_verbs_ep_nop(uct_rc_verbs_ep_t *ep)
     wr.exp_send_flags = IBV_EXP_SEND_FENCE;
     wr.comp_mask      = 0;
     UCT_RC_CHECK_RES(&iface->super, &ep->super);
-    uct_rc_verbs_exp_post_send(ep, &wr, IBV_EXP_SEND_SIGNALED);
+    uct_rc_verbs_exp_post_send(ep, &wr, IBV_EXP_SEND_SIGNALED, INT_MAX);
     return UCS_OK;
 #else
     return UCS_ERR_UNSUPPORTED;
@@ -515,9 +521,13 @@ ucs_status_t uct_rc_verbs_ep_flush(uct_ep_h tl_ep, unsigned flags,
         }
     }
 
-    uct_rc_txqp_add_send_comp(&iface->super, &ep->super.txqp, comp, ep->txcnt.pi);
-    UCT_TL_EP_STAT_FLUSH_WAIT(&ep->super.super);
-    return UCS_INPROGRESS;
+    status = uct_rc_txqp_add_flush_comp(&iface->super, &ep->super.txqp, comp,
+                                        ep->txcnt.pi);
+    if (status == UCS_INPROGRESS) {
+        UCT_TL_EP_STAT_FLUSH_WAIT(&ep->super.super);
+    }
+
+    return status;
 }
 
 ucs_status_t uct_rc_verbs_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
@@ -547,7 +557,7 @@ ucs_status_t uct_rc_verbs_ep_fc_ctrl(uct_ep_t *tl_ep, unsigned op,
     iface->verbs_common.inl_sge[0].addr    = (uintptr_t)hdr;
     iface->verbs_common.inl_sge[0].length  = sizeof(*hdr);
 
-    uct_rc_verbs_ep_post_send(iface, ep, &fc_wr, IBV_SEND_INLINE);
+    uct_rc_verbs_ep_post_send(iface, ep, &fc_wr, IBV_SEND_INLINE, INT_MAX);
     return UCS_OK;
 }
 
@@ -567,7 +577,10 @@ ucs_status_t uct_rc_verbs_ep_tag_eager_short(uct_ep_h tl_ep, uct_tag_t tag,
     uct_rc_iface_fill_tmh(&tmh, tag, 0, IBV_EXP_TMH_EAGER);
     uct_rc_verbs_iface_fill_inl_sge(&iface->verbs_common, &tmh, sizeof(tmh), data, length);
 
-    uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_am_wr, IBV_SEND_INLINE);
+    uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_am_wr, IBV_SEND_INLINE, INT_MAX);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, SHORT, length);
+
     return UCS_OK;
 }
 
@@ -591,7 +604,10 @@ ssize_t uct_rc_verbs_ep_tag_eager_bcopy(uct_ep_h tl_ep, uct_tag_t tag,
     UCT_RC_IFACE_GET_TM_BCOPY_DESC(iface, &iface->tx.mp, desc, tag, app_ctx,
                                    pack_cb, arg, length);
     UCT_RC_VERBS_FILL_SGE(wr, sge, length + sizeof(struct ibv_exp_tmh));
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, 0);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, 0, INT_MAX);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, BCOPY, length);
+
     return length;
 }
 
@@ -609,7 +625,8 @@ ucs_status_t uct_rc_verbs_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     size_t sge_cnt;
     uint32_t app_ctx;
 
-    UCT_CHECK_IOV_SIZE(iovcnt, 1ul, "uct_rc_verbs_ep_tag_eager_zcopy");
+    UCT_CHECK_IOV_SIZE(iovcnt, uct_ib_iface_get_max_iov(&iface->super.super) - 1,
+                       "uct_rc_verbs_ep_tag_eager_zcopy");
     UCT_RC_CHECK_ZCOPY_DATA(sizeof(struct ibv_exp_tmh),
                             uct_iov_total_length(iov, iovcnt),
                             iface->super.super.config.seg_size);
@@ -626,7 +643,11 @@ ucs_status_t uct_rc_verbs_ep_tag_eager_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
     wr.num_sge = sge_cnt + 1;
     wr.sg_list = sge;
 
-    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, send_flags);
+    uct_rc_verbs_ep_post_send_desc(ep, &wr, desc, send_flags, INT_MAX);
+
+    UCT_TL_EP_STAT_OP(&ep->super.super, TAG, ZCOPY,
+                      uct_iov_total_length(iov, iovcnt));
+
     return UCS_INPROGRESS;
 }
 
@@ -657,7 +678,7 @@ ucs_status_ptr_t uct_rc_verbs_ep_tag_rndv_zcopy(uct_ep_h tl_ep, uct_tag_t tag,
                                                  iface->super.tm.max_rndv_data,
                                                  tmh_len, tag, iov, comp);
 
-    uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_am_wr, IBV_SEND_INLINE);
+    uct_rc_verbs_ep_post_send(iface, ep, &iface->inl_am_wr, IBV_SEND_INLINE, INT_MAX);
     return (ucs_status_ptr_t)((uint64_t)rndv_idx);
 }
 
@@ -688,7 +709,7 @@ ucs_status_t uct_rc_verbs_ep_tag_rndv_request(uct_ep_h tl_ep, uct_tag_t tag,
     uct_rc_iface_fill_tmh(&tmh, tag, app_ctx, IBV_EXP_TMH_EAGER);
     uct_rc_verbs_iface_fill_inl_sge(&iface->verbs_common, &tmh, sizeof(tmh),
                                     header, header_length);
-    uct_rc_verbs_ep_post_send(iface, ep, &wr, IBV_SEND_INLINE);
+    uct_rc_verbs_ep_post_send(iface, ep, &wr, IBV_SEND_INLINE, INT_MAX);
     return UCS_OK;
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_iface.c
index 98a395de5..7182b6353 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rc/verbs/rc_verbs_iface.c
@@ -202,6 +202,7 @@ static ucs_status_t uct_rc_verbs_iface_query(uct_iface_h tl_iface, uct_iface_att
                                 verbs_common->config.max_inline,
                                 verbs_common->config.max_inline,
                                 verbs_common->config.short_desc_size,
+                                uct_ib_iface_get_max_iov(&iface->super.super) - 1,
                                 uct_ib_iface_get_max_iov(&iface->super.super) - 1);
     if (status != UCS_OK) {
         return status;
@@ -221,12 +222,16 @@ static UCS_CLASS_INIT_FUNC(uct_rc_verbs_iface_t, uct_md_h md, uct_worker_h worke
     uct_rc_verbs_iface_config_t *config =
                     ucs_derived_of(tl_config, uct_rc_verbs_iface_config_t);
     ucs_status_t status;
+    uct_ib_iface_init_attr_t init_attr = {};
     struct ibv_qp_cap cap;
     struct ibv_qp *qp;
 
+    init_attr.res_domain_key = UCT_IB_IFACE_NULL_RES_DOMAIN_KEY;
+    init_attr.tm_cap_bit     = IBV_EXP_TM_CAP_RC;
+    init_attr.fc_req_size    = sizeof(uct_rc_fc_request_t);
+
     UCS_CLASS_CALL_SUPER_INIT(uct_rc_iface_t, &uct_rc_verbs_iface_ops, md,
-                              worker, params, &config->super, 0,
-                              sizeof(uct_rc_fc_request_t), IBV_EXP_TM_CAP_RC);
+                              worker, params, &config->super, &init_attr);
 
     self->config.tx_max_wr           = ucs_min(config->verbs_common.tx_max_wr,
                                                self->super.config.tx_qp_len);
@@ -301,14 +306,12 @@ static uct_rc_iface_ops_t uct_rc_verbs_iface_ops = {
     .ep_put_zcopy             = uct_rc_verbs_ep_put_zcopy,
     .ep_get_bcopy             = uct_rc_verbs_ep_get_bcopy,
     .ep_get_zcopy             = uct_rc_verbs_ep_get_zcopy,
-    .ep_atomic_add64          = uct_rc_verbs_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_rc_verbs_ep_atomic_fadd64,
-    .ep_atomic_swap64         = uct_rc_verbs_ep_atomic_swap64,
     .ep_atomic_cswap64        = uct_rc_verbs_ep_atomic_cswap64,
-    .ep_atomic_add32          = uct_rc_verbs_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_rc_verbs_ep_atomic_fadd32,
-    .ep_atomic_swap32         = uct_rc_verbs_ep_atomic_swap32,
+    .ep_atomic64_post         = uct_rc_verbs_ep_atomic64_post,
+    .ep_atomic64_fetch        = uct_rc_verbs_ep_atomic64_fetch,
     .ep_atomic_cswap32        = uct_rc_verbs_ep_atomic_cswap32,
+    .ep_atomic32_post         = uct_rc_verbs_ep_atomic32_post,
+    .ep_atomic32_fetch        = uct_rc_verbs_ep_atomic32_fetch,
     .ep_pending_add           = uct_rc_ep_pending_add,
     .ep_pending_purge         = uct_rc_ep_pending_purge,
     .ep_flush                 = uct_rc_verbs_ep_flush,
@@ -340,10 +343,11 @@ static uct_rc_iface_ops_t uct_rc_verbs_iface_ops = {
     .iface_get_device_address = uct_ib_iface_get_device_address,
     .iface_is_reachable       = uct_rc_iface_is_reachable,
     },
-    .arm_tx_cq                = uct_ib_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_ib_iface_arm_rx_cq,
+    .arm_cq                   = uct_ib_iface_arm_cq,
+    .event_cq                 = (void*)ucs_empty_function,
     .handle_failure           = uct_rc_verbs_handle_failure,
-    .set_ep_failed            = uct_rc_verbs_ep_set_failed
+    .set_ep_failed            = uct_rc_verbs_ep_set_failed,
+    .create_qp                = uct_ib_iface_create_qp
     },
     .fc_ctrl                  = uct_rc_verbs_ep_fc_ctrl,
     .fc_handler               = uct_rc_iface_fc_handler
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_def.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_def.h
index c1cddcc6a..fe4a24536 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_def.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_def.h
@@ -25,6 +25,7 @@ typedef struct uct_rdmacm_ep      uct_rdmacm_ep_t;
 
 typedef struct uct_rdmacm_priv_data_hdr {
     uint8_t length;     /* length of the private data */
+    int8_t  status;
 } uct_rdmacm_priv_data_hdr_t;
 
 typedef struct uct_rdmacm_ctx {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.c
index 63331cfe2..5679a133f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.c
@@ -5,6 +5,16 @@
 
 #include "rdmacm_ep.h"
 
+
+#define UCT_RDMACM_CB_FLAGS_CHECK(_flags) \
+    do { \
+        UCT_CB_FLAGS_CHECK(_flags); \
+        if (!((_flags) & UCT_CB_FLAG_ASYNC)) { \
+            return UCS_ERR_UNSUPPORTED; \
+        } \
+    } while (0)
+
+
 ucs_status_t uct_rdmacm_ep_resolve_addr(uct_rdmacm_ep_t *ep)
 {
     uct_rdmacm_iface_t *iface = ucs_derived_of(ep->super.super.iface, uct_rdmacm_iface_t);
@@ -74,11 +84,11 @@ static inline void uct_rdmacm_ep_add_to_pending(uct_rdmacm_iface_t *iface, uct_r
 
 static UCS_CLASS_INIT_FUNC(uct_rdmacm_ep_t, uct_iface_t *tl_iface,
                            const ucs_sock_addr_t *sockaddr,
-                           const void *priv_data, size_t length)
+                           uct_sockaddr_priv_pack_callback_t pack_cb,
+                           void *arg, uint32_t cb_flags)
 {
     uct_rdmacm_iface_t *iface = ucs_derived_of(tl_iface, uct_rdmacm_iface_t);
     char ip_port_str[UCS_SOCKADDR_STRING_LEN];
-    uct_rdmacm_priv_data_hdr_t hdr;
     ucs_status_t status;
 
     UCS_CLASS_CALL_SUPER_INIT(uct_base_ep_t, &iface->super);
@@ -88,18 +98,16 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_ep_t, uct_iface_t *tl_iface,
         return UCS_ERR_UNSUPPORTED;
     }
 
+    UCT_RDMACM_CB_FLAGS_CHECK(cb_flags);
+
     /* Initialize these fields before calling rdma_resolve_addr to avoid a race
      * where they are used before being initialized (from the async thread
      * - after an RDMA_CM_EVENT_ROUTE_RESOLVED event) */
-    hdr.length           = length;
-    self->priv_data      = ucs_malloc(sizeof(hdr) + length, "client private data");
-    if (self->priv_data == NULL) {
-        status = UCS_ERR_NO_MEMORY;
-        goto err;
-    }
-
-    memcpy(self->priv_data, &hdr, sizeof(hdr));
-    memcpy(self->priv_data + sizeof(hdr), priv_data, length);
+    self->pack_cb       = pack_cb;
+    self->pack_cb_arg   = arg;
+    self->pack_cb_flags = cb_flags;
+    pthread_mutex_init(&self->ops_mutex, NULL);
+    ucs_queue_head_init(&self->ops);
 
     /* Save the remote address */
     if (sockaddr->addr->sa_family == AF_INET) {
@@ -109,7 +117,7 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_ep_t, uct_iface_t *tl_iface,
     } else {
         ucs_error("rdmacm ep: unknown remote sa_family=%d", sockaddr->addr->sa_family);
         status = UCS_ERR_IO_ERROR;
-        goto err_free_priv_data;
+        goto err;
     }
 
     self->slow_prog_id = UCS_CALLBACKQ_ID_NULL;
@@ -118,7 +126,7 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_ep_t, uct_iface_t *tl_iface,
     if (status == UCS_ERR_NO_RESOURCE) {
         goto add_to_pending;
     } else if (status != UCS_OK) {
-        goto err_free_priv_data;
+        goto err;
     }
 
     self->is_on_pending = 0;
@@ -130,7 +138,7 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_ep_t, uct_iface_t *tl_iface,
      * All endpoints share the interface's event_channel. */
     status = uct_rdmacm_ep_resolve_addr(self);
     if (status != UCS_OK) {
-        goto err_free_priv_data;
+        goto err;
     }
 
     goto out;
@@ -145,11 +153,12 @@ out:
                iface, iface->event_ch, iface->cm_id,
                ucs_sockaddr_str((struct sockaddr *)sockaddr->addr,
                                 ip_port_str, UCS_SOCKADDR_STRING_LEN));
+    self->status = UCS_INPROGRESS;
     return UCS_OK;
 
-err_free_priv_data:
-    ucs_free(self->priv_data);
 err:
+    pthread_mutex_destroy(&self->ops_mutex);
+
     return status;
 }
 
@@ -171,6 +180,11 @@ static UCS_CLASS_CLEANUP_FUNC(uct_rdmacm_ep_t)
     uct_worker_progress_unregister_safe(&iface->super.worker->super,
                                         &self->slow_prog_id);
 
+    pthread_mutex_destroy(&self->ops_mutex);
+    if (!ucs_queue_is_empty(&self->ops)) {
+        ucs_warn("destroying endpoint %p with not completed operations", self);
+    }
+
     /* mark this ep as destroyed so that arriving events on it won't try to
      * use it */
     if (self->cm_id_ctx != NULL) {
@@ -179,35 +193,63 @@ static UCS_CLASS_CLEANUP_FUNC(uct_rdmacm_ep_t)
         ucs_debug("ep destroy: cm_id %p", cm_id_ctx->cm_id);
     }
     UCS_ASYNC_UNBLOCK(iface->super.worker->async);
-
-    ucs_free(self->priv_data);
 }
 
 UCS_CLASS_DEFINE(uct_rdmacm_ep_t, uct_base_ep_t)
 UCS_CLASS_DEFINE_NEW_FUNC(uct_rdmacm_ep_t, uct_ep_t, uct_iface_t*,
                           const ucs_sock_addr_t *,
-                          const void *, size_t);
+                          uct_sockaddr_priv_pack_callback_t, void *,
+                          uint32_t);
 UCS_CLASS_DEFINE_DELETE_FUNC(uct_rdmacm_ep_t, uct_ep_t);
 
 static unsigned uct_rdmacm_client_err_handle_progress(void *arg)
 {
-    uct_rdmacm_ep_t *ep = arg;
-    ucs_trace_func("err_handle ep=%p",ep);
+    uct_rdmacm_ep_t *rdmacm_ep = arg;
+    uct_rdmacm_iface_t *iface = ucs_derived_of(rdmacm_ep->super.super.iface,
+                                               uct_rdmacm_iface_t);
 
-    ep->slow_prog_id = UCS_CALLBACKQ_ID_NULL;
-    uct_set_ep_failed(&UCS_CLASS_NAME(uct_rdmacm_ep_t), &ep->super.super,
-                      ep->super.super.iface, UCS_ERR_IO_ERROR);
+    ucs_trace_func("err_handle ep=%p", rdmacm_ep);
+    UCS_ASYNC_BLOCK(iface->super.worker->async);
+
+    rdmacm_ep->slow_prog_id = UCS_CALLBACKQ_ID_NULL;
+    uct_set_ep_failed(&UCS_CLASS_NAME(uct_rdmacm_ep_t), &rdmacm_ep->super.super,
+                      rdmacm_ep->super.super.iface, rdmacm_ep->status);
+
+    UCS_ASYNC_UNBLOCK(iface->super.worker->async);
     return 0;
 }
 
-void uct_rdmacm_ep_set_failed(uct_iface_t *iface, uct_ep_h ep)
+void uct_rdmacm_ep_set_failed(uct_iface_t *iface, uct_ep_h ep, ucs_status_t status)
 {
     uct_rdmacm_iface_t *rdmacm_iface = ucs_derived_of(iface, uct_rdmacm_iface_t);
-    uct_rdmacm_ep_t *rdmacm_ep = ucs_derived_of(ep, uct_rdmacm_ep_t);
+    uct_rdmacm_ep_t *rdmacm_ep       = ucs_derived_of(ep, uct_rdmacm_ep_t);
+
+    if (rdmacm_iface->super.err_handler_flags & UCT_CB_FLAG_ASYNC) {
+        uct_set_ep_failed(&UCS_CLASS_NAME(uct_rdmacm_ep_t), &rdmacm_ep->super.super,
+                          &rdmacm_iface->super.super, status);
+    } else {
+        /* invoke the error handling flow from the main thread */
+        rdmacm_ep->status = status;
+        uct_worker_progress_register_safe(&rdmacm_iface->super.worker->super,
+                                          uct_rdmacm_client_err_handle_progress,
+                                          rdmacm_ep, UCS_CALLBACKQ_FLAG_ONESHOT,
+                                          &rdmacm_ep->slow_prog_id);
+    }
+}
 
-    /* invoke the error handling flow from the main thread */
-    uct_worker_progress_register_safe(&rdmacm_iface->super.worker->super,
-                                      uct_rdmacm_client_err_handle_progress,
-                                      rdmacm_ep, UCS_CALLBACKQ_FLAG_ONESHOT,
-                                      &rdmacm_ep->slow_prog_id);
+/**
+ * Caller must lock ep->ops_mutex
+ */
+void uct_rdmacm_ep_invoke_completions(uct_rdmacm_ep_t *ep, ucs_status_t status)
+{
+    uct_rdmacm_ep_op_t *op;
+
+    ucs_assert(pthread_mutex_trylock(&ep->ops_mutex) == EBUSY);
+
+    ucs_queue_for_each_extract(op, &ep->ops, queue_elem, 1) {
+        pthread_mutex_unlock(&ep->ops_mutex);
+        uct_invoke_completion(op->user_comp, status);
+        ucs_free(op);
+        pthread_mutex_lock(&ep->ops_mutex);
+    }
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.h
index 436670d29..232f00d0a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_ep.h
@@ -8,21 +8,40 @@
 
 #include "rdmacm_iface.h"
 
+
+typedef struct uct_rdmacm_ep_op uct_rdmacm_ep_op_t;
+
+struct uct_rdmacm_ep_op {
+    ucs_queue_elem_t    queue_elem;
+    uct_completion_t    *user_comp;
+};
+
+
 struct uct_rdmacm_ep {
     uct_base_ep_t                      super;
-    void                               *priv_data;
-    ucs_list_link_t                    list_elem;       /* for the pending_eps_list*/
-    struct sockaddr_storage            remote_addr;
+    uct_sockaddr_priv_pack_callback_t  pack_cb;
+    void                               *pack_cb_arg;
+    uint32_t                           pack_cb_flags;
     int                                is_on_pending;
+
+    pthread_mutex_t                    ops_mutex;  /* guards ops and status */
+    ucs_queue_head_t                   ops;
+    ucs_status_t                       status;     /* client EP status */
+
+    ucs_list_link_t                    list_elem;  /* for the pending_eps_list */
+    struct sockaddr_storage            remote_addr;
     uct_worker_cb_id_t                 slow_prog_id;
     uct_rdmacm_ctx_t                   *cm_id_ctx;
 };
 
 UCS_CLASS_DECLARE_NEW_FUNC(uct_rdmacm_ep_t, uct_ep_t, uct_iface_t*,
                            const ucs_sock_addr_t *,
-                           const void *, size_t);
+                           uct_sockaddr_priv_pack_callback_t, void *,
+                           uint32_t);
 UCS_CLASS_DECLARE_DELETE_FUNC(uct_rdmacm_ep_t, uct_ep_t);
 
-void uct_rdmacm_ep_set_failed(uct_iface_t *iface, uct_ep_h ep);
+void uct_rdmacm_ep_set_failed(uct_iface_t *iface, uct_ep_h ep, ucs_status_t status);
+
+void uct_rdmacm_ep_invoke_completions(uct_rdmacm_ep_t *ep, ucs_status_t status);
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_iface.c
index 5e7337b1d..accf09aab 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_iface.c
@@ -8,6 +8,12 @@
 #include <uct/base/uct_worker.h>
 #include <ucs/sys/string.h>
 
+
+enum uct_rdmacm_process_event_flags {
+    UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG = UCS_BIT(0),
+    UCT_RDMACM_PROCESS_EVENT_ACK_EVENT_FLAG     = UCS_BIT(1)
+};
+
 static ucs_config_field_t uct_rdmacm_iface_config_table[] = {
     {"BACKLOG", "1024",
      "Maximum number of pending connections for an rdma_cm_id.",
@@ -56,12 +62,87 @@ static ucs_status_t uct_rdmacm_iface_get_address(uct_iface_h tl_iface, uct_iface
     return UCS_OK;
 }
 
+static ucs_status_t uct_rdmacm_accept(struct rdma_cm_id *id)
+{
+    /* The server will not send any reply data back to the client */
+    struct rdma_conn_param conn_param = {0};
+
+    /* Accepting the connection will generate the RDMA_CM_EVENT_ESTABLISHED
+     * event on the client side. */
+    if (rdma_accept(id, &conn_param)) {
+        ucs_error("rdma_accept(to id=%p) failed: %m", id);
+        return UCS_ERR_IO_ERROR;
+    }
+
+    return UCS_OK;
+}
+
+static ucs_status_t uct_rdmacm_iface_accept(uct_iface_h tl_iface,
+                                            uct_conn_request_h conn_request)
+{
+    struct rdma_cm_event *event = conn_request;
+    ucs_status_t         status;
+
+    ucs_trace("accepting event %p with id %p", event, event->id);
+    status = uct_rdmacm_accept(event->id);
+    rdma_destroy_id(event->id);
+    rdma_ack_cm_event(event);
+
+    return status;
+}
+
+static ucs_status_t uct_rdmacm_iface_reject(uct_iface_h tl_iface,
+                                            uct_conn_request_h conn_request)
+{
+    struct rdma_cm_event       *event = conn_request;
+    ucs_status_t               status = UCS_OK;
+    uct_rdmacm_priv_data_hdr_t hdr    = {
+        .length = 0,
+        .status = UCS_ERR_REJECTED
+    };
+
+    ucs_trace("rejecting event %p with id %p", event, event->id);
+    if (rdma_reject(event->id, &hdr, sizeof(hdr))) {
+        ucs_warn("rdma_reject(id=%p) failed: %m", event->id);
+        status = UCS_ERR_IO_ERROR;
+    }
+
+    rdma_destroy_id(event->id);
+    rdma_ack_cm_event(event);
+    return status;
+}
+
+static ucs_status_t uct_rdmacm_ep_flush(uct_ep_h tl_ep, unsigned flags,
+                                        uct_completion_t *comp)
+{
+    uct_rdmacm_ep_t    *ep = ucs_derived_of(tl_ep, uct_rdmacm_ep_t);
+    ucs_status_t       status;
+    uct_rdmacm_ep_op_t *op;
+
+    pthread_mutex_lock(&ep->ops_mutex);
+    status = ep->status;
+    if ((status == UCS_INPROGRESS) && (comp != NULL)) {
+        op = ucs_malloc(sizeof(*op), "uct_rdmacm_ep_flush op");
+        if (op != NULL) {
+            op->user_comp = comp;
+            ucs_queue_push(&ep->ops, &op->queue_elem);
+        } else {
+            status = UCS_ERR_NO_MEMORY;
+        }
+    }
+    pthread_mutex_unlock(&ep->ops_mutex);
+
+    return status;
+}
+
 static uct_iface_ops_t uct_rdmacm_iface_ops = {
     .ep_create_sockaddr       = UCS_CLASS_NEW_FUNC_NAME(uct_rdmacm_ep_t),
     .ep_destroy               = UCS_CLASS_DELETE_FUNC_NAME(uct_rdmacm_ep_t),
-    .ep_flush                 = uct_base_ep_flush,
+    .ep_flush                 = uct_rdmacm_ep_flush,
     .ep_fence                 = uct_base_ep_fence,
     .ep_pending_purge         = ucs_empty_function,
+    .iface_accept             = uct_rdmacm_iface_accept,
+    .iface_reject             = uct_rdmacm_iface_reject,
     .iface_progress_enable    = (void*)ucs_empty_function_return_success,
     .iface_progress_disable   = (void*)ucs_empty_function_return_success,
     .iface_progress           = ucs_empty_function_return_zero,
@@ -110,18 +191,22 @@ void uct_rdmacm_iface_client_start_next_ep(uct_rdmacm_iface_t *iface)
             break;
         }
 
-        uct_rdmacm_ep_set_failed(&iface->super.super, &ep->super.super);
+        uct_rdmacm_ep_set_failed(&iface->super.super, &ep->super.super, status);
     }
 
     UCS_ASYNC_UNBLOCK(iface->super.worker->async);
 }
 
 static void uct_rdmacm_client_handle_failure(uct_rdmacm_iface_t *iface,
-                                             uct_rdmacm_ep_t *ep)
+                                             uct_rdmacm_ep_t *ep,
+                                             ucs_status_t status)
 {
     ucs_assert(!iface->is_server);
     if (ep != NULL) {
-        uct_rdmacm_ep_set_failed(&iface->super.super, &ep->super.super);
+        pthread_mutex_lock(&ep->ops_mutex);
+        uct_rdmacm_ep_set_failed(&iface->super.super, &ep->super.super, status);
+        uct_rdmacm_ep_invoke_completions(ep, status);
+        pthread_mutex_unlock(&ep->ops_mutex);
     }
 }
 
@@ -130,40 +215,20 @@ static void uct_rdmacm_iface_process_conn_req(uct_rdmacm_iface_t *iface,
                                               struct sockaddr *remote_addr)
 {
     uct_rdmacm_priv_data_hdr_t *hdr;
-    struct rdma_conn_param conn_param;
-    char ip_port_str[UCS_SOCKADDR_STRING_LEN];
-    ucs_status_t status;
 
     hdr = (uct_rdmacm_priv_data_hdr_t*) event->param.ud.private_data;
-
+    ucs_assert(hdr->status == UCS_OK);
 
     /* TODO check the iface's cb_flags to determine when to invoke this callback.
      * currently only UCT_CB_FLAG_ASYNC is supported so the cb is invoked from here */
-    status = iface->conn_request_cb(iface->conn_request_arg,
-                                    event->param.ud.private_data +
-                                    /* private data */
-                                    sizeof(uct_rdmacm_priv_data_hdr_t),
-                                    /* length */
-                                    hdr->length);
-    if (status != UCS_OK) {
-        rdma_reject(event->id, NULL, 0);
-        return;
-    }
-
-    /* The server will not send any reply data back to the client */
-    memset(&conn_param, 0, sizeof(conn_param));
-    /* Accepting the connection will generate the RDMA_CM_EVENT_ESTABLISHED
-     * event on the client side. */
-    if (rdma_accept(event->id, &conn_param)) {
-        ucs_error("rdma_accept(to addr=%s) failed: %m.",
-                  ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
-        rdma_reject(event->id, NULL, 0);
-        return;
-    }
-
-    /* Destroy the new rdma_cm_id which was created when receiving the
-     * RDMA_CM_EVENT_CONNECT_REQUEST event. (this is not the listening rdma_cm_id)*/
-    rdma_destroy_id(event->id);
+    iface->conn_request_cb(&iface->super.super, iface->conn_request_arg,
+                           /* connection request*/
+                           event,
+                           /* private data */
+                           UCS_PTR_BYTE_OFFSET(event->param.ud.private_data,
+                                               sizeof(uct_rdmacm_priv_data_hdr_t)),
+                           /* length */
+                           hdr->length);
 }
 
 /**
@@ -173,7 +238,7 @@ static void uct_rdmacm_iface_process_conn_req(uct_rdmacm_iface_t *iface,
 static void uct_rdmacm_iface_release_cm_id(uct_rdmacm_iface_t *iface,
                                           uct_rdmacm_ctx_t *cm_id_ctx)
 {
-    ucs_debug("destroying cm_id %p", cm_id_ctx->cm_id);
+    ucs_trace("destroying cm_id %p", cm_id_ctx->cm_id);
 
     ucs_list_del(&cm_id_ctx->list);
     if (cm_id_ctx->ep != NULL) {
@@ -184,17 +249,28 @@ static void uct_rdmacm_iface_release_cm_id(uct_rdmacm_iface_t *iface,
     iface->cm_id_quota++;
 }
 
-static int uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface, struct rdma_cm_event *event)
+static void uct_rdmacm_iface_cm_id_to_dev_name(struct rdma_cm_id *cm_id,
+                                               char *dev_name)
+{
+    ucs_snprintf_zero(dev_name, UCT_DEVICE_NAME_MAX, "%s:%d",
+                      ibv_get_device_name(cm_id->verbs->device), cm_id->port_num);
+}
+
+static unsigned
+uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface,
+                               struct rdma_cm_event *event)
 {
     struct sockaddr *remote_addr = rdma_get_peer_addr(event->id);
     uct_rdmacm_md_t *rdmacm_md   = (uct_rdmacm_md_t *)iface->super.md;
+    unsigned ret_flags           = UCT_RDMACM_PROCESS_EVENT_ACK_EVENT_FLAG;
+    uct_rdmacm_ep_t *ep          = NULL;
     char ip_port_str[UCS_SOCKADDR_STRING_LEN];
-    uct_rdmacm_priv_data_hdr_t *hdr;
+    char dev_name[UCT_DEVICE_NAME_MAX];
+    uct_rdmacm_priv_data_hdr_t hdr;
     struct rdma_conn_param conn_param;
     uct_rdmacm_ctx_t *cm_id_ctx;
-    uct_rdmacm_ep_t *ep = NULL;
-    int destroy_cm_id = 0;
-
+    ssize_t priv_data_ret;
+    ucs_status_t status;
 
     if (iface->is_server) {
         ucs_assert((iface->cm_id == event->id) ||
@@ -205,23 +281,26 @@ static int uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface, struct rdma
         ep = cm_id_ctx->ep;
     }
 
-    ucs_debug("rdmacm event (fd=%d cm_id %p) on %s (ep=%p): %s. Peer: %s.",
+    ucs_trace("rdmacm event (fd=%d cm_id %p) on %s (ep=%p): %s. Peer: %s.",
               iface->event_ch->fd, event->id, (iface->is_server ? "server" : "client"),
               ep, rdma_event_str(event->event),
               ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
 
+    status = UCS_ERR_UNREACHABLE;
     /* The following applies for rdma_cm_id of type RDMA_PS_UDP only */
     switch (event->event) {
     case RDMA_CM_EVENT_ADDR_RESOLVED:
         /* Client - resolve the route to the server */
         if (ep == NULL) {
             /* received an event on an non-existing ep - an already destroyed ep */
-            destroy_cm_id = 1;
-        } else if (rdma_resolve_route(event->id, UCS_MSEC_PER_SEC * rdmacm_md->addr_resolve_timeout)) {
+            ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+        } else if (rdma_resolve_route(event->id, UCS_MSEC_PER_SEC *
+                                                 rdmacm_md->addr_resolve_timeout)) {
             ucs_error("rdma_resolve_route(to addr=%s) failed: %m",
-                      ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
-            destroy_cm_id = 1;
-            uct_rdmacm_client_handle_failure(iface, ep);
+                      ucs_sockaddr_str(remote_addr, ip_port_str,
+                                       UCS_SOCKADDR_STRING_LEN));
+            ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+            uct_rdmacm_client_handle_failure(iface, ep, UCS_ERR_INVALID_ADDR);
         }
         break;
 
@@ -229,20 +308,44 @@ static int uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface, struct rdma
         /* Client - send a connection request to the server */
         if (ep == NULL) {
             /* received an event on an non-existing ep - an already destroyed ep */
-            destroy_cm_id = 1;
+            ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
         } else {
-            hdr = (uct_rdmacm_priv_data_hdr_t*)ep->priv_data;
-
             memset(&conn_param, 0, sizeof(conn_param));
-            conn_param.private_data     = ep->priv_data;
+            conn_param.private_data = ucs_alloca(UCT_RDMACM_MAX_CONN_PRIV +
+                                                 sizeof(uct_rdmacm_priv_data_hdr_t));
+
+            uct_rdmacm_iface_cm_id_to_dev_name(ep->cm_id_ctx->cm_id, dev_name);
+            /* TODO check the ep's cb_flags to determine when to invoke this callback.
+             * currently only UCT_CB_FLAG_ASYNC is supported so the cb is invoked from here */
+            priv_data_ret = ep->pack_cb(ep->pack_cb_arg, dev_name,
+                                        (void*)(conn_param.private_data +
+                                        sizeof(uct_rdmacm_priv_data_hdr_t)));
+            if (priv_data_ret < 0) {
+                ucs_trace("rdmacm client (iface=%p cm_id=%p fd=%d) failed to fill "
+                          "private data. status: %s",
+                          iface, event->id, iface->event_ch->fd,
+                          ucs_status_string(priv_data_ret));
+                ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+                uct_rdmacm_client_handle_failure(iface, ep, priv_data_ret);
+                break;
+            }
+
+            hdr.length = (uint8_t)priv_data_ret;
+            hdr.status = UCS_OK;
+            UCS_STATIC_ASSERT(sizeof(hdr) == sizeof(uct_rdmacm_priv_data_hdr_t));
+            /* The private_data starts with the header of the user's private data
+             * and then the private data itself */
+            memcpy((void*)conn_param.private_data, &hdr, sizeof(uct_rdmacm_priv_data_hdr_t));
             conn_param.private_data_len = sizeof(uct_rdmacm_priv_data_hdr_t) +
-                                          hdr->length;
+                                          hdr.length;
 
             if (rdma_connect(event->id, &conn_param)) {
                 ucs_error("rdma_connect(to addr=%s) failed: %m",
-                          ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
-                destroy_cm_id = 1;
-                uct_rdmacm_client_handle_failure(iface, ep);
+                          ucs_sockaddr_str(remote_addr, ip_port_str,
+                                           UCS_SOCKADDR_STRING_LEN));
+                ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+                uct_rdmacm_client_handle_failure(iface, ep,
+                                                 UCS_ERR_SOME_CONNECTS_FAILED);
             }
         }
         break;
@@ -251,39 +354,58 @@ static int uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface, struct rdma
         /* Server - handle a connection request from the client */
         ucs_assert(iface->is_server);
         uct_rdmacm_iface_process_conn_req(iface, event, remote_addr);
+        ret_flags &= ~UCT_RDMACM_PROCESS_EVENT_ACK_EVENT_FLAG;
         break;
 
     case RDMA_CM_EVENT_REJECTED:
         /* Client - server rejected the connection request */
-        ucs_warn("rdmacm connection request to %s rejected",
-                  ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
+        ucs_warn("rdmacm connection request to %s rejected, id %p",
+                  ucs_sockaddr_str(remote_addr, ip_port_str,
+                                   UCS_SOCKADDR_STRING_LEN), event->id);
 
-        destroy_cm_id = 1;
-        uct_rdmacm_client_handle_failure(iface, ep);
+        ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+        uct_rdmacm_client_handle_failure(iface, ep, UCS_ERR_REJECTED);
         break;
 
     case RDMA_CM_EVENT_ESTABLISHED:
         /* Client - connection is ready */
-        destroy_cm_id = 1;
+        ucs_assert(!iface->is_server);
+        ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+        if (ep != NULL) {
+            pthread_mutex_lock(&ep->ops_mutex);
+            ep->status = UCS_OK;
+            uct_rdmacm_ep_invoke_completions(ep, UCS_OK);
+            pthread_mutex_unlock(&ep->ops_mutex);
+        }
         break;
 
     /* client error events */
+    case RDMA_CM_EVENT_UNREACHABLE:
+        hdr = *(uct_rdmacm_priv_data_hdr_t *)event->param.conn.private_data;
+        if ((event->param.conn.private_data_len > 0) &&
+            (hdr.status == UCS_ERR_REJECTED)) {
+            ucs_assert(hdr.length == 0);
+            ucs_assert(event->param.conn.private_data_len >= sizeof(hdr));
+            ucs_assert(!iface->is_server);
+            status = UCS_ERR_REJECTED;
+        }
+        /* Fall through */
     case RDMA_CM_EVENT_ADDR_ERROR:
     case RDMA_CM_EVENT_ROUTE_ERROR:
     case RDMA_CM_EVENT_CONNECT_RESPONSE:
-    case RDMA_CM_EVENT_UNREACHABLE:
-
     /* client and server error events */
     case RDMA_CM_EVENT_CONNECT_ERROR:
     case RDMA_CM_EVENT_DISCONNECTED:
         /* Server/Client - connection was disconnected */
-        ucs_error("received event %s. status = %d. Peer: %s.",
-                  rdma_event_str(event->event), event->status,
-                  ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
+        if (status != UCS_ERR_REJECTED) {
+            ucs_error("received event %s. status = %d. Peer: %s.",
+                      rdma_event_str(event->event), event->status,
+                      ucs_sockaddr_str(remote_addr, ip_port_str, UCS_SOCKADDR_STRING_LEN));
+        }
 
         if (!iface->is_server) {
-            destroy_cm_id = 1;
-            uct_rdmacm_client_handle_failure(iface, ep);
+            ret_flags |= UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG;
+            uct_rdmacm_client_handle_failure(iface, ep, status);
         }
         break;
 
@@ -292,15 +414,16 @@ static int uct_rdmacm_iface_process_event(uct_rdmacm_iface_t *iface, struct rdma
         break;
     }
 
-    return destroy_cm_id;
+    return ret_flags;
 }
 
 static void uct_rdmacm_iface_event_handler(int fd, void *arg)
 {
-    uct_rdmacm_iface_t *iface = arg;
-    struct rdma_cm_event *event;
-    int ret, destroy_cm_id;
-    uct_rdmacm_ctx_t *cm_id_ctx = NULL;
+    uct_rdmacm_iface_t             *iface     = arg;
+    uct_rdmacm_ctx_t               *cm_id_ctx = NULL;
+    struct rdma_cm_event           *event;
+    unsigned                       proc_event_flags;
+    int                            ret;
 
     for (;;) {
         /* Fetch an event */
@@ -314,17 +437,20 @@ static void uct_rdmacm_iface_event_handler(int fd, void *arg)
             return;
         }
 
-        destroy_cm_id = uct_rdmacm_iface_process_event(iface, event);
+        proc_event_flags = uct_rdmacm_iface_process_event(iface, event);
         if (!iface->is_server) {
             cm_id_ctx = (uct_rdmacm_ctx_t *)event->id->context;
         }
 
-        ret = rdma_ack_cm_event(event);
-        if (ret) {
-            ucs_warn("rdma_ack_cm_event() failed: %m");
+        if (proc_event_flags & UCT_RDMACM_PROCESS_EVENT_ACK_EVENT_FLAG) {
+            ret = rdma_ack_cm_event(event);
+            if (ret) {
+                ucs_warn("rdma_ack_cm_event() failed: %m");
+            }
         }
 
-        if (destroy_cm_id && (cm_id_ctx != NULL)) {
+        if ((proc_event_flags & UCT_RDMACM_PROCESS_EVENT_DESTROY_CM_ID_FLAG) &&
+            (cm_id_ctx != NULL)) {
             uct_rdmacm_iface_release_cm_id(iface, cm_id_ctx);
             uct_rdmacm_iface_client_start_next_ep(iface);
         }
@@ -407,8 +533,8 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_iface_t, uct_md_h md, uct_worker_h worker,
                                    ip_port_str, UCS_SOCKADDR_STRING_LEN),
                   ntohs(rdma_get_src_port(self->cm_id)));
 
-        if (params->mode.sockaddr.cb_flags != UCT_CB_FLAG_ASYNC) {
-            ucs_fatal("UCT_CB_FLAG_SYNC is not supported");
+        if (!(params->mode.sockaddr.cb_flags & UCT_CB_FLAG_ASYNC)) {
+            ucs_fatal("Synchronous callback is not supported");
         }
 
         self->cb_flags         = params->mode.sockaddr.cb_flags;
@@ -416,7 +542,8 @@ static UCS_CLASS_INIT_FUNC(uct_rdmacm_iface_t, uct_md_h md, uct_worker_h worker,
         self->conn_request_arg = params->mode.sockaddr.conn_request_arg;
         self->is_server        = 1;
     } else {
-        self->is_server               = 0;
+        self->cm_id            = NULL;
+        self->is_server        = 0;
     }
 
     self->cm_id_quota = config->cm_id_quota;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_md.c
index b28bea1b4..98d6a118f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/rdmacm/rdmacm_md.c
@@ -106,6 +106,25 @@ static int uct_rdmacm_is_addr_route_resolved(struct rdma_cm_id *cm_id,
     return 1;
 }
 
+static int uct_rdmacm_is_sockaddr_inaddr_any(struct sockaddr *addr)
+{
+    struct sockaddr_in6 *addr_in6;
+    struct sockaddr_in *addr_in;
+
+    switch (addr->sa_family) {
+    case AF_INET:
+        addr_in = (struct sockaddr_in *)addr;
+        return addr_in->sin_addr.s_addr == INADDR_ANY;
+    case AF_INET6:
+        addr_in6 = (struct sockaddr_in6 *)addr;
+        return !memcmp(&addr_in6->sin6_addr, &in6addr_any, sizeof(addr_in6->sin6_addr));
+    default:
+        ucs_debug("Invalid address family: %d", addr->sa_family);
+    }
+
+    return 0;
+}
+
 int uct_rdmacm_is_sockaddr_accessible(uct_md_h md, const ucs_sock_addr_t *sockaddr,
                                       uct_sockaddr_accessibility_t mode)
 {
@@ -139,6 +158,11 @@ int uct_rdmacm_is_sockaddr_accessible(uct_md_h md, const ucs_sock_addr_t *sockad
                                        ip_port_str, UCS_SOCKADDR_STRING_LEN));
             goto out_destroy_id;
         }
+
+        if (uct_rdmacm_is_sockaddr_inaddr_any((struct sockaddr *)sockaddr->addr)) {
+            is_accessible = 1;
+            goto out_print;
+        }
     }
 
     /* Client and server sides check if can access the given sockaddr.
@@ -150,6 +174,7 @@ int uct_rdmacm_is_sockaddr_accessible(uct_md_h md, const ucs_sock_addr_t *sockad
         goto out_destroy_id;
     }
 
+out_print:
     ucs_debug("address %s (port %d) is accessible from rdmacm_md %p with mode: %d",
               ucs_sockaddr_str((struct sockaddr *)sockaddr->addr, ip_port_str,
                                UCS_SOCKADDR_STRING_LEN),
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.c
index 1d11f3928..185470434 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.c
@@ -19,6 +19,7 @@
 
 #include <uct/ib/mlx5/ib_mlx5_log.h>
 #include <uct/ib/mlx5/ib_mlx5.inl>
+#include <uct/ib/mlx5/ib_mlx5_dv.h>
 
 #include <uct/ib/ud/base/ud_iface.h>
 #include <uct/ib/ud/base/ud_ep.h>
@@ -33,6 +34,10 @@ static ucs_config_field_t uct_ud_mlx5_iface_config_table[] = {
 
   {"", "", NULL,
    ucs_offsetof(uct_ud_mlx5_iface_config_t, mlx5_common),
+   UCS_CONFIG_TYPE_TABLE(uct_ib_mlx5_iface_config_table)},
+
+  {"", "", NULL,
+   ucs_offsetof(uct_ud_mlx5_iface_config_t, ud_mlx5_common),
    UCS_CONFIG_TYPE_TABLE(uct_ud_mlx5_iface_common_config_table)},
 
   {NULL}
@@ -46,7 +51,8 @@ uct_ud_mlx5_ep_ctrl_av_size(uct_ud_mlx5_ep_t *ep)
 
 static UCS_F_ALWAYS_INLINE void
 uct_ud_mlx5_post_send(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
-                      uint8_t se, struct mlx5_wqe_ctrl_seg *ctrl, size_t wqe_size)
+                      uint8_t se, struct mlx5_wqe_ctrl_seg *ctrl, size_t wqe_size,
+                      int max_log_sge)
 {
     struct mlx5_wqe_datagram_seg *dgram = (void*)(ctrl + 1);
 
@@ -58,7 +64,7 @@ uct_ud_mlx5_post_send(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
 
     uct_ib_mlx5_log_tx(&iface->super.super, IBV_QPT_UD, ctrl,
                        iface->tx.wq.qstart, iface->tx.wq.qend,
-                       uct_ud_dump_packet);
+                       max_log_sge, NULL, uct_ud_dump_packet);
     iface->super.tx.available -= uct_ib_mlx5_post_send(&iface->tx.wq, ctrl,
                                                        wqe_size);
     ucs_assert((int16_t)iface->tx.wq.bb_max >= iface->super.tx.available);
@@ -66,7 +72,7 @@ uct_ud_mlx5_post_send(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
 
 static UCS_F_ALWAYS_INLINE void
 uct_ud_mlx5_ep_tx_skb(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
-                      uct_ud_send_skb_t *skb, uint8_t se)
+                      uct_ud_send_skb_t *skb, uint8_t se, int max_log_sge)
 {
     size_t ctrl_av_size = uct_ud_mlx5_ep_ctrl_av_size(ep);
     struct mlx5_wqe_ctrl_seg *ctrl;
@@ -76,7 +82,7 @@ uct_ud_mlx5_ep_tx_skb(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
     dptr = uct_ib_mlx5_txwq_wrap_exact(&iface->tx.wq, (void*)ctrl + ctrl_av_size);
     uct_ib_mlx5_set_data_seg(dptr, skb->neth, skb->len, skb->lkey);
     UCT_UD_EP_HOOK_CALL_TX(&ep->super, skb->neth);
-    uct_ud_mlx5_post_send(iface, ep, se, ctrl, ctrl_av_size + sizeof(*dptr));
+    uct_ud_mlx5_post_send(iface, ep, se, ctrl, ctrl_av_size + sizeof(*dptr), max_log_sge);
 }
 
 static inline void
@@ -93,7 +99,7 @@ uct_ud_mlx5_ep_tx_inl(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
     uct_ib_mlx5_inline_copy(inl + 1, buf, length, &iface->tx.wq);
     UCT_UD_EP_HOOK_CALL_TX(&ep->super, (uct_ud_neth_t *)buf);
     uct_ud_mlx5_post_send(iface, ep, se, ctrl,
-                          ctrl_av_size + sizeof(*inl) + length);
+                          ctrl_av_size + sizeof(*inl) + length, INT_MAX);
 }
 
 
@@ -107,7 +113,7 @@ static void uct_ud_mlx5_ep_tx_ctl_skb(uct_ud_ep_t *ud_ep, uct_ud_send_skb_t *skb
 
     se = solicited ? MLX5_WQE_CTRL_SOLICITED : 0;
     if (skb->len >= iface->super.config.max_inline) {
-        uct_ud_mlx5_ep_tx_skb(iface, ep, skb, se);
+        uct_ud_mlx5_ep_tx_skb(iface, ep, skb, se, INT_MAX);
     } else {
         uct_ud_mlx5_ep_tx_inl(iface, ep, skb->neth, skb->len, se);
     }
@@ -185,7 +191,7 @@ uct_ud_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
                     0, iface->super.config.max_inline, "am_short");
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     skb = uct_ud_ep_get_tx_skb(&iface->super, &ep->super);
     if (!skb) {
         uct_ud_leave(&iface->super);
@@ -208,10 +214,9 @@ uct_ud_mlx5_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
     uct_ib_mlx5_inline_copy(am + 1, buffer, length, &iface->tx.wq);
 
     wqe_size += ctrl_av_size + sizeof(*inl);
-    UCT_CHECK_LENGTH(wqe_size, 0, UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB,
-                     "am_short");
+    UCT_CHECK_LENGTH(wqe_size, 0, UCT_IB_MLX5_MAX_SEND_WQE_SIZE, "am_short");
     UCT_UD_EP_HOOK_CALL_TX(&ep->super, neth);
-    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size);
+    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size, INT_MAX);
 
     skb->len = sizeof(*neth) + sizeof(*am);
     memcpy(skb->neth, neth, skb->len);
@@ -234,7 +239,7 @@ static ssize_t uct_ud_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
     size_t length;
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     status = uct_ud_am_common(&iface->super, &ep->super, id, &skb);
     if (status != UCS_OK) {
         uct_ud_leave(&iface->super);
@@ -244,7 +249,7 @@ static ssize_t uct_ud_mlx5_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
     length = uct_ud_skb_bcopy(skb, pack_cb, arg);
     UCT_UD_CHECK_BCOPY_LENGTH(&iface->super, length);
 
-    uct_ud_mlx5_ep_tx_skb(iface, ep, skb, 0);
+    uct_ud_mlx5_ep_tx_skb(iface, ep, skb, 0, INT_MAX);
     uct_ud_iface_complete_tx_skb(&iface->super, &ep->super, skb);
     UCT_TL_EP_STAT_OP(&ep->super.super, AM, BCOPY, length);
     uct_ud_leave(&iface->super);
@@ -275,7 +280,6 @@ uct_ud_mlx5_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *header,
                               uct_iov_total_length(iov, iovcnt));
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
 
     skb = uct_ud_ep_get_tx_skb(&iface->super, &ep->super);
     if (!skb) {
@@ -299,10 +303,11 @@ uct_ud_mlx5_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *header,
                                  UCT_IB_MLX5_WQE_SEG_SIZE);
     wqe_size += uct_ib_mlx5_set_data_seg_iov(&iface->tx.wq, (void *)ctrl + wqe_size,
                                              iov, iovcnt);
-    ucs_assert(wqe_size <= (UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB));
+    ucs_assert(wqe_size <= UCT_IB_MLX5_MAX_SEND_WQE_SIZE);
 
     UCT_UD_EP_HOOK_CALL_TX(&ep->super, neth);
-    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size);
+    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size,
+                          UCT_IB_MAX_ZCOPY_LOG_SGE(&iface->super.super));
 
     skb->len = sizeof(*neth) + header_length;
     memcpy(skb->neth, neth, sizeof(*neth));
@@ -332,7 +337,7 @@ uct_ud_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer, unsigned length,
     size_t wqe_size;
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     skb = uct_ud_ep_get_tx_skb(&iface->super, &ep->super);
     if (!skb) {
         uct_ud_leave(&iface->super);
@@ -358,10 +363,9 @@ uct_ud_mlx5_ep_put_short(uct_ep_h tl_ep, const void *buffer, unsigned length,
     uct_ib_mlx5_inline_copy(put_hdr + 1, buffer, length, &iface->tx.wq);
 
     wqe_size += ctrl_av_size + sizeof(*inl);
-    UCT_CHECK_LENGTH(wqe_size, 0, UCT_IB_MLX5_MAX_BB * MLX5_SEND_WQE_BB,
-                     "put_short");
+    UCT_CHECK_LENGTH(wqe_size, 0, UCT_IB_MLX5_MAX_SEND_WQE_SIZE, "put_short");
     UCT_UD_EP_HOOK_CALL_TX(&ep->super, neth);
-    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size);
+    uct_ud_mlx5_post_send(iface, ep, 0, ctrl, wqe_size, INT_MAX);
 
     skb->len = sizeof(*neth) + sizeof(*put_hdr);
     memcpy(skb->neth, neth, skb->len);
@@ -387,7 +391,7 @@ uct_ud_mlx5_iface_poll_rx(uct_ud_mlx5_iface_t *iface, int is_async)
     ucs_prefetch(packet + UCT_IB_GRH_LEN);
     desc   = (uct_ib_iface_recv_desc_t *)(packet - iface->super.super.config.rx_hdr_offset);
 
-    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->rx.cq);
+    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->cq[UCT_IB_DIR_RX]);
     if (cqe == NULL) {
         count = 0;
         goto out;
@@ -428,18 +432,22 @@ out:
     return count;
 }
 
-static UCS_F_ALWAYS_INLINE void
+static UCS_F_ALWAYS_INLINE unsigned
 uct_ud_mlx5_iface_poll_tx(uct_ud_mlx5_iface_t *iface)
 {
     struct mlx5_cqe64 *cqe;
 
-    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->tx.cq);
+    cqe = uct_ib_mlx5_poll_cq(&iface->super.super, &iface->cq[UCT_IB_DIR_TX]);
     if (cqe == NULL) {
-        return;
+        return 0;
     }
+
     ucs_memory_cpu_load_fence();
+
     uct_ib_mlx5_log_cqe(cqe);
-    iface->super.tx.available = uct_ib_mlx5_txwq_update_bb(&iface->tx.wq, ntohs(cqe->wqe_counter));
+    iface->super.tx.available = uct_ib_mlx5_txwq_update_bb(&iface->tx.wq,
+                                                           ntohs(cqe->wqe_counter));
+    return 1;
 }
 
 static unsigned uct_ud_mlx5_iface_progress(uct_iface_h tl_iface)
@@ -450,6 +458,7 @@ static unsigned uct_ud_mlx5_iface_progress(uct_iface_h tl_iface)
 
     uct_ud_enter(&iface->super);
     uct_ud_iface_dispatch_zcopy_comps(&iface->super);
+
     status = uct_ud_iface_dispatch_pending_rx(&iface->super);
     if (ucs_likely(status == UCS_OK)) {
         do {
@@ -457,22 +466,29 @@ static unsigned uct_ud_mlx5_iface_progress(uct_iface_h tl_iface)
             count += n;
         } while ((n > 0) && (count < iface->super.super.config.rx_max_poll));
     }
-    uct_ud_mlx5_iface_poll_tx(iface);
+
+    count += uct_ud_mlx5_iface_poll_tx(iface);
     uct_ud_iface_progress_pending(&iface->super, 0);
     uct_ud_leave(&iface->super);
     return count;
 }
 
-static void uct_ud_mlx5_iface_async_progress(uct_ud_iface_t *ud_iface)
+static unsigned uct_ud_mlx5_iface_async_progress(uct_ud_iface_t *ud_iface)
 {
     uct_ud_mlx5_iface_t *iface = ucs_derived_of(ud_iface, uct_ud_mlx5_iface_t);
-    unsigned count;
+    unsigned n, count;
 
+    count = 0;
     do {
-        count = uct_ud_mlx5_iface_poll_rx(iface, 1);
-    } while (count > 0);
-    uct_ud_mlx5_iface_poll_tx(iface);
+        n = uct_ud_mlx5_iface_poll_rx(iface, 1);
+        count += n;
+    } while (n > 0);
+
+    count += uct_ud_mlx5_iface_poll_tx(iface);
+
     uct_ud_iface_progress_pending(&iface->super, 1);
+
+    return count;
 }
 
 static ucs_status_t
@@ -502,17 +518,21 @@ uct_ud_mlx5_ep_create_ah(uct_ud_mlx5_iface_t *iface, uct_ud_mlx5_ep_t *ep,
                          const uct_ud_iface_addr_t *if_addr)
 {
     ucs_status_t status;
+    uint32_t remote_qpn;
     int is_global;
 
-    status = uct_ud_mlx5_iface_get_av(&iface->super.super, &iface->mlx5_common,
+    status = uct_ud_mlx5_iface_get_av(&iface->super.super, &iface->ud_mlx5_common,
                                       ib_addr, ep->super.path_bits, &ep->av,
                                       &ep->grh_av, &is_global);
     if (status != UCS_OK) {
         return status;
     }
 
+    remote_qpn      = uct_ib_unpack_uint24(if_addr->qp_num);
     ep->is_global   = is_global;
-    ep->av.dqp_dct |= htonl(uct_ib_unpack_uint24(if_addr->qp_num));
+    ep->av.dqp_dct |= htonl(remote_qpn);
+    uct_ib_mlx5_iface_set_av_sport(&iface->super.super, &ep->av,
+                                   remote_qpn ^ iface->super.qp->qp_num);
     return UCS_OK;
 }
 
@@ -591,19 +611,18 @@ uct_ud_mlx5_ep_connect_to_ep(uct_ep_h tl_ep,
     return UCS_OK;
 }
 
-static ucs_status_t uct_ud_mlx5_iface_arm_tx_cq(uct_ib_iface_t *ib_iface)
-{
-    uct_ud_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_ud_mlx5_iface_t);
-    uct_ib_mlx5_update_cq_ci(iface->super.super.send_cq, iface->tx.cq.cq_ci);
-    return uct_ib_iface_arm_tx_cq(ib_iface);
-}
-
-static ucs_status_t uct_ud_mlx5_iface_arm_rx_cq(uct_ib_iface_t *ib_iface,
-                                                int solicited)
+static ucs_status_t uct_ud_mlx5_iface_arm_cq(uct_ib_iface_t *ib_iface,
+                                             uct_ib_dir_t dir,
+                                             int solicited)
 {
     uct_ud_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_ud_mlx5_iface_t);
-    uct_ib_mlx5_update_cq_ci(iface->super.super.recv_cq, iface->rx.cq.cq_ci);
-    return uct_ib_iface_arm_rx_cq(ib_iface, solicited);
+#if HAVE_DECL_MLX5DV_INIT_OBJ
+    return uct_ib_mlx5dv_arm_cq(&iface->cq[dir], solicited);
+#else
+    uct_ib_mlx5_update_cq_ci(iface->super.super.cq[dir],
+                             iface->cq[dir].cq_ci);
+    return uct_ib_iface_arm_cq(ib_iface, dir, solicited);
+#endif
 }
 
 static ucs_status_t uct_ud_mlx5_ep_set_failed(uct_ib_iface_t *iface,
@@ -613,8 +632,27 @@ static ucs_status_t uct_ud_mlx5_ep_set_failed(uct_ib_iface_t *iface,
                              &iface->super.super, status);
 }
 
+static void uct_ud_mlx5_iface_event_cq(uct_ib_iface_t *ib_iface,
+                                       uct_ib_dir_t dir)
+{
+    uct_ud_mlx5_iface_t *iface = ucs_derived_of(ib_iface, uct_ud_mlx5_iface_t);
+
+    iface->cq[dir].cq_sn++;
+}
+
 static void UCS_CLASS_DELETE_FUNC_NAME(uct_ud_mlx5_iface_t)(uct_iface_t*);
 
+static void uct_ud_mlx5_iface_handle_failure(uct_ib_iface_t *iface, void *arg,
+                                             ucs_status_t status)
+{
+    if (status == UCS_ERR_ENDPOINT_TIMEOUT) {
+        uct_ud_iface_handle_failure(iface, arg, status);
+    } else {
+        /* Local side failure - treat as fatal */
+        uct_ib_mlx5_completion_with_err(iface, arg, UCS_LOG_LEVEL_FATAL);
+    }
+}
+
 static uct_ud_iface_ops_t uct_ud_mlx5_iface_ops = {
     {
     {
@@ -644,13 +682,15 @@ static uct_ud_iface_ops_t uct_ud_mlx5_iface_ops = {
     .iface_get_address        = uct_ud_iface_get_address,
     .iface_is_reachable       = uct_ib_iface_is_reachable
     },
-    .arm_tx_cq                = uct_ud_mlx5_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_ud_mlx5_iface_arm_rx_cq,
-    .handle_failure           = uct_ud_iface_handle_failure,
-    .set_ep_failed            = uct_ud_mlx5_ep_set_failed
+    .arm_cq                   = uct_ud_mlx5_iface_arm_cq,
+    .event_cq                 = uct_ud_mlx5_iface_event_cq,
+    .handle_failure           = uct_ud_mlx5_iface_handle_failure,
+    .set_ep_failed            = uct_ud_mlx5_ep_set_failed,
+    .create_qp                = uct_ib_iface_create_qp
     },
     .async_progress           = uct_ud_mlx5_iface_async_progress,
     .tx_skb                   = uct_ud_mlx5_ep_tx_ctl_skb,
+    .ep_free                  = UCS_CLASS_DELETE_FUNC_NAME(uct_ud_mlx5_ep_t)
 };
 
 static UCS_CLASS_INIT_FUNC(uct_ud_mlx5_iface_t,
@@ -660,28 +700,33 @@ static UCS_CLASS_INIT_FUNC(uct_ud_mlx5_iface_t,
 {
     uct_ud_mlx5_iface_config_t *config = ucs_derived_of(tl_config,
                                                         uct_ud_mlx5_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     ucs_status_t status;
     int i;
 
     ucs_trace_func("");
 
+    init_attr.res_domain_key = UCT_IB_IFACE_NULL_RES_DOMAIN_KEY;
+    init_attr.flags          = UCT_IB_CQ_IGNORE_OVERRUN;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_ud_iface_t, &uct_ud_mlx5_iface_ops,
-                              md, worker, params, 0, &config->super);
+                              md, worker, params, &config->super, &init_attr);
 
     uct_ib_iface_set_max_iov(&self->super.super, UCT_IB_MLX5_AM_ZCOPY_MAX_IOV);
     self->super.config.max_inline = UCT_IB_MLX5_AM_MAX_SHORT(UCT_IB_MLX5_AV_FULL_SIZE);
 
-    status = uct_ib_mlx5_get_cq(self->super.super.send_cq, &self->tx.cq);
+    status = uct_ib_mlx5_get_cq(self->super.super.cq[UCT_IB_DIR_TX], &self->cq[UCT_IB_DIR_TX]);
     if (status != UCS_OK) {
         return status;
     }
 
-    status = uct_ib_mlx5_get_cq(self->super.super.recv_cq, &self->rx.cq);
+    status = uct_ib_mlx5_get_cq(self->super.super.cq[UCT_IB_DIR_RX], &self->cq[UCT_IB_DIR_RX]);
     if (status != UCS_OK) {
         return status;
     }
 
-    status = uct_ib_mlx5_txwq_init(self->super.super.super.worker, &self->tx.wq,
+    status = uct_ib_mlx5_txwq_init(self->super.super.super.worker,
+                                   config->mlx5_common.mmio_mode, &self->tx.wq,
                                    self->super.qp);
     if (status != UCS_OK) {
         return status;
@@ -694,7 +739,8 @@ static UCS_CLASS_INIT_FUNC(uct_ud_mlx5_iface_t,
     }
 
     status = uct_ud_mlx5_iface_common_init(&self->super.super,
-                                           &self->mlx5_common, &config->mlx5_common);
+                                           &self->ud_mlx5_common,
+                                           &config->ud_mlx5_common);
     if (status != UCS_OK) {
         return status;
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.h
index e7c193783..3cd43f1e5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5.h
@@ -22,7 +22,8 @@ typedef struct {
 
 typedef struct {
     uct_ud_iface_config_t               super;
-    uct_ud_mlx5_iface_common_config_t   mlx5_common;
+    uct_ib_mlx5_iface_config_t          mlx5_common;
+    uct_ud_mlx5_iface_common_config_t   ud_mlx5_common;
 } uct_ud_mlx5_iface_config_t;
 
 
@@ -30,13 +31,12 @@ typedef struct {
     uct_ud_iface_t                      super;
     struct {
         uct_ib_mlx5_txwq_t              wq;
-        uct_ib_mlx5_cq_t                cq;
     } tx;
     struct {
         uct_ib_mlx5_rxwq_t              wq;
-        uct_ib_mlx5_cq_t                cq;
     } rx;
-    uct_ud_mlx5_iface_common_t          mlx5_common;
+    uct_ib_mlx5_cq_t                    cq[UCT_IB_DIR_NUM];
+    uct_ud_mlx5_iface_common_t          ud_mlx5_common;
 } uct_ud_mlx5_iface_t;
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5_common.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5_common.c
index 466a1e513..23b56dc6f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5_common.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/accel/ud_mlx5_common.c
@@ -49,14 +49,15 @@ ucs_status_t uct_ud_mlx5_iface_get_av(uct_ib_iface_t *iface,
     *is_global = ah_attr.is_global;
 
     uct_ib_mlx5_get_av(ah, &mlx5_av);
-    ibv_destroy_ah(ah);
 
     base_av->stat_rate_sl = mlx5_av_base(&mlx5_av)->stat_rate_sl;
     base_av->fl_mlid      = mlx5_av_base(&mlx5_av)->fl_mlid;
     base_av->rlid         = mlx5_av_base(&mlx5_av)->rlid;
+    base_av->dqp_dct      = 0;
 
-    base_av->dqp_dct = (ud_common_iface->config.compact_av) ? 0 :
-                        UCT_IB_MLX5_EXTENDED_UD_AV;
+    if (!ud_common_iface->config.compact_av || ah_attr.is_global) {
+        base_av->dqp_dct |= UCT_IB_MLX5_EXTENDED_UD_AV;
+    }
 
     ucs_assertv_always((UCT_IB_MLX5_AV_FULL_SIZE > UCT_IB_MLX5_AV_BASE_SIZE) ||
                        (base_av->dqp_dct & UCT_IB_MLX5_EXTENDED_UD_AV),
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.c
index cc5f13e60..40f92dc5d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.c
@@ -12,6 +12,15 @@
 #include <uct/ib/base/ib_verbs.h>
 #include <ucs/debug/memtrack.h>
 #include <ucs/debug/log.h>
+#include <ucs/time/time.h>
+
+
+/* Must be less then peer_timeout to avoid false positive errors taking into
+ * account timer resolution and not too small to avoid performance degradation
+ */
+#define UCT_UD_SLOW_TIMER_MAX_TICK(_iface)  ((_iface)->config.peer_timeout / 3)
+
+static void uct_ud_ep_do_pending_ctl(uct_ud_ep_t *ep, uct_ud_iface_t *iface);
 
 static void uct_ud_peer_name(uct_ud_peer_name_t *peer)
 {
@@ -99,29 +108,57 @@ static void uct_ud_ep_reset(uct_ud_ep_t *ep)
     ep->resend.pos       = ucs_queue_iter_begin(&ep->tx.window);
     ep->resend.psn       = ep->tx.psn;
     ep->resend.max_psn   = ep->tx.acked_psn;
+    ep->rx_creq_count    = 0;
 
     ep->rx.acked_psn = UCT_UD_INITIAL_PSN - 1;
     ucs_frag_list_init(ep->tx.psn-1, &ep->rx.ooo_pkts, 0 /*TODO: ooo support */
                        UCS_STATS_ARG(ep->super.stats));
 }
 
+static ucs_status_t uct_ud_ep_free_by_timeout(uct_ud_ep_t *ep,
+                                              uct_ud_iface_t *iface)
+{
+    uct_ud_iface_ops_t *ops;
+    ucs_time_t         diff;
+
+    diff = ucs_twheel_get_time(&iface->async.slow_timer) - ep->close_time;
+    if (diff > iface->config.peer_timeout) {
+        ucs_debug("ud_ep %p is destroyed after %fs with timeout %fs\n",
+                  ep, ucs_time_to_sec(diff),
+                  ucs_time_to_sec(iface->config.peer_timeout));
+        ops = ucs_derived_of(iface->super.ops, uct_ud_iface_ops_t);
+        ops->ep_free(&ep->super.super);
+        return UCS_OK;
+    }
+    return UCS_INPROGRESS;
+}
+
 static void uct_ud_ep_slow_timer(ucs_wtimer_t *self)
 {
-    uct_ud_ep_t *ep = ucs_container_of(self, uct_ud_ep_t, slow_timer);
-    uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface,
-                                           uct_ud_iface_t);
-    ucs_time_t now;
-    ucs_time_t diff;
+    uct_ud_ep_t        *ep    = ucs_container_of(self, uct_ud_ep_t, slow_timer);
+    uct_ud_iface_t     *iface = ucs_derived_of(ep->super.super.iface,
+                                               uct_ud_iface_t);
+    ucs_time_t         now;
+    ucs_time_t         diff;
+    ucs_status_t       status;
 
     UCT_UD_EP_HOOK_CALL_TIMER(ep);
-    now = ucs_twheel_get_time(&iface->async.slow_timer);
-    diff = now - ep->tx.send_time;
 
     if (ucs_queue_is_empty(&ep->tx.window)) {
+        /* Do not free the EP until all scheduled communications are done. */
+        if (ep->flags & UCT_UD_EP_FLAG_DISCONNECTED) {
+            status = uct_ud_ep_free_by_timeout(ep, iface);
+            if (status == UCS_INPROGRESS) {
+                goto again;
+            }
+        }
         return;
     }
 
+    now = ucs_twheel_get_time(&iface->async.slow_timer);
+    diff = now - ep->tx.send_time;
     if (diff > iface->config.peer_timeout) {
+        ucs_debug("ep %p: timeout of %.2f sec", ep, ucs_time_to_sec(diff));
         iface->super.ops->handle_failure(&iface->super, ep,
                                          UCS_ERR_ENDPOINT_TIMEOUT);
         return;
@@ -140,9 +177,11 @@ static void uct_ud_ep_slow_timer(ucs_wtimer_t *self)
         uct_ud_ep_ctl_op_add(iface, ep, UCT_UD_EP_OP_ACK_REQ);
     }
 
+again:
     /* Cool down the timer on rescheduling/resending */
     ep->tx.slow_tick *= iface->config.slow_timer_backoff;
-    ep->tx.slow_tick = ucs_min(ep->tx.slow_tick, iface->config.peer_timeout/3);
+    ep->tx.slow_tick = ucs_min(ep->tx.slow_tick,
+                               UCT_UD_SLOW_TIMER_MAX_TICK(iface));
     ucs_wtimer_add(&iface->async.slow_timer, &ep->slow_timer, ep->tx.slow_tick);
 }
 
@@ -176,7 +215,6 @@ uct_ud_ep_pending_cancel_cb(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
 {
     uct_ud_ep_t *ep = ucs_container_of(ucs_arbiter_elem_group(elem),
                                        uct_ud_ep_t, tx.pending.group);
-    uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface, uct_ud_iface_t);
     uct_pending_req_t *req;
 
     /* we may have pending op on ep */
@@ -188,7 +226,6 @@ uct_ud_ep_pending_cancel_cb(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
     /* uct user should not have anything pending */
     req = ucs_container_of(elem, uct_pending_req_t, priv);
     ucs_warn("ep=%p removing user pending req=%p", ep, req);
-    iface->tx.pending_q_len--;
 
     /* return ignored by arbiter */
     return UCS_ARBITER_CB_RESULT_REMOVE_ELEM;
@@ -283,6 +320,7 @@ ucs_status_t uct_ud_ep_create_connected_common(uct_ud_iface_t *iface,
     ep = uct_ud_iface_cep_lookup(iface, ib_addr, if_addr, UCT_UD_EP_CONN_ID_MAX);
     if (ep) {
         uct_ud_ep_set_state(ep, UCT_UD_EP_FLAG_CREQ_NOTSENT);
+        ep->flags &= ~UCT_UD_EP_FLAG_PRIVATE;
         *new_ep_p = ep;
         *skb_p    = NULL;
         return UCS_ERR_ALREADY_EXISTS;
@@ -475,7 +513,7 @@ static void uct_ud_ep_rx_creq(uct_ud_iface_t *iface, uct_ud_neth_t *neth)
             /* simultanuous CREQ */
             ep->dest_ep_id = uct_ib_unpack_uint24(ctl->conn_req.ep_addr.ep_id);
             ep->rx.ooo_pkts.head_sn = neth->psn;
-            uct_ud_peer_copy(&ep->peer, (void*)&ctl->peer);
+            uct_ud_peer_copy(&ep->peer, ucs_unaligned_ptr(&ctl->peer));
             ucs_debug("simultanuous CREQ ep=%p"
                       "(iface=%p conn_id=%d ep_id=%d, dest_ep_id=%d rx_psn=%u)",
                       ep, iface, ep->conn_id, ep->ep_id,
@@ -490,10 +528,17 @@ static void uct_ud_ep_rx_creq(uct_ud_iface_t *iface, uct_ud_neth_t *neth)
         }
     }
 
+    ++ep->rx_creq_count;
+
     ucs_assert_always(ctl->conn_req.conn_id == ep->conn_id);
     ucs_assert_always(uct_ib_unpack_uint24(ctl->conn_req.ep_addr.ep_id) == ep->dest_ep_id);
     /* creq must always have same psn */
-    ucs_assert_always(ep->rx.ooo_pkts.head_sn == neth->psn);
+    ucs_assertv_always(ep->rx.ooo_pkts.head_sn == neth->psn,
+                       "iface=%p ep=%p conn_id=%d ep_id=%d, dest_ep_id=%d rx_psn=%u "
+                       "neth_psn=%u ep_flags=0x%x ctl_ops=0x%x rx_creq_count=%d",
+                       iface, ep, ep->conn_id, ep->ep_id, ep->dest_ep_id,
+                       ep->rx.ooo_pkts.head_sn, neth->psn, ep->flags,
+                       ep->tx.pending.ops, ep->rx_creq_count);
     /* scedule connection reply op */
     UCT_UD_EP_HOOK_CALL_RX(ep, neth, sizeof(*neth) + sizeof(*ctl));
     if (uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_CREQ)) {
@@ -521,7 +566,7 @@ static void uct_ud_ep_rx_ctl(uct_ud_iface_t *iface, uct_ud_ep_t *ep,
     ep->rx.ooo_pkts.head_sn = neth->psn;
     ep->dest_ep_id = ctl->conn_rep.src_ep_id;
     ucs_arbiter_group_schedule(&iface->tx.pending_q, &ep->tx.pending.group);
-    uct_ud_peer_copy(&ep->peer, (void*)&ctl->peer);
+    uct_ud_peer_copy(&ep->peer, ucs_unaligned_ptr(&ctl->peer));
     uct_ud_ep_set_state(ep, UCT_UD_EP_FLAG_CREP_RCVD);
 }
 
@@ -536,6 +581,15 @@ uct_ud_send_skb_t *uct_ud_ep_prepare_creq(uct_ud_ep_t *ep)
     ucs_assert_always(ep->dest_ep_id == UCT_UD_EP_NULL_ID);
     ucs_assert_always(ep->ep_id != UCT_UD_EP_NULL_ID);
 
+    /* CREQ should not be sent if CREP for the counter CREQ is scheduled
+     * (or sent already) */
+    ucs_assertv_always(!uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_CREP) &&
+                       !(ep->flags & UCT_UD_EP_FLAG_CREP_SENT),
+                       "iface=%p ep=%p conn_id=%d rx_psn=%u ep_flags=0x%x "
+                       "ctl_ops=0x%x rx_creq_count=%d",
+                       iface, ep, ep->conn_id, ep->rx.ooo_pkts.head_sn,
+                       ep->flags, ep->tx.pending.ops, ep->rx_creq_count);
+
     skb = uct_ud_iface_get_tx_skb(iface, ep);
     if (!skb) {
         return NULL;
@@ -564,7 +618,7 @@ uct_ud_send_skb_t *uct_ud_ep_prepare_creq(uct_ud_ep_t *ep)
         return NULL;
     }
 
-    uct_ud_peer_name((void*)&creq->peer);
+    uct_ud_peer_name(ucs_unaligned_ptr(&creq->peer));
 
     skb->len = sizeof(*neth) + sizeof(*creq) + iface->super.addr_size;
     return skb;
@@ -639,10 +693,16 @@ void uct_ud_ep_process_rx(uct_ud_iface_t *iface, uct_ud_neth_t *neth, unsigned b
     }
 
     if (ucs_unlikely(is_async &&
-                     (iface->super.super.am[am_id].flags & UCT_CB_FLAG_SYNC))) {
+                     !(iface->super.super.am[am_id].flags & UCT_CB_FLAG_ASYNC))) {
         skb->u.am.len = byte_len - sizeof(*neth);
         ucs_queue_push(&iface->rx.pending_q, &skb->u.am.queue);
     } else {
+        /* Avoid reordering with respect to pending operations, if user AM handler
+         * initiates sends from any endpoint created on the iface.
+         * This flag would be cleared after all incoming messages
+         * are processed. */
+        uct_ud_iface_raise_pending_async_ev(iface);
+
         uct_ib_iface_invoke_am_desc(&iface->super, am_id, neth + 1,
                                     byte_len - sizeof(*neth), &skb->super);
     }
@@ -709,11 +769,24 @@ ucs_status_t uct_ud_ep_flush_nolock(uct_ud_iface_t *iface, uct_ud_ep_t *ep,
         skb = ucs_queue_tail_elem_non_empty(&ep->tx.window, uct_ud_send_skb_t, queue);
         psn = skb->neth->psn;
         if (!(skb->flags & UCT_UD_SEND_SKB_FLAG_ACK_REQ)) {
-            /* If we didn't ask for ACK on last skb, schedule an ACK message.
+            /* If we didn't ask for ACK on last skb, send an ACK_REQ message.
+             * It will speed up the flush because we will not have to wait untill
+             * retransmit is triggered.
              * Also, prevent from sending more control messages like this after
              * first time by turning on the flag on the last skb.
              */
-            uct_ud_ep_ctl_op_add_safe(iface, ep, UCT_UD_SEND_SKB_FLAG_ACK_REQ);
+
+            /* Since the function can be called from the arbiter context it is
+             * impossible to schedule a control operation. So just raise a
+             * flag and if there is no other control send ACK_REQ directly.
+             *
+             * If there is other control arbiter will take care of it.
+             */
+            ep->tx.pending.ops |= UCT_UD_EP_OP_ACK_REQ;
+            if (uct_ud_ep_ctl_op_check_ex(ep, UCT_UD_EP_OP_ACK_REQ)) {
+                uct_ud_ep_do_pending_ctl(ep, iface);
+            }
+
             skb->flags |= UCT_UD_SEND_SKB_FLAG_ACK_REQ;
         }
 
@@ -774,18 +847,23 @@ ucs_status_t uct_ud_ep_flush(uct_ep_h ep_h, unsigned flags,
         uct_ep_pending_purge(ep_h, NULL, 0);
         /* Open window after cancellation for next sending */
         uct_ud_ep_ca_ack(ep);
+        status = UCS_OK;
+        goto out;
+    }
 
-        uct_ud_leave(iface);
-        return UCS_OK;
+    if (ucs_unlikely(uct_ud_iface_has_pending_async_ev(iface))) {
+        status = UCS_ERR_NO_RESOURCE;
+        goto out;
     }
 
-    uct_ud_iface_progress_pending_tx(iface);
     status = uct_ud_ep_flush_nolock(iface, ep, comp);
     if (status == UCS_OK) {
         UCT_TL_EP_STAT_FLUSH(&ep->super);
     } else if (status == UCS_INPROGRESS) {
         UCT_TL_EP_STAT_FLUSH_WAIT(&ep->super);
     }
+
+out:
     uct_ud_leave(iface);
     return status;
 }
@@ -800,6 +878,15 @@ static uct_ud_send_skb_t *uct_ud_ep_prepare_crep(uct_ud_ep_t *ep)
     ucs_assert_always(ep->dest_ep_id != UCT_UD_EP_NULL_ID);
     ucs_assert_always(ep->ep_id != UCT_UD_EP_NULL_ID);
 
+    /* Check that CREQ is neither sheduled nor waiting for CREP ack */
+    ucs_assertv_always(!uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_CREQ) &&
+                       ucs_queue_is_empty(&ep->tx.window),
+                       "iface=%p ep=%p conn_id=%d ep_id=%d, dest_ep_id=%d rx_psn=%u "
+                       "ep_flags=0x%x ctl_ops=0x%x rx_creq_count=%d",
+                       iface, ep, ep->conn_id, ep->ep_id, ep->dest_ep_id,
+                       ep->rx.ooo_pkts.head_sn, ep->flags, ep->tx.pending.ops,
+                       ep->rx_creq_count);
+
     skb = uct_ud_iface_get_tx_skb(iface, ep);
     if (!skb) {
         return NULL;
@@ -816,7 +903,7 @@ static uct_ud_send_skb_t *uct_ud_ep_prepare_crep(uct_ud_ep_t *ep)
     crep->type               = UCT_UD_PACKET_CREP;
     crep->conn_rep.src_ep_id = ep->ep_id;
 
-    uct_ud_peer_name((void*)&crep->peer);
+    uct_ud_peer_name(ucs_unaligned_ptr(&crep->peer));
 
     skb->len = sizeof(*neth) + sizeof(*crep);
     uct_ud_ep_ctl_op_del(ep, UCT_UD_EP_OP_CREP);
@@ -916,7 +1003,7 @@ static void uct_ud_ep_do_pending_ctl(uct_ud_ep_t *ep, uct_ud_iface_t *iface)
         skb =  uct_ud_ep_resend(ep);
     } else if (uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_ACK)) {
         if (uct_ud_ep_is_connected(ep)) {
-            skb = (void*)&iface->tx.skb_inl.super;
+            skb = ucs_unaligned_ptr(&iface->tx.skb_inl.super);
             uct_ud_neth_ctl_ack(ep, skb->neth);
         } else {
             /* Do not send ACKs if not connected yet. It may happen if
@@ -926,7 +1013,7 @@ static void uct_ud_ep_do_pending_ctl(uct_ud_ep_t *ep, uct_ud_iface_t *iface)
         }
         uct_ud_ep_ctl_op_del(ep, UCT_UD_EP_OP_ACK);
     } else if (uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_ACK_REQ)) {
-        skb = (void*)&iface->tx.skb_inl.super;
+        skb = ucs_unaligned_ptr(&iface->tx.skb_inl.super);
         uct_ud_neth_ctl_ack_req(ep, skb->neth);
         uct_ud_ep_ctl_op_del(ep, UCT_UD_EP_OP_ACK_REQ);
     } else if (uct_ud_ep_ctl_op_isany(ep)) {
@@ -1032,16 +1119,18 @@ uct_ud_ep_do_pending(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
 
     /* user pending can be send iff
      * - not in async progress
-     * - there are only low priority ctl pending or not ctl at all
+     * - there are no high priority pending control messages
      */
-    if (!in_async_progress &&
-            (uct_ud_ep_ctl_op_check_ex(ep, UCT_UD_EP_OP_CTL_LOW_PRIO) ||
-             !uct_ud_ep_ctl_op_isany(ep))) {
+    if (!in_async_progress && !uct_ud_ep_ctl_op_check(ep, UCT_UD_EP_OP_CTL_HI_PRIO)) {
         uct_pending_req_t *req;
         ucs_status_t status;
 
         req = ucs_container_of(elem, uct_pending_req_t, priv);
+
+        ucs_assert(!(ep->flags & UCT_UD_EP_FLAG_IN_PENDING));
+        ep->flags |= UCT_UD_EP_FLAG_IN_PENDING;
         status = req->func(req);
+        ep->flags &= ~UCT_UD_EP_FLAG_IN_PENDING;
 
         if (status == UCS_INPROGRESS) {
             return UCS_ARBITER_CB_RESULT_NEXT_GROUP;
@@ -1053,15 +1142,25 @@ uct_ud_ep_do_pending(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
             uct_ud_ep_do_pending_ctl(ep, iface);
             return uct_ud_ep_ctl_op_next(ep);
         }
-        iface->tx.pending_q_len--;
         return UCS_ARBITER_CB_RESULT_REMOVE_ELEM;
     }
+
     /* try to send ctl messages */
     uct_ud_ep_do_pending_ctl(ep, iface);
-    return uct_ud_ep_ctl_op_next(ep);
+    if (in_async_progress) {
+        return uct_ud_ep_ctl_op_next(ep);
+    } else {
+        /* we still didn't process the current pending request because of hi-prio
+         * control messages, so cannot stop sending yet. If we stop, not all
+         * resources will be exhausted and out-of-order with pending can occur.
+         * (pending control ops may be cleared by uct_ud_ep_do_pending_ctl)
+         */
+        return UCS_ARBITER_CB_RESULT_NEXT_GROUP;
+    }
 }
 
-ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep_h, uct_pending_req_t *req)
+ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep_h, uct_pending_req_t *req,
+                                   unsigned flags)
 {
     uct_ud_ep_t *ep = ucs_derived_of(ep_h, uct_ud_ep_t);
     uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface,
@@ -1069,8 +1168,15 @@ ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep_h, uct_pending_req_t *req)
 
     uct_ud_enter(iface);
 
-    /* try to flush pending queue first */
-    uct_ud_iface_progress_pending(iface, 0);
+    /* if there was an async progress all 'send' ops return
+     * UCS_ERR_NO_RESOURCE. If we return UCS_ERR_BUSY there will
+     * be a deadlock.
+     * So we must skip a resource check and add a pending op in order to
+     * avoid a deadlock.
+     */
+    if (ucs_unlikely(uct_ud_iface_has_pending_async_ev(iface))) {
+        goto add_req;
+    }
 
     if (uct_ud_iface_can_tx(iface) &&
         uct_ud_iface_has_skbs(iface) &&
@@ -1081,12 +1187,14 @@ ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep_h, uct_pending_req_t *req)
         return UCS_ERR_BUSY;
     }
 
+add_req:
     ucs_arbiter_elem_init((ucs_arbiter_elem_t *)req->priv);
     ucs_arbiter_group_push_elem(&ep->tx.pending.group,
                                 (ucs_arbiter_elem_t *)req->priv);
     ucs_arbiter_group_schedule(&iface->tx.pending_q, &ep->tx.pending.group);
+    ucs_trace_data("ud ep %p: added pending req %p tx_psn %d acked_psn %d cwnd %d",
+                   ep, req, ep->tx.psn, ep->tx.acked_psn, ep->ca.cwnd);
 
-    iface->tx.pending_q_len++;
     uct_ud_leave(iface);
     return UCS_OK;
 }
@@ -1097,10 +1205,9 @@ uct_ud_ep_pending_purge_cb(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
 {
     uct_ud_ep_t *ep = ucs_container_of(ucs_arbiter_elem_group(elem),
                                        uct_ud_ep_t, tx.pending.group);
-    uct_pending_req_t *req;
-    uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface, uct_ud_iface_t);
     uct_purge_cb_args_t *cb_args    = arg;
     uct_pending_purge_callback_t cb = cb_args->cb;
+    uct_pending_req_t *req;
 
     if (&ep->tx.pending.elem == elem) {
         /* return ignored by arbiter */
@@ -1112,7 +1219,6 @@ uct_ud_ep_pending_purge_cb(ucs_arbiter_t *arbiter, ucs_arbiter_elem_t *elem,
     } else {
         ucs_debug("ep=%p cancelling user pending request %p", ep, req);
     }
-    iface->tx.pending_q_len--;
 
     /* return ignored by arbiter */
     return UCS_ARBITER_CB_RESULT_REMOVE_ELEM;
@@ -1140,24 +1246,22 @@ void uct_ud_ep_pending_purge(uct_ep_h ep_h, uct_pending_purge_callback_t cb,
 
 void  uct_ud_ep_disconnect(uct_ep_h tl_ep)
 {
-    uct_ud_ep_t *ep = ucs_derived_of(tl_ep, uct_ud_ep_t);
-    /*
-     * At the moment scedule flush and keep ep
-     * until interface is destroyed. User should not send any
-     * new data
-     * In the future consider doin full fledged disconnect
-     * protocol. Kind of TCP (FIN/ACK). Doing this will save memory
-     * on the other hand active ep will need more memory to keep its state
-     * and such protocol will add extra complexity
-     */
+    uct_ud_ep_t    *ep    = ucs_derived_of(tl_ep, uct_ud_ep_t);
+    uct_ud_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ud_iface_t);
+
+    ucs_debug("ep %p: disconnect", ep);
 
-    ucs_trace_func("");
     /* cancel user pending */
     uct_ud_ep_pending_purge(tl_ep, NULL, NULL);
 
     /* schedule flush */
     uct_ud_ep_flush(tl_ep, 0, NULL);
 
+    /* the EP will be destroyed by interface destroy or timeout in
+     * uct_ud_ep_slow_timer
+     */
+    ep->close_time = ucs_twheel_get_time(&iface->async.slow_timer);
     ep->flags |= UCT_UD_EP_FLAG_DISCONNECTED;
-    /* TODO: at least in debug mode keep and check tl_ep state  */
+    ucs_wtimer_add(&iface->async.slow_timer, &ep->slow_timer,
+                   UCT_UD_SLOW_TIMER_MAX_TICK(iface));
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.h
index dd93e28da..95fe92fad 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_ep.h
@@ -188,16 +188,23 @@ enum {
     UCT_UD_EP_FLAG_ASYNC_COMPS       = UCS_BIT(0), /* set if there are completions that
                                                     * were picked by async thread and queued */
     UCT_UD_EP_FLAG_DISCONNECTED      = UCS_BIT(1), /* set if the endpoint was disconnected */
+    UCT_UD_EP_FLAG_PRIVATE           = UCS_BIT(2), /* EP is was created as internal */
 
     /* debug flags */
-    UCT_UD_EP_FLAG_PRIVATE           = UCS_BIT(2), /* EP is was created as internal */
     UCT_UD_EP_FLAG_CREQ_RCVD         = UCS_BIT(3), /* CREQ message was received */
     UCT_UD_EP_FLAG_CREP_RCVD         = UCS_BIT(4), /* CREP message was received */
     UCT_UD_EP_FLAG_CREQ_SENT         = UCS_BIT(5), /* CREQ message was sent */
     UCT_UD_EP_FLAG_CREP_SENT         = UCS_BIT(6), /* CREP message was sent */
-    UCT_UD_EP_FLAG_CREQ_NOTSENT      = UCS_BIT(7)  /* CREQ message is NOT sent, because
+    UCT_UD_EP_FLAG_CREQ_NOTSENT      = UCS_BIT(7), /* CREQ message is NOT sent, because
                                                       connection establishment process
                                                       is driven by remote side. */
+
+    /* Endpoint is currently executing the pending queue */
+#if ENABLE_ASSERT
+    UCT_UD_EP_FLAG_IN_PENDING        = UCS_BIT(8)
+#else
+    UCT_UD_EP_FLAG_IN_PENDING        = 0
+#endif
 };
 
 typedef struct uct_ud_peer_name {
@@ -221,15 +228,6 @@ struct uct_ud_ep {
          UCS_STATS_NODE_DECLARE(stats);
          UCT_UD_EP_HOOK_DECLARE(tx_hook);
     } tx;
-    struct {
-         uct_ud_psn_t           psn;       /* last psn that was retransmitted */
-         uct_ud_psn_t           max_psn;   /* max psn that should be retransmitted */
-         ucs_queue_iter_t       pos;       /* points to the part of tx window that needs to be resent */
-    } resend;
-    struct {
-        uct_ud_psn_t  wmax;
-        uct_ud_psn_t  cwnd;
-    } ca;
     struct {
         uct_ud_psn_t        acked_psn;    /* Last psn we acked */
         ucs_frag_list_t     ooo_pkts;     /* Out of order packets that can not be processed yet,
@@ -237,11 +235,22 @@ struct uct_ud_ep {
         UCS_STATS_NODE_DECLARE(stats);
         UCT_UD_EP_HOOK_DECLARE(rx_hook);
     } rx;
+    struct {
+        uct_ud_psn_t  wmax;
+        uct_ud_psn_t  cwnd;
+    } ca;
+    struct UCS_S_PACKED {
+         uct_ud_psn_t           psn;       /* last psn that was retransmitted */
+         uct_ud_psn_t           max_psn;   /* max psn that should be retransmitted */
+         ucs_queue_iter_t       pos;       /* points to the part of tx window that needs to be resent */
+    } resend;
     ucs_list_link_t  cep_list;
     uint32_t         conn_id;      /* connection id. assigned in connect_to_iface() */
-    ucs_wtimer_t     slow_timer;
-    uint8_t          flags;
+    uint16_t         flags;
     uint8_t          path_bits;
+    uint8_t          rx_creq_count; /* TODO: remove when reason for DUP/OOO CREQ is found */
+    ucs_wtimer_t     slow_timer;
+    ucs_time_t       close_time;   /* timestamp of closure */
     UCS_STATS_NODE_DECLARE(stats);
     UCT_UD_EP_HOOK_DECLARE(timer_hook);
 #if ENABLE_DEBUG_DATA
@@ -266,7 +275,8 @@ ucs_status_t uct_ud_ep_connect_to_ep(uct_ud_ep_t *ep,
                                      const uct_ib_address_t *ib_addr,
                                      const uct_ud_ep_addr_t *ep_addr);
 
-ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep, uct_pending_req_t *n);
+ucs_status_t uct_ud_ep_pending_add(uct_ep_h ep, uct_pending_req_t *n,
+                                   unsigned flags);
 
 void   uct_ud_ep_pending_purge(uct_ep_h ep, uct_pending_purge_callback_t cb,
                                void *arg);
@@ -372,7 +382,10 @@ uct_ud_ep_ctl_op_isany(uct_ud_ep_t *ep)
 static UCS_F_ALWAYS_INLINE int
 uct_ud_ep_ctl_op_check_ex(uct_ud_ep_t *ep, uint32_t ops)
 {
-    return ep->tx.pending.ops == ops;
+    /* check that at least one the given ops is set and
+     * all ops not given are not set */
+    return (ep->tx.pending.ops & ops) &&
+           ((ep->tx.pending.ops & ~ops) == 0);
 }
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.c
index 05d653816..db2f1331a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.c
@@ -99,10 +99,9 @@ uct_ud_iface_cep_lookup_peer(uct_ud_iface_t *iface,
 {
     uint32_t dest_qpn = uct_ib_unpack_uint24(src_if_addr->qp_num);
     union ibv_gid dgid;
-    uint8_t is_global;
     uint16_t dlid;
 
-    uct_ib_address_unpack(src_ib_addr, &dlid, &is_global, &dgid);
+    uct_ib_address_unpack(src_ib_addr, &dlid, &dgid);
     return uct_ud_iface_cep_lookup_addr(iface, dlid, &dgid, dest_qpn);
 }
 
@@ -150,11 +149,10 @@ ucs_status_t uct_ud_iface_cep_insert(uct_ud_iface_t *iface,
     uint32_t dest_qpn = uct_ib_unpack_uint24(src_if_addr->qp_num);
     uct_ud_iface_peer_t *peer;
     union ibv_gid dgid;
-    uint8_t is_global;
     uct_ud_ep_t *cep;
     uint16_t dlid;
 
-    uct_ib_address_unpack(src_ib_addr, &dlid, &is_global, &dgid);
+    uct_ib_address_unpack(src_ib_addr, &dlid, &dgid);
     peer = uct_ud_iface_cep_lookup_addr(iface, dlid, &dgid, dest_qpn);
     if (peer == NULL) {
         peer = malloc(sizeof *peer);
@@ -250,21 +248,13 @@ static void uct_ud_iface_send_skb_init(uct_iface_h tl_iface, void *obj,
 static ucs_status_t
 uct_ud_iface_create_qp(uct_ud_iface_t *self, const uct_ud_iface_config_t *config)
 {
-    /* TODO: exp attrs autoconf */
-    struct ibv_exp_qp_init_attr qp_init_attr;
+    uct_ib_qp_attr_t qp_init_attr      = {};
     struct ibv_qp_attr qp_attr;
+    static ucs_status_t status;
     int ret;
 
-    /* Create QP */
-    memset(&qp_init_attr, 0, sizeof(qp_init_attr));
-    qp_init_attr.qp_context          = NULL;
-    qp_init_attr.send_cq             = self->super.send_cq;
-    qp_init_attr.recv_cq             = self->super.recv_cq;
-    qp_init_attr.srq                 = NULL; /* TODO */
     qp_init_attr.qp_type             = IBV_QPT_UD;
     qp_init_attr.sq_sig_all          = 0;
-
-    /* TODO: cap setting */
     qp_init_attr.cap.max_send_wr     = config->super.tx.queue_len;
     qp_init_attr.cap.max_recv_wr     = config->super.rx.queue_len;
     qp_init_attr.cap.max_send_sge    = 2;
@@ -272,28 +262,9 @@ uct_ud_iface_create_qp(uct_ud_iface_t *self, const uct_ud_iface_config_t *config
     qp_init_attr.cap.max_inline_data = ucs_max(config->super.tx.min_inline,
                                                UCT_UD_MIN_INLINE);
 
-#if HAVE_VERBS_EXP_H
-    qp_init_attr.pd                  = uct_ib_iface_md(&self->super)->pd;
-    qp_init_attr.comp_mask           = IBV_QP_INIT_ATTR_PD;
-    /* TODO: inline rcv */
-#if 0
-    if (mxm_ud_ep_opts(ep)->ud.ib.rx.max_inline > 0) {
-        qp_init_attr.comp_mask      |= IBV_EXP_QP_INIT_ATTR_INL_RECV;
-        qp_init_attr.max_inl_recv    = mxm_ud_ep_opts(ep)->ud.ib.rx.max_inline;
-    }
-#endif
-    self->qp = ibv_exp_create_qp(uct_ib_iface_device(&self->super)->ibv_context,
-                                 &qp_init_attr);
-#else
-    self->qp = ibv_exp_create_qp(uct_ib_iface_md(&self->super)->pd, &qp_init_attr);
-#endif
-    if (self->qp == NULL) {
-        ucs_error("Failed to create qp: %s [inline: %u rsge: %u ssge: %u rwr: %u swr: %u]",
-                  strerror(errno),
-                  qp_init_attr.cap.max_inline_data, qp_init_attr.cap.max_recv_sge,
-                  qp_init_attr.cap.max_send_sge, qp_init_attr.cap.max_recv_wr,
-                  qp_init_attr.cap.max_send_wr);
-        goto err;
+    status = self->super.ops->create_qp(&self->super, &qp_init_attr, &self->qp);
+    if (status != UCS_OK) {
+        return status;
     }
 
     self->config.max_inline = qp_init_attr.cap.max_inline_data;
@@ -330,16 +301,9 @@ uct_ud_iface_create_qp(uct_ud_iface_t *self, const uct_ud_iface_config_t *config
         goto err_destroy_qp;
     }
 
-    ucs_debug("iface=%p: created qp 0x%x max_send_wr %u max_recv_wr %u max_inline %u",
-            self, self->qp->qp_num,
-            qp_init_attr.cap.max_send_wr,
-            qp_init_attr.cap.max_recv_wr,
-            qp_init_attr.cap.max_inline_data);
-
     return UCS_OK;
 err_destroy_qp:
     ibv_destroy_qp(self->qp);
-err:
     return UCS_ERR_INVALID_PARAM;
 }
 
@@ -417,17 +381,16 @@ static void uct_ud_iface_calc_gid_len(uct_ud_iface_t *iface)
 
 UCS_CLASS_INIT_FUNC(uct_ud_iface_t, uct_ud_iface_ops_t *ops, uct_md_h md,
                     uct_worker_h worker, const uct_iface_params_t *params,
-                    unsigned ud_rx_priv_len,
-                    const uct_ud_iface_config_t *config)
+                    const uct_ud_iface_config_t *config,
+                    uct_ib_iface_init_attr_t *init_attr)
 {
-    unsigned rx_priv_len, rx_hdr_len;
     ucs_status_t status;
     size_t data_size;
     int mtu;
 
-    ucs_trace_func("%s: iface=%p ops=%p worker=%p rx_headroom=%zu ud_rx_priv_len=%u",
+    ucs_trace_func("%s: iface=%p ops=%p worker=%p rx_headroom=%zu",
                    params->mode.device.dev_name, self, ops, worker,
-                   params->rx_headroom, ud_rx_priv_len);
+                   params->rx_headroom);
 
     if (config->super.tx.queue_len <= UCT_UD_TX_MODERATION) {
         ucs_error("%s ud iface tx queue is too short (%d <= %d)",
@@ -441,15 +404,15 @@ UCS_CLASS_INIT_FUNC(uct_ud_iface_t, uct_ud_iface_ops_t *ops, uct_md_h md,
         return status;
     }
 
-    rx_priv_len = ud_rx_priv_len +
-                  sizeof(uct_ud_recv_skb_t) - sizeof(uct_ib_iface_recv_desc_t);
-    rx_hdr_len  = UCT_IB_GRH_LEN + sizeof(uct_ud_neth_t);
+    init_attr->rx_priv_len = sizeof(uct_ud_recv_skb_t) -
+                             sizeof(uct_ib_iface_recv_desc_t);
+    init_attr->rx_hdr_len  = UCT_IB_GRH_LEN + sizeof(uct_ud_neth_t);
+    init_attr->tx_cq_len   = config->super.tx.queue_len;
+    init_attr->rx_cq_len   = config->super.rx.queue_len;
+    init_attr->seg_size    = ucs_min(mtu, config->super.super.max_bcopy);
 
     UCS_CLASS_CALL_SUPER_INIT(uct_ib_iface_t, &ops->super, md, worker,
-                              params, rx_priv_len, rx_hdr_len,
-                              config->super.tx.queue_len,
-                              config->super.rx.queue_len,
-                              mtu, &config->super);
+                              params, &config->super, init_attr);
 
     if (self->super.super.worker->async == NULL) {
         ucs_error("%s ud iface must have valid async context", params->mode.device.dev_name);
@@ -464,7 +427,7 @@ UCS_CLASS_INIT_FUNC(uct_ud_iface_t, uct_ud_iface_ops_t *ops, uct_md_h md,
     self->config.tx_qp_len       = config->super.tx.queue_len;
     self->config.peer_timeout    = ucs_time_from_sec(config->peer_timeout);
     self->config.check_grh_dgid  = (config->dgid_check &&
-                                    (self->super.addr_type == UCT_IB_ADDRESS_TYPE_ETH));
+                                    IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(&self->super)));
 
     if (config->slow_timer_backoff <= 0.) {
         ucs_error("The slow timer back off should be > 0 (%lf)",
@@ -518,13 +481,13 @@ UCS_CLASS_INIT_FUNC(uct_ud_iface_t, uct_ud_iface_ops_t *ops, uct_md_h md,
     self->tx.resend_skbs_quota = 0;
 
     ucs_arbiter_init(&self->tx.pending_q);
-    self->tx.pending_q_len = 0;
 
     ucs_queue_head_init(&self->tx.async_comp_q);
-    self->tx.in_pending = 0;
 
     ucs_queue_head_init(&self->rx.pending_q);
 
+    self->tx.async_before_pending = 0;
+
     uct_ud_iface_calc_gid_len(self);
 
     status = UCS_STATS_NODE_ALLOC(&self->stats, &uct_ud_iface_stats_class,
@@ -563,7 +526,6 @@ static UCS_CLASS_CLEANUP_FUNC(uct_ud_iface_t)
     ucs_debug("iface(%p): ptr_array cleanup", self);
     ucs_ptr_array_cleanup(&self->eps);
     ucs_arbiter_cleanup(&self->tx.pending_q);
-    ucs_assert(self->tx.pending_q_len == 0);
     UCS_STATS_NODE_FREE(self->stats);
     uct_ud_leave(self);
 }
@@ -664,7 +626,11 @@ ucs_status_t uct_ud_iface_flush(uct_iface_h tl_iface, unsigned flags,
 
     uct_ud_enter(iface);
 
-    uct_ud_iface_progress_pending_tx(iface);
+    if (ucs_unlikely(uct_ud_iface_has_pending_async_ev(iface))) {
+        UCT_TL_IFACE_STAT_FLUSH_WAIT(&iface->super.super);
+        uct_ud_leave(iface);
+        return UCS_INPROGRESS;
+    }
 
     count = 0;
     ucs_ptr_array_for_each(ep, i, &iface->eps) {
@@ -680,6 +646,7 @@ ucs_status_t uct_ud_iface_flush(uct_iface_h tl_iface, unsigned flags,
         UCT_TL_IFACE_STAT_FLUSH_WAIT(&iface->super.super);
         return UCS_INPROGRESS;
     }
+
     UCT_TL_IFACE_STAT_FLUSH(&iface->super.super);
     return UCS_OK;
 }
@@ -742,11 +709,34 @@ static void uct_ud_iface_free_resend_skbs(uct_ud_iface_t *iface)
     }
 }
 
+static void uct_ud_ep_dispatch_err_comp(uct_ud_ep_t *ep, uct_ud_send_skb_t *skb)
+{
+    uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface, uct_ud_iface_t);
+    ucs_status_t status;
+
+    ucs_assert(ep->tx.err_skb_count > 0);
+    --ep->tx.err_skb_count;
+
+    if ((ep->tx.err_skb_count > 0) || (ep->flags & UCT_UD_EP_FLAG_DISCONNECTED)) {
+        return;
+    }
+
+    if (ep->flags & UCT_UD_EP_FLAG_PRIVATE) {
+        uct_ep_destroy(&ep->super.super);
+        return;
+    }
+
+    status = iface->super.ops->set_ep_failed(&iface->super, &ep->super.super,
+                                             skb->status);
+    if (status != UCS_OK) {
+        ucs_fatal("transport error: %s", ucs_status_string(status));
+    }
+}
+
 void uct_ud_iface_dispatch_async_comps_do(uct_ud_iface_t *iface)
 {
     uct_ud_comp_desc_t *cdesc;
     uct_ud_send_skb_t  *skb;
-    ucs_status_t        status;
     uct_ud_ep_t *ep;
 
     do {
@@ -761,17 +751,7 @@ void uct_ud_iface_dispatch_async_comps_do(uct_ud_iface_t *iface)
         }
 
         if (ucs_unlikely(skb->flags & UCT_UD_SEND_SKB_FLAG_ERR)) {
-            ucs_assert(ep->tx.err_skb_count > 0);
-            --ep->tx.err_skb_count;
-            if ((ep->tx.err_skb_count == 0) &&
-                !(ep->flags & UCT_UD_EP_FLAG_DISCONNECTED)) {
-                status = iface->super.ops->set_ep_failed(&iface->super,
-                                                         &ep->super.super,
-                                                         skb->status);
-                if (status != UCS_OK) {
-                    ucs_fatal("transport error: %s", ucs_status_string(status));
-                }
-            }
+            uct_ud_ep_dispatch_err_comp(ep, skb);
         }
 
         ep->flags &= ~UCT_UD_EP_FLAG_ASYNC_COMPS;
@@ -830,7 +810,14 @@ static void uct_ud_iface_free_pending_rx(uct_ud_iface_t *iface)
 
 static inline void uct_ud_iface_async_progress(uct_ud_iface_t *iface)
 {
-    ucs_derived_of(iface->super.ops, uct_ud_iface_ops_t)->async_progress(iface);
+    unsigned ev_count;
+    uct_ud_iface_ops_t *ops;
+
+    ops = ucs_derived_of(iface->super.ops, uct_ud_iface_ops_t);
+    ev_count = ops->async_progress(iface);
+    if (ev_count > 0) {
+        uct_ud_iface_raise_pending_async_ev(iface);
+    }
 }
 
 static void uct_ud_iface_timer(int timer_id, void *arg)
@@ -892,7 +879,7 @@ ucs_status_t uct_ud_iface_event_arm(uct_iface_h tl_iface, unsigned events)
     }
 
     if (events & UCT_EVENT_SEND_COMP) {
-        status = iface->super.ops->arm_tx_cq(&iface->super);
+        status = iface->super.ops->arm_cq(&iface->super, UCT_IB_DIR_TX, 0);
         if (status != UCS_OK) {
             goto out;
         }
@@ -900,7 +887,7 @@ ucs_status_t uct_ud_iface_event_arm(uct_iface_h tl_iface, unsigned events)
 
     if (events & (UCT_EVENT_SEND_COMP | UCT_EVENT_RECV)) {
         /* we may get send completion through ACKs as well */
-        status = iface->super.ops->arm_rx_cq(&iface->super, 0);
+        status = iface->super.ops->arm_cq(&iface->super, UCT_IB_DIR_RX, 0);
         if (status != UCS_OK) {
             goto out;
         }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.h
index e4c6db537..ab36374bc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_iface.h
@@ -103,9 +103,10 @@ static inline ucs_status_t uct_ud_iface_null_hook(uct_ud_iface_t *iface,
 
 typedef struct uct_ud_iface_ops {
     uct_ib_iface_ops_t        super;
-    void                      (*async_progress)(uct_ud_iface_t *iface);
+    unsigned                  (*async_progress)(uct_ud_iface_t *iface);
     void                      (*tx_skb)(uct_ud_ep_t *ep, uct_ud_send_skb_t *skb,
                                         int solicited);
+    void                      (*ep_free)(uct_ep_h ep);
 } uct_ud_iface_ops_t;
 
 struct uct_ud_iface {
@@ -122,14 +123,14 @@ struct uct_ud_iface {
         uct_ud_send_skb_t     *skb; /* ready to use skb */
         uct_ud_send_skb_inl_t  skb_inl;
         ucs_mpool_t            mp;
+        /* got async events but pending queue was not dispatched */
+        uint8_t                async_before_pending;
         int16_t                available;
         unsigned               unsignaled;
         /* pool of skbs that are reserved for retransmissions */
         ucs_queue_head_t       resend_skbs;
         unsigned               resend_skbs_quota;
         ucs_arbiter_t          pending_q;
-        int                    pending_q_len;
-        int                    in_pending;
         ucs_queue_head_t       async_comp_q;
     } tx;
     struct {
@@ -154,7 +155,8 @@ struct uct_ud_iface {
 
 UCS_CLASS_DECLARE(uct_ud_iface_t, uct_ud_iface_ops_t*, uct_md_h,
                   uct_worker_h, const uct_iface_params_t*,
-                  unsigned, const uct_ud_iface_config_t*)
+                  const uct_ud_iface_config_t*,
+                  uct_ib_iface_init_attr_t*)
 
 struct uct_ud_ctl_hdr {
     uint8_t                    type;
@@ -213,7 +215,7 @@ uct_ud_send_skb_t *uct_ud_iface_resend_skb_get(uct_ud_iface_t *iface);
 static inline void
 uct_ud_iface_resend_skb_put(uct_ud_iface_t *iface, uct_ud_send_skb_t *skb)
 {
-    if (skb != (void*)&iface->tx.skb_inl.super) {
+    if (skb != ucs_unaligned_ptr(&iface->tx.skb_inl.super)) {
         ucs_queue_push(&iface->tx.resend_skbs, &skb->queue);
     }
 }
@@ -376,26 +378,29 @@ uct_ud_iface_get_async_time(uct_ud_iface_t *iface)
 static UCS_F_ALWAYS_INLINE void
 uct_ud_iface_progress_pending(uct_ud_iface_t *iface, const uintptr_t is_async)
 {
+    if (!is_async) {
+        iface->tx.async_before_pending = 0;
+    }
 
     if (!uct_ud_iface_can_tx(iface)) {
         return;
     }
 
-    iface->tx.in_pending = 1;
-    ucs_arbiter_dispatch(&iface->tx.pending_q, 1,
-             uct_ud_ep_do_pending, (void *)is_async);
-    iface->tx.in_pending = 0;
+    ucs_arbiter_dispatch(&iface->tx.pending_q, 1, uct_ud_ep_do_pending,
+                         (void *)is_async);
+}
+
+static UCS_F_ALWAYS_INLINE int
+uct_ud_iface_has_pending_async_ev(uct_ud_iface_t *iface)
+{
+    return iface->tx.async_before_pending;
 }
 
 static UCS_F_ALWAYS_INLINE void
-uct_ud_iface_progress_pending_tx(uct_ud_iface_t *iface)
+uct_ud_iface_raise_pending_async_ev(uct_ud_iface_t *iface)
 {
-    if (ucs_unlikely(iface->tx.pending_q_len > 0 &&
-                     iface->tx.in_pending == 0)) {
-        iface->tx.in_pending = 1;
-        ucs_arbiter_dispatch(&iface->tx.pending_q, 1,
-                             uct_ud_ep_do_pending, (void *)0);
-        iface->tx.in_pending = 0;
+    if (!ucs_arbiter_is_empty(&iface->tx.pending_q)) {
+        iface->tx.async_before_pending = 1;
     }
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_inl.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_inl.h
index 55e481613..b7f6b4439 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_inl.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/base/ud_inl.h
@@ -7,7 +7,6 @@
 static UCS_F_ALWAYS_INLINE void
 uct_ud_ep_ctl_op_schedule(uct_ud_iface_t *iface, uct_ud_ep_t *ep)
 {
-    ucs_assert(!iface->tx.in_pending);
     ucs_arbiter_group_push_elem(&ep->tx.pending.group,
                                 &ep->tx.pending.elem);
     ucs_arbiter_group_schedule(&iface->tx.pending_q, &ep->tx.pending.group);
@@ -23,15 +22,6 @@ uct_ud_ep_ctl_op_add(uct_ud_iface_t *iface, uct_ud_ep_t *ep, int op)
     uct_ud_ep_ctl_op_schedule(iface, ep);
 }
 
-static UCS_F_ALWAYS_INLINE void
-uct_ud_ep_ctl_op_add_safe(uct_ud_iface_t *iface, uct_ud_ep_t *ep, int op)
-{
-    ep->tx.pending.ops |= op;
-    if (!iface->tx.in_pending) {
-        uct_ud_ep_ctl_op_schedule(iface, ep);
-    }
-}
-
 static UCS_F_ALWAYS_INLINE void
 uct_ud_ep_tx_stop(uct_ud_ep_t *ep)
 {
@@ -81,8 +71,9 @@ static UCS_F_ALWAYS_INLINE uct_ud_send_skb_t *
 uct_ud_ep_get_tx_skb(uct_ud_iface_t *iface, uct_ud_ep_t *ep)
 {
     if (ucs_unlikely(!uct_ud_ep_is_connected(ep) ||
-                     uct_ud_ep_no_window(ep))) {
-        ucs_trace_data("iface=%p ep=%p (%d->%d) no ep resources (psn=%u max_psn=%u)",
+                     uct_ud_ep_no_window(ep) ||
+                     uct_ud_iface_has_pending_async_ev(iface))) {
+        ucs_trace_poll("iface=%p ep=%p (%d->%d) no ep resources (psn=%u max_psn=%u)",
                        iface, ep, ep->ep_id, ep->dest_ep_id,
                        (unsigned)ep->tx.psn,
                        (unsigned)ep->tx.max_psn);
@@ -170,6 +161,17 @@ uct_ud_am_common(uct_ud_iface_t *iface, uct_ud_ep_t *ep, uint8_t id,
         return UCS_ERR_NO_RESOURCE;
     }
 
+    /* either we are executing pending operations, or there are no any pending
+     * elements, or the only pending element is for sending control messages
+     * (we don't care about reordering with respect to control messages)
+     */
+    ucs_assertv((ep->flags & UCT_UD_EP_FLAG_IN_PENDING) ||
+                ucs_arbiter_group_is_empty(&ep->tx.pending.group) ||
+                ucs_arbiter_elem_is_only(&ep->tx.pending.group, &ep->tx.pending.elem),
+                "out-of-order send detected for ep %p am %d ep_pending %d arbtail %p arbelem %p",
+                ep, id, (ep->flags & UCT_UD_EP_FLAG_IN_PENDING),
+                ep->tx.pending.group.tail,
+                &ep->tx.pending.elem);
     uct_ud_am_set_neth(skb->neth, ep, id);
 
     *skb_p = skb;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/verbs/ud_verbs.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/verbs/ud_verbs.c
index 1209ca133..3d3bd5040 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/verbs/ud_verbs.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ib/ud/verbs/ud_verbs.c
@@ -50,10 +50,6 @@ UCS_CLASS_INIT_FUNC(uct_ud_verbs_ep_t, uct_iface_h tl_iface)
 static UCS_CLASS_CLEANUP_FUNC(uct_ud_verbs_ep_t)
 {
     ucs_trace_func("");
-    if (self->ah) {
-        ibv_destroy_ah(self->ah);
-        self->ah = NULL;
-    }
 }
 
 UCS_CLASS_DEFINE(uct_ud_verbs_ep_t, uct_ud_ep_t);
@@ -90,7 +86,7 @@ uct_ud_verbs_ep_tx_inlv(uct_ud_verbs_iface_t *iface, uct_ud_verbs_ep_t *ep,
     ret = ibv_post_send(iface->super.qp, &iface->tx.wr_inl, &bad_wr);
     ucs_assertv(ret == 0, "ibv_post_send() returned %d (%m)", ret);
     uct_ib_log_post_send(&iface->super.super, iface->super.qp, &iface->tx.wr_inl,
-                         uct_ud_dump_packet);
+                         INT_MAX, uct_ud_dump_packet);
     --iface->super.tx.available;
 }
 
@@ -109,7 +105,7 @@ uct_ud_verbs_ep_tx_skb(uct_ud_verbs_iface_t *iface,
     ret = ibv_post_send(iface->super.qp, &iface->tx.wr_skb, &bad_wr);
     ucs_assertv(ret == 0, "ibv_post_send() returned %d (%m)", ret);
     uct_ib_log_post_send(&iface->super.super, iface->super.qp, &iface->tx.wr_skb,
-                         uct_ud_dump_packet);
+                         INT_MAX, uct_ud_dump_packet);
     --iface->super.tx.available;
 }
 
@@ -145,7 +141,7 @@ ucs_status_t uct_ud_verbs_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t hdr,
                      0, iface->super.config.max_inline, "am_short");
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     status = uct_ud_am_common(&iface->super, &ep->super, id, &skb);
     if (status != UCS_OK) {
         uct_ud_leave(&iface->super);
@@ -180,7 +176,7 @@ static ssize_t uct_ud_verbs_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
     size_t length;
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     status = uct_ud_am_common(&iface->super, &ep->super, id, &skb);
     if (status != UCS_OK) {
         uct_ud_leave(&iface->super);
@@ -217,7 +213,7 @@ uct_ud_verbs_ep_am_zcopy(uct_ep_h tl_ep, uint8_t id, const void *header,
                               uct_iov_total_length(iov, iovcnt));
 
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     status = uct_ud_am_common(&iface->super, &ep->super, id, &skb);
     if (status != UCS_OK) {
         uct_ud_leave(&iface->super);
@@ -256,7 +252,7 @@ ucs_status_t uct_ud_verbs_ep_put_short(uct_ep_h tl_ep,
 
     /* TODO: UCT_CHECK_LENGTH(length <= iface->config.max_inline, "put_short"); */
     uct_ud_enter(&iface->super);
-    uct_ud_iface_progress_pending_tx(&iface->super);
+
     skb = uct_ud_ep_get_tx_skb(&iface->super, &ep->super);
     if (!skb) {
         uct_ud_leave(&iface->super);
@@ -290,7 +286,7 @@ uct_ud_verbs_iface_poll_tx(uct_ud_verbs_iface_t *iface)
     struct ibv_wc wc;
     int ret;
 
-    ret = ibv_poll_cq(iface->super.super.send_cq, 1, &wc);
+    ret = ibv_poll_cq(iface->super.super.cq[UCT_IB_DIR_TX], 1, &wc);
     if (ucs_unlikely(ret < 0)) {
         ucs_fatal("Failed to poll send CQ");
         return 0;
@@ -319,7 +315,7 @@ uct_ud_verbs_iface_poll_rx(uct_ud_verbs_iface_t *iface, int is_async)
     void *packet;
     int i;
 
-    status = uct_ib_poll_cq(iface->super.super.recv_cq, &num_wcs, wc);
+    status = uct_ib_poll_cq(iface->super.super.cq[UCT_IB_DIR_RX], &num_wcs, wc);
     if (status != UCS_OK) {
         num_wcs = 0;
         goto out;
@@ -353,16 +349,21 @@ static ucs_status_t uct_ud_verbs_ep_set_failed(uct_ib_iface_t *iface,
                              &iface->super.super, status);
 }
 
-static void uct_ud_verbs_iface_async_progress(uct_ud_iface_t *ud_iface)
+static unsigned uct_ud_verbs_iface_async_progress(uct_ud_iface_t *ud_iface)
 {
     uct_ud_verbs_iface_t *iface = ucs_derived_of(ud_iface, uct_ud_verbs_iface_t);
-    unsigned count;
+    unsigned count, n;
 
+    count = 0;
     do {
-        count = uct_ud_verbs_iface_poll_rx(iface, 1);
-    } while (count > 0);
-    uct_ud_verbs_iface_poll_tx(iface);
+        n = uct_ud_verbs_iface_poll_rx(iface, 1);
+        count += n;
+    } while (n > 0);
+
+    count += uct_ud_verbs_iface_poll_tx(iface);
+
     uct_ud_iface_progress_pending(&iface->super, 1);
+    return count;
 }
 
 static unsigned uct_ud_verbs_iface_progress(uct_iface_h tl_iface)
@@ -382,8 +383,10 @@ static unsigned uct_ud_verbs_iface_progress(uct_iface_h tl_iface)
     } else {
         count = 0;
     }
+
     uct_ud_iface_progress_pending(&iface->super, 0);
     uct_ud_leave(&iface->super);
+
     return count;
 }
 
@@ -513,13 +516,15 @@ static uct_ud_iface_ops_t uct_ud_verbs_iface_ops = {
     .iface_get_address        = uct_ud_iface_get_address,
     .iface_is_reachable       = uct_ib_iface_is_reachable
     },
-    .arm_tx_cq                = uct_ib_iface_arm_tx_cq,
-    .arm_rx_cq                = uct_ib_iface_arm_rx_cq,
+    .arm_cq                   = uct_ib_iface_arm_cq,
+    .event_cq                 = (void*)ucs_empty_function,
     .handle_failure           = uct_ud_iface_handle_failure,
-    .set_ep_failed            = uct_ud_verbs_ep_set_failed
+    .set_ep_failed            = uct_ud_verbs_ep_set_failed,
+    .create_qp                = uct_ib_iface_create_qp
     },
     .async_progress           = uct_ud_verbs_iface_async_progress,
-    .tx_skb                   = uct_ud_verbs_ep_tx_ctl_skb
+    .tx_skb                   = uct_ud_verbs_ep_tx_ctl_skb,
+    .ep_free                  = UCS_CLASS_DELETE_FUNC_NAME(uct_ud_verbs_ep_t)
 };
 
 static UCS_F_NOINLINE void
@@ -562,12 +567,15 @@ static UCS_CLASS_INIT_FUNC(uct_ud_verbs_iface_t, uct_md_h md, uct_worker_h worke
 {
     uct_ud_iface_config_t *config = ucs_derived_of(tl_config,
                                                    uct_ud_iface_config_t);
+    uct_ib_iface_init_attr_t init_attr = {};
     ucs_status_t status;
 
     ucs_trace_func("");
 
+    init_attr.res_domain_key = UCT_IB_IFACE_NULL_RES_DOMAIN_KEY;
+
     UCS_CLASS_CALL_SUPER_INIT(uct_ud_iface_t, &uct_ud_verbs_iface_ops, md,
-                              worker, params, 0, config);
+                              worker, params, config, &init_attr);
 
     memset(&self->tx.wr_inl, 0, sizeof(self->tx.wr_inl));
     self->tx.wr_inl.opcode            = IBV_WR_SEND;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_ep.c
index b61d8a503..29ef06b5a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_ep.c
@@ -46,42 +46,6 @@ UCS_CLASS_DEFINE_DELETE_FUNC(uct_rocm_cma_ep_t, uct_ep_t);
                    (_rkey))
 
 
-/** Convert pointer to pointer which could be used for 
-  * GPU access.
-*/
-static ucs_status_t uct_rocm_cma_ptr_to_gpu_ptr(void *ptr, void **gpu_address,
-                                                size_t size, int any_memory,
-                                                int *locked)
-{
-    /* Assume that we do not need to lock memory */
-    *locked = 0;
-
-    /* Try to get GPU address if any */
-    if (!uct_rocm_is_ptr_gpu_accessible(ptr, gpu_address)) {
-        /* We do not have GPU address. Check what to do. */
-        if (!any_memory) {
-            /* We do not want to deal with memory about which
-             * ROCm stack is not aware */
-            ucs_warn("Address %p is not GPU registered", ptr);
-            return UCS_ERR_INVALID_ADDR;
-        } else {
-            /* Register / lock this memory for GPU access */
-            hsa_status_t status =  uct_rocm_memory_lock(ptr, size, gpu_address);
-
-            if (status != HSA_STATUS_SUCCESS) {
-                ucs_error("Could not lock  %p. Status %d", ptr, status);
-                return UCS_ERR_INVALID_ADDR;
-            } else {
-                ucs_trace("Lock address %p as GPU %p", ptr, *gpu_address);
-                /* We locked this memory. Set the flag to be aware that
-                 * we need to unlock it later */
-                *locked = 1;
-            }
-        }
-    }
-
-    return UCS_OK;
-}
 
 /** Release GPU address if it was previously locked */
 static void uct_rocm_cma_unlock_ptrs(void **local_ptr, int *locked,
@@ -131,8 +95,8 @@ ucs_status_t uct_rocm_cma_ep_common_zcopy(uct_ep_h tl_ep,
     size_t length = 0;
     HsaMemoryRange local_iov[UCT_SM_MAX_IOV];
     HsaMemoryRange remote_iov;
-    ucs_status_t ucs_status;
     HSAKMT_STATUS hsa_status;
+    hsa_status_t status;
     void   *local_ptr[UCT_SM_MAX_IOV];
     int     local_ptr_locked[UCT_SM_MAX_IOV];
     uint64_t remote_gpu_address;
@@ -212,15 +176,15 @@ ucs_status_t uct_rocm_cma_ep_common_zcopy(uct_ep_h tl_ep,
              * If this is memory was not yet registered with ROCm stack and
              * flag "any_memory" is set than lock this memory.
              */
-            ucs_status = uct_rocm_cma_ptr_to_gpu_ptr(local_ptr[local_iov_it],
-                                                     &local_iov[local_iov_it].MemoryAddress,
-                                                     local_iov[local_iov_it].SizeInBytes,
-                                                     rocm_md->any_memory,
-                                                     &local_ptr_locked[local_iov_it]);
+            status = uct_rocm_cma_ptr_to_gpu_ptr(local_ptr[local_iov_it],
+                                                 &local_iov[local_iov_it].MemoryAddress,
+                                                 local_iov[local_iov_it].SizeInBytes,
+                                                 rocm_md->any_memory,
+                                                 &local_ptr_locked[local_iov_it]);
 
-            if (ucs_status != UCS_OK) {
+            if (status != HSA_STATUS_SUCCESS) {
                 uct_rocm_cma_unlock_ptrs(local_ptr, local_ptr_locked, local_iov_it);
-                return ucs_status;
+                return UCS_ERR_INVALID_ADDR;
             }
 
             ucs_trace("[%d] Local address %p (GPU ptr %p), Local Size 0x%x",
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_md.c
index 9bc7c7638..24821f783 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_cma_md.c
@@ -99,27 +99,14 @@ static ucs_status_t uct_rocm_cma_mem_reg(uct_md_h md, void *address, size_t leng
     /* Assume memory is already GPU accessible */
     key->is_locked = 0;
 
-    /* Check if memory is already GPU accessible. If yes then GPU
-     * address will be returned.
-     * Note that we could have case of "malloc"-ed memory which
-     * was "locked" outside of UCX. In this case CPU address may be 
-     * not the same as GPU one.
-     */
-    if (!uct_rocm_is_ptr_gpu_accessible(address, &gpu_address)) {
-        if (!rocm_md->any_memory) {
-            ucs_warn("Address %p is not GPU allocated.", address);
-            return UCS_ERR_INVALID_ADDR;
-        } else {
-            status = uct_rocm_memory_lock(address, length, &gpu_address);
-
-            if (status != HSA_STATUS_SUCCESS) {
-                ucs_error("Could not lock  %p. Status %d", address, status);
-                return UCS_ERR_INVALID_ADDR;
-            } else {
-                ucs_trace("Lock address %p as GPU %p", address, gpu_address);
-                key->is_locked = 1; /* Set flag that memory was locked by us */
-            }
-        }
+    status = uct_rocm_cma_ptr_to_gpu_ptr(address, &gpu_address,
+                                         length, rocm_md->any_memory,
+                                         &key->is_locked);
+    if (status != HSA_STATUS_SUCCESS) {
+        ucs_error("Failed to convert cma %p for gpu access", address);
+        *memh_p = NULL;
+        ucs_free(key);
+        return UCS_ERR_INVALID_ADDR;
     }
 
     key->length         = length;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.c
index 016c7d843..68f420a2c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.c
@@ -200,12 +200,17 @@ end:
     return status;
 }
 
-int uct_rocm_is_ptr_gpu_accessible(void *ptr, void **gpu_ptr)
+/* Checks if ptr is ROCm allocated, if so then set need_lock to 0 */
+static hsa_status_t uct_rocm_check_ptr_info(void *ptr, void **gpu_ptr,
+                                            int *need_lock)
 {
+    hsa_status_t status;
     hsa_amd_pointer_info_t info;
     info.size = sizeof(hsa_amd_pointer_info_t);
 
-    hsa_status_t status = hsa_amd_pointer_info(ptr, (hsa_amd_pointer_info_t *)&info,
+    *need_lock = 1;
+    *gpu_ptr = 0;
+    status = hsa_amd_pointer_info(ptr, (hsa_amd_pointer_info_t *)&info,
                                                NULL, NULL, NULL);
 
     if (status == HSA_STATUS_SUCCESS) {
@@ -224,26 +229,27 @@ int uct_rocm_is_ptr_gpu_accessible(void *ptr, void **gpu_ptr)
                 else if (info.type == HSA_EXT_POINTER_TYPE_HSA) {
                     /* This is the GPU pointer */
                     *gpu_ptr += ptr - info.agentBaseAddress;
+                    *need_lock = 0;
                 }
                 else {
                     /* Assume that "ptr" is GPU pointer */
                     *gpu_ptr += ptr - info.agentBaseAddress;
+                    *need_lock = 0;
                 }
             }
-
-            ucs_trace("%p is GPU accessible (agent addr %p, Host Base %p)",
+            ucs_trace("%p info (agent addr %p, Host Base %p)",
                       ptr, info.agentBaseAddress, info.hostBaseAddress);
-            return 1;
         }
     }
 
     ucs_trace_func("%p is not GPU accessible", ptr);
-    return 0;
+    return status;
 }
 
 
 hsa_status_t uct_rocm_memory_lock(void *ptr, size_t size, void **gpu_ptr)
 {
+
     /* We need to lock / register memory on all GPUs because we do not know
        the location of other memory */
     hsa_status_t status = hsa_amd_memory_lock(ptr, size,
@@ -257,3 +263,41 @@ hsa_status_t uct_rocm_memory_lock(void *ptr, size_t size, void **gpu_ptr)
 
     return status;
 }
+
+/** Convert pointer to pointer which could be used for GPU access.
+*/
+hsa_status_t uct_rocm_cma_ptr_to_gpu_ptr(void *ptr, void **gpu_address,
+                                         size_t size, int any_memory,
+                                         int *locked)
+{
+    hsa_status_t status;
+    int need_lock = 0;
+
+    /* Assume that we do not need to lock memory */
+    *locked = 0;
+
+    if (!any_memory) {
+        /* We do not want to deal with memory about which
+         * ROCm stack is not aware */
+        ucs_warn("Address %p is not GPU registered", ptr);
+        return HSA_STATUS_ERROR;
+    }
+
+    status = uct_rocm_check_ptr_info(ptr, gpu_address, &need_lock);
+    if (status != HSA_STATUS_SUCCESS || !need_lock)
+        return status;
+
+    /* Register / lock this memory for GPU access */
+    status =  uct_rocm_memory_lock(ptr, size, gpu_address);
+    if (status != HSA_STATUS_SUCCESS) {
+        ucs_error("Could not lock  %p. Status %d", ptr, status);
+        return status;
+    } else {
+        ucs_trace("Lock address %p as GPU %p", ptr, *gpu_address);
+        /* We locked this memory. Set the flag to be aware that
+         * we need to unlock it later */
+        *locked = 1;
+    }
+
+    return status;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.h
index 4205d2157..827cee1bd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/rocm/rocm_common.h
@@ -20,16 +20,19 @@ hsa_status_t uct_rocm_init();
 
 
 /**
- * @brief Check if memory is GPU accessible
+ * @brief Convert pointer to pointer which could be used for GPU access
  *
  * @param [in] ptr Pointer to memory
  * @param [out] gpu_ptr If not NULL return host address to be used for
  *                      GPU access.
- *
- * @return  true if GPU accessible false otherwise
+ * @param [out] indicates if memory is locked for GPU access. If locked
+ *              then hsa_amd_unlock() should be called
+ * @return  HSA status
  *
 */
-int uct_rocm_is_ptr_gpu_accessible(void *ptr, void **gpu_ptr);
+hsa_status_t uct_rocm_cma_ptr_to_gpu_ptr(void *ptr, void **gpu_address,
+                                         size_t size, int any_memory,
+                                         int *locked);
 
 
 /**
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.c
index cce8dad5e..e6910f3ec 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.c
@@ -54,39 +54,145 @@ ucs_status_t uct_sm_ep_get_bcopy(uct_ep_h tl_ep, uct_unpack_callback_t unpack_cb
     return UCS_OK;
 }
 
-ucs_status_t uct_sm_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_sm_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey)
 {
-    uint64_t *ptr = (uint64_t *)(rkey + remote_addr);
-    ucs_atomic_add64(ptr, add);
-    uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_ADD64 [add %"PRIu64"]", add);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
+    uint32_t *ptr = (uint32_t *)(rkey + remote_addr);
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        ucs_atomic_add32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_ADD32 [value %"PRIu32"]", value);
+        break;
+    case UCT_ATOMIC_OP_AND:
+        ucs_atomic_and32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_AND32 [value %"PRIu32"]", value);
+        break;
+    case UCT_ATOMIC_OP_OR:
+        ucs_atomic_or32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_OR32 [value %"PRIu32"]", value);
+        break;
+    case UCT_ATOMIC_OP_XOR:
+        ucs_atomic_xor32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_XOR32 [value %"PRIu32"]", value);
+        break;
+    default:
+        ucs_assertv(0, "incorrect opcode: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(ep, uct_base_ep_t));
     return UCS_OK;
 }
 
-ucs_status_t uct_sm_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp)
+ucs_status_t uct_sm_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey)
 {
     uint64_t *ptr = (uint64_t *)(rkey + remote_addr);
-    *result = ucs_atomic_fadd64(ptr, add);
-    uct_sm_ep_trace_data(remote_addr, rkey,
-    		             "ATOMIC_FADD64 [add %"PRIu64" result %"PRIu64"]",
-                         add, *result);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        ucs_atomic_add64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_ADD64 [value %"PRIu64"]", value);
+        break;
+    case UCT_ATOMIC_OP_AND:
+        ucs_atomic_and64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_AND64 [value %"PRIu64"]", value);
+        break;
+    case UCT_ATOMIC_OP_OR:
+        ucs_atomic_or64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_OR64 [value %"PRIu64"]", value);
+        break;
+    case UCT_ATOMIC_OP_XOR:
+        ucs_atomic_xor64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_XOR64 [value %"PRIu64"]", value);
+        break;
+    default:
+        ucs_assertv(0, "incorrect opcode: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(ep, uct_base_ep_t));
     return UCS_OK;
 }
 
-ucs_status_t uct_sm_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp)
+ucs_status_t uct_sm_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                      uint64_t value, uint64_t *result,
+                                      uint64_t remote_addr, uct_rkey_t rkey,
+                                      uct_completion_t *comp)
 {
     uint64_t *ptr = (uint64_t *)(rkey + remote_addr);
-    *result = ucs_atomic_swap64(ptr, swap);
-    uct_sm_ep_trace_data(remote_addr, rkey,
-                         "ATOMIC_SWAP64 [swap %"PRIu64" result %"PRIu64"]",
-                         swap, *result);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        *result = ucs_atomic_fadd64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FADD64 [value %"PRIu64
+                             " result %"PRIu64"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_AND:
+        *result = ucs_atomic_fand64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FAND64 [value %"PRIu64
+                             " result %"PRIu64"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_OR:
+        *result = ucs_atomic_for64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FOR64 [value %"PRIu64
+                             " result %"PRIu64"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_XOR:
+        *result = ucs_atomic_fxor64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FXOR64 [value %"PRIu64
+                             " result %"PRIu64"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_SWAP:
+        *result = ucs_atomic_swap64(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_SWAP64 [value %"PRIu64
+                             " result %"PRIu64"]", value, *result);
+        break;
+    default:
+        ucs_assertv(0, "incorrect opcode: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(ep, uct_base_ep_t));
+    return UCS_OK;
+}
+
+ucs_status_t uct_sm_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                      uint32_t value, uint32_t *result,
+                                      uint64_t remote_addr, uct_rkey_t rkey,
+                                      uct_completion_t *comp)
+{
+    uint32_t *ptr = (uint32_t *)(rkey + remote_addr);
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        *result = ucs_atomic_fadd32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FADD32 [value %"PRIu32
+                             " result %"PRIu32"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_AND:
+        *result = ucs_atomic_fand32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FAND32 [value %"PRIu32
+                             " result %"PRIu32"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_OR:
+        *result = ucs_atomic_for32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FOR32 [value %"PRIu32
+                             " result %"PRIu32"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_XOR:
+        *result = ucs_atomic_fxor32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FXOR32 [value %"PRIu32
+                             " result %"PRIu32"]", value, *result);
+        break;
+    case UCT_ATOMIC_OP_SWAP:
+        *result = ucs_atomic_swap32(ptr, value);
+        uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_SWAP32 [value %"PRIu32
+                             " result %"PRIu32"]", value, *result);
+        break;
+    default:
+        ucs_assertv(0, "incorrect opcode: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+
+    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(ep, uct_base_ep_t));
     return UCS_OK;
 }
 
@@ -104,40 +210,6 @@ ucs_status_t uct_sm_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare,
     return UCS_OK;
 }
 
-ucs_status_t uct_sm_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey)
-{
-    uint32_t *ptr = (uint32_t *)(rkey + remote_addr);
-    ucs_atomic_add32(ptr, add);
-    uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_ADD32 [add %"PRIu32"]", add);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return UCS_OK;
-}
-
-ucs_status_t uct_sm_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp)
-{
-    uint32_t *ptr = (uint32_t *)(rkey + remote_addr);
-    *result = ucs_atomic_fadd32(ptr, add);
-    uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_FADD32 [add %"PRIu32
-    		             " result %"PRIu32"]", add, *result);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return UCS_OK;
-}
-
-ucs_status_t uct_sm_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp)
-{
-    uint32_t *ptr = (uint32_t *)(rkey + remote_addr);
-    *result = ucs_atomic_swap32(ptr, swap);
-    uct_sm_ep_trace_data(remote_addr, rkey, "ATOMIC_SWAP32 [swap %"PRIu32
-    		             " result %"PRIu32"]", swap, *result);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return UCS_OK;
-}
-
 ucs_status_t uct_sm_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare,
                                       uint32_t swap, uint64_t remote_addr,
                                       uct_rkey_t rkey, uint32_t *result,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.h
index 9940b3b07..78454fbbe 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/base/sm_ep.h
@@ -20,30 +20,25 @@ ucs_status_t uct_sm_ep_get_bcopy(uct_ep_h ep, uct_unpack_callback_t unpack_cb,
                                  uint64_t remote_addr, uct_rkey_t rkey,
                                  uct_completion_t *comp);
 
-ucs_status_t uct_sm_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey);
-ucs_status_t uct_sm_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp);
-ucs_status_t uct_sm_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint64_t *result, uct_completion_t *comp);
 ucs_status_t uct_sm_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare,
                                       uint64_t swap, uint64_t remote_addr,
                                       uct_rkey_t rkey, uint64_t *result,
                                       uct_completion_t *comp);
-
-ucs_status_t uct_sm_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                    uint64_t remote_addr, uct_rkey_t rkey);
-ucs_status_t uct_sm_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp);
-ucs_status_t uct_sm_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                     uint64_t remote_addr, uct_rkey_t rkey,
-                                     uint32_t *result, uct_completion_t *comp);
 ucs_status_t uct_sm_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare,
                                       uint32_t swap, uint64_t remote_addr,
                                       uct_rkey_t rkey, uint32_t *result,
                                       uct_completion_t *comp);
+ucs_status_t uct_sm_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_sm_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                      uint64_t value, uint64_t *result,
+                                      uint64_t remote_addr, uct_rkey_t rkey,
+                                      uct_completion_t *comp);
+ucs_status_t uct_sm_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                     uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_sm_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                      uint32_t value, uint32_t *result,
+                                      uint64_t remote_addr, uct_rkey_t rkey,
+                                      uct_completion_t *comp);
 
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/knem/knem_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/knem/knem_md.c
index 34f42e681..ba2865ccd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/knem/knem_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/knem/knem_md.c
@@ -9,6 +9,7 @@
 #include "knem_io.h"
 
 #include <ucs/arch/cpu.h>
+#include <ucm/api/ucm.h>
 
 static ucs_config_field_t uct_knem_md_config_table[] = {
     {"", "", NULL,
@@ -278,7 +279,8 @@ static uct_md_ops_t uct_knem_md_rcache_ops = {
 
 
 static ucs_status_t uct_knem_rcache_mem_reg_cb(void *context, ucs_rcache_t *rcache,
-                                               void *arg, ucs_rcache_region_t *rregion)
+                                               void *arg, ucs_rcache_region_t *rregion,
+                                               uint16_t rcache_mem_reg_flags)
 {
     uct_knem_rcache_region_t *region = ucs_derived_of(rregion, uct_knem_rcache_region_t);
     uct_knem_md_t *md                = context;
@@ -286,7 +288,9 @@ static ucs_status_t uct_knem_rcache_mem_reg_cb(void *context, ucs_rcache_t *rcac
 
     return uct_knem_mem_reg_internal(&md->super, (void*)region->super.super.start,
                                      region->super.super.end - region->super.super.start,
-                                     *flags, 1, &region->key);
+                                     *flags,
+                                     rcache_mem_reg_flags & UCS_RCACHE_MEM_REG_HIDE_ERRORS,
+                                     &region->key);
 }
 
 static void uct_knem_rcache_mem_dereg_cb(void *context, ucs_rcache_t *rcache,
@@ -346,6 +350,7 @@ static ucs_status_t uct_knem_md_open(const char *md_name,
         rcache_params.region_struct_size = sizeof(uct_knem_rcache_region_t);
         rcache_params.alignment          = md_config->rcache.alignment;
         rcache_params.max_alignment      = ucs_get_page_size();
+        rcache_params.ucm_events         = UCM_EVENT_VM_UNMAPPED;
         rcache_params.ucm_event_priority = md_config->rcache.event_prio;
         rcache_params.context            = knem_md;
         rcache_params.ops                = &uct_knem_rcache_ops;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.c
index 547ff838e..5d3699348 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.c
@@ -220,6 +220,7 @@ uct_mm_ep_am_common_send(unsigned is_short, uct_mm_ep_t *ep, uct_mm_iface_t *ifa
 
     UCT_CHECK_AM_ID(am_id);
 
+retry:
     head = ep->fifo_ctl->head;
     /* check if there is room in the remote process's receive FIFO to write */
     if (!UCT_MM_EP_IS_ABLE_TO_SEND(head, ep->cached_tail, iface->config.fifo_size)) {
@@ -240,9 +241,9 @@ uct_mm_ep_am_common_send(unsigned is_short, uct_mm_ep_t *ep, uct_mm_iface_t *ifa
 
     status = uct_mm_ep_get_remote_elem(ep, head, &elem);
     if (status != UCS_OK) {
-        ucs_trace_poll("couldn't get an available FIFO element");
-        UCS_STATS_UPDATE_COUNTER(ep->super.stats, UCT_EP_STAT_NO_RES, 1);
-        return status;
+        ucs_assert(status == UCS_ERR_NO_RESOURCE);
+        ucs_trace_poll("couldn't get an available FIFO element. retrying");
+        goto retry;
     }
 
     if (is_short) {
@@ -329,13 +330,15 @@ static inline int uct_mm_ep_has_tx_resources(uct_mm_ep_t *ep)
                                      iface->config.fifo_size);
 }
 
-ucs_status_t uct_mm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n)
+ucs_status_t uct_mm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                   unsigned flags)
 {
     uct_mm_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_mm_iface_t);
     uct_mm_ep_t *ep = ucs_derived_of(tl_ep, uct_mm_ep_t);
 
     /* check if resources became available */
     if (uct_mm_ep_has_tx_resources(ep)) {
+        ucs_assert(ucs_arbiter_group_is_empty(&ep->arb_group));
         return UCS_ERR_BUSY;
     }
 
@@ -360,8 +363,7 @@ ucs_arbiter_cb_result_t uct_mm_ep_process_pending(ucs_arbiter_t *arbiter,
 
     /* update the local tail with its actual value from the remote peer
      * making sure that the pending sends would use the real tail value */
-    ucs_memory_cpu_load_fence();
-    ep->cached_tail = ep->fifo_ctl->tail;
+    uct_mm_ep_update_cached_tail(ep);
 
     if (!uct_mm_ep_has_tx_resources(ep)) {
         return UCS_ARBITER_CB_RESULT_RESCHED_GROUP;
@@ -417,10 +419,15 @@ ucs_status_t uct_mm_ep_flush(uct_ep_h tl_ep, unsigned flags,
 {
     uct_mm_ep_t *ep = ucs_derived_of(tl_ep, uct_mm_ep_t);
 
-    uct_mm_ep_update_cached_tail(ep);
-
     if (!uct_mm_ep_has_tx_resources(ep)) {
-        return UCS_ERR_NO_RESOURCE;
+        if (!ucs_arbiter_group_is_empty(&ep->arb_group)) {
+            return UCS_ERR_NO_RESOURCE;
+        } else {
+            uct_mm_ep_update_cached_tail(ep);
+            if (!uct_mm_ep_has_tx_resources(ep)) {
+                return UCS_ERR_NO_RESOURCE;
+            }
+        }
     }
 
     ucs_memory_cpu_store_fence();
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.h
index e6e7d22f0..45b987907 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_ep.h
@@ -52,7 +52,8 @@ ssize_t uct_mm_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id, uct_pack_callback_t pack_
 ucs_status_t uct_mm_ep_flush(uct_ep_h tl_ep, unsigned flags,
                              uct_completion_t *comp);
 
-ucs_status_t uct_mm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n);
+ucs_status_t uct_mm_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                   unsigned flags);
 
 void uct_mm_ep_pending_purge(uct_ep_h ep, uct_pending_purge_callback_t cb,
                              void *arg);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_iface.c
index 632972861..09d1a05af 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_iface.c
@@ -117,14 +117,6 @@ static ucs_status_t uct_mm_iface_query(uct_iface_h tl_iface,
     iface_attr->max_conn_priv           = 0;
     iface_attr->cap.flags               = UCT_IFACE_FLAG_PUT_SHORT           |
                                           UCT_IFACE_FLAG_PUT_BCOPY           |
-                                          UCT_IFACE_FLAG_ATOMIC_ADD32        |
-                                          UCT_IFACE_FLAG_ATOMIC_ADD64        |
-                                          UCT_IFACE_FLAG_ATOMIC_FADD64       |
-                                          UCT_IFACE_FLAG_ATOMIC_FADD32       |
-                                          UCT_IFACE_FLAG_ATOMIC_SWAP64       |
-                                          UCT_IFACE_FLAG_ATOMIC_SWAP32       |
-                                          UCT_IFACE_FLAG_ATOMIC_CSWAP64      |
-                                          UCT_IFACE_FLAG_ATOMIC_CSWAP32      |
                                           UCT_IFACE_FLAG_ATOMIC_CPU          |
                                           UCT_IFACE_FLAG_GET_BCOPY           |
                                           UCT_IFACE_FLAG_AM_SHORT            |
@@ -135,6 +127,19 @@ static ucs_status_t uct_mm_iface_query(uct_iface_h tl_iface,
                                           UCT_IFACE_FLAG_EVENT_RECV_SIG      |
                                           UCT_IFACE_FLAG_CONNECT_TO_IFACE;
 
+    iface_attr->cap.atomic32.op_flags   =
+    iface_attr->cap.atomic64.op_flags   = UCS_BIT(UCT_ATOMIC_OP_ADD)         |
+                                          UCS_BIT(UCT_ATOMIC_OP_AND)         |
+                                          UCS_BIT(UCT_ATOMIC_OP_OR)          |
+                                          UCS_BIT(UCT_ATOMIC_OP_XOR);
+    iface_attr->cap.atomic32.fop_flags  =
+    iface_attr->cap.atomic64.fop_flags  = UCS_BIT(UCT_ATOMIC_OP_ADD)         |
+                                          UCS_BIT(UCT_ATOMIC_OP_AND)         |
+                                          UCS_BIT(UCT_ATOMIC_OP_OR)          |
+                                          UCS_BIT(UCT_ATOMIC_OP_XOR)         |
+                                          UCS_BIT(UCT_ATOMIC_OP_SWAP)        |
+                                          UCS_BIT(UCT_ATOMIC_OP_CSWAP);
+
     iface_attr->latency.overhead        = 80e-9; /* 80 ns */
     iface_attr->latency.growth          = 0;
     iface_attr->bandwidth               = 6911 * 1024.0 * 1024.0;
@@ -302,14 +307,12 @@ static uct_iface_ops_t uct_mm_iface_ops = {
     .ep_get_bcopy             = uct_sm_ep_get_bcopy,
     .ep_am_short              = uct_mm_ep_am_short,
     .ep_am_bcopy              = uct_mm_ep_am_bcopy,
-    .ep_atomic_add64          = uct_sm_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_sm_ep_atomic_fadd64,
     .ep_atomic_cswap64        = uct_sm_ep_atomic_cswap64,
-    .ep_atomic_swap64         = uct_sm_ep_atomic_swap64,
-    .ep_atomic_add32          = uct_sm_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_sm_ep_atomic_fadd32,
+    .ep_atomic64_post         = uct_sm_ep_atomic64_post,
+    .ep_atomic64_fetch        = uct_sm_ep_atomic64_fetch,
     .ep_atomic_cswap32        = uct_sm_ep_atomic_cswap32,
-    .ep_atomic_swap32         = uct_sm_ep_atomic_swap32,
+    .ep_atomic32_post         = uct_sm_ep_atomic32_post,
+    .ep_atomic32_fetch        = uct_sm_ep_atomic32_fetch,
     .ep_pending_add           = uct_mm_ep_pending_add,
     .ep_pending_purge         = uct_mm_ep_pending_purge,
     .ep_flush                 = uct_mm_ep_flush,
@@ -366,8 +369,8 @@ ucs_status_t uct_mm_allocate_fifo_mem(uct_mm_iface_t *iface,
     size_to_alloc = UCT_MM_GET_FIFO_SIZE(iface);
 
     status = uct_mm_md_mapper_ops(md)->alloc(md, &size_to_alloc, config->hugetlb_mode,
-                                             0, &iface->shared_mem, &iface->fifo_mm_id,
-                                             &iface->path UCS_MEMTRACK_NAME("mm fifo"));
+                                             0, "mm fifo", &iface->shared_mem,
+                                             &iface->fifo_mm_id, &iface->path);
     if (status != UCS_OK) {
         ucs_error("Failed to allocate memory for the receive FIFO in mm. size: %zu : %m",
                    size_to_alloc);
@@ -461,6 +464,11 @@ static UCS_CLASS_INIT_FUNC(uct_mm_iface_t, uct_md_h md, uct_worker_h worker,
 
     ucs_trace_func("Creating an MM iface=%p worker=%p", self, worker);
 
+    if (ucs_derived_of(worker, uct_priv_worker_t)->thread_mode == UCS_THREAD_MODE_MULTI) {
+        ucs_error("Shared memory transport does not support multi-threaded worker");
+        return UCS_ERR_INVALID_PARAM;
+    }
+
     /* check that the fifo size, from the user, is a power of two and bigger than 1 */
     if ((mm_config->fifo_size <= 1) || ucs_is_pow2(mm_config->fifo_size) != 1) {
         ucs_error("The MM FIFO size must be a power of two and bigger than 1.");
@@ -491,7 +499,7 @@ static UCS_CLASS_INIT_FUNC(uct_mm_iface_t, uct_md_h md, uct_worker_h worker,
                                      (mm_config->fifo_size * mm_config->release_fifo_factor),
                                      1)));
     self->fifo_mask                = mm_config->fifo_size - 1;
-    self->fifo_shift               = ucs_count_zero_bits(mm_config->fifo_size);
+    self->fifo_shift               = ucs_count_trailing_zero_bits(mm_config->fifo_size);
     self->rx_headroom              = params->rx_headroom;
     self->release_desc.cb          = uct_mm_iface_release_desc;
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.c
index e6942ff64..e46ff50df 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.c
@@ -23,7 +23,8 @@ ucs_config_field_t uct_mm_md_config_table[] = {
 };
 
 ucs_status_t uct_mm_mem_alloc(uct_md_h md, size_t *length_p, void **address_p,
-                              unsigned flags, uct_mem_h *memh_p UCS_MEMTRACK_ARG)
+                              unsigned flags, const char *alloc_name,
+                              uct_mem_h *memh_p)
 {
     ucs_status_t status;
     uct_mm_seg_t *seg;
@@ -36,8 +37,8 @@ ucs_status_t uct_mm_mem_alloc(uct_md_h md, size_t *length_p, void **address_p,
 
 
     status = uct_mm_md_mapper_ops(md)->alloc(md, length_p, UCS_TRY, flags,
-                                             address_p, &seg->mmid, &seg->path
-                                             UCS_MEMTRACK_VAL);
+                                             alloc_name, address_p, &seg->mmid,
+                                             &seg->path);
     if (status != UCS_OK) {
         ucs_free(seg);
         return status;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.h
index d3557a15a..02b8b40a0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_md.h
@@ -49,8 +49,8 @@ typedef struct uct_mm_mapper_ops {
     ucs_status_t (*dereg)(uct_mm_id_t mm_id);
 
     ucs_status_t (*alloc)(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
-                          unsigned flags, void **address_p, uct_mm_id_t *mmid_p,
-                          const char **path_p UCS_MEMTRACK_ARG);
+                          unsigned flags, const char *alloc_name, void **address_p,
+                          uct_mm_id_t *mmid_p, const char **path_p);
 
     ucs_status_t (*attach)(uct_mm_id_t mmid, size_t length,
                            void *remote_address, void **address, uint64_t *cookie,
@@ -142,7 +142,8 @@ typedef struct uct_mm_md {
 
 
 ucs_status_t uct_mm_mem_alloc(uct_md_h md, size_t *length_p, void **address_p,
-                              unsigned flags, uct_mem_h *memh_p UCS_MEMTRACK_ARG);
+                              unsigned flags, const char *alloc_name,
+                              uct_mem_h *memh_p);
 
 ucs_status_t uct_mm_mem_free(uct_md_h md, uct_mem_h memh);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_posix.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_posix.c
index 6d5ca90ac..278d397ed 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_posix.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_posix.c
@@ -250,8 +250,8 @@ out:
 
 static ucs_status_t
 uct_posix_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
-                unsigned md_map_flags, void **address_p, uct_mm_id_t *mmid_p,
-                const char **path_p UCS_MEMTRACK_ARG)
+                unsigned md_map_flags, const char *alloc_name, void **address_p,
+                uct_mm_id_t *mmid_p, const char **path_p)
 {
     ucs_status_t status;
     int shm_fd = -1;
@@ -370,7 +370,8 @@ uct_posix_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
            goto out_ok;
        }
 
-       ucs_debug("mm failed to allocate %zu bytes without hugetlb %m", *length_p);
+       ucs_debug("mm failed to allocate %zu bytes without hugetlb for %s: %m",
+                 *length_p, alloc_name);
     }
 
 err_shm_unlink:
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_sysv.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_sysv.c
index 1cffaf712..4bcb94be7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_sysv.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_sysv.c
@@ -27,8 +27,8 @@ static ucs_config_field_t uct_sysv_md_config_table[] = {
 
 static ucs_status_t
 uct_sysv_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
-               unsigned md_map_flags, void **address_p, uct_mm_id_t *mmid_p,
-               const char **path_p UCS_MEMTRACK_ARG)
+               unsigned md_map_flags, const char *alloc_name, void **address_p,
+               uct_mm_id_t *mmid_p, const char **path_p)
 {
     ucs_status_t status = UCS_ERR_NO_MEMORY;
     int flags, shmid = 0;
@@ -47,7 +47,7 @@ uct_sysv_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
 
     if (hugetlb != UCS_NO) {
         status = ucs_sysv_alloc(length_p, (*length_p) * 2, address_p,
-                                flags | SHM_HUGETLB, &shmid UCS_MEMTRACK_VAL);
+                                flags | SHM_HUGETLB, alloc_name, &shmid);
         if (status == UCS_OK) {
             goto out_ok;
         }
@@ -56,8 +56,8 @@ uct_sysv_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
     }
 
     if (hugetlb != UCS_YES) {
-        status = ucs_sysv_alloc(length_p, SIZE_MAX, address_p, flags , &shmid
-                                UCS_MEMTRACK_VAL);
+        status = ucs_sysv_alloc(length_p, SIZE_MAX, address_p, flags, alloc_name,
+                                &shmid);
         if (status == UCS_OK) {
             goto out_ok;
         }
@@ -66,7 +66,7 @@ uct_sysv_alloc(uct_md_h md, size_t *length_p, ucs_ternary_value_t hugetlb,
     }
 
 err:
-    ucs_error("failed to allocate %zu bytes with mm", *length_p);
+    ucs_error("failed to allocate %zu bytes with mm for %s", *length_p, alloc_name);
     return status;
 
 out_ok:
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_xpmem.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_xpmem.c
index 4394d1750..f323a6611 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_xpmem.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/mm/mm_xpmem.c
@@ -147,9 +147,9 @@ static ucs_status_t uct_xpmem_detach(uct_mm_remote_seg_t *mm_desc)
 
 static ucs_status_t uct_xpmem_alloc(uct_md_h md, size_t *length_p,
                                     ucs_ternary_value_t hugetlb,
-                                    unsigned md_map_flags, void **address_p,
-                                    uct_mm_id_t *mmid_p, const char **path_p
-                                    UCS_MEMTRACK_ARG)
+                                    unsigned md_map_flags, const char *alloc_name,
+                                    void **address_p, uct_mm_id_t *mmid_p,
+                                    const char **path_p)
 {
     ucs_status_t status;
     int mmap_flags;
@@ -170,11 +170,13 @@ static ucs_status_t uct_xpmem_alloc(uct_md_h md, size_t *length_p,
     /* TBD: any ideas for better allocation */
     status = ucs_mmap_alloc(length_p, address_p, mmap_flags UCS_MEMTRACK_VAL);
     if (status != UCS_OK) {
-        ucs_error("Failed to allocate %zu bytes of memory", *length_p);
+        ucs_error("Failed to allocate %zu bytes of memory for %s", *length_p,
+                  alloc_name);
         goto out;
     }
 
-    ucs_trace("xpmem allocated address %p length %zu", *address_p, *length_p);
+    ucs_trace("xpmem allocated address %p length %zu for %s", *address_p,
+              *length_p, alloc_name);
 
     status = uct_xmpem_reg(*address_p, *length_p, mmid_p);
     if (UCS_OK != status) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.c
new file mode 100644
index 000000000..889855964
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.c
@@ -0,0 +1,386 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#include "self.h"
+
+#include <uct/sm/base/sm_ep.h>
+#include <ucs/type/class.h>
+#include <ucs/sys/string.h>
+#include <ucs/arch/cpu.h>
+#include "self.h"
+
+
+#define UCT_SELF_NAME "self"
+
+#define UCT_SELF_IFACE_SEND_BUFFER_GET(_iface) \
+    ({ /* use buffers from mpool to avoid buffer re-usage */ \
+       /* till operation completes */ \
+        void *ptr = ucs_mpool_get_inline(&(_iface)->msg_mp); \
+        if (ucs_unlikely(ptr == NULL)) { \
+                return UCS_ERR_NO_MEMORY; \
+        } \
+        ptr; \
+    })
+
+
+/* Forward declarations */
+static uct_iface_ops_t uct_self_iface_ops;
+static uct_md_component_t uct_self_md;
+
+
+static ucs_status_t uct_self_iface_query(uct_iface_h tl_iface, uct_iface_attr_t *attr)
+{
+    uct_self_iface_t *iface = ucs_derived_of(tl_iface, uct_self_iface_t);
+
+    ucs_trace_func("iface=%p", iface);
+    memset(attr, 0, sizeof(*attr));
+
+    attr->iface_addr_len         = sizeof(uct_self_iface_addr_t);
+    attr->device_addr_len        = 0;
+    attr->ep_addr_len            = 0;
+    attr->max_conn_priv          = 0;
+    attr->cap.flags              = UCT_IFACE_FLAG_CONNECT_TO_IFACE |
+                                   UCT_IFACE_FLAG_AM_SHORT         |
+                                   UCT_IFACE_FLAG_AM_BCOPY         |
+                                   UCT_IFACE_FLAG_PUT_SHORT        |
+                                   UCT_IFACE_FLAG_PUT_BCOPY        |
+                                   UCT_IFACE_FLAG_GET_BCOPY        |
+                                   UCT_IFACE_FLAG_ATOMIC_CPU       |
+                                   UCT_IFACE_FLAG_PENDING          |
+                                   UCT_IFACE_FLAG_CB_SYNC          |
+                                   UCT_IFACE_FLAG_EP_CHECK;
+
+    attr->cap.atomic32.op_flags   =
+    attr->cap.atomic64.op_flags   = UCS_BIT(UCT_ATOMIC_OP_ADD)     |
+                                    UCS_BIT(UCT_ATOMIC_OP_AND)     |
+                                    UCS_BIT(UCT_ATOMIC_OP_OR)      |
+                                    UCS_BIT(UCT_ATOMIC_OP_XOR);
+    attr->cap.atomic32.fop_flags  =
+    attr->cap.atomic64.fop_flags  = UCS_BIT(UCT_ATOMIC_OP_ADD)     |
+                                    UCS_BIT(UCT_ATOMIC_OP_AND)     |
+                                    UCS_BIT(UCT_ATOMIC_OP_OR)      |
+                                    UCS_BIT(UCT_ATOMIC_OP_XOR)     |
+                                    UCS_BIT(UCT_ATOMIC_OP_SWAP)    |
+                                    UCS_BIT(UCT_ATOMIC_OP_CSWAP);
+
+    attr->cap.put.max_short       = UINT_MAX;
+    attr->cap.put.max_bcopy       = SIZE_MAX;
+    attr->cap.put.min_zcopy       = 0;
+    attr->cap.put.max_zcopy       = 0;
+    attr->cap.put.opt_zcopy_align = 1;
+    attr->cap.put.align_mtu       = attr->cap.put.opt_zcopy_align;
+    attr->cap.put.max_iov         = 1;
+
+    attr->cap.get.max_bcopy       = SIZE_MAX;
+    attr->cap.get.min_zcopy       = 0;
+    attr->cap.get.max_zcopy       = 0;
+    attr->cap.get.opt_zcopy_align = 1;
+    attr->cap.get.align_mtu       = attr->cap.get.opt_zcopy_align;
+    attr->cap.get.max_iov         = 1;
+
+    attr->cap.am.max_short        = iface->send_size;
+    attr->cap.am.max_bcopy        = iface->send_size;
+    attr->cap.am.min_zcopy        = 0;
+    attr->cap.am.max_zcopy        = 0;
+    attr->cap.am.opt_zcopy_align  = 1;
+    attr->cap.am.align_mtu        = attr->cap.am.opt_zcopy_align;
+    attr->cap.am.max_hdr          = 0;
+    attr->cap.am.max_iov          = 1;
+
+    attr->latency.overhead        = 0;
+    attr->latency.growth          = 0;
+    attr->bandwidth               = 6911 * 1024.0 * 1024.0;
+    attr->overhead                = 10e-9;
+    attr->priority                = 0;
+
+    return UCS_OK;
+}
+
+static ucs_status_t uct_self_iface_get_address(uct_iface_h tl_iface,
+                                               uct_iface_addr_t *addr)
+{
+    const uct_self_iface_t *iface = ucs_derived_of(tl_iface, uct_self_iface_t);
+
+    *(uct_self_iface_addr_t*)addr = iface->id;
+    return UCS_OK;
+}
+
+static int uct_self_iface_is_reachable(const uct_iface_h tl_iface,
+                                       const uct_device_addr_t *dev_addr,
+                                       const uct_iface_addr_t *iface_addr)
+{
+    const uct_self_iface_t     *iface = ucs_derived_of(tl_iface, uct_self_iface_t);
+    const uct_self_iface_addr_t *addr = (const uct_self_iface_addr_t*)iface_addr;
+
+    return (addr != NULL) && (iface->id == *addr);
+}
+
+static void uct_self_iface_sendrecv_am(uct_self_iface_t *iface, uint8_t am_id,
+                                       void *buffer, size_t length, const char *title)
+{
+    ucs_status_t UCS_V_UNUSED status;
+
+    uct_iface_trace_am(&iface->super, UCT_AM_TRACE_TYPE_SEND, am_id,
+                       buffer, length, "TX: AM_%s", title);
+    uct_iface_trace_am(&iface->super, UCT_AM_TRACE_TYPE_RECV, am_id,
+                       buffer, length, "RX: AM_%s", title);
+
+    status = uct_iface_invoke_am(&iface->super, am_id, buffer,
+                                 length, 0);
+    ucs_assert(status == UCS_OK);
+    ucs_mpool_put_inline(buffer);
+}
+
+static ucs_mpool_ops_t uct_self_iface_mpool_ops = {
+    .chunk_alloc   = ucs_mpool_chunk_malloc,
+    .chunk_release = ucs_mpool_chunk_free,
+    .obj_init      = NULL,
+    .obj_cleanup   = NULL
+};
+
+static UCS_CLASS_DEFINE_DELETE_FUNC(uct_self_iface_t, uct_iface_t);
+
+static UCS_CLASS_INIT_FUNC(uct_self_iface_t, uct_md_h md, uct_worker_h worker,
+                           const uct_iface_params_t *params,
+                           const uct_iface_config_t *tl_config)
+{
+    ucs_status_t status;
+
+    if (!(params->open_mode & UCT_IFACE_OPEN_MODE_DEVICE)) {
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    if (ucs_derived_of(worker, uct_priv_worker_t)->thread_mode == UCS_THREAD_MODE_MULTI) {
+        ucs_error("Self transport does not support multi-threaded worker");
+        return UCS_ERR_INVALID_PARAM;
+    }
+
+    if (strcmp(params->mode.device.dev_name, UCT_SELF_NAME) != 0) {
+        ucs_error("No device was found: %s", params->mode.device.dev_name);
+        return UCS_ERR_NO_DEVICE;
+    }
+
+    UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &uct_self_iface_ops, md, worker,
+                              params, tl_config UCS_STATS_ARG(params->stats_root)
+                              UCS_STATS_ARG(UCT_SELF_NAME));
+
+    self->id          = ucs_generate_uuid((uintptr_t)self);
+    self->send_size   = tl_config->max_bcopy;
+
+    status = ucs_mpool_init(&self->msg_mp, 0, self->send_size, 0,
+                            UCS_SYS_CACHE_LINE_SIZE,
+                            2, /* 2 elements are enough for most of communications */
+                            UINT_MAX, &uct_self_iface_mpool_ops, "self_msg_desc");
+
+    if (UCS_STATUS_IS_ERR(status)) {
+        return status;
+    }
+
+    ucs_debug("created self iface id 0x%lx send_size %zu", self->id,
+              self->send_size);
+    return UCS_OK;
+}
+
+static UCS_CLASS_CLEANUP_FUNC(uct_self_iface_t)
+{
+    ucs_mpool_cleanup(&self->msg_mp, 1);
+}
+
+UCS_CLASS_DEFINE(uct_self_iface_t, uct_base_iface_t);
+static UCS_CLASS_DEFINE_NEW_FUNC(uct_self_iface_t, uct_iface_t, uct_md_h,
+                                 uct_worker_h, const uct_iface_params_t*,
+                                 const uct_iface_config_t*);
+
+static ucs_status_t uct_self_query_tl_resources(uct_md_h md,
+                                                uct_tl_resource_desc_t **resource_p,
+                                                unsigned *num_resources_p)
+{
+    uct_tl_resource_desc_t *resource = 0;
+
+    ucs_trace_func("md=%p", md);
+
+    resource = ucs_calloc(1, sizeof(*resource), "resource desc");
+    if (NULL == resource) {
+        ucs_error("Failed to allocate memory");
+        return UCS_ERR_NO_MEMORY;
+    }
+
+    ucs_snprintf_zero(resource->tl_name, sizeof(resource->tl_name), "%s",
+                      UCT_SELF_NAME);
+    ucs_snprintf_zero(resource->dev_name, sizeof(resource->dev_name), "%s",
+                      UCT_SELF_NAME);
+    resource->dev_type = UCT_DEVICE_TYPE_SELF;
+
+    *num_resources_p = 1;
+    *resource_p      = resource;
+    return UCS_OK;
+}
+
+static UCS_CLASS_INIT_FUNC(uct_self_ep_t, uct_iface_t *tl_iface,
+                           const uct_device_addr_t *dev_addr,
+                           const uct_iface_addr_t *iface_addr)
+{
+    uct_self_iface_t *iface = ucs_derived_of(tl_iface, uct_self_iface_t);
+
+    UCS_CLASS_CALL_SUPER_INIT(uct_base_ep_t, &iface->super)
+    return UCS_OK;
+}
+
+static UCS_CLASS_CLEANUP_FUNC(uct_self_ep_t)
+{
+}
+
+UCS_CLASS_DEFINE(uct_self_ep_t, uct_base_ep_t);
+UCS_CLASS_DEFINE_NEW_FUNC(uct_self_ep_t, uct_ep_t, uct_iface_t *,
+                          const uct_device_addr_t *, const uct_iface_addr_t *);
+UCS_CLASS_DEFINE_DELETE_FUNC(uct_self_ep_t, uct_ep_t);
+
+
+ucs_status_t uct_self_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t header,
+                                  const void *payload, unsigned length)
+{
+    uct_self_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_self_iface_t);
+    uct_self_ep_t UCS_V_UNUSED *ep = ucs_derived_of(tl_ep, uct_self_ep_t);
+    size_t total_length;
+    void *send_buffer;
+
+    UCT_CHECK_AM_ID(id);
+
+    total_length = length + sizeof(header);
+    UCT_CHECK_LENGTH(total_length, 0, iface->send_size, "am_short");
+
+    send_buffer = UCT_SELF_IFACE_SEND_BUFFER_GET(iface);
+    *(uint64_t*)send_buffer = header;
+    memcpy(send_buffer + sizeof(uint64_t), payload, length);
+
+    UCT_TL_EP_STAT_OP(&ep->super, AM, SHORT, total_length);
+    uct_self_iface_sendrecv_am(iface, id, send_buffer, total_length, "SHORT");
+    return UCS_OK;
+}
+
+ssize_t uct_self_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
+                             uct_pack_callback_t pack_cb, void *arg,
+                             unsigned flags)
+{
+    uct_self_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_self_iface_t);
+    uct_self_ep_t UCS_V_UNUSED *ep = ucs_derived_of(tl_ep, uct_self_ep_t);
+    size_t length;
+    void *send_buffer;
+
+    UCT_CHECK_AM_ID(id);
+
+    send_buffer = UCT_SELF_IFACE_SEND_BUFFER_GET(iface);
+    length = pack_cb(send_buffer, arg);
+
+    UCT_CHECK_LENGTH(length, 0, iface->send_size, "am_bcopy");
+    UCT_TL_EP_STAT_OP(&ep->super, AM, BCOPY, length);
+
+    uct_self_iface_sendrecv_am(iface, id, send_buffer, length, "BCOPY");
+    return length;
+}
+
+static uct_iface_ops_t uct_self_iface_ops = {
+    .ep_put_short             = uct_sm_ep_put_short,
+    .ep_put_bcopy             = uct_sm_ep_put_bcopy,
+    .ep_get_bcopy             = uct_sm_ep_get_bcopy,
+    .ep_am_short              = uct_self_ep_am_short,
+    .ep_am_bcopy              = uct_self_ep_am_bcopy,
+    .ep_atomic_cswap64        = uct_sm_ep_atomic_cswap64,
+    .ep_atomic64_post         = uct_sm_ep_atomic64_post,
+    .ep_atomic64_fetch        = uct_sm_ep_atomic64_fetch,
+    .ep_atomic_cswap32        = uct_sm_ep_atomic_cswap32,
+    .ep_atomic32_post         = uct_sm_ep_atomic32_post,
+    .ep_atomic32_fetch        = uct_sm_ep_atomic32_fetch,
+    .ep_flush                 = uct_base_ep_flush,
+    .ep_fence                 = uct_base_ep_fence,
+    .ep_check                 = ucs_empty_function_return_success,
+    .ep_pending_add           = ucs_empty_function_return_busy,
+    .ep_pending_purge         = ucs_empty_function,
+    .ep_create_connected      = UCS_CLASS_NEW_FUNC_NAME(uct_self_ep_t),
+    .ep_destroy               = UCS_CLASS_DELETE_FUNC_NAME(uct_self_ep_t),
+    .iface_flush              = uct_base_iface_flush,
+    .iface_fence              = uct_base_iface_fence,
+    .iface_progress_enable    = ucs_empty_function,
+    .iface_progress_disable   = ucs_empty_function,
+    .iface_progress           = ucs_empty_function_return_zero,
+    .iface_close              = UCS_CLASS_DELETE_FUNC_NAME(uct_self_iface_t),
+    .iface_query              = uct_self_iface_query,
+    .iface_get_device_address = ucs_empty_function_return_success,
+    .iface_get_address        = uct_self_iface_get_address,
+    .iface_is_reachable       = uct_self_iface_is_reachable
+};
+
+UCT_TL_COMPONENT_DEFINE(uct_self_tl, uct_self_query_tl_resources, uct_self_iface_t,
+                        UCT_SELF_NAME, "SELF_", uct_iface_config_table, uct_iface_config_t);
+UCT_MD_REGISTER_TL(&uct_self_md, &uct_self_tl);
+
+static ucs_status_t uct_self_md_query(uct_md_h md, uct_md_attr_t *attr)
+{
+    /* Dummy memory registration provided. No real memory handling exists */
+    attr->cap.flags         = UCT_MD_FLAG_REG |
+                              UCT_MD_FLAG_NEED_RKEY; /* TODO ignore rkey in rma/amo ops */
+    attr->cap.reg_mem_types = UCS_BIT(UCT_MD_MEM_TYPE_HOST);
+    attr->cap.mem_type      = UCT_MD_MEM_TYPE_HOST;
+    attr->cap.max_alloc     = 0;
+    attr->cap.max_reg       = ULONG_MAX;
+    attr->rkey_packed_size  = 0; /* uct_md_query adds UCT_MD_COMPONENT_NAME_MAX to this */
+    attr->reg_cost.overhead = 0;
+    attr->reg_cost.growth   = 0;
+    memset(&attr->local_cpus, 0xff, sizeof(attr->local_cpus));
+    return UCS_OK;
+}
+
+static ucs_status_t uct_self_query_md_resources(uct_md_resource_desc_t **resources_p,
+                                                unsigned *num_resources_p)
+{
+    return uct_single_md_resource(&uct_self_md, resources_p, num_resources_p);
+}
+
+static ucs_status_t uct_self_mem_reg(uct_md_h md, void *address, size_t length,
+                                     unsigned flags, uct_mem_h *memh_p)
+{
+    /* We have to emulate memory registration. Return dummy pointer */
+    *memh_p = (void *) 0xdeadbeef;
+    return UCS_OK;
+}
+
+static ucs_status_t uct_self_md_open(const char *md_name, const uct_md_config_t *md_config,
+                                     uct_md_h *md_p)
+{
+    static uct_md_ops_t md_ops = {
+        .close        = (void*)ucs_empty_function,
+        .query        = uct_self_md_query,
+        .mkey_pack    = ucs_empty_function_return_success,
+        .mem_reg      = uct_self_mem_reg,
+        .mem_dereg    = ucs_empty_function_return_success,
+        .is_mem_type_owned = (void *)ucs_empty_function_return_zero,
+    };
+    static uct_md_t md = {
+        .ops          = &md_ops,
+        .component    = &uct_self_md
+    };
+
+    *md_p = &md;
+    return UCS_OK;
+}
+
+static ucs_status_t uct_self_md_rkey_unpack(uct_md_component_t *mdc,
+                                            const void *rkey_buffer, uct_rkey_t *rkey_p,
+                                            void **handle_p)
+{
+    /**
+     * Pseudo stub function for the key unpacking
+     * Need rkey == 0 due to work with same process to reuse uct_base_[put|get|atomic]*
+     */
+    *rkey_p   = 0;
+    *handle_p = NULL;
+    return UCS_OK;
+}
+
+static UCT_MD_COMPONENT_DEFINE(uct_self_md, UCT_SELF_NAME,
+                               uct_self_query_md_resources, uct_self_md_open, NULL,
+                               uct_self_md_rkey_unpack,
+                               ucs_empty_function_return_success, "SELF_",
+                               uct_md_config_table, uct_md_config_t);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.h
new file mode 100644
index 000000000..21c4bf9a7
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self.h
@@ -0,0 +1,30 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ * Copyright (C) ARM Ltd. 2016-2017.  ALL RIGHTS RESERVED.
+ * See file LICENSE for terms.
+ */
+
+#ifndef UCT_SELF_H
+#define UCT_SELF_H
+
+#include <uct/base/uct_iface.h>
+#include <uct/base/uct_md.h>
+
+
+typedef uint64_t uct_self_iface_addr_t;
+
+
+typedef struct uct_self_iface {
+    uct_base_iface_t      super;
+    uct_self_iface_addr_t id;           /* Unique identifier for the instance */
+    size_t                send_size;    /* Maximum size for payload */
+    ucs_mpool_t           msg_mp;       /* Messages memory pool */
+} uct_self_iface_t;
+
+
+typedef struct uct_self_ep {
+    uct_base_ep_t         super;
+} uct_self_ep_t;
+
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.c
deleted file mode 100644
index d692f4127..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.c
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#include "self_ep.h"
-#include "self_iface.h"
-
-static UCS_CLASS_INIT_FUNC(uct_self_ep_t, uct_iface_t *tl_iface,
-                           const uct_device_addr_t *dev_addr,
-                           const uct_iface_addr_t *iface_addr)
-{
-    uct_self_iface_t *local_iface = 0;
-
-    ucs_trace_func("Creating an EP for loop-back transport self=%p", self);
-    local_iface = ucs_derived_of(tl_iface, uct_self_iface_t);
-    UCS_CLASS_CALL_SUPER_INIT(uct_base_ep_t, &local_iface->super)
-    return UCS_OK;
-}
-
-static UCS_CLASS_CLEANUP_FUNC(uct_self_ep_t)
-{
-    ucs_trace_func("self=%p", self);
-}
-
-UCS_CLASS_DEFINE(uct_self_ep_t, uct_base_ep_t);
-UCS_CLASS_DEFINE_NEW_FUNC(uct_self_ep_t, uct_ep_t, uct_iface_t *,
-                          const uct_device_addr_t *, const uct_iface_addr_t *);
-UCS_CLASS_DEFINE_DELETE_FUNC(uct_self_ep_t, uct_ep_t);
-
-/**
- * Reserve the buffer and set the descriptor empty for later initialization
- * in case if UCS_ERR_NO_RESOURCE obtained from active message handler
- */
-static void UCS_F_ALWAYS_INLINE uct_self_ep_am_reserve_buffer(uct_self_iface_t *self_iface,
-                                                              void *desc)
-{
-    uct_recv_desc(desc) = &self_iface->release_desc;
-}
-
-ucs_status_t uct_self_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t header,
-                                  const void *payload, unsigned length)
-{
-    ucs_status_t status;
-    uct_self_iface_t *self_iface = 0;
-    uct_self_ep_t *self_ep = 0;
-    void *desc = 0, *p_data = 0;
-    unsigned total_length = 0;
-    uct_recv_desc_t *cur_desc;
-
-    self_ep = ucs_derived_of(tl_ep, uct_self_ep_t);
-    self_iface = ucs_derived_of(self_ep->super.super.iface, uct_self_iface_t);
-    total_length = length + sizeof(header);
-
-    /* Send part */
-    UCT_CHECK_AM_ID(id);
-    UCT_CHECK_LENGTH(total_length, 0, self_iface->data_length, "am_short");
-    UCT_TL_IFACE_GET_TX_DESC(&self_iface->super, &self_iface->msg_desc_mp,
-                             cur_desc, return UCS_ERR_NO_MEMORY);
-
-    desc = cur_desc + 1;
-    p_data = desc + self_iface->rx_headroom;
-    *(typeof(header)*) p_data = header;
-    memcpy(p_data + sizeof(header), payload, length);
-
-    UCT_TL_EP_STAT_OP(&self_ep->super, AM, SHORT, total_length);
-    uct_iface_trace_am(&self_iface->super, UCT_AM_TRACE_TYPE_SEND, id, p_data,
-                       total_length, "TX: AM_SHORT");
-
-    /* Receive part */
-    uct_iface_trace_am(&self_iface->super, UCT_AM_TRACE_TYPE_RECV, id, p_data,
-                       total_length, "RX: AM_SHORT");
-    status = uct_iface_invoke_am(&self_iface->super, id, p_data, total_length,
-                                 UCT_CB_PARAM_FLAG_DESC);
-
-    if (ucs_unlikely(UCS_INPROGRESS == status)) {
-        uct_self_ep_am_reserve_buffer(self_iface, desc);
-        status = UCS_OK;
-    } else {
-        UCT_TL_IFACE_PUT_DESC(cur_desc);
-    }
-
-    return status;
-}
-
-ssize_t uct_self_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
-                             uct_pack_callback_t pack_cb, void *arg,
-                             unsigned flags)
-{
-    ucs_status_t status;
-    uct_self_iface_t *self_iface = 0;
-    uct_self_ep_t *self_ep = 0;
-    void *desc = 0, *payload = 0;
-    ssize_t length = 0;
-    uct_recv_desc_t *cur_desc;
-
-    self_ep = ucs_derived_of(tl_ep, uct_self_ep_t);
-    self_iface = ucs_derived_of(self_ep->super.super.iface, uct_self_iface_t);
-
-    /* Send part */
-    UCT_CHECK_AM_ID(id);
-    UCT_TL_IFACE_GET_TX_DESC(&self_iface->super, &self_iface->msg_desc_mp,
-                             cur_desc, return UCS_ERR_NO_MEMORY);
-
-    desc = cur_desc + 1;
-    payload = desc + self_iface->rx_headroom;
-    length = pack_cb(payload, arg);
-
-    UCT_CHECK_LENGTH(length, 0, self_iface->data_length, "am_bcopy");
-    UCT_TL_EP_STAT_OP(&self_ep->super, AM, BCOPY, length);
-    uct_iface_trace_am(&self_iface->super, UCT_AM_TRACE_TYPE_SEND, id, payload,
-                       length, "TX: AM_BCOPY");
-
-    /* Receive part */
-    uct_iface_trace_am(&self_iface->super, UCT_AM_TRACE_TYPE_RECV, id, payload,
-                       length, "RX: AM_BCOPY");
-    status = uct_iface_invoke_am(&self_iface->super, id, payload, length,
-                                 UCT_CB_PARAM_FLAG_DESC);
-
-    if (ucs_unlikely(UCS_INPROGRESS == status)) {
-        uct_self_ep_am_reserve_buffer(self_iface, desc);
-        status = UCS_OK;
-    } else {
-        UCT_TL_IFACE_PUT_DESC(cur_desc);
-    }
-
-    return length;
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.h
deleted file mode 100644
index de2813031..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_ep.h
+++ /dev/null
@@ -1,28 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#ifndef UCT_SELF_EP_H
-#define UCT_SELF_EP_H
-
-#include <uct/base/uct_iface.h>
-
-
-typedef struct uct_self_ep {
-    uct_base_ep_t super;
-} uct_self_ep_t;
-
-UCS_CLASS_DECLARE_NEW_FUNC(uct_self_ep_t, uct_ep_t, uct_iface_t *,
-                           const uct_device_addr_t *, const uct_iface_addr_t *);
-UCS_CLASS_DECLARE_DELETE_FUNC(uct_self_ep_t, uct_ep_t);
-
-ucs_status_t uct_self_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t header,
-                                  const void *payload, unsigned length);
-ssize_t uct_self_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
-                             uct_pack_callback_t pack_cb, void *arg,
-                             unsigned flags);
-
-ucs_status_t uct_self_ep_check(uct_ep_h ep, unsigned flags, uct_completion_t *comp);
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.c
deleted file mode 100644
index 6c81f2f96..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.c
+++ /dev/null
@@ -1,243 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#include "self_iface.h"
-#include "self_md.h"
-#include "self_ep.h"
-
-#include <uct/sm/base/sm_ep.h>
-#include <ucs/type/class.h>
-#include <ucs/sys/string.h>
-
-
-static ucs_config_field_t uct_self_iface_config_table[] = {
-    {"", "", NULL,
-     ucs_offsetof(uct_self_iface_config_t, super),
-     UCS_CONFIG_TYPE_TABLE(uct_iface_config_table)},
-
-     UCT_IFACE_MPOOL_CONFIG_FIELDS("", 16384, 16, "",
-                                   ucs_offsetof(uct_self_iface_config_t, mp), ""),
-
-    {NULL}
-};
-
-static ucs_status_t uct_self_iface_query(uct_iface_h iface, uct_iface_attr_t *attr)
-{
-    uct_self_iface_t *self_iface = ucs_derived_of(iface, uct_self_iface_t);
-
-    ucs_trace_func("iface=%p", iface);
-    memset(attr, 0, sizeof(*attr));
-
-    attr->iface_addr_len         = sizeof(uct_self_iface_addr_t);
-    attr->device_addr_len        = 0;
-    attr->ep_addr_len            = 0; /* No UCT_IFACE_FLAG_CONNECT_TO_EP supported */
-    attr->max_conn_priv          = 0;
-    attr->cap.flags              = UCT_IFACE_FLAG_CONNECT_TO_IFACE |
-                                   UCT_IFACE_FLAG_AM_SHORT         |
-                                   UCT_IFACE_FLAG_AM_BCOPY         |
-                                   UCT_IFACE_FLAG_PUT_SHORT        |
-                                   UCT_IFACE_FLAG_PUT_BCOPY        |
-                                   UCT_IFACE_FLAG_GET_BCOPY        |
-                                   UCT_IFACE_FLAG_ATOMIC_ADD32     |
-                                   UCT_IFACE_FLAG_ATOMIC_ADD64     |
-                                   UCT_IFACE_FLAG_ATOMIC_FADD64    |
-                                   UCT_IFACE_FLAG_ATOMIC_FADD32    |
-                                   UCT_IFACE_FLAG_ATOMIC_SWAP64    |
-                                   UCT_IFACE_FLAG_ATOMIC_SWAP32    |
-                                   UCT_IFACE_FLAG_ATOMIC_CSWAP64   |
-                                   UCT_IFACE_FLAG_ATOMIC_CSWAP32   |
-                                   UCT_IFACE_FLAG_ATOMIC_CPU       |
-                                   UCT_IFACE_FLAG_PENDING          |
-                                   UCT_IFACE_FLAG_CB_SYNC          |
-                                   UCT_IFACE_FLAG_EP_CHECK;
-
-    attr->cap.put.max_short       = UINT_MAX;
-    attr->cap.put.max_bcopy       = SIZE_MAX;
-    attr->cap.put.min_zcopy       = 0;
-    attr->cap.put.max_zcopy       = 0;
-    attr->cap.put.opt_zcopy_align = UCS_SYS_CACHE_LINE_SIZE;
-    attr->cap.put.align_mtu       = attr->cap.put.opt_zcopy_align;
-    attr->cap.put.max_iov         = 1;
-
-    attr->cap.get.max_bcopy       = SIZE_MAX;
-    attr->cap.get.min_zcopy       = 0;
-    attr->cap.get.max_zcopy       = 0;
-    attr->cap.get.opt_zcopy_align = UCS_SYS_CACHE_LINE_SIZE;
-    attr->cap.get.align_mtu       = attr->cap.get.opt_zcopy_align;
-    attr->cap.get.max_iov         = 1;
-
-    attr->cap.am.max_short        = self_iface->data_length;
-    attr->cap.am.max_bcopy        = self_iface->data_length;
-    attr->cap.am.min_zcopy        = 0;
-    attr->cap.am.max_zcopy        = 0;
-    attr->cap.am.opt_zcopy_align  = UCS_SYS_CACHE_LINE_SIZE;
-    attr->cap.am.align_mtu        = attr->cap.am.opt_zcopy_align;
-    attr->cap.am.max_hdr          = 0;
-    attr->cap.am.max_iov          = 1;
-
-    attr->latency.overhead        = 0;
-    attr->latency.growth          = 0;
-    attr->bandwidth               = 6911 * 1024.0 * 1024.0;
-    attr->overhead                = 10e-9;
-    attr->priority                = 0;
-
-    return UCS_OK;
-}
-
-static ucs_status_t uct_self_iface_get_address(uct_iface_h iface,
-                                               uct_iface_addr_t *addr)
-{
-    const uct_self_iface_t *self_iface = 0;
-
-    ucs_trace_func("iface=%p", iface);
-    self_iface = ucs_derived_of(iface, uct_self_iface_t);
-    *(uct_self_iface_addr_t*)addr = self_iface->id;
-    return UCS_OK;
-}
-
-static int uct_self_iface_is_reachable(const uct_iface_h iface, const uct_device_addr_t *dev_addr,
-                                       const uct_iface_addr_t *iface_addr)
-{
-    const uct_self_iface_t *self_iface = NULL;
-    const uct_self_iface_addr_t *self_addr = NULL;
-
-    if (NULL == iface_addr) {
-        return 0;
-    }
-    self_iface = ucs_derived_of(iface, uct_self_iface_t);
-    self_addr = (const uct_self_iface_addr_t *) iface_addr;
-    ucs_trace_func("iface=%p id=%lx addr=%lx", iface, self_iface->id, *self_addr);
-    return  self_iface->id == *self_addr;
-}
-
-static void uct_self_iface_release_desc(uct_recv_desc_t *self, void *desc)
-{
-    uct_recv_desc_t *self_desc = (uct_recv_desc_t *)desc - 1;
-    ucs_mpool_put(self_desc);
-}
-
-static UCS_CLASS_DEFINE_DELETE_FUNC(uct_self_iface_t, uct_iface_t);
-
-static uct_iface_ops_t uct_self_iface_ops = {
-    .ep_put_short             = uct_sm_ep_put_short,
-    .ep_put_bcopy             = uct_sm_ep_put_bcopy,
-    .ep_get_bcopy             = uct_sm_ep_get_bcopy,
-    .ep_am_short              = uct_self_ep_am_short,
-    .ep_am_bcopy              = uct_self_ep_am_bcopy,
-    .ep_atomic_add64          = uct_sm_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_sm_ep_atomic_fadd64,
-    .ep_atomic_cswap64        = uct_sm_ep_atomic_cswap64,
-    .ep_atomic_swap64         = uct_sm_ep_atomic_swap64,
-    .ep_atomic_add32          = uct_sm_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_sm_ep_atomic_fadd32,
-    .ep_atomic_cswap32        = uct_sm_ep_atomic_cswap32,
-    .ep_atomic_swap32         = uct_sm_ep_atomic_swap32,
-    .ep_flush                 = uct_base_ep_flush,
-    .ep_fence                 = uct_base_ep_fence,
-    .ep_check                 = ucs_empty_function_return_success,
-    .ep_pending_add           = ucs_empty_function_return_busy,
-    .ep_pending_purge         = ucs_empty_function,
-    .ep_create_connected      = UCS_CLASS_NEW_FUNC_NAME(uct_self_ep_t),
-    .ep_destroy               = UCS_CLASS_DELETE_FUNC_NAME(uct_self_ep_t),
-    .iface_flush              = uct_base_iface_flush,
-    .iface_fence              = uct_base_iface_fence,
-    .iface_progress_enable    = ucs_empty_function,
-    .iface_progress_disable   = ucs_empty_function,
-    .iface_progress           = ucs_empty_function_return_zero,
-    .iface_close              = UCS_CLASS_DELETE_FUNC_NAME(uct_self_iface_t),
-    .iface_query              = uct_self_iface_query,
-    .iface_get_device_address = ucs_empty_function_return_success,
-    .iface_get_address        = uct_self_iface_get_address,
-    .iface_is_reachable       = uct_self_iface_is_reachable
-};
-
-static UCS_CLASS_INIT_FUNC(uct_self_iface_t, uct_md_h md, uct_worker_h worker,
-                           const uct_iface_params_t *params,
-                           const uct_iface_config_t *tl_config)
-{
-    ucs_status_t status;
-    uct_self_iface_config_t *self_config = 0;
-
-    ucs_trace_func("Creating a loop-back transport self=%p rxh=%lu",
-                   self, params->rx_headroom);
-
-    ucs_assert(params->open_mode & UCT_IFACE_OPEN_MODE_DEVICE);
-
-    if (strcmp(params->mode.device.dev_name, UCT_SELF_NAME) != 0) {
-        ucs_error("No device was found: %s", params->mode.device.dev_name);
-        return UCS_ERR_NO_DEVICE;
-    }
-
-    UCS_CLASS_CALL_SUPER_INIT(uct_base_iface_t, &uct_self_iface_ops, md, worker,
-                              params, tl_config UCS_STATS_ARG(params->stats_root)
-                              UCS_STATS_ARG(UCT_SELF_NAME));
-
-    self_config = ucs_derived_of(tl_config, uct_self_iface_config_t);
-
-    self->id              = ucs_generate_uuid((uintptr_t)self);
-    self->rx_headroom     = params->rx_headroom;
-    self->data_length     = self_config->super.max_bcopy;
-    self->release_desc.cb = uct_self_iface_release_desc;
-
-    /* create a memory pool for data transferred */
-    status = uct_iface_mpool_init(&self->super,
-                                  &self->msg_desc_mp,
-                                  sizeof(uct_recv_desc_t) + self->rx_headroom +
-                                                            self->data_length,
-                                  sizeof(uct_recv_desc_t) + self->rx_headroom,
-                                  UCS_SYS_CACHE_LINE_SIZE,
-                                  &self_config->mp,
-                                  256,
-                                  ucs_empty_function,
-                                  "self_msg_desc");
-    if (UCS_OK != status) {
-        ucs_error("Failed to create a memory pool for the loop-back transport");
-        return status;
-    }
-
-    ucs_debug("Created a loop-back iface. id=0x%lx, len=%u, tx_hdr=%lu",
-              self->id, self->data_length, self->rx_headroom);
-    return UCS_OK;
-}
-
-static UCS_CLASS_CLEANUP_FUNC(uct_self_iface_t)
-{
-    ucs_trace_func("self=%p", self);
-
-    ucs_mpool_cleanup(&self->msg_desc_mp, 1);
-}
-
-UCS_CLASS_DEFINE(uct_self_iface_t, uct_base_iface_t);
-static UCS_CLASS_DEFINE_NEW_FUNC(uct_self_iface_t, uct_iface_t, uct_md_h,
-                                 uct_worker_h, const uct_iface_params_t*,
-                                 const uct_iface_config_t*);
-
-static ucs_status_t uct_self_query_tl_resources(uct_md_h md,
-                                                uct_tl_resource_desc_t **resource_p,
-                                                unsigned *num_resources_p)
-{
-    uct_tl_resource_desc_t *resource = 0;
-
-    ucs_trace_func("md=%p", md);
-    resource = ucs_calloc(1, sizeof(*resource), "resource desc");
-    if (NULL == resource) {
-        ucs_error("Failed to allocate memory");
-        return UCS_ERR_NO_MEMORY;
-    }
-
-    ucs_snprintf_zero(resource->tl_name, sizeof(resource->tl_name), "%s",
-                      UCT_SELF_NAME);
-    ucs_snprintf_zero(resource->dev_name, sizeof(resource->dev_name), "%s",
-                      UCT_SELF_NAME);
-    resource->dev_type = UCT_DEVICE_TYPE_SELF;
-
-    *num_resources_p = 1;
-    *resource_p      = resource;
-    return UCS_OK;
-}
-
-UCT_TL_COMPONENT_DEFINE(uct_self_tl, uct_self_query_tl_resources, uct_self_iface_t,
-                        UCT_SELF_NAME, "SELF_", uct_self_iface_config_table, uct_self_iface_config_t);
-UCT_MD_REGISTER_TL(&uct_self_md, &uct_self_tl);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.h
deleted file mode 100644
index 772692707..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_iface.h
+++ /dev/null
@@ -1,30 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * Copyright (C) ARM Ltd. 2016-2017.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#ifndef UCT_SELF_IFACE_H
-#define UCT_SELF_IFACE_H
-
-#include <uct/base/uct_iface.h>
-#include <ucs/arch/cpu.h>
-
-typedef uint64_t uct_self_iface_addr_t;
-
-typedef struct uct_self_iface {
-    uct_base_iface_t      super;
-    uct_self_iface_addr_t id;           /* Unique identifier for the instance */
-    size_t                rx_headroom;  /* User data size precedes payload */
-    unsigned              data_length;  /* Maximum size for payload */
-    uct_recv_desc_t       release_desc; /* Callback to desc release func */
-    ucs_mpool_t           msg_desc_mp;  /* Messages memory pool */
-} UCS_V_ALIGNED(UCS_SYS_CACHE_LINE_SIZE) uct_self_iface_t;
-
-typedef struct uct_self_iface_config {
-    uct_iface_config_t       super;
-    uct_iface_mpool_config_t mp;
-} uct_self_iface_config_t;
-
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.c
deleted file mode 100644
index 01641b4dd..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.c
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#include "self_md.h"
-
-
-static ucs_status_t uct_self_md_query(uct_md_h md, uct_md_attr_t *attr)
-{
-    /* Dummy memory registration provided. No real memory handling exists */
-    attr->cap.flags         = UCT_MD_FLAG_REG |
-                              UCT_MD_FLAG_NEED_RKEY; /* TODO ignore rkey in rma/amo ops */
-    attr->cap.reg_mem_types = UCS_BIT(UCT_MD_MEM_TYPE_HOST);
-    attr->cap.mem_type      = UCT_MD_MEM_TYPE_HOST;
-    attr->cap.max_alloc     = 0;
-    attr->cap.max_reg       = ULONG_MAX;
-    attr->rkey_packed_size  = 0; /* uct_md_query adds UCT_MD_COMPONENT_NAME_MAX to this */
-    attr->reg_cost.overhead = 0;
-    attr->reg_cost.growth   = 0;
-    memset(&attr->local_cpus, 0xff, sizeof(attr->local_cpus));
-    return UCS_OK;
-}
-
-static ucs_status_t uct_self_query_md_resources(uct_md_resource_desc_t **resources_p,
-                                                unsigned *num_resources_p)
-{
-    return uct_single_md_resource(&uct_self_md, resources_p, num_resources_p);
-}
-
-static ucs_status_t uct_self_mem_reg(uct_md_h md, void *address, size_t length,
-                                     unsigned flags, uct_mem_h *memh_p)
-{
-    /* We have to emulate memory registration. Return dummy pointer */
-    *memh_p = (void *) 0xdeadbeef;
-    return UCS_OK;
-}
-
-static ucs_status_t uct_self_md_open(const char *md_name, const uct_md_config_t *md_config,
-                                     uct_md_h *md_p)
-{
-    static uct_md_ops_t md_ops = {
-        .close        = (void*)ucs_empty_function,
-        .query        = uct_self_md_query,
-        .mkey_pack    = ucs_empty_function_return_success,
-        .mem_reg      = uct_self_mem_reg,
-        .mem_dereg    = ucs_empty_function_return_success,
-        .is_mem_type_owned = (void *)ucs_empty_function_return_zero,
-    };
-    static uct_md_t md = {
-        .ops          = &md_ops,
-        .component    = &uct_self_md
-    };
-
-    *md_p = &md;
-    return UCS_OK;
-}
-/**
- * Pseudo stub function for the key unpacking
- * Need rkey == 0 due to work with same process to reuse uct_base_[put|get|atomic]*
- */
-static ucs_status_t uct_self_md_rkey_unpack(uct_md_component_t *mdc,
-                                            const void *rkey_buffer, uct_rkey_t *rkey_p,
-                                            void **handle_p)
-{
-    *rkey_p   = 0;
-    *handle_p = NULL;
-    return UCS_OK;
-}
-
-UCT_MD_COMPONENT_DEFINE(uct_self_md, UCT_SELF_NAME,
-                        uct_self_query_md_resources, uct_self_md_open, NULL,
-                        uct_self_md_rkey_unpack,
-                        ucs_empty_function_return_success, "SELF_",
-                        uct_md_config_table, uct_md_config_t);
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.h
deleted file mode 100644
index c2ee021d7..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/sm/self/self_md.h
+++ /dev/null
@@ -1,16 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#ifndef UCT_SELF_MD_H
-#define UCT_SELF_MD_H
-
-#include <uct/base/uct_md.h>
-
-#define UCT_SELF_NAME "self"
-
-extern uct_md_component_t uct_self_md;
-
-
-#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp.h
index a355e2929..4d8aff87b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp.h
@@ -120,7 +120,8 @@ ssize_t uct_tcp_ep_am_bcopy(uct_ep_h uct_ep, uint8_t am_id,
                             uct_pack_callback_t pack_cb, void *arg,
                             unsigned flags);
 
-ucs_status_t uct_tcp_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req);
+ucs_status_t uct_tcp_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req,
+                                    unsigned flags);
 
 void uct_tcp_ep_pending_purge(uct_ep_h tl_ep, uct_pending_purge_callback_t cb,
                               void *arg);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_ep.c
index 0268b39eb..b03815080 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_ep.c
@@ -209,6 +209,8 @@ unsigned uct_tcp_ep_progress_rx(uct_tcp_ep_t *ep)
 
     /* Receive next chunk of data */
     recv_length = iface->config.buf_size - ep->length;
+    ucs_assertv(recv_length > 0, "ep=%p", ep);
+
     status = uct_tcp_recv(ep->fd, ep->buf + ep->length, &recv_length);
     if (status != UCS_OK) {
         if (status == UCS_ERR_CANCELED) {
@@ -225,6 +227,8 @@ unsigned uct_tcp_ep_progress_rx(uct_tcp_ep_t *ep)
     /* Parse received active messages */
     while ((remainder = ep->length - ep->offset) >= sizeof(*hdr)) {
         hdr = ep->buf + ep->offset;
+        ucs_assert(hdr->length <= (iface->config.buf_size - sizeof(uct_tcp_am_hdr_t)));
+
         if (remainder < sizeof(*hdr) + hdr->length) {
             break;
         }
@@ -272,6 +276,9 @@ ssize_t uct_tcp_ep_am_bcopy(uct_ep_h uct_ep, uint8_t am_id,
     hdr->length = packed_length = pack_cb(hdr + 1, arg);
     ep->length  = sizeof(*hdr) + packed_length;
 
+    UCT_CHECK_LENGTH(hdr->length, 0,
+                     iface->config.buf_size - sizeof(uct_tcp_am_hdr_t),
+                     "am_bcopy");
     UCT_TL_EP_STAT_OP(&ep->super, AM, BCOPY, hdr->length);
     uct_iface_trace_am(&iface->super, UCT_AM_TRACE_TYPE_SEND, hdr->am_id,
                        hdr + 1, hdr->length, "SEND fd %d", ep->fd);
@@ -284,7 +291,8 @@ ssize_t uct_tcp_ep_am_bcopy(uct_ep_h uct_ep, uint8_t am_id,
     return packed_length;
 }
 
-ucs_status_t uct_tcp_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req)
+ucs_status_t uct_tcp_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *req,
+                                    unsigned flags)
 {
     uct_tcp_ep_t *ep = ucs_derived_of(tl_ep, uct_tcp_ep_t);
 
@@ -314,7 +322,6 @@ ucs_status_t uct_tcp_ep_flush(uct_ep_h tl_ep, unsigned flags,
         return UCS_ERR_NO_RESOURCE;
     }
 
-    ucs_assert(ucs_queue_is_empty(&ep->pending_q));
     UCT_TL_EP_STAT_FLUSH(&ep->super);
     return UCS_OK;
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_iface.c
index e0bfe8a8b..68f0e49e5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_iface.c
@@ -173,6 +173,14 @@ static ucs_status_t uct_tcp_iface_flush(uct_iface_h tl_iface, unsigned flags,
     return UCS_OK;
 }
 
+static void uct_tcp_iface_listen_close(uct_tcp_iface_t *iface)
+{
+    if (iface->listen_fd != -1) {
+        close(iface->listen_fd);
+        iface->listen_fd = -1;
+    }
+}
+
 static void uct_tcp_iface_connect_handler(int listen_fd, void *arg)
 {
     uct_tcp_iface_t *iface = arg;
@@ -189,6 +197,7 @@ static void uct_tcp_iface_connect_handler(int listen_fd, void *arg)
     if (fd < 0) {
         if ((errno != EAGAIN) && (errno != EINTR)) {
             ucs_error("accept() failed: %m");
+            uct_tcp_iface_listen_close(iface);
         }
         return;
     }
@@ -275,6 +284,11 @@ static UCS_CLASS_INIT_FUNC(uct_tcp_iface_t, uct_md_h md, uct_worker_h worker,
     self->sockopt.sndbuf         = config->sockopt_sndbuf;
     ucs_list_head_init(&self->ep_list);
 
+    if (ucs_derived_of(worker, uct_priv_worker_t)->thread_mode == UCS_THREAD_MODE_MULTI) {
+        ucs_error("TCP transport does not support multi-threaded worker");
+        return UCS_ERR_INVALID_PARAM;
+    }
+
     status = uct_tcp_netif_inaddr(self->if_name, &self->config.ifaddr,
                                   &self->config.netmask);
     if (status != UCS_OK) {
@@ -366,7 +380,7 @@ static UCS_CLASS_CLEANUP_FUNC(uct_tcp_iface_t)
         uct_tcp_ep_destroy(&ep->super.super);
     }
 
-    close(self->listen_fd);
+    uct_tcp_iface_listen_close(self);
     close(self->epfd);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_net.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_net.c
index a82138899..4aaf88055 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_net.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/tcp/tcp_net.c
@@ -45,10 +45,12 @@ ucs_status_t uct_tcp_netif_caps(const char *if_name, double *latency_p,
     ucs_status_t status;
     struct ifreq ifr;
     size_t mtu, ll_headers;
+    int speed_known;
     short ether_type;
 
     memset(&ifr, 0, sizeof(ifr));
 
+    speed_known  = 0;
     edata.cmd    = ETHTOOL_GSET;
     ifr.ifr_data = (void*)&edata;
     status = ucs_netif_ioctl(if_name, SIOCETHTOOL, &ifr);
@@ -59,15 +61,15 @@ ucs_status_t uct_tcp_netif_caps(const char *if_name, double *latency_p,
         speed_mbps = edata.speed;
 #endif
 #if HAVE_DECL_SPEED_UNKNOWN
-        if (speed_mbps == SPEED_UNKNOWN) {
+        speed_known = speed_mbps != SPEED_UNKNOWN;
 #else
-        if ((speed_mbps == 0) || ((uint16_t)speed_mbps == (uint16_t)-1)) {
+        speed_known = (speed_mbps != 0) && ((uint16_t)speed_mbps != (uint16_t)-1);
 #endif
-            ucs_error("speed of %s is UNKNOWN", if_name);
-            return UCS_ERR_NO_DEVICE;
-        }
-    } else {
-        speed_mbps = 100; /* Default value if SIOCETHTOOL is not supported */
+    }
+
+    if (!speed_known) {
+        speed_mbps = 100;
+        ucs_debug("speed of %s is UNKNOWN, assuming %d Mbps", if_name, speed_mbps);
     }
 
     status = ucs_netif_ioctl(if_name, SIOCGIFHWADDR, &ifr);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_def.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_def.h
index 88701e8bf..c3e9720e0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_def.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_def.h
@@ -60,7 +60,7 @@ if (uct_ugni_check_lock_needed(_cdm)) {    \
 }
 #else
 #define uct_ugni_cdm_init_lock(x) UCS_OK
-#define uct_ugni_cdm_destroy_lock(x) UCS_OK
+#define uct_ugni_cdm_destroy_lock(x) {}
 #define uct_ugni_cdm_lock(x)
 #define uct_ugni_cdm_unlock(x)
 #define uct_ugni_check_lock_needed(x) 0
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_device.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_device.c
index 7795f6f8e..251065291 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_device.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_device.c
@@ -27,7 +27,7 @@ typedef struct uct_ugni_job_info {
     int                 pmi_rank_id;                    /**< rank id assigned by PMI */
     int                 num_devices;                    /**< Number of devices */
     uct_ugni_device_t   devices[UCT_UGNI_MAX_DEVICES];  /**< Array of devices */
-    bool                initialized;                    /**< Info status */
+    int                 initialized;                    /**< Info status */
 } uct_ugni_job_info_t;
 
 static uct_ugni_job_info_t job_info = {
@@ -36,7 +36,7 @@ static uct_ugni_job_info_t job_info = {
     .pmi_num_of_ranks   = 0,
     .pmi_rank_id        = 0,
     .num_devices        = -1,
-    .initialized        = false,
+    .initialized        = 0,
 };
 
 uint32_t ugni_domain_counter = 0;
@@ -168,7 +168,7 @@ static ucs_status_t uct_ugni_fetch_pmi()
     ucs_debug("PMI cookie %d", job_info.cookie);
 
     /* Context and domain is activated */
-    job_info.initialized = true;
+    job_info.initialized = 1;
     ucs_debug("UGNI job info was activated");
     return UCS_OK;
 }
@@ -447,12 +447,8 @@ ucs_status_t uct_ugni_create_md_cdm(uct_ugni_cdm_t *cdm)
 ucs_status_t uct_ugni_destroy_cdm(uct_ugni_cdm_t *cdm)
 {
     gni_return_t ugni_rc;
-    ucs_status_t status;
 
-    status = uct_ugni_cdm_destroy_lock(cdm);
-    if (UCS_OK != status) {
-        ucs_error("Couldn't destroy cdm lock.");
-    }
+    uct_ugni_cdm_destroy_lock(cdm);
 
     ucs_trace_func("cdm=%p", cdm);
     ugni_rc = GNI_CdmDestroy(cdm->cdm_handle);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.c
index 51d588af4..71125e20f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.c
@@ -12,7 +12,8 @@
 SGLIB_DEFINE_LIST_FUNCTIONS(uct_ugni_ep_t, uct_ugni_ep_compare, next);
 SGLIB_DEFINE_HASHED_CONTAINER_FUNCTIONS(uct_ugni_ep_t, UCT_UGNI_HASH_SIZE, uct_ugni_ep_hash);
 
-ucs_status_t uct_ugni_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n){
+ucs_status_t uct_ugni_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                     unsigned flags){
     uct_ugni_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_iface_t);
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.h
index 53ddb247a..52743ddd6 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_ep.h
@@ -35,7 +35,8 @@ UCS_CLASS_DECLARE_DELETE_FUNC(uct_ugni_ep_t, uct_ep_t);
 uct_ugni_ep_t *uct_ugni_iface_lookup_ep(uct_ugni_iface_t *iface, uintptr_t hash_key);
 ucs_status_t ugni_connect_ep(uct_ugni_iface_t *iface, const uct_devaddr_ugni_t *dev_addr,
                              const uct_sockaddr_ugni_t *iface_addr, uct_ugni_ep_t *ep);
-ucs_status_t uct_ugni_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n);
+ucs_status_t uct_ugni_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                     unsigned flags);
 void uct_ugni_ep_pending_purge(uct_ep_h tl_ep, uct_pending_purge_callback_t cb,
                                void *arg);
 ucs_arbiter_cb_result_t uct_ugni_ep_process_pending(ucs_arbiter_t *arbiter,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_types.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_types.h
index 5c2f7ec0e..23e1447f8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_types.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/base/ugni_types.h
@@ -11,7 +11,6 @@
 #include <uct/base/uct_md.h>
 #include <ucs/datastruct/arbiter.h>
 #include <gni_pub.h>
-#include <stdbool.h>
 
 typedef struct uct_ugni_device {
     gni_nic_device_t type;                      /**< Device type */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.c
index 63994f8e5..486e5b527 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.c
@@ -243,51 +243,6 @@ static void uct_ugni_amo_unpack64(uct_completion_t *self, ucs_status_t status)
     uct_ugni_invoke_orig_comp(fma, status);
 }
 
-ucs_status_t uct_ugni_ep_atomic_add64(uct_ep_h tl_ep, uint64_t add,
-                                      uint64_t remote_addr, uct_rkey_t rkey)
-{
-    uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
-    uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
-    uct_ugni_rdma_fetch_desc_t *fma;
-
-    UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
-                             return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC_ADD,
-                            add, 0, NULL, remote_addr,
-                            rkey, LEN_64, ep, NULL, NULL, NULL);
-    ucs_trace_data("Posting AMO ADD, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx64" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, add,
-                   (void *)fma->super.desc.remote_addr,
-                   fma->super.desc.remote_mem_hndl.qword1,
-                   fma->super.desc.remote_mem_hndl.qword2);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return uct_ugni_post_fma(iface, ep, &fma->super, UCS_OK);
-}
-
-ucs_status_t uct_ugni_ep_atomic_fadd64(uct_ep_h tl_ep, uint64_t add,
-                                       uint64_t remote_addr, uct_rkey_t rkey,
-                                       uint64_t *result, uct_completion_t *comp)
-{
-    uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
-    uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
-    uct_ugni_rdma_fetch_desc_t *fma;
-
-    UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
-                             return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC_FADD,
-                            add, 0, fma + 1, remote_addr,
-                            rkey, LEN_64, ep, comp, uct_ugni_amo_unpack64, (void *)result);
-    ucs_trace_data("Posting AMO FADD, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx64" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, add,
-                   (void *)fma->super.desc.remote_addr,
-                   fma->super.desc.remote_mem_hndl.qword1,
-                   fma->super.desc.remote_mem_hndl.qword2);
-    UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return uct_ugni_post_fma(iface, ep, &fma->super, UCS_INPROGRESS);
-}
-
 ucs_status_t uct_ugni_ep_atomic_cswap64(uct_ep_h tl_ep, uint64_t compare, uint64_t swap,
                                         uint64_t remote_addr, uct_rkey_t rkey,
                                         uint64_t *result, uct_completion_t *comp)
@@ -321,9 +276,9 @@ static void uct_ugni_amo_unpack32(uct_completion_t *self, ucs_status_t status)
     uct_ugni_invoke_orig_comp(fma, status);
 }
 
-ucs_status_t uct_ugni_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
-                                       uint64_t remote_addr, uct_rkey_t rkey,
-                                       uint64_t *result, uct_completion_t *comp)
+ucs_status_t uct_ugni_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+                                        uint64_t remote_addr, uct_rkey_t rkey,
+                                        uint32_t *result, uct_completion_t *comp)
 {
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
     uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
@@ -331,12 +286,12 @@ ucs_status_t uct_ugni_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
 
     UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
                              return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_FSWAP,
-                            swap, 0, fma + 1, remote_addr,
-                            rkey, LEN_64, ep, comp, uct_ugni_amo_unpack64, (void *)result);
-    ucs_trace_data("Posting AMO SWAP, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx64" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, swap,
+    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_FCSWAP_S,
+                            (uint64_t)compare, (uint64_t)swap, fma + 1, remote_addr,
+                            rkey, LEN_32, ep, comp, uct_ugni_amo_unpack32, (void *)result);
+    ucs_trace_data("Posting AMO CSWAP, GNI_PostFma of size %"PRIx64" value"
+                   "%"PRIx32" compare %"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
+                   fma->super.desc.length, swap, compare,
                    (void *)fma->super.desc.remote_addr,
                    fma->super.desc.remote_mem_hndl.qword1,
                    fma->super.desc.remote_mem_hndl.qword2);
@@ -344,8 +299,9 @@ ucs_status_t uct_ugni_ep_atomic_swap64(uct_ep_h tl_ep, uint64_t swap,
     return uct_ugni_post_fma(iface, ep, &fma->super, UCS_INPROGRESS);
 }
 
-ucs_status_t uct_ugni_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
-                                      uint64_t remote_addr, uct_rkey_t rkey)
+ucs_status_t uct_ugni_ep_atomic_op32(uct_ep_h tl_ep, uint32_t op,
+                                     uint64_t remote_addr, uct_rkey_t rkey,
+                                     gni_fma_cmd_type_t op_type, char *op_str)
 {
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
     uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
@@ -353,12 +309,12 @@ ucs_status_t uct_ugni_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
 
     UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
                              return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_IADD_S,
-                            (uint64_t)add, 0, NULL, remote_addr,
+    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, op_type,
+                            (uint64_t)op, 0, NULL, remote_addr,
                             rkey, LEN_32, ep, NULL, NULL, NULL);
-    ucs_trace_data("Posting AMO ADD, GNI_PostFma of size %"PRIx64" value"
+    ucs_trace_data("Posting AMO %s, GNI_PostFma of size %"PRIx64" value"
                    "%"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, add,
+                   op_str, fma->super.desc.length, op,
                    (void *)fma->super.desc.remote_addr,
                    fma->super.desc.remote_mem_hndl.qword1,
                    fma->super.desc.remote_mem_hndl.qword2);
@@ -366,9 +322,31 @@ ucs_status_t uct_ugni_ep_atomic_add32(uct_ep_h tl_ep, uint32_t add,
     return uct_ugni_post_fma(iface, ep, &fma->super, UCS_OK);
 }
 
-ucs_status_t uct_ugni_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
-                                       uint64_t remote_addr, uct_rkey_t rkey,
-                                       uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_ugni_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                       uint64_t remote_addr, uct_rkey_t rkey)
+{
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_ugni_ep_atomic_op32(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC2_IADD_S, "ADD");
+    case UCT_ATOMIC_OP_XOR:
+        return uct_ugni_ep_atomic_op32(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC2_XOR_S, "XOR");
+    case UCT_ATOMIC_OP_AND:
+        return uct_ugni_ep_atomic_op32(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC2_AND_S, "AND");
+    case UCT_ATOMIC_OP_OR:
+        return uct_ugni_ep_atomic_op32(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC2_OR_S, "OR");
+    default:
+        ucs_assertv(0, "incorrect opcode for atomic: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+}
+
+ucs_status_t uct_ugni_ep_atomic_op64(uct_ep_h tl_ep, uint64_t op,
+                                     uint64_t remote_addr, uct_rkey_t rkey,
+                                     gni_fma_cmd_type_t op_type, char *op_str)
 {
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
     uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
@@ -376,22 +354,45 @@ ucs_status_t uct_ugni_ep_atomic_fadd32(uct_ep_h tl_ep, uint32_t add,
 
     UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
                              return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_FIADD_S,
-                            (uint64_t)add, 0, fma + 1, remote_addr,
-                            rkey, LEN_32, ep, comp, uct_ugni_amo_unpack32, (void *)result);
-    ucs_trace_data("Posting AMO FADD, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, add,
+    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, op_type,
+                            op, 0, NULL, remote_addr,
+                            rkey, LEN_64, ep, NULL, NULL, NULL);
+    ucs_trace_data("Posting AMO %s, GNI_PostFma of size %"PRIx64" value"
+                   "%"PRIx64" to %p, with [%"PRIx64" %"PRIx64"]",
+                   op_str, fma->super.desc.length, op,
                    (void *)fma->super.desc.remote_addr,
                    fma->super.desc.remote_mem_hndl.qword1,
                    fma->super.desc.remote_mem_hndl.qword2);
     UCT_TL_EP_STAT_ATOMIC(ucs_derived_of(tl_ep, uct_base_ep_t));
-    return uct_ugni_post_fma(iface, ep, &fma->super, UCS_INPROGRESS);
+    return uct_ugni_post_fma(iface, ep, &fma->super, UCS_OK);
+}
+
+ucs_status_t uct_ugni_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                       uint64_t remote_addr, uct_rkey_t rkey)
+{
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_ugni_ep_atomic_op64(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC_ADD, "ADD");
+    case UCT_ATOMIC_OP_XOR:
+        return uct_ugni_ep_atomic_op64(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC_XOR, "XOR");
+   case UCT_ATOMIC_OP_AND:
+        return uct_ugni_ep_atomic_op64(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC_AND, "AND"); 
+   case UCT_ATOMIC_OP_OR:
+        return uct_ugni_ep_atomic_op64(ep, value, remote_addr, rkey,
+                                       GNI_FMA_ATOMIC_OR, "OR");
+    default:
+        ucs_assertv(0, "incorrect opcode for atomic: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
 }
 
-ucs_status_t uct_ugni_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
-                                       uint64_t remote_addr, uct_rkey_t rkey,
-                                       uint32_t *result, uct_completion_t *comp)
+ucs_status_t uct_ugni_ep_atomic_fop64(uct_ep_h tl_ep, uint64_t op,
+                                     uint64_t remote_addr, uct_rkey_t rkey,
+                                     uint64_t *result, uct_completion_t *comp,
+                                     gni_fma_cmd_type_t op_type, char *op_str)
 {
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
     uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
@@ -399,12 +400,12 @@ ucs_status_t uct_ugni_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
 
     UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
                              return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_FSWAP_S,
-                            (uint64_t)swap, 0, fma + 1, remote_addr,
-                            rkey, LEN_32, ep, comp, uct_ugni_amo_unpack64, (void *)result);
-    ucs_trace_data("Posting AMO SWAP, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, swap,
+    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, op_type,
+                            op, 0, fma + 1, remote_addr,
+                            rkey, LEN_64, ep, comp, uct_ugni_amo_unpack64, (void *)result);
+    ucs_trace_data("Posting AMO %s, GNI_PostFma of size %"PRIx64" value"
+                   "%"PRIx64" to %p, with [%"PRIx64" %"PRIx64"]",
+                   op_str, fma->super.desc.length, op,
                    (void *)fma->super.desc.remote_addr,
                    fma->super.desc.remote_mem_hndl.qword1,
                    fma->super.desc.remote_mem_hndl.qword2);
@@ -412,9 +413,37 @@ ucs_status_t uct_ugni_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
     return uct_ugni_post_fma(iface, ep, &fma->super, UCS_INPROGRESS);
 }
 
-ucs_status_t uct_ugni_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
+ucs_status_t uct_ugni_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                        uint64_t value, uint64_t *result,
                                         uint64_t remote_addr, uct_rkey_t rkey,
-                                        uint32_t *result, uct_completion_t *comp)
+                                        uct_completion_t *comp)
+{
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_ugni_ep_atomic_fop64(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC_FADD, "FADD");
+    case UCT_ATOMIC_OP_SWAP:
+        return uct_ugni_ep_atomic_fop64(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FSWAP, "FSWAP");
+    case UCT_ATOMIC_OP_XOR:
+        return uct_ugni_ep_atomic_fop64(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC_FXOR, "FXOR");
+    case UCT_ATOMIC_OP_AND:
+        return uct_ugni_ep_atomic_fop64(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC_FAND, "FAND");
+    case UCT_ATOMIC_OP_OR:
+        return uct_ugni_ep_atomic_fop64(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC_FOR, "FOR");
+    default:
+        ucs_assertv(0, "incorrect opcode for atomic: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+}
+
+ucs_status_t uct_ugni_ep_atomic_fop32(uct_ep_h tl_ep, uint32_t op,
+                                      uint64_t remote_addr, uct_rkey_t rkey,
+                                      uint32_t *result, uct_completion_t *comp,
+                                      gni_fma_cmd_type_t op_type, char *op_str)
 {
     uct_ugni_ep_t *ep = ucs_derived_of(tl_ep, uct_ugni_ep_t);
     uct_ugni_rdma_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_rdma_iface_t);
@@ -422,12 +451,12 @@ ucs_status_t uct_ugni_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32
 
     UCT_TL_IFACE_GET_TX_DESC(&iface->super.super, &iface->free_desc_famo, fma,
                              return UCS_ERR_NO_RESOURCE);
-    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, GNI_FMA_ATOMIC2_FCSWAP_S,
-                            (uint64_t)compare, (uint64_t)swap, fma + 1, remote_addr,
+    uct_ugni_format_fma_amo(fma, GNI_POST_AMO, op_type,
+                            (uint64_t)op, 0, fma + 1, remote_addr,
                             rkey, LEN_32, ep, comp, uct_ugni_amo_unpack32, (void *)result);
-    ucs_trace_data("Posting AMO CSWAP, GNI_PostFma of size %"PRIx64" value"
-                   "%"PRIx32" compare %"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
-                   fma->super.desc.length, swap, compare,
+    ucs_trace_data("Posting AMO %s, GNI_PostFma of size %"PRIx64" value"
+                   "%"PRIx32" to %p, with [%"PRIx64" %"PRIx64"]",
+                   op_str, fma->super.desc.length, op,
                    (void *)fma->super.desc.remote_addr,
                    fma->super.desc.remote_mem_hndl.qword1,
                    fma->super.desc.remote_mem_hndl.qword2);
@@ -435,6 +464,33 @@ ucs_status_t uct_ugni_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32
     return uct_ugni_post_fma(iface, ep, &fma->super, UCS_INPROGRESS);
 }
 
+ucs_status_t uct_ugni_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                        uint32_t value, uint32_t *result,
+                                        uint64_t remote_addr, uct_rkey_t rkey,
+                                        uct_completion_t *comp)
+{
+    switch (opcode) {
+    case UCT_ATOMIC_OP_ADD:
+        return uct_ugni_ep_atomic_fop32(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FIADD_S, "ADD");
+    case UCT_ATOMIC_OP_SWAP:
+        return uct_ugni_ep_atomic_fop32(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FSWAP_S, "SWAP");
+    case UCT_ATOMIC_OP_XOR:
+        return uct_ugni_ep_atomic_fop32(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FXOR_S, "XOR");
+    case UCT_ATOMIC_OP_AND:
+        return uct_ugni_ep_atomic_fop32(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FAND_S, "AND"); 
+    case UCT_ATOMIC_OP_OR:
+        return uct_ugni_ep_atomic_fop32(ep, value, remote_addr, rkey, result, comp,
+                                        GNI_FMA_ATOMIC2_FOR_S, "OR");
+    default:
+        ucs_assertv(0, "incorrect opcode for atomic: %d", opcode);
+        return UCS_ERR_UNSUPPORTED;
+    }
+}
+
 static void uct_ugni_unalign_fma_get_cb(uct_completion_t *self, ucs_status_t status)
 {
     uct_ugni_rdma_fetch_desc_t *fma = (uct_ugni_rdma_fetch_desc_t *)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.h
index 1466ad9d8..a8711b892 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_ep.h
@@ -45,6 +45,18 @@ ucs_status_t uct_ugni_ep_atomic_swap32(uct_ep_h tl_ep, uint32_t swap,
 ucs_status_t uct_ugni_ep_atomic_cswap32(uct_ep_h tl_ep, uint32_t compare, uint32_t swap,
                                         uint64_t remote_addr, uct_rkey_t rkey,
                                         uint32_t *result, uct_completion_t *comp);
+ucs_status_t uct_ugni_ep_atomic64_post(uct_ep_h ep, unsigned opcode, uint64_t value,
+                                       uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_ugni_ep_atomic32_post(uct_ep_h ep, unsigned opcode, uint32_t value,
+                                       uint64_t remote_addr, uct_rkey_t rkey);
+ucs_status_t uct_ugni_ep_atomic64_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                        uint64_t value, uint64_t *result,
+                                        uint64_t remote_addr, uct_rkey_t rkey,
+                                        uct_completion_t *comp);
+ucs_status_t uct_ugni_ep_atomic32_fetch(uct_ep_h ep, uct_atomic_op_t opcode,
+                                        uint32_t value, uint32_t *result,
+                                        uint64_t remote_addr, uct_rkey_t rkey,
+                                        uct_completion_t *comp);
 ucs_status_t uct_ugni_ep_get_bcopy(uct_ep_h tl_ep,
                                    uct_unpack_callback_t unpack_cb,
                                    void *arg, size_t length,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_iface.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_iface.c
index a07bdce51..77a395ef5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_iface.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/rdma/ugni_rdma_iface.c
@@ -62,22 +62,39 @@ static ucs_status_t uct_ugni_rdma_iface_query(uct_iface_h tl_iface, uct_iface_at
     iface_attr->cap.flags              = UCT_IFACE_FLAG_PUT_SHORT |
                                          UCT_IFACE_FLAG_PUT_BCOPY |
                                          UCT_IFACE_FLAG_PUT_ZCOPY |
-                                         UCT_IFACE_FLAG_ATOMIC_CSWAP64 |
-                                         UCT_IFACE_FLAG_ATOMIC_FADD64  |
-                                         UCT_IFACE_FLAG_ATOMIC_ADD64   |
-                                         UCT_IFACE_FLAG_ATOMIC_DEVICE  |
                                          UCT_IFACE_FLAG_GET_BCOPY      |
                                          UCT_IFACE_FLAG_GET_ZCOPY      |
                                          UCT_IFACE_FLAG_CONNECT_TO_IFACE |
+                                         UCT_IFACE_FLAG_ATOMIC_DEVICE |
                                          UCT_IFACE_FLAG_PENDING;
 
+    iface_attr->cap.atomic64.op_flags  = UCS_BIT(UCT_ATOMIC_OP_ADD)|
+                                         UCS_BIT(UCT_ATOMIC_OP_AND) |
+                                         UCS_BIT(UCT_ATOMIC_OP_OR)  |
+                                         UCS_BIT(UCT_ATOMIC_OP_XOR);
+
+    iface_attr->cap.atomic64.fop_flags = UCS_BIT(UCT_ATOMIC_OP_ADD)    |
+                                         UCS_BIT(UCT_ATOMIC_OP_AND)   |
+                                         UCS_BIT(UCT_ATOMIC_OP_OR)    |
+                                         UCS_BIT(UCT_ATOMIC_OP_XOR)   |
+                                         UCS_BIT(UCT_ATOMIC_OP_SWAP)  |
+                                         UCS_BIT(UCT_ATOMIC_OP_CSWAP);
+
+
     if (uct_ugni_check_device_type(&iface->super, GNI_DEVICE_ARIES)) {
-        iface_attr->cap.flags         |= UCT_IFACE_FLAG_PUT_SHORT |
-                                         UCT_IFACE_FLAG_ATOMIC_SWAP64 |
-                                         UCT_IFACE_FLAG_ATOMIC_SWAP32 |
-                                         UCT_IFACE_FLAG_ATOMIC_FADD32 |
-                                         UCT_IFACE_FLAG_ATOMIC_ADD32 |
-                                         UCT_IFACE_FLAG_ATOMIC_CSWAP32;
+        iface_attr->cap.flags              |= UCT_IFACE_FLAG_PUT_SHORT;
+
+        iface_attr->cap.atomic64.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_SWAP);
+        iface_attr->cap.atomic32.op_flags  |= UCS_BIT(UCT_ATOMIC_OP_ADD) |
+                                              UCS_BIT(UCT_ATOMIC_OP_AND) |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR);
+        iface_attr->cap.atomic32.fop_flags |= UCS_BIT(UCT_ATOMIC_OP_ADD)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_AND)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_OR)    |
+                                              UCS_BIT(UCT_ATOMIC_OP_XOR)   |
+                                              UCS_BIT(UCT_ATOMIC_OP_SWAP)  |
+                                              UCS_BIT(UCT_ATOMIC_OP_CSWAP);
     }
     iface_attr->overhead               = 80e-9; /* 80 ns */
     iface_attr->latency.overhead       = 900e-9; /* 900 ns */
@@ -169,14 +186,12 @@ static uct_iface_ops_t uct_ugni_aries_rdma_iface_ops = {
     .ep_get_bcopy             = uct_ugni_ep_get_bcopy,
     .ep_get_zcopy             = uct_ugni_ep_get_zcopy,
     .ep_am_short              = uct_ugni_ep_am_short,
-    .ep_atomic_add64          = uct_ugni_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_ugni_ep_atomic_fadd64,
     .ep_atomic_cswap64        = uct_ugni_ep_atomic_cswap64,
-    .ep_atomic_swap64         = uct_ugni_ep_atomic_swap64,
-    .ep_atomic_add32          = uct_ugni_ep_atomic_add32,
-    .ep_atomic_fadd32         = uct_ugni_ep_atomic_fadd32,
     .ep_atomic_cswap32        = uct_ugni_ep_atomic_cswap32,
-    .ep_atomic_swap32         = uct_ugni_ep_atomic_swap32,
+    .ep_atomic64_post         = uct_ugni_ep_atomic64_post,
+    .ep_atomic32_post         = uct_ugni_ep_atomic32_post,
+    .ep_atomic64_fetch        = uct_ugni_ep_atomic64_fetch,
+    .ep_atomic32_fetch        = uct_ugni_ep_atomic32_fetch,
     .ep_pending_add           = uct_ugni_ep_pending_add,
     .ep_pending_purge         = uct_ugni_ep_pending_purge,
     .ep_flush                 = uct_ugni_ep_flush,
@@ -202,8 +217,6 @@ static uct_iface_ops_t uct_ugni_gemini_rdma_iface_ops = {
     .ep_get_bcopy             = uct_ugni_ep_get_bcopy,
     .ep_get_zcopy             = uct_ugni_ep_get_zcopy,
     .ep_am_short              = uct_ugni_ep_am_short,
-    .ep_atomic_add64          = uct_ugni_ep_atomic_add64,
-    .ep_atomic_fadd64         = uct_ugni_ep_atomic_fadd64,
     .ep_atomic_cswap64        = uct_ugni_ep_atomic_cswap64,
     .ep_pending_add           = uct_ugni_ep_pending_add,
     .ep_pending_purge         = uct_ugni_ep_pending_purge,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.c b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.c
index aa2ffe274..99c31f460 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.c
@@ -11,10 +11,11 @@
 
 #define uct_ugni_udt_can_send(_ep) ((uct_ugni_ep_can_send(&_ep->super)) && (_ep->posted_desc == NULL))
 
-ucs_status_t uct_ugni_udt_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n)
+ucs_status_t uct_ugni_udt_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                         unsigned flags)
 {
     uct_ugni_iface_t *iface = ucs_derived_of(tl_ep->iface, uct_ugni_iface_t);
-    ucs_status_t status = uct_ugni_ep_pending_add(tl_ep, n);
+    ucs_status_t status = uct_ugni_ep_pending_add(tl_ep, n, flags);
 
     if (UCS_OK == status) {
         uct_worker_progress_add_safe(iface->super.worker, uct_ugni_udt_progress,
diff --git a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.h b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.h
index 05559e77b..48d11de38 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/src/uct/ugni/udt/ugni_udt_ep.h
@@ -29,7 +29,8 @@ ucs_status_t uct_ugni_udt_ep_am_short(uct_ep_h tl_ep, uint8_t id, uint64_t heade
 ssize_t uct_ugni_udt_ep_am_bcopy(uct_ep_h tl_ep, uint8_t id,
                                  uct_pack_callback_t pack_cb, void *arg,
                                  unsigned flags);
-ucs_status_t uct_ugni_udt_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n);
+ucs_status_t uct_ugni_udt_ep_pending_add(uct_ep_h tl_ep, uct_pending_req_t *n,
+                                         unsigned flags);
 ucs_arbiter_cb_result_t uct_ugni_udt_ep_process_pending(ucs_arbiter_t *arbiter,
                                                         ucs_arbiter_elem_t *elem,
                                                         void *arg);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/test/apps/Makefile.am
index b4ea0552c..4341b5659 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/apps/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/Makefile.am
@@ -6,23 +6,28 @@
 # See file LICENSE for terms.
 #
 
+if HAVE_CXX11
+SUBDIRS = sockaddr
+endif
+
 noinst_PROGRAMS = \
 	test_dlopen
 
 
 objdir = $(shell sed -n -e 's/^objdir=\(.*\)$$/\1/p' $(LIBTOOL))
 
-test_dlopen_SOURCES  = dlopen.c
+test_dlopen_SOURCES  = test_dlopen.c
 test_dlopen_CPPFLAGS = $(BASE_CPPFLAGS) -g -DUCP_LIB_PATH=$(abs_top_builddir)/src/ucp/$(objdir)/libucp.so
 test_dlopen_CFLAGS   = $(BASE_CFLAGS)
 test_dlopen_LDADD    = -ldl
 
-if HAVE_PROFILING
-noinst_PROGRAMS += \
-	test_profiling
-
-test_profiling_SOURCES  = profiling.c
-test_profiling_LDADD    = $(top_builddir)/src/ucs/libucs.la -lm
-test_profiling_CPPFLAGS = $(BASE_CPPFLAGS) -g -I$(top_srcdir)/src
-test_profiling_CFLAGS   = $(BASE_CFLAGS)
+if HAVE_TCMALLOC
+noinst_PROGRAMS       += test_tcmalloc
+test_tcmalloc_SOURCES  = test_tcmalloc.c
+test_tcmalloc_CPPFLAGS = $(BASE_CPPFLAGS) -g \
+				-I$(abs_top_builddir)/src \
+				-I$(abs_top_srcdir)/src
+test_tcmalloc_CFLAGS   = $(BASE_CFLAGS)
+test_tcmalloc_LDADD    = -ldl $(TCMALLOC_LIB) \
+                          $(top_builddir)/src/ucp/libucp.la
 endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/Makefile.am
new file mode 100644
index 000000000..7ce7a019f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/Makefile.am
@@ -0,0 +1,23 @@
+#
+# Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+#
+# See file LICENSE for terms.
+#
+
+noinst_PROGRAMS = sa
+
+noinst_HEADERS = \
+	sa_base.h \
+	sa_tcp.h \
+	sa_util.h
+
+sa_CXXFLAGS = \
+	-std=c++11 -g -Wall -Werror
+
+sa_CPPFLAGS = $(BASE_CPPFLAGS)
+
+sa_SOURCES = \
+	sa_base.cc \
+	sa_main.cc \
+	sa_tcp.cc \
+	sa_util.cc
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.cc b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.cc
new file mode 100644
index 000000000..dfb7389ff
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.cc
@@ -0,0 +1,37 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "sa_base.h"
+#include "sa_tcp.h"
+#include "sa_util.h"
+
+#include <cstring>
+
+
+connection::~connection() {
+}
+
+void connection::set_id(uint64_t id) {
+    m_id = id;
+}
+
+uint64_t connection::id() const {
+    return m_id;
+}
+
+worker::~worker() {
+}
+
+std::shared_ptr<worker> worker::make(const std::string& mode,
+                                     const struct sockaddr *listen_addr,
+                                     socklen_t addrlen)
+{
+    if (mode == "tcp") {
+        return std::make_shared<tcp_worker>(listen_addr, addrlen);
+    } else {
+        throw error("invalid mode: " + mode);
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.h b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.h
new file mode 100644
index 000000000..531f044d8
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_base.h
@@ -0,0 +1,68 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef SA_BASE_H_
+#define SA_BASE_H_
+
+#include "sa_util.h"
+
+#include <sys/socket.h>
+#include <functional>
+#include <cstdint>
+#include <cstddef>
+#include <memory>
+
+
+/* interface for classes which generate events */
+class event_source {
+public:
+    virtual void add_to_evpoll(evpoll_set& evpoll) = 0;
+};
+
+
+/* one data connection */
+class connection : public event_source {
+public:
+    virtual ~connection();
+
+    virtual size_t send(const char *buffer, size_t size) = 0;
+
+    virtual size_t recv(char *buffer, size_t size) = 0;
+
+    virtual bool is_closed() const = 0;
+
+    uint64_t id() const;
+
+protected:
+    void set_id(uint64_t id);
+
+private:
+    uint64_t m_id;
+};
+
+typedef std::shared_ptr<connection> conn_ptr_t;
+
+
+/* communication context */
+class worker : public event_source {
+public:
+    typedef std::function<void(conn_ptr_t)>         conn_handler_t;
+    typedef std::function<void(uint64_t, uint32_t)> data_handler_t;
+
+    virtual ~worker();
+
+    virtual conn_ptr_t connect(const struct sockaddr *addr, socklen_t addrlen) = 0;
+
+    virtual void wait(const evpoll_set& evpoll, conn_handler_t conn_handler,
+                      data_handler_t data_handler, int timeout_ms) = 0;
+
+    /* factory function to create workers of given type */
+    static std::shared_ptr<worker> make(const std::string& mode,
+                                        const struct sockaddr *listen_addr,
+                                        socklen_t addrlen);
+};
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_main.cc b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_main.cc
new file mode 100644
index 000000000..57ccf8814
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_main.cc
@@ -0,0 +1,420 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "sa_base.h"
+
+#include <iostream>
+#include <vector>
+#include <fstream>
+#include <sstream>
+#include <cstring>
+#include <map>
+#include <sys/epoll.h>
+#include <getopt.h>
+#include <netdb.h>
+#include <unistd.h>
+
+
+class application {
+public:
+    class usage_exception : public error {
+    public:
+        usage_exception(const std::string& message = "");
+    };
+
+    application(int argc, char **argv);
+
+    int run();
+
+    static void usage(const std::string& error);
+
+private:
+    typedef struct {
+        std::string        hostname;
+        int                port;
+    } dest_t;
+
+    typedef std::vector<dest_t> dest_vec_t;
+
+    enum connection_type {
+        CONNECTION_CLIENT,
+        CONNECTION_SERVER
+    };
+
+    struct params {
+        params() : port(0),
+                   total_conns(1000),
+                   conn_ratio(1.5),
+                   request_size(32),
+                   response_size(1024) {
+        }
+
+        std::string         mode;
+        int                 port;
+        int                 total_conns;
+        double              conn_ratio;
+        size_t              request_size;
+        size_t              response_size;
+        dest_vec_t          dests;
+   };
+
+    struct connection_state {
+        conn_ptr_t          conn_ptr;
+        connection_type     conn_type;
+        size_t              bytes_sent;
+        size_t              bytes_recvd;
+        std::string         send_data;
+        std::string         recv_data;
+    };
+
+    typedef std::shared_ptr<connection_state>    conn_state_ptr_t;
+    typedef std::map<uint64_t, conn_state_ptr_t> conn_map_t;
+
+    void parse_hostfile(const std::string& filename);
+
+    void initiate_connections();
+
+    int max_conns_inflight() const;
+
+    void create_worker();
+
+    void add_connection(conn_ptr_t conn_ptr, connection_type conn_type);
+
+    conn_ptr_t connect(const dest_t& dst);
+
+    void advance_connection(conn_state_ptr_t s, uint32_t events);
+
+    void connection_completed(conn_state_ptr_t s);
+
+    static void pton(const dest_t& dst, struct sockaddr_storage& saddr,
+                     socklen_t &addrlen);
+
+    template <typename O>
+    friend typename O::__basic_ostream& operator<<(O& os, connection_type conn_type);
+
+    params                  m_params;
+    std::shared_ptr<worker> m_worker;
+    evpoll_set              m_evpoll;
+    conn_map_t              m_connections;
+    int                     m_num_conns_inflight;
+    int                     m_num_conns_started;
+};
+
+
+application::usage_exception::usage_exception(const std::string& message) :
+                error(message) {
+};
+
+application::application(int argc, char **argv) : m_num_conns_inflight(0),
+                m_num_conns_started(0) {
+    int c;
+
+    while ( (c = getopt(argc, argv, "p:f:m:r:n:S:s:vh")) != -1 ) {
+        switch (c) {
+        case 'p':
+            m_params.port = atoi(optarg);
+            break;
+        case 'f':
+            parse_hostfile(optarg);
+            break;
+        case 'm':
+            m_params.mode = optarg;
+            break;
+        case 'r':
+            m_params.conn_ratio = atof(optarg);
+            break;
+        case 'n':
+            m_params.total_conns = atoi(optarg);
+            break;
+        case 'S':
+            m_params.request_size = atoi(optarg);
+            break;
+        case 's':
+            m_params.response_size = atoi(optarg);
+            break;
+        case 'v':
+            log::more_verbose();
+            break;
+        default:
+            throw usage_exception();
+        }
+    }
+
+    if (m_params.mode.empty()) {
+        throw usage_exception("missing mode argument");
+    }
+
+    if (m_params.dests.empty()) {
+        throw usage_exception("no remote destinations specified");
+    }
+
+    if (m_params.port == 0) {
+        throw usage_exception("local port not specified");
+    }
+}
+
+int application::run() {
+    LOG_INFO << "starting application with "
+             << max_conns_inflight() << " simultaneous connections, "
+             << m_params.total_conns << " total";
+
+    create_worker();
+
+    while ((m_num_conns_started > m_params.total_conns) || !m_connections.empty()) {
+        initiate_connections();
+        m_worker->wait(m_evpoll,
+                       [this](conn_ptr_t conn) {
+                           LOG_DEBUG << "accepted new connection";
+                           add_connection(conn, CONNECTION_SERVER);
+                       },
+                       [this](uint64_t conn_id, uint32_t events) {
+                           LOG_DEBUG << "new event on connection id "
+                                     << conn_id << " events "
+                                     << ((events & EPOLLIN ) ? "i" : "")
+                                     << ((events & EPOLLOUT) ? "o" : "")
+                                     << ((events & EPOLLERR) ? "e" : "")
+                                     ;
+                           advance_connection(m_connections.at(conn_id), events);
+                       },
+                       -1);
+    }
+
+    LOG_INFO << "all connections completed";
+
+    m_worker.reset();
+    return 0;
+}
+
+void application::create_worker() {
+    struct sockaddr_in inaddr_any;
+    memset(&inaddr_any, 0, sizeof(inaddr_any));
+    inaddr_any.sin_family      = AF_INET;
+    inaddr_any.sin_port        = htons(m_params.port);
+    inaddr_any.sin_addr.s_addr = INADDR_ANY;
+
+    m_worker = worker::make(m_params.mode, reinterpret_cast<struct sockaddr *>(&inaddr_any),
+                            sizeof(inaddr_any));
+    m_worker->add_to_evpoll(m_evpoll);
+}
+
+std::shared_ptr<connection> application::connect(const dest_t& dst) {
+    struct sockaddr_storage saddr;
+    socklen_t addrlen;
+    pton(dst, saddr, addrlen);
+    return m_worker->connect(reinterpret_cast<const struct sockaddr*>(&saddr),
+                             addrlen);
+}
+
+template <typename O>
+typename O::__basic_ostream& operator<<(O& os, application::connection_type conn_type) {
+    switch (conn_type) {
+    case application::CONNECTION_CLIENT:
+        return os << "client";
+    case application::CONNECTION_SERVER:
+        return os << "server";
+    default:
+        return os;
+    }
+}
+
+void application::add_connection(conn_ptr_t conn_ptr, connection_type conn_type) {
+    auto s = std::make_shared<connection_state>();
+    s->conn_type   = conn_type;
+    s->conn_ptr    = conn_ptr;
+    s->bytes_sent  = 0;
+    s->bytes_recvd = 0;
+
+    switch (s->conn_type) {
+    case CONNECTION_CLIENT:
+        s->send_data.assign(m_params.request_size, 'r');
+        s->recv_data.resize(m_params.response_size);
+        break;
+    case CONNECTION_SERVER:
+        s->send_data.resize(m_params.response_size);
+        s->recv_data.resize(m_params.request_size);
+        break;
+    }
+
+    LOG_DEBUG << "add " << conn_type << " connection with id " << conn_ptr->id();
+    conn_ptr->add_to_evpoll(m_evpoll);
+    m_connections[conn_ptr->id()] = s;
+    advance_connection(s, 0);
+}
+
+void application::initiate_connections() {
+    int max = max_conns_inflight();
+    while ((m_num_conns_started < m_params.total_conns) && (m_num_conns_inflight < max)) {
+        /* coverity[dont_call] */
+        const dest_t& dest = m_params.dests[::rand() % m_params.dests.size()];
+        ++m_num_conns_started;
+        ++m_num_conns_inflight;
+        LOG_DEBUG << "connecting to " << dest.hostname << ":" << dest.port;
+        add_connection(connect(dest), CONNECTION_CLIENT);
+    }
+}
+
+int application::max_conns_inflight() const {
+    return m_params.conn_ratio * m_params.dests.size() + 0.5;
+}
+
+void application::advance_connection(conn_state_ptr_t s, uint32_t events) {
+    LOG_DEBUG << "advance " << s->conn_type << " connection id " << s->conn_ptr->id()
+              << " total sent " << s->bytes_sent << ", received " << s->bytes_recvd;
+    switch (s->conn_type) {
+    case CONNECTION_CLIENT:
+        if (s->bytes_sent < m_params.request_size) {
+            /* more data should be sent */
+            size_t nsent = s->conn_ptr->send(&s->send_data[s->bytes_sent],
+                                             m_params.request_size - s->bytes_sent);
+            LOG_DEBUG << "sent " << nsent << " bytes on connection id "
+                      << s->conn_ptr->id();
+            s->bytes_sent += nsent;
+        }
+        if (events & EPOLLIN) {
+            size_t nrecv = s->conn_ptr->recv(&s->recv_data[s->bytes_recvd],
+                                             m_params.response_size - s->bytes_recvd);
+            LOG_DEBUG << "received " << nrecv << " bytes on connection id "
+                       << s->conn_ptr->id();
+            s->bytes_recvd += nrecv;
+        }
+        if (s->bytes_recvd == m_params.response_size) {
+            connection_completed(s);
+        }
+        break;
+    case CONNECTION_SERVER:
+        if (events & EPOLLIN) {
+            size_t nrecv = s->conn_ptr->recv(&s->recv_data[s->bytes_recvd],
+                                             m_params.request_size - s->bytes_recvd);
+            LOG_DEBUG << "received " << nrecv << " bytes on connection id "
+                      << s->conn_ptr->id();
+            s->bytes_recvd += nrecv;
+        }
+        if ((s->bytes_recvd == m_params.request_size) &&
+            (s->bytes_sent < m_params.response_size)) {
+            /* more data should be sent */
+            size_t nsent = s->conn_ptr->send(&s->send_data[s->bytes_sent],
+                                             m_params.response_size - s->bytes_sent);
+            LOG_DEBUG << "sent " << nsent << " bytes on connection id "
+                      << s->conn_ptr->id();
+            s->bytes_sent += nsent;
+        }
+        if (s->conn_ptr->is_closed()) {
+            connection_completed(s);
+        }
+        break;
+    }
+}
+
+void application::connection_completed(conn_state_ptr_t s) {
+    LOG_DEBUG << "completed " << s->conn_type << " connection id " << s->conn_ptr->id();
+    m_connections.erase(s->conn_ptr->id());
+    --m_num_conns_inflight;
+}
+
+void application::pton(const dest_t& dst, struct sockaddr_storage& saddr,
+                       socklen_t &addrlen) {
+
+    struct hostent *he = gethostbyname(dst.hostname.c_str());
+    if (he == NULL || he->h_addr_list == NULL) {
+        throw error("host " + dst.hostname + " not found: "+ hstrerror(h_errno));
+    }
+
+    memset(&saddr, 0, sizeof(saddr));
+    saddr.ss_family = he->h_addrtype;
+
+    void *addr;
+    int addr_datalen = 0;
+    switch (saddr.ss_family) {
+    case AF_INET:
+        reinterpret_cast<struct sockaddr_in*>(&saddr)->sin_port =
+                        htons(dst.port);
+        addr         = &reinterpret_cast<struct sockaddr_in*>(&saddr)->sin_addr;
+        addrlen      = sizeof(struct sockaddr_in);
+        addr_datalen = sizeof(struct in_addr);
+        break;
+    case AF_INET6:
+        reinterpret_cast<struct sockaddr_in6*>(&saddr)->sin6_port =
+                        htons(dst.port);
+        addr         = &reinterpret_cast<struct sockaddr_in6*>(&saddr)->sin6_addr;
+        addrlen      = sizeof(struct sockaddr_in6);
+        addr_datalen = sizeof(struct in6_addr);
+        break;
+    default:
+        throw error("unsupported address family");
+    }
+
+    if (he->h_length != addr_datalen) {
+        throw error("mismatching address length");
+    }
+
+    memcpy(addr, he->h_addr_list[0], addr_datalen);
+}
+
+void application::usage(const std::string& error) {
+    if (!error.empty()) {
+        std::cout << "Error: " << error << std::endl;
+        std::cout << std::endl;
+    }
+
+    params defaults;
+    std::cout << "Usage: ./sa [ options ]" << std::endl;
+    std::cout << "Options:"                                                           << std::endl;
+    std::cout << "    -m <mode>    Application mode (tcp)"                            << std::endl;
+    std::cout << "    -p <port>    Local port number to listen on"                    << std::endl;
+    std::cout << "    -f <file>    File with list of hosts and ports to connect to"   << std::endl;
+    std::cout << "                 Each line in the file is formatter as follows:"    << std::endl;
+    std::cout << "                    <address> <port>"                               << std::endl;
+    std::cout << "    -r <ratio>   How many in-flight connection to hold as multiple" << std::endl;
+    std::cout << "                 of number of possible destinations (" << defaults.conn_ratio << ")" << std::endl;
+    std::cout << "    -n <count>   How many total exchanges to perform (" << defaults.total_conns << ")" << std::endl;
+    std::cout << "    -S <size>    Request message size, in bytes (" << defaults.request_size << ")" << std::endl;
+    std::cout << "    -s <size>    Response message size, in bytes (" << defaults.response_size << ")" << std::endl;
+    std::cout << "    -v           Increase verbosity level (may be specified several times)" << std::endl;
+}
+
+void application::parse_hostfile(const std::string& filename) {
+    std::ifstream f(filename.c_str());
+    if (!f) {
+        throw error("failed to open '" + filename + "'");
+    }
+
+    /*
+     * Each line in the file contains 2 whitespace-separated tokens: host-name
+     * and port number.
+     */
+    std::string line;
+    int lineno = 1;
+    while (std::getline(f, line)) {
+        std::stringstream ss(line);
+        if (line.empty()) {
+            continue;
+        }
+
+        dest_t dest;
+        if ((ss >> dest.hostname) && (ss >> dest.port)) {
+            m_params.dests.push_back(dest);
+        } else {
+            std::stringstream errss;
+            errss << "syntax error in file '" << filename << "' line " << lineno <<
+                     " near `" << line << "'";
+            throw error(errss.str());
+        }
+        ++lineno;
+    }
+}
+
+int main(int argc, char **argv)
+{
+    try {
+        application app(argc, argv);
+        return app.run();
+    } catch (application::usage_exception& e) {
+        application::usage(e.what());
+        return -127;
+    } catch (error& e) {
+        std::cerr << "Error: " << e.what() << std::endl;
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.cc b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.cc
new file mode 100644
index 000000000..7d15776f3
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.cc
@@ -0,0 +1,127 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "sa_tcp.h"
+
+#include <sys/socket.h>
+#include <sys/epoll.h>
+#include <sys/fcntl.h>
+#include <arpa/inet.h>
+#include <unistd.h>
+#include <cstring>
+#include <cerrno>
+
+
+tcp_socket::tcp_socket() : file_desc(create_socket()) {
+}
+
+tcp_socket::tcp_socket(int fd) : file_desc(fd) {
+}
+
+tcp_socket::~tcp_socket() {
+}
+
+int tcp_socket::create_socket() {
+    int fd = ::socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
+    if (fd < 0) {
+        throw sys_error("failed to create tcp socket", errno);
+    }
+    return fd;
+}
+
+tcp_connection::tcp_connection(const struct sockaddr *addr, socklen_t addrlen) :
+                m_is_closed(false) {
+    initialize();
+    int ret = ::connect(m_socket, addr, addrlen);
+    if ((ret < 0) && (errno != EINPROGRESS)) {
+        throw sys_error("failed to connect tcp socket", errno);
+    }
+}
+
+tcp_connection::tcp_connection(int fd) : m_socket(fd), m_is_closed(false) {
+    initialize();
+}
+
+void tcp_connection::initialize() {
+    int ret = fcntl(m_socket, F_SETFL, fcntl(m_socket, F_GETFL) | O_NONBLOCK);
+    if (ret < 0) {
+        throw sys_error("failed to set tcp socket to nonblocking", errno);
+    }
+
+    set_id(m_socket);
+}
+
+void tcp_connection::add_to_evpoll(evpoll_set& evpoll) {
+    evpoll.add(m_socket, EPOLLIN | EPOLLOUT | EPOLLERR | EPOLLET);
+}
+
+size_t tcp_connection::send(const char *buffer, size_t size) {
+    ssize_t ret = ::send(m_socket, buffer, size, 0);
+    if (ret < 0) {
+        if (errno != EAGAIN) {
+            throw sys_error("failed to send on tcp socket", errno);
+        }
+        return 0;
+    }
+    return ret;
+}
+
+size_t tcp_connection::recv(char *buffer, size_t size) {
+    ssize_t ret = ::recv(m_socket, buffer, size, 0);
+    if (ret < 0) {
+        if (errno != EAGAIN) {
+            throw sys_error("failed to receive from tcp socket", errno);
+        }
+        return 0;
+    }
+    if (ret == 0) {
+        m_is_closed = true;
+    }
+    return ret;
+}
+
+bool tcp_connection::is_closed() const {
+    return m_is_closed;
+}
+
+tcp_worker::tcp_worker(const struct sockaddr *listen_addr, socklen_t addrlen) {
+    int retb = ::bind(m_server_socket, listen_addr, addrlen);
+    if (retb != 0) {
+        throw sys_error("failed to bind tcp socket", errno);
+    }
+
+    int retl = ::listen(m_server_socket, 1024);
+    if (retl != 0) {
+        throw sys_error("failed to listen on tcp socket", errno);
+    }
+}
+
+void tcp_worker::add_to_evpoll(evpoll_set& evpoll) {
+    evpoll.add(m_server_socket, EPOLLIN | EPOLLERR);
+}
+
+void tcp_worker::wait(const evpoll_set& evpoll, conn_handler_t conn_handler,
+                      data_handler_t data_handler, int timeout_ms) {
+    std::vector<evpoll_set::event> events;
+    evpoll.wait(events, timeout_ms);
+    for (auto ev : events) {
+        if (ev.fd == m_server_socket) {
+            int ret = accept(m_server_socket, NULL, NULL);
+            if (ret < 0) {
+                throw sys_error("failed to accept", errno);
+            }
+            auto conn = std::make_shared<tcp_connection>(ret);
+            conn_handler(conn);
+        } else {
+            data_handler(ev.fd, ev.ev_flags);
+        }
+    }
+}
+
+std::shared_ptr<connection> tcp_worker::connect(const struct sockaddr *addr,
+                                                socklen_t addrlen) {
+    return std::make_shared<tcp_connection>(addr, addrlen);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.h b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.h
new file mode 100644
index 000000000..f8c371733
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_tcp.h
@@ -0,0 +1,64 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef SA_TCP_H_
+#define SA_TCP_H_
+
+#include "sa_base.h"
+#include "sa_util.h"
+
+
+class tcp_socket : public file_desc {
+public:
+    tcp_socket();
+
+    tcp_socket(int fd);
+
+    virtual ~tcp_socket();
+
+private:
+    static int create_socket();
+};
+
+
+class tcp_connection : public connection {
+public:
+    tcp_connection(const struct sockaddr *addr, socklen_t addrlen);
+
+    tcp_connection(int fd);
+
+    virtual void add_to_evpoll(evpoll_set& evpoll);
+
+    virtual size_t send(const char *buffer, size_t size);
+
+    virtual size_t recv(char *buffer, size_t size);
+
+    virtual bool is_closed() const;
+
+private:
+    void initialize();
+
+    tcp_socket m_socket;
+    bool       m_is_closed;
+};
+
+
+class tcp_worker : public worker {
+public:
+    tcp_worker(const struct sockaddr *listen_addr, socklen_t addrlen);
+
+    virtual void add_to_evpoll(evpoll_set& evpoll);
+
+    virtual conn_ptr_t connect(const struct sockaddr *addr, socklen_t addrlen);
+
+    virtual void wait(const evpoll_set& evpoll, conn_handler_t conn_handler,
+                      data_handler_t data_handler, int timeout_ms);
+
+private:
+    tcp_socket m_server_socket;
+};
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.cc b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.cc
new file mode 100644
index 000000000..9e2a97e67
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.cc
@@ -0,0 +1,124 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include "sa_util.h"
+
+#include <sys/epoll.h>
+#include <sys/time.h>
+#include <unistd.h>
+#include <cstring>
+#include <climits>
+
+
+error::error(const std::string& message) : m_message(message) {
+}
+
+error::~error() throw() {
+}
+
+const char* error::what() const throw() {
+    return m_message.c_str();
+}
+
+sys_error::~sys_error() throw() {
+}
+
+sys_error::sys_error(const std::string& message, int errn) :
+    error(message + ": " + strerror(errn) + " (" + std::to_string(errn) + ")") {
+}
+
+file_desc::file_desc(int fd) : m_fd(fd) {
+}
+
+file_desc::~file_desc() {
+    int ret = ::close(m_fd);
+    if (ret < 0) {
+        fprintf(stderr, "Warning: failed to close fd %d: %m", m_fd);
+    }
+}
+
+file_desc::operator int() const {
+    return m_fd;
+}
+
+evpoll_set::evpoll_set() : file_desc(create_epfd()) {
+}
+
+void evpoll_set::add(int fd, uint32_t ev_flags) {
+    struct epoll_event ev;
+    memset(&ev, 0, sizeof(ev));
+    ev.events  = ev_flags;
+    ev.data.fd = fd;
+    int ret = ::epoll_ctl(*this, EPOLL_CTL_ADD, fd, &ev);
+    if (ret != 0) {
+        throw sys_error("failed to add fd to epoll", errno);
+    }
+}
+
+void evpoll_set::wait(std::vector<event>& events, int timeout_ms) const {
+    static const size_t maxevents = 32;
+    struct epoll_event ev_array[maxevents];
+
+    LOG_DEBUG << "epoll_wait with timeout " << timeout_ms << " milliseconds";
+    int ret = epoll_wait(*this, ev_array, maxevents, timeout_ms);
+    if (ret < 0) {
+        if (errno != EINTR) {
+            throw sys_error("epoll_wait failed", errno);
+        }
+    } else {
+        for (int i = 0; i < ret; ++i) {
+            event ev = { ev_array[i].data.fd, ev_array[i].events };
+            events.push_back(ev);
+        }
+    }
+}
+
+int evpoll_set::create_epfd() {
+    int fd = epoll_create(1);
+    if (fd < 0) {
+        throw sys_error("failed to create epoll set", errno);
+    }
+    return fd;
+}
+
+log::level_t log::m_log_level = INFO;
+
+log::log(log::level_t level, const std::string& file, int line) :
+            m_enabled(level <= m_log_level) {
+    if (m_enabled) {
+        struct timeval tv;
+        gettimeofday(&tv, NULL);
+
+        char cstr[64];
+        snprintf(cstr, sizeof(cstr), "[%lu.%06lu] %12s:%-5d",
+                 tv.tv_sec, tv.tv_usec, basename(file.c_str()), line);
+        m_msg << cstr << " " << level_str(level) << "   ";
+    }
+}
+
+log::~log() {
+    if (m_enabled) {
+        m_msg << std::endl;
+        std::cout << m_msg.str() << std::flush;
+    }
+}
+
+std::string log::level_str(log::level_t level) {
+    switch (level) {
+    case INFO:
+        return "INFO ";
+    case DEBUG:
+        return "DEBUG";
+    default:
+        throw error("invalid log level");
+    }
+}
+
+void log::more_verbose() {
+    if (m_log_level == INFO) {
+        m_log_level = DEBUG;
+    }
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.h b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.h
new file mode 100644
index 000000000..7cd46bcec
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/sockaddr/sa_util.h
@@ -0,0 +1,107 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#ifndef SA_UTIL_H_
+#define SA_UTIL_H_
+
+#include <iostream>
+#include <sstream>
+#include <string>
+#include <vector>
+
+
+/* runtime error exception */
+class error : public std::exception {
+public:
+    error(const std::string& message);
+
+    virtual ~error() throw();
+
+    virtual const char* what() const throw();
+
+private:
+    std::string m_message;
+};
+
+
+/* system error exception */
+class sys_error : public error {
+public:
+    virtual ~sys_error() throw();
+
+    sys_error(const std::string& message, int errn);
+};
+
+
+/* file descriptor wrapper which closes the file automatically */
+class file_desc {
+public:
+    file_desc(int fd);
+
+    virtual ~file_desc();
+
+    operator int() const;
+
+private:
+    file_desc(const file_desc&);
+
+    const file_desc& operator=(const file_desc&);
+
+    int m_fd;
+};
+
+
+/* event poll set */
+class evpoll_set : public file_desc {
+public:
+    struct event {
+        int      fd;
+        uint32_t ev_flags;
+    };
+
+    evpoll_set();
+
+    void add(int fd, uint32_t ev_flags);
+
+    void wait(std::vector<event>& events, int timeout_ms = -1) const;
+
+private:
+    static int create_epfd();
+};
+
+#define LOG_INFO \
+    log(log::INFO, __FILE__, __LINE__)
+#define LOG_DEBUG \
+    log(log::DEBUG, __FILE__, __LINE__)
+
+/* logger */
+class log {
+public:
+    typedef enum {
+        INFO,
+        DEBUG
+    } level_t;
+
+    log(level_t level, const std::string& file, int line);
+    ~log();
+
+    template <typename T>
+    log& operator<<(const T& value) {
+        m_msg << value;
+        return *this;
+    }
+
+    static void more_verbose();
+
+private:
+    static std::string level_str(level_t level);
+
+    static level_t     m_log_level;
+    const bool         m_enabled;
+    std::ostringstream m_msg;
+};
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/dlopen.c b/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_dlopen.c
similarity index 100%
rename from test/apps/dlopen.c
rename to test/apps/test_dlopen.c
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_tcmalloc.c b/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_tcmalloc.c
new file mode 100644
index 000000000..a987ed756
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_tcmalloc.c
@@ -0,0 +1,33 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include <ucp/api/ucp.h>
+#include <dlfcn.h>
+#include <stdio.h>
+
+
+int main(int argc, char **argv)
+{
+    ucp_params_t params;
+    ucs_status_t status;
+    ucp_context_h context;
+
+    params.field_mask = UCP_PARAM_FIELD_FEATURES;
+    params.features   = UCP_FEATURE_TAG;
+
+    status = ucp_init(&params, NULL, &context);
+    if (status != UCS_OK) {
+        return -1;
+    }
+
+    dlopen("libselinux.so", RTLD_LAZY);
+
+    ucp_cleanup(context);
+
+    printf("SUCCESS\n");
+    return 0;
+}
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_ucx_tls.py b/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_ucx_tls.py
index 6e62944c5..fcf1c8738 100755
--- a/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_ucx_tls.py
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/apps/test_ucx_tls.py
@@ -9,6 +9,8 @@ import sys
 import subprocess
 import os
 import re
+from distutils.version import LooseVersion
+
 
 #expected AM transport selections per given number of eps
 mlx4_am = {
@@ -31,7 +33,7 @@ mlx5_am = {
  1000000 :      "dc_mlx5",
 }
 
-mlx5_am_roce = {
+mlx5_am_no_dc = {
        2 :      "rc_mlx5",
       16 :      "rc_mlx5",
       32 :      "rc_mlx5",
@@ -62,12 +64,13 @@ mlx4_am_override = {
  1000000 :      "rc",
 }
 
-am_tls = {
-    "mlx4"          : mlx4_am,
-    "mlx5"          : mlx5_am,
-    "mlx5_roce"     : mlx5_am_roce,
-    "mlx4_override" : mlx4_am_override,
-    "mlx5_override" : mlx5_am_override
+am_tls =  {
+    "mlx4"            : mlx4_am,
+    "mlx5"            : mlx5_am,
+    "mlx5_roce_dc"    : mlx5_am,       # mlx5 RoCE port which supports DC
+    "mlx5_roce_no_dc" : mlx5_am_no_dc, # mlx5 RoCE port which doesn't support DC
+    "mlx4_override"   : mlx4_am_override,
+    "mlx5_override"   : mlx5_am_override
 }
 
 def find_am_transport(dev, neps, override = 0) :
@@ -111,7 +114,11 @@ for dev in sorted(dev_list):
         dev_tl_override_map = am_tls[dev[0:dev.index('_')] + "_override"]
         override = 1
     else:
-        dev_tl_map = am_tls[dev[0:dev.index('_')]+"_roce"]
+        fw_ver = open("/sys/class/infiniband/%s/fw_ver" % dev).read()
+        if LooseVersion(fw_ver) >= LooseVersion("16.23.0"):
+            dev_tl_map = am_tls[dev[0:dev.index('_')]+"_roce_dc"]
+        else:
+            dev_tl_map = am_tls[dev[0:dev.index('_')]+"_roce_no_dc"]
         override = 0
 
     for n_eps in sorted(dev_tl_map):
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/examples/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/test/examples/Makefile.am
index 73ec1931a..3eecc2af0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/examples/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/examples/Makefile.am
@@ -1,5 +1,5 @@
 #
-# Copyright (C) Mellanox Technologies Ltd. 2001-2014.  ALL RIGHTS RESERVED.
+# Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
 #
 # Copyright (C) UT-Battelle, LLC. 2015. ALL RIGHTS RESERVED.
 # See file LICENSE for terms.
@@ -9,10 +9,53 @@ examplesdir = $(pkgdatadir)/examples
 dist_examples_DATA = \
 	ucx_hello_world.h \
 	ucp_hello_world.c \
-	uct_hello_world.c
+	uct_hello_world.c \
+	ucx_profiling.c \
+	ucp_client_server.c
+
+EXAMPLE_CC_FLAGS = -lucs -lucm -I$(includedir) -L$(libdir) -Wall -Werror -Wl,-rpath,$(libdir)
 
 installcheck-local:
 	@echo "INSTALLCHECK: Compiling examples with installed library"
-	$(CC) -o uct_hello_world $(examplesdir)/uct_hello_world.c -luct -lucs -I$(includedir) -L$(libdir) -pedantic -Werror -Wl,-rpath,$(libdir)
-	$(CC) -o ucp_hello_world $(examplesdir)/ucp_hello_world.c -lucp -lucs -I$(includedir) -L$(libdir) -pedantic -Werror -Wl,-rpath,$(libdir)
-	$(RM) *.o uct_hello_world ucp_hello_world
+	$(CC) -o uct_hello_world   $(examplesdir)/uct_hello_world.c   -luct $(EXAMPLE_CC_FLAGS) -pedantic
+	$(CC) -o ucp_hello_world   $(examplesdir)/ucp_hello_world.c   -lucp $(EXAMPLE_CC_FLAGS) -pedantic
+	$(CC) -o ucp_client_server $(examplesdir)/ucp_client_server.c -lucp $(EXAMPLE_CC_FLAGS) -pedantic
+	$(CC) -o ucx_profiling     $(examplesdir)/ucx_profiling.c     -lucs $(EXAMPLE_CC_FLAGS) -Wall -lm
+	$(RM) *.o uct_hello_world ucp_hello_world ucp_client_server ucx_profiling
+
+if HAVE_EXAMPLES
+
+EXAMPLE_INCLUDE_FLAGS = \
+	-I$(abs_top_srcdir)/src \
+	-I$(abs_top_builddir)/src
+
+EXAMPLE_CPPFLAGS = \
+	$(EXAMPLE_INCLUDE_FLAGS)
+
+noinst_PROGRAMS = \
+	ucp_hello_world \
+	uct_hello_world \
+	ucx_profiling \
+	ucp_client_server
+
+ucp_hello_world_SOURCES  = ucp_hello_world.c
+ucp_hello_world_CFLAGS   = $(BASE_CFLAGS)
+ucp_hello_world_CPPFLAGS = $(EXAMPLE_CPPFLAGS)
+ucp_hello_world_LDADD    = $(top_builddir)/src/ucp/libucp.la
+
+uct_hello_world_SOURCES  = uct_hello_world.c
+uct_hello_world_CFLAGS   = $(BASE_CFLAGS)
+uct_hello_world_CPPFLAGS = $(EXAMPLE_CPPFLAGS)
+uct_hello_world_LDADD    = $(top_builddir)/src/uct/libuct.la
+
+ucp_client_server_SOURCES  = ucp_client_server.c
+ucp_client_server_CFLAGS   = $(BASE_CFLAGS)
+ucp_client_server_CPPFLAGS = $(EXAMPLE_CPPFLAGS)
+ucp_client_server_LDADD    = $(top_builddir)/src/ucp/libucp.la
+
+ucx_profiling_SOURCES  = ucx_profiling.c
+ucx_profiling_CFLAGS   = $(BASE_CFLAGS)
+ucx_profiling_CPPFLAGS = $(EXAMPLE_CPPFLAGS)
+ucx_profiling_LDADD    = $(top_builddir)/src/ucs/libucs.la
+
+endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_client_server.c b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_client_server.c
new file mode 100644
index 000000000..44f49260d
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_client_server.c
@@ -0,0 +1,485 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+/*
+ * UCP client - server example utility
+ * -----------------------------------------------
+ *
+ * Server side:
+ *
+ *    ./ucp_client_server
+ *
+ * Client side:
+ *
+ *    ./ucp_client_server -a <server-ip>
+ *
+ * Notes:
+ *
+ *    - The server will listen to incoming connection requests on INADDR_ANY.
+ *    - The client needs to pass the IP address of the server side to connect to
+ *      as an argument to the test.
+ *    - Currently, the passed IP needs to be an IPoIB or a RoCE address.
+ *    - The port which the server side would listen on can be modified with the
+ *      '-p' option and should be used on both sides. The default port to use is
+ *      13337.
+ */
+
+#include <ucp/api/ucp.h>
+
+#include <string.h>    /* memset */
+#include <arpa/inet.h> /* inet_addr */
+#include <unistd.h>    /* getopt */
+#include <stdlib.h>    /* atoi */
+
+
+const char test_message[] = "UCX Client-Server Hello World";
+static uint16_t server_port = 13337;
+
+#define TEST_STRING_LEN sizeof(test_message)
+
+
+/**
+ * Server context to be used in the user's accept callback.
+ * It holds the server's endpoint which will be created upon accepting a
+ * connection request from the client.
+ */
+typedef struct ucx_server_ctx {
+    ucp_ep_h     ep;
+} ucx_server_ctx_t;
+
+
+/**
+ * Stream request context. Holds a value to indicate whether or not the
+ * request is completed.
+ */
+typedef struct test_req {
+    int complete;
+} test_req_t;
+
+
+/**
+ * The callback on the receiving side, which is invoked upon receiving the
+ * stream message.
+ */
+static void stream_recv_cb(void *request, ucs_status_t status, size_t length)
+{
+    test_req_t *req = request;
+
+    req->complete = 1;
+
+    printf("stream_recv_cb returned with status %d (%s), length: %lu\n",
+           status, ucs_status_string(status), length);
+}
+
+/**
+ * The callback on the sending side, which is invoked after finishing sending
+ * the stream message.
+ */
+static void stream_send_cb(void *request, ucs_status_t status)
+{
+    test_req_t *req = request;
+
+    req->complete = 1;
+
+    printf("stream_send_cb returned with status %d (%s)\n",
+           status, ucs_status_string(status));
+}
+
+/**
+ * The callback on the server side which is invoked upon receiving a connection
+ * request from the client.
+ */
+static void server_accept_cb(ucp_ep_h ep, void *arg)
+{
+    ucx_server_ctx_t *context = arg;
+
+    /* Save the server's endpoint in the user's context, for future usage */
+    context->ep = ep;
+}
+
+/**
+ * Set an address for the server to listen on - INADDR_ANY on a well known port.
+ */
+void set_listen_addr(struct sockaddr_in *listen_addr)
+{
+    /* The server will listen on INADDR_ANY */
+    memset(listen_addr, 0, sizeof(struct sockaddr_in));
+    listen_addr->sin_family      = AF_INET;
+    listen_addr->sin_addr.s_addr = INADDR_ANY;
+    listen_addr->sin_port        = server_port;
+}
+
+/**
+ * Set an address to connect to. A given IP address on a well known port.
+ */
+void set_connect_addr(const char *address_str, struct sockaddr_in *connect_addr)
+{
+    memset(connect_addr, 0, sizeof(struct sockaddr_in));
+    connect_addr->sin_family      = AF_INET;
+    connect_addr->sin_addr.s_addr = inet_addr(address_str);
+    connect_addr->sin_port        = server_port;
+}
+
+/**
+ * Initialize the server side. The server starts listening on the set address
+ * and waits for its connected endpoint to be created.
+ */
+static int start_server(ucp_worker_h ucp_worker, ucx_server_ctx_t *context,
+                        ucp_listener_h *listener)
+{
+    struct sockaddr_in listen_addr;
+    ucp_listener_params_t params;
+    ucs_status_t status;
+
+    set_listen_addr(&listen_addr);
+
+    params.field_mask         = UCP_LISTENER_PARAM_FIELD_SOCK_ADDR |
+                                UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER;
+    params.sockaddr.addr      = (const struct sockaddr*)&listen_addr;
+    params.sockaddr.addrlen   = sizeof(listen_addr);
+    params.accept_handler.cb  = server_accept_cb;
+    params.accept_handler.arg = context;
+
+    /* Create a listener on the server side to listen on the given address.*/
+    status = ucp_listener_create(ucp_worker, &params, listener);
+    if (status != UCS_OK) {
+        fprintf(stderr, "failed to listen (%s)\n", ucs_status_string(status));
+    }
+
+    return status;
+}
+
+/**
+ * Initialize the client side. Create an endpoint from the client side to be
+ * connected to the remote server (to the given IP).
+ */
+static int start_client(ucp_worker_h ucp_worker, const char *ip,
+                        ucp_ep_h *client_ep)
+{
+    ucp_ep_params_t ep_params;
+    struct sockaddr_in connect_addr;
+    ucs_status_t status;
+
+    set_connect_addr(ip, &connect_addr);
+
+    /*
+     * Endpoint field mask bits:
+     * UCP_EP_PARAM_FIELD_FLAGS             - Use the value of the 'flags' field.
+     * UCP_EP_PARAM_FIELD_SOCK_ADDR         - Use a remote sockaddr to connect
+     *                                        to the remote peer.
+     * UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE - Error handling mode - this flag
+     *                                        is temporarily required since the
+     *                                        endpoint will be closed with
+     *                                        UCP_EP_CLOSE_MODE_FORCE which
+     *                                        requires this mode.
+     *                                        Once UCP_EP_CLOSE_MODE_FORCE is
+     *                                        removed, the error handling mode
+     *                                        will be removed.
+     */
+    ep_params.field_mask       = UCP_EP_PARAM_FIELD_FLAGS     |
+                                 UCP_EP_PARAM_FIELD_SOCK_ADDR |
+                                 UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
+    ep_params.err_mode         = UCP_ERR_HANDLING_MODE_PEER;
+    ep_params.flags            = UCP_EP_PARAMS_FLAGS_CLIENT_SERVER;
+    ep_params.sockaddr.addr    = (struct sockaddr*)&connect_addr;
+    ep_params.sockaddr.addrlen = sizeof(connect_addr);
+
+    status = ucp_ep_create(ucp_worker, &ep_params, client_ep);
+    if (status != UCS_OK) {
+        fprintf(stderr, "failed to connect to %s (%s)\n", ip, ucs_status_string(status));
+    }
+
+    return status;
+}
+
+/**
+ * Print the received message on the server side or the sent data on the client
+ * side.
+ */
+static void print_result(int is_server, char *recv_message)
+{
+    if (is_server) {
+        printf("\n\n----- UCP TEST SUCCESS -------\n\n");
+        printf("%s", recv_message);
+        printf("\n\n------------------------------\n\n");
+    } else {
+        printf("\n\n-----------------------------------------\n\n");
+        printf("Client sent message: \n%s.\nlength: %ld\n",
+               test_message, TEST_STRING_LEN);
+        printf("\n-----------------------------------------\n\n");
+    }
+}
+
+static void request_wait(ucp_worker_h ucp_worker, test_req_t *request)
+{
+    while (request->complete == 0) {
+        ucp_worker_progress(ucp_worker);
+    }
+
+    /* This request may be reused so initialize it for next time */
+    request->complete = 0;
+    ucp_request_free(request);
+}
+
+/**
+ * Send and receive a message using the Stream API.
+ * The client sends a message to the server and waits until the send it completed.
+ * The server receives a message from the client and waits for its completion.
+ */
+static int send_recv_stream(ucp_worker_h ucp_worker, ucp_ep_h ep, int is_server)
+{
+    char recv_message[TEST_STRING_LEN]= "";
+    test_req_t *request;
+    size_t length;
+    int ret = 0;
+
+    if (!is_server) {
+        /* Client sends a message to the server using the stream API */
+        request = ucp_stream_send_nb(ep, test_message, 1,
+                                     ucp_dt_make_contig(TEST_STRING_LEN),
+                                     stream_send_cb, 0);
+        if (UCS_PTR_IS_ERR(request)) {
+            fprintf(stderr, "unable to send UCX message (%s)\n",
+                    ucs_status_string(UCS_PTR_STATUS(request)));
+            ret = -1;
+            goto out;
+        } else if (UCS_PTR_STATUS(request) != UCS_OK) {
+            request_wait(ucp_worker, request);
+        }
+    } else {
+        /* Server receives a message from the client using the stream API */
+        request = ucp_stream_recv_nb(ep, &recv_message, 1,
+                                     ucp_dt_make_contig(TEST_STRING_LEN),
+                                     stream_recv_cb, &length , 0);
+        if (UCS_PTR_IS_ERR(request)) {
+            fprintf(stderr, "unable to receive UCX message (%s)\n",
+                    ucs_status_string(UCS_PTR_STATUS(request)));
+            ret = -1;
+            goto out;
+        } else {
+            request_wait(ucp_worker, request);
+            printf("UCX data message was received\n");
+        }
+    }
+
+    print_result(is_server, recv_message);
+
+out:
+    return ret;
+}
+
+/**
+ * Close the given endpoint.
+ * Currently closing the endpoint with UCP_EP_CLOSE_MODE_FORCE since we currently
+ * cannot rely on the client side to be present during the server's endpoint
+ * closing process.
+ */
+static void ep_close(ucp_worker_h ucp_worker, ucp_ep_h ep)
+{
+    ucs_status_t status;
+    void *close_req;
+
+    close_req = ucp_ep_close_nb(ep, UCP_EP_CLOSE_MODE_FORCE);
+    if (UCS_PTR_IS_PTR(close_req)) {
+        do {
+            ucp_worker_progress(ucp_worker);
+            status = ucp_request_check_status(close_req);
+        } while (status == UCS_INPROGRESS);
+
+        ucp_request_free(close_req);
+    } else if (UCS_PTR_STATUS(close_req) != UCS_OK) {
+        fprintf(stderr, "failed to close ep %p\n", (void*)ep);
+    }
+}
+
+/**
+ * A callback to be invoked by UCX in order to initialize the user's request.
+ */
+static void request_init(void *request)
+{
+    test_req_t *req = request;
+    req->complete = 0;
+}
+
+/**
+ * Print this application's usage help message.
+ */
+static void usage()
+{
+    fprintf(stderr, "Usage: ucp_client_server [parameters]\n");
+    fprintf(stderr, "UCP client-server example utility\n");
+    fprintf(stderr, "\nParameters are:\n");
+    fprintf(stderr, " -a Set IP address of the server "
+                    "(required for client and should not be specified "
+                    "for the server)\n");
+    fprintf(stderr, " -p Set alternative server port (default:13337)\n");
+    fprintf(stderr, "\n");
+}
+
+/**
+ * Parse the command line arguments.
+ */
+static int parse_cmd(int argc, char *const argv[], char **server_addr)
+{
+    int c = 0;
+    opterr = 0;
+
+    while ((c = getopt(argc, argv, "a:p:")) != -1) {
+        switch (c) {
+        case 'a':
+            *server_addr = optarg;
+            break;
+        case 'p':
+            server_port = atoi(optarg);
+            if (server_port < 0) {
+                fprintf(stderr, "Wrong server port number %d\n", server_port);
+                return -1;
+            }
+            break;
+        default:
+            usage();
+            return -1;
+        }
+    }
+
+    return 0;
+}
+
+/**
+ * Initialize the UCP context and worker.
+ */
+static int init_context(ucp_context_h *ucp_context, ucp_worker_h *ucp_worker)
+{
+    /* UCP objects */
+    ucp_worker_params_t worker_params;
+    ucp_params_t ucp_params;
+    ucs_status_t status;
+    int ret = 0;
+
+    memset(&ucp_params, 0, sizeof(ucp_params));
+    memset(&worker_params, 0, sizeof(worker_params));
+
+    /* UCP initialization */
+    ucp_params.field_mask   = UCP_PARAM_FIELD_FEATURES     |
+                              UCP_PARAM_FIELD_REQUEST_SIZE |
+                              UCP_PARAM_FIELD_REQUEST_INIT;
+    ucp_params.features     = UCP_FEATURE_STREAM;
+
+    ucp_params.request_size = sizeof(test_req_t);
+    ucp_params.request_init = request_init;
+
+    status = ucp_init(&ucp_params, NULL, ucp_context);
+    if (status != UCS_OK) {
+        fprintf(stderr, "failed to ucp_init (%s)\n", ucs_status_string(status));
+        ret = -1;
+        goto err;
+    }
+
+    worker_params.field_mask  = UCP_WORKER_PARAM_FIELD_THREAD_MODE;
+    worker_params.thread_mode = UCS_THREAD_MODE_SINGLE;
+
+    status = ucp_worker_create(*ucp_context, &worker_params, ucp_worker);
+    if (status != UCS_OK) {
+        fprintf(stderr, "failed to ucp_worker_create (%s)\n", ucs_status_string(status));
+        ret = -1;
+        goto err_cleanup;
+    }
+
+    return ret;
+
+err_cleanup:
+    ucp_cleanup(*ucp_context);
+
+err:
+    return ret;
+}
+
+
+int main(int argc, char **argv)
+{
+    ucx_server_ctx_t context;
+    int is_server, ret;
+    char *server_addr = NULL;
+
+    /* UCP objects */
+    ucp_context_h ucp_context;
+    ucp_listener_h listener;
+    ucp_worker_h ucp_worker;
+    ucs_status_t status;
+    ucp_ep_h ep;
+
+    ret = parse_cmd(argc, argv, &server_addr);
+    if (ret != 0) {
+        goto err;
+    }
+
+    /* Initialize the UCX required objects */
+    ret = init_context(&ucp_context, &ucp_worker);
+    if (ret != 0) {
+        goto err;
+    }
+
+    /* Client-Server initialization */
+    if (server_addr == NULL) {
+        /* Server side */
+        is_server = 1;
+
+        /* Initialize the server's endpoint to NULL. Once the server's endpoint
+         * is created, this field will have a valid value. */
+        context.ep = NULL;
+
+        status = start_server(ucp_worker, &context, &listener);
+        if (status != UCS_OK) {
+            fprintf(stderr, "failed to start server\n");
+            goto err_worker;
+        }
+
+        /* Server is always up */
+        while (1) {
+            printf("Waiting for connection...\n");
+
+            /* Wait for the server's callback to set the context->ep field, thus
+             * indicating that the server's endpoint was created and is ready to
+             * be used. The client side should initiate the connection, leading
+             * to this ep's creation */
+            while (context.ep == NULL) {
+                ucp_worker_progress(ucp_worker);
+            }
+
+            /* Client-Server communication via Stream API */
+            send_recv_stream(ucp_worker, context.ep, is_server);
+
+            /* Close the endpoint to the client */
+            ep_close(ucp_worker, context.ep);
+
+            /* Initialize server's endpoint for the next connection with a new
+             * client */
+            context.ep = NULL;
+        }
+    } else {
+        /* Client side */
+        is_server = 0;
+        status = start_client(ucp_worker, server_addr, &ep);
+        if (status != UCS_OK) {
+            fprintf(stderr, "failed to start client\n");
+            goto err_worker;
+        }
+
+        /* Client-Server communication via Stream API */
+        ret = send_recv_stream(ucp_worker, ep, is_server);
+
+        /* Close the endpoint to the server */
+        ep_close(ucp_worker, ep);
+    }
+
+err_worker:
+    ucp_worker_destroy(ucp_worker);
+
+    ucp_cleanup(ucp_context);
+
+err:
+    return ret;
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_hello_world.c b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_hello_world.c
index 49a3a4ad1..dbaeff8dc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_hello_world.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucp_hello_world.c
@@ -1,11 +1,14 @@
 /**
 * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+* Copyright (C) Advanced Micro Devices, Inc. 2018. ALL RIGHTS RESERVED.
 *
 * See file LICENSE for terms.
 */
 
-#define HAVE_CONFIG_H /* Force using config.h, so test would fail if header
-                         actually tries to use it */
+#ifndef HAVE_CONFIG_H
+#  define HAVE_CONFIG_H /* Force using config.h, so test would fail if header
+                           actually tries to use it */
+#endif
 
 /*
  * UCP hello world client / server example utility
@@ -88,7 +91,7 @@ static void request_init(void *request)
     ctx->completed = 0;
 }
 
-static void send_handle(void *request, ucs_status_t status)
+static void send_handler(void *request, ucs_status_t status)
 {
     struct ucx_context *context = (struct ucx_context *) request;
 
@@ -108,7 +111,7 @@ static void failure_handler(void *arg, ucp_ep_h ep, ucs_status_t status)
     *arg_status = status;
 }
 
-static void recv_handle(void *request, ucs_status_t status,
+static void recv_handler(void *request, ucs_status_t status,
                         ucp_tag_recv_info_t *info)
 {
     struct ucx_context *context = (struct ucx_context *) request;
@@ -197,7 +200,7 @@ static int run_ucx_client(ucp_worker_h ucp_worker)
 
     request = ucp_tag_send_nb(server_ep, msg, msg_len,
                               ucp_dt_make_contig(1), tag,
-                              send_handle);
+                              send_handler);
     if (UCS_PTR_IS_ERR(request)) {
         fprintf(stderr, "unable to send UCX address message\n");
         free(msg);
@@ -247,7 +250,7 @@ static int run_ucx_client(ucp_worker_h ucp_worker)
 
     request = ucp_tag_msg_recv_nb(ucp_worker, msg, info_tag.length,
                                   ucp_dt_make_contig(1), msg_tag,
-                                  recv_handle);
+                                  recv_handler);
 
     if (UCS_PTR_IS_ERR(request)) {
         fprintf(stderr, "unable to receive UCX data message (%u)\n",
@@ -324,7 +327,7 @@ static int run_ucx_server(ucp_worker_h ucp_worker)
     msg = malloc(info_tag.length);
     CHKERR_JUMP(!msg, "allocate memory\n", err);
     request = ucp_tag_msg_recv_nb(ucp_worker, msg, info_tag.length,
-                                  ucp_dt_make_contig(1), msg_tag, recv_handle);
+                                  ucp_dt_make_contig(1), msg_tag, recv_handler);
 
     if (UCS_PTR_IS_ERR(request)) {
         fprintf(stderr, "unable to receive UCX address message (%s)\n",
@@ -373,7 +376,7 @@ static int run_ucx_server(ucp_worker_h ucp_worker)
 
     request = ucp_tag_send_nb(client_ep, msg, msg_len,
                               ucp_dt_make_contig(1), tag,
-                              send_handle);
+                              send_handler);
     if (UCS_PTR_IS_ERR(request)) {
         fprintf(stderr, "unable to send UCX data message\n");
         free(msg);
@@ -386,7 +389,7 @@ static int run_ucx_server(ucp_worker_h ucp_worker)
     }
 
     status = flush_ep(ucp_worker, client_ep);
-    fprintf(stderr, "ucp_ep_flush is completed with status %d (%s)\n",
+    printf("flush_ep completed with status %d (%s)\n",
             status, ucs_status_string(status));
 
     ret = 0;
@@ -599,78 +602,3 @@ int parse_cmd(int argc, char * const argv[], char **server_name)
     }
     return UCS_OK;
 }
-
-int run_server()
-{
-    struct sockaddr_in inaddr;
-    int lsock  = -1;
-    int dsock  = -1;
-    int optval = 1;
-    int ret;
-
-    lsock = socket(AF_INET, SOCK_STREAM, 0);
-    CHKERR_JUMP(lsock < 0, "open server socket\n", err);
-
-    optval = 1;
-    ret = setsockopt(lsock, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof(optval));
-    CHKERR_JUMP(ret < 0, "setsockopt server\n", err_sock);
-
-    inaddr.sin_family      = AF_INET;
-    inaddr.sin_port        = htons(server_port);
-    inaddr.sin_addr.s_addr = INADDR_ANY;
-    memset(inaddr.sin_zero, 0, sizeof(inaddr.sin_zero));
-    ret = bind(lsock, (struct sockaddr*)&inaddr, sizeof(inaddr));
-    CHKERR_JUMP(ret < 0, "bind server\n", err_sock);
-
-    ret = listen(lsock, 0);
-    CHKERR_JUMP(ret < 0, "listen server\n", err_sock);
-
-    printf("Waiting for connection...\n");
-
-    /* Accept next connection */
-    dsock = accept(lsock, NULL, NULL);
-    CHKERR_JUMP(dsock < 0, "accept server\n", err_sock);
-
-    close(lsock);
-
-    return dsock;
-
-err_sock:
-    close(lsock);
-
-err:
-    return -1;
-}
-
-int run_client(const char *server)
-{
-    struct sockaddr_in conn_addr;
-    struct hostent *he;
-    int connfd;
-    int ret;
-
-    connfd = socket(AF_INET, SOCK_STREAM, 0);
-    if (connfd < 0) {
-        fprintf(stderr, "socket() failed: %s\n", strerror(errno));
-        return -1;
-    }
-
-    he = gethostbyname(server);
-    CHKERR_JUMP((he == NULL || he->h_addr_list == NULL), "found host\n", err_conn);
-
-    conn_addr.sin_family = he->h_addrtype;
-    conn_addr.sin_port   = htons(server_port);
-
-    memcpy(&conn_addr.sin_addr, he->h_addr_list[0], he->h_length);
-    memset(conn_addr.sin_zero, 0, sizeof(conn_addr.sin_zero));
-
-    ret = connect(connfd, (struct sockaddr*)&conn_addr, sizeof(conn_addr));
-    CHKERR_JUMP(ret < 0, "connect client\n", err_conn);
-
-    return connfd;
-
-err_conn:
-    close(connfd);
-
-    return -1;
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/examples/uct_hello_world.c b/src/mpid/ch4/netmod/ucx/ucx/test/examples/uct_hello_world.c
index c42259471..c19602afd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/examples/uct_hello_world.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/examples/uct_hello_world.c
@@ -98,14 +98,22 @@ void am_short_params_pack(char *buf, size_t len, am_short_args_t *args)
     }
 }
 
-ucs_status_t do_am_short(uct_ep_h ep, uint8_t id, const cmd_args_t *cmd_args,
-                         char *buf)
+ucs_status_t do_am_short(iface_info_t *if_info, uct_ep_h ep, uint8_t id,
+                         const cmd_args_t *cmd_args, char *buf)
 {
+    ucs_status_t    status;
     am_short_args_t send_args;
+
     am_short_params_pack(buf, cmd_args->test_strlen, &send_args);
-    /* Send active message to remote endpoint */
-    return uct_ep_am_short(ep, id, send_args.header, send_args.payload,
-                           send_args.len);
+
+    do {
+        /* Send active message to remote endpoint */
+        status = uct_ep_am_short(ep, id, send_args.header, send_args.payload,
+                                 send_args.len);
+        uct_worker_progress(if_info->worker);
+    } while (status == UCS_ERR_NO_RESOURCE);
+
+    return status;
 }
 
 /* Pack callback for am_bcopy */
@@ -116,8 +124,8 @@ size_t am_bcopy_data_pack_cb(void *dest, void *arg)
     return bc_args->len;
 }
 
-ucs_status_t do_am_bcopy(uct_ep_h ep, uint8_t id, const cmd_args_t *cmd_args,
-                         char *buf)
+ucs_status_t do_am_bcopy(iface_info_t *if_info, uct_ep_h ep, uint8_t id,
+                         const cmd_args_t *cmd_args, char *buf)
 {
     am_bcopy_args_t args;
     ssize_t len;
@@ -126,7 +134,10 @@ ucs_status_t do_am_bcopy(uct_ep_h ep, uint8_t id, const cmd_args_t *cmd_args,
     args.len  = cmd_args->test_strlen;
 
     /* Send active message to remote endpoint */
-    len = uct_ep_am_bcopy(ep, id, am_bcopy_data_pack_cb, &args, 0);
+    do {
+        len = uct_ep_am_bcopy(ep, id, am_bcopy_data_pack_cb, &args, 0);
+        uct_worker_progress(if_info->worker);
+    } while (len == UCS_ERR_NO_RESOURCE);
     /* Negative len is an error code */
     return (len >= 0) ? UCS_OK : len;
 }
@@ -148,7 +159,7 @@ ucs_status_t do_am_zcopy(iface_info_t *if_info, uct_ep_h ep, uint8_t id,
     zcopy_comp_t comp;
 
     ucs_status_t status = uct_md_mem_reg(if_info->pd, buf, cmd_args->test_strlen,
-                                         0, &memh);
+                                         UCT_MD_MEM_ACCESS_RMA, &memh);
     iov.buffer          = buf;
     iov.length          = cmd_args->test_strlen;
     iov.memh            = memh;
@@ -161,8 +172,12 @@ ucs_status_t do_am_zcopy(iface_info_t *if_info, uct_ep_h ep, uint8_t id,
     comp.memh           = memh;
 
     if (status == UCS_OK) {
-        status = uct_ep_am_zcopy(ep, id, NULL, 0, &iov, 1, 0,
-                                 (uct_completion_t *)&comp);
+        do {
+            status = uct_ep_am_zcopy(ep, id, NULL, 0, &iov, 1, 0,
+                                     (uct_completion_t *)&comp);
+            uct_worker_progress(if_info->worker);
+        } while (status == UCS_ERR_NO_RESOURCE);
+
         if (status == UCS_INPROGRESS) {
             while (!desc_holder) {
                 /* Explicitly progress outstanding active message request */
@@ -276,7 +291,7 @@ static ucs_status_t dev_tl_lookup(const cmd_args_t *cmd_args,
     int                     j;
 
     status = uct_query_md_resources(&md_resources, &num_md_resources);
-    CHKERR_JUMP(UCS_OK != status, "query for protected domain resources", error_ret);
+    CHKERR_JUMP(UCS_OK != status, "query for memory domain resources", error_ret);
 
     /* Iterate through protected domain resources */
     for (i = 0; i < num_md_resources; ++i) {
@@ -285,7 +300,7 @@ static ucs_status_t dev_tl_lookup(const cmd_args_t *cmd_args,
 
         status = uct_md_open(md_resources[i].md_name, md_config, &iface_p->pd);
         uct_config_release(md_config);
-        CHKERR_JUMP(UCS_OK != status, "open protected domains", release_pd);
+        CHKERR_JUMP(UCS_OK != status, "open memory domains", release_pd);
 
         status = uct_md_query_tl_resources(iface_p->pd, &tl_resources, &num_tl_resources);
         CHKERR_JUMP(UCS_OK != status, "query transport resources", close_pd);
@@ -473,7 +488,7 @@ int main(int argc, char **argv)
     uct_device_addr_t   *peer_dev   = NULL;
     uct_iface_addr_t    *own_iface;
     uct_iface_addr_t    *peer_iface = NULL;
-    uct_ep_addr_t       *own_ep;
+    uct_ep_addr_t       *own_ep     = NULL;
     uct_ep_addr_t       *peer_ep    = NULL;
     ucs_status_t        status      = UCS_OK; /* status codes for UCS */
     uct_ep_h            ep;                   /* Remote endpoint */
@@ -584,8 +599,7 @@ int main(int argc, char **argv)
 
     /*Set active message handler */
     status = uct_iface_set_am_handler(if_info.iface, id, hello_world,
-                                      &cmd_args.func_am_type,
-                                      UCT_CB_FLAG_SYNC);
+                                      &cmd_args.func_am_type, 0);
     CHKERR_JUMP(UCS_OK != status, "set callback", out_free_ep);
 
     if (cmd_args.server_name) {
@@ -594,9 +608,9 @@ int main(int argc, char **argv)
 
         /* Send active message to remote endpoint */
         if (cmd_args.func_am_type == FUNC_AM_SHORT) {
-            status = do_am_short(ep, id, &cmd_args, str);
+            status = do_am_short(&if_info, ep, id, &cmd_args, str);
         } else if (cmd_args.func_am_type == FUNC_AM_BCOPY) {
-            status = do_am_bcopy(ep, id, &cmd_args, str);
+            status = do_am_bcopy(&if_info, ep, id, &cmd_args, str);
         } else if (cmd_args.func_am_type == FUNC_AM_ZCOPY) {
             status = do_am_zcopy(&if_info, ep, id, &cmd_args, str);
         }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/apps/profiling.c b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucx_profiling.c
similarity index 86%
rename from test/apps/profiling.c
rename to test/examples/ucx_profiling.c
index 13472053b..38c3eb645 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/apps/profiling.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/examples/ucx_profiling.c
@@ -1,13 +1,12 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
 
-#define HAVE_PROFILING 1
-#include <ucs/debug/profile.h>
-
+#include <ucs/profile/profile_on.h>
 #include <stdio.h>
+#include <math.h>
 
 
 /* calc_pi() would be profiled */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/Makefile.am
index 1ca9649f9..29492aaf6 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/Makefile.am
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/Makefile.am
@@ -6,20 +6,21 @@
 # See file LICENSE for terms.
 #
 
+if HAVE_GTEST
 
 # Set default configuration for running tests
 UCX_HANDLE_ERRORS        ?= freeze
 UCX_LOG_LEVEL            ?= info
-UCX_MEM_LOG_LEVEL        ?= info
 UCX_LOG_PRINT_ENABLE     ?= y
 GTEST_FILTER             ?= *
 GTEST_EXTRA_ARGS         ?=
 LAUNCHER                 ?=
 VALGRIND_EXTRA_ARGS      ?=
 
+SUBDIRS = ucm/test_dlopen
+
 export UCX_HANDLE_ERRORS
 export UCX_LOG_LEVEL
-export UCX_MEM_LOG_LEVEL
 export UCX_LOG_PRINT_ENABLE
 
 GTEST_ARGS = \
@@ -57,7 +58,8 @@ gtest_CPPFLAGS = \
 
 gtest_LDFLAGS  = $(GTEST_LDFLAGS) -no-install
 gtest_CFLAGS   = $(BASE_CFLAGS)
-gtest_CXXFLAGS = $(BASE_CXXFLAGS) $(GTEST_CXXFLAGS) -g -fno-tree-vectorize
+gtest_CXXFLAGS = $(BASE_CXXFLAGS) $(GTEST_CXXFLAGS) -fno-tree-vectorize \
+				 -DGTEST_UCM_HOOK_LIB_DIR="\"${abs_builddir}/ucm/test_dlopen/.libs\""
 
 gtest_SOURCES = \
 	common/gtest-all.cc \
@@ -70,9 +72,11 @@ gtest_SOURCES = \
 	ucm/malloc_hook.cc \
 	\
 	uct/test_amo.cc \
-	uct/test_amo_add.cc \
+	uct/test_amo_add_xor.cc \
+	uct/test_amo_and_or.cc \
 	uct/test_amo_cswap.cc \
-	uct/test_amo_fadd.cc \
+	uct/test_amo_fadd_fxor.cc \
+	uct/test_amo_fand_for.cc \
 	uct/test_amo_swap.cc \
 	uct/test_event.cc \
 	uct/test_fence.cc \
@@ -121,6 +125,7 @@ gtest_SOURCES = \
 	ucp/test_ucp_fence.cc \
 	ucp/test_ucp_sockaddr.cc \
 	ucp/ucp_test.cc \
+	ucp/ucp_datatype.cc \
 	\
 	ucs/test_algorithm.cc \
 	ucs/test_arbiter.cc \
@@ -138,12 +143,13 @@ gtest_SOURCES = \
 	ucs/test_pgtable.cc \
 	ucs/test_profile.cc \
 	ucs/test_rcache.cc \
+	ucs/test_memtype_cache.cc \
 	ucs/test_stats.cc \
+	ucs/test_strided_alloc.cc \
 	ucs/test_sys.cc \
 	ucs/test_time.cc \
 	ucs/test_twheel.cc \
 	ucs/test_frag_list.cc \
-	ucs/test_hash_perf.cc \
 	ucs/test_type.cc \
 	ucs/test_log.cc
 
@@ -200,7 +206,8 @@ noinst_HEADERS = \
 	ucp/test_ucp_atomic.h \
 	ucp/test_ucp_memheap.h \
 	ucp/test_ucp_tag.h \
-	ucp/ucp_test.h
+	ucp/ucp_test.h \
+	ucp/ucp_datatype.h
 
 .PHONY: test test gdb valgrind fix_rpath ucx
 
@@ -237,7 +244,7 @@ list: gtest
 #
 test: ucx gtest
 	@rm -f core.*
-	$(LAUNCHER) $(abs_builddir)/gtest $(GTEST_ARGS)
+	$(LAUNCHER) stdbuf -e0 -o0 $(abs_builddir)/gtest $(GTEST_ARGS)
 
 #
 # Run unit tests with GDB
@@ -245,7 +252,6 @@ test: ucx gtest
 test_gdb: ucx gtest
 	echo -e 'r\ninit-if-undefined $$_exitcode=-1\nif $$_exitcode>=0\n\tq\nend' > .gdbcommands
 	$(LAUNCHER) env UCX_HANDLE_ERRORS=none \
-		UCX_GDB_PATH="" \
 		gdb -x .gdbcommands --args $(GDB_ARGS) \
 			$(abs_builddir)/gtest $(GTEST_ARGS)
 
@@ -254,4 +260,5 @@ test_gdb: ucx gtest
 #
 test_valgrind: ucx gtest
 	$(LAUNCHER) env LD_LIBRARY_PATH="$(VALGRIND_LIBPATH):${LD_LIBRARY_PATH}" \
-	valgrind $(VALGRIND_ARGS) $(abs_builddir)/gtest $(GTEST_ARGS)
+	stdbuf -e0 -o0 valgrind $(VALGRIND_ARGS) $(abs_builddir)/gtest $(GTEST_ARGS)
+endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/main.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/main.cc
index 0cde3173a..f5540b82f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/main.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/main.cc
@@ -8,6 +8,7 @@
 #  include "config.h"
 #endif
 #include <ucs/config/parser.h>
+#include <ucs/config/global_opts.h>
 #include <ucs/sys/sys.h>
 #include <ucm/api/ucm.h>
 #include "test_helpers.h"
@@ -79,8 +80,10 @@ int main(int argc, char **argv) {
         modify_config_for_valgrind("IB_TX_BUFS_GROW", "64");
         modify_config_for_valgrind("RC_TX_CQ_LEN", "256");
         modify_config_for_valgrind("CM_TIMEOUT", "600ms");
-        ucm_config_modify("MALLOC_RELOC", "y"); /* Test reloc hooks with valgrind,
-                                                   though it's generally unsafe. */
+        ucm_global_opts.enable_malloc_reloc = 1; /* Test reloc hooks with valgrind,
+                                                    though it's generally unsafe. */
     }
+    ucs_global_opts.warn_unused_env_vars = 0; /* Avoid warnings if not all
+                                                 config vars are being used */
     return RUN_ALL_TESTS();
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.cc
index 6499fd5e6..341023be8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.cc
@@ -16,6 +16,7 @@ pthread_mutex_t test_base::m_logger_mutex = PTHREAD_MUTEX_INITIALIZER;
 unsigned test_base::m_total_warnings = 0;
 unsigned test_base::m_total_errors   = 0;
 std::vector<std::string> test_base::m_errors;
+std::vector<std::string> test_base::m_warnings;
 
 test_base::test_base() :
                 m_state(NEW),
@@ -23,7 +24,8 @@ test_base::test_base() :
                 m_num_threads(1),
                 m_num_valgrind_errors_before(0),
                 m_num_errors_before(0),
-                m_num_warnings_before(0)
+                m_num_warnings_before(0),
+                m_num_log_handlers_before(0)
 {
     push_config();
 }
@@ -93,13 +95,14 @@ void test_base::modify_config(const std::string& name, const std::string& value,
                               bool optional)
 {
     ucs_status_t status = ucs_global_opts_set_value(name.c_str(), value.c_str());
-    if ((status == UCS_OK) || (optional && (status == UCS_ERR_NO_ELEM))) {
-        return;
+    if ((status == UCS_ERR_NO_ELEM) && optional) {
+        m_env_stack.push_back(new scoped_setenv(("UCX_" + name).c_str(),
+                                                value.c_str()));
+    } else if (status != UCS_OK) {
+        GTEST_FAIL() << "Invalid UCS configuration for " << name << " : "
+                     << value << ", error message: "
+                     << ucs_status_string(status) << "(" << status << ")";
     }
-
-    GTEST_FAIL() << "Invalid UCS configuration for " << name << " : "
-                    << value << ", error message: "
-                    << ucs_status_string(status) << "(" << status << ")";
 }
 
 void test_base::push_config()
@@ -121,21 +124,6 @@ void test_base::pop_config()
     m_config_stack.pop_back();
 }
 
-void test_base::hide_errors()
-{
-    ucs_log_push_handler(hide_errors_logger);
-}
-
-void test_base::wrap_errors()
-{
-    ucs_log_push_handler(wrap_errors_logger);
-}
-
-void test_base::restore_errors()
-{
-    ucs_log_pop_handler();
-}
-
 ucs_log_func_rc_t
 test_base::count_warns_logger(const char *file, unsigned line, const char *function,
                               ucs_log_level_t level, const char *message, va_list ap)
@@ -175,6 +163,24 @@ test_base::hide_errors_logger(const char *file, unsigned line, const char *funct
     return UCS_LOG_FUNC_RC_STOP;
 }
 
+ucs_log_func_rc_t
+test_base::hide_warns_logger(const char *file, unsigned line, const char *function,
+                             ucs_log_level_t level, const char *message, va_list ap)
+{
+    if (level == UCS_LOG_LEVEL_WARN) {
+        pthread_mutex_lock(&m_logger_mutex);
+        va_list ap2;
+        va_copy(ap2, ap);
+        m_warnings.push_back(format_message(message, ap2));
+        va_end(ap2);
+        level = UCS_LOG_LEVEL_DEBUG;
+        pthread_mutex_unlock(&m_logger_mutex);
+    }
+
+    ucs_log_default_handler(file, line, function, level, message, ap);
+    return UCS_LOG_FUNC_RC_STOP;
+}
+
 ucs_log_func_rc_t
 test_base::wrap_errors_logger(const char *file, unsigned line, const char *function,
                               ucs_log_level_t level, const char *message, va_list ap)
@@ -199,6 +205,7 @@ void test_base::SetUpProxy() {
     m_num_errors_before          = m_total_errors;
 
     m_errors.clear();
+    m_num_log_handlers_before    = ucs_log_num_handlers();
     ucs_log_push_handler(count_warns_logger);
 
     try {
@@ -223,9 +230,15 @@ void test_base::TearDownProxy() {
         cleanup();
     }
 
-    ucs_log_pop_handler();
     m_errors.clear();
 
+    ucs_log_pop_handler();
+
+    unsigned num_not_removed = ucs_log_num_handlers() - m_num_log_handlers_before;
+    if (num_not_removed != 0) {
+         ADD_FAILURE() << num_not_removed << " log handlers were not removed";
+    }
+
     int num_valgrind_errors = VALGRIND_COUNT_ERRORS - m_num_valgrind_errors_before;
     if (num_valgrind_errors > 0) {
         ADD_FAILURE() << "Got " << num_valgrind_errors << " valgrind errors during the test";
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.h
index e96972c08..0991dd296 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test.h
@@ -38,11 +38,16 @@ public:
     virtual void push_config();
     virtual void pop_config();
 
-    static void hide_errors();
-    static void wrap_errors();
-    static void restore_errors();
-
 protected:
+    class scoped_log_handler {
+    public:
+        scoped_log_handler(ucs_log_func_t handler) {
+            ucs_log_push_handler(handler);
+        }
+        ~scoped_log_handler() {
+            ucs_log_pop_handler();
+        }
+    };
 
     typedef enum {
         NEW, RUNNING, SKIPPED, ABORTED, FINISHED
@@ -61,24 +66,6 @@ protected:
 
     virtual void test_body() = 0;
 
-    state_t              m_state;
-    bool                 m_initialized;
-    unsigned             m_num_threads;
-    config_stack_t       m_config_stack;
-    int                  m_num_valgrind_errors_before;
-    unsigned             m_num_errors_before;
-    unsigned             m_num_warnings_before;
-
-    static pthread_mutex_t          m_logger_mutex;
-    static unsigned                 m_total_errors;
-    static unsigned                 m_total_warnings;
-    static std::vector<std::string> m_errors;
-
-private:
-    void skipped(const test_skip_exception& e);
-    void run();
-    static void *thread_func(void *arg);
-
     static ucs_log_func_rc_t
     count_warns_logger(const char *file, unsigned line, const char *function,
                        ucs_log_level_t level, const char *message, va_list ap);
@@ -87,10 +74,35 @@ private:
     hide_errors_logger(const char *file, unsigned line, const char *function,
                        ucs_log_level_t level, const char *message, va_list ap);
 
+    static ucs_log_func_rc_t
+    hide_warns_logger(const char *file, unsigned line, const char *function,
+                      ucs_log_level_t level, const char *message, va_list ap);
+
     static ucs_log_func_rc_t
     wrap_errors_logger(const char *file, unsigned line, const char *function,
                        ucs_log_level_t level, const char *message, va_list ap);
 
+    state_t                         m_state;
+    bool                            m_initialized;
+    unsigned                        m_num_threads;
+    config_stack_t                  m_config_stack;
+    ptr_vector<scoped_setenv>       m_env_stack;
+    int                             m_num_valgrind_errors_before;
+    unsigned                        m_num_errors_before;
+    unsigned                        m_num_warnings_before;
+    unsigned                        m_num_log_handlers_before;
+
+    static pthread_mutex_t          m_logger_mutex;
+    static unsigned                 m_total_errors;
+    static unsigned                 m_total_warnings;
+    static std::vector<std::string> m_errors;
+    static std::vector<std::string> m_warnings;
+
+private:
+    void skipped(const test_skip_exception& e);
+    void run();
+    static void *thread_func(void *arg);
+
     pthread_barrier_t    m_barrier;
 };
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.cc
index af672dcdc..a609e1843 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.cc
@@ -10,6 +10,7 @@
 #include <ucs/sys/sys.h>
 #include <ucs/time/time.h>
 #include <ucs/sys/string.h>
+#include <sys/resource.h>
 
 namespace ucs {
 
@@ -25,19 +26,19 @@ int test_time_multiplier()
     return factor;
 }
 
-std::ostream& operator<<(std::ostream& os, const std::vector<char>& vec) {
-    static const size_t LIMIT = 100;
-    size_t i = 0;
-    for (std::vector<char>::const_iterator iter = vec.begin(); iter != vec.end(); ++iter) {
-        if (i >= LIMIT) {
-            os << "...";
-            break;
-        }
-        int n = static_cast<unsigned char>(*iter);
-        os << "[" << i << "]=" << n << " ";
-        ++i;
+int max_tcp_connections()
+{
+    int max_conn = 65535 - 1024; /* limit on number of ports */
+
+    /* Limit numer of endpoints to number of open files, for TCP */
+    struct rlimit rlim;
+    int ret = getrlimit(RLIMIT_NOFILE, &rlim);
+    if (ret == 0) {
+        /* assume no more than 100 fd-s are already used */
+        max_conn = ucs_min((static_cast<int>(rlim.rlim_cur) - 100) / 2, max_conn);
     }
-    return os << std::endl;
+
+    return max_conn;
 }
 
 void fill_random(void *data, size_t size)
@@ -89,19 +90,42 @@ bool is_inet_addr(const struct sockaddr* ifa_addr) {
     return ifa_addr->sa_family == AF_INET;
 }
 
-bool is_ib_netdev(const char *ifa_name) {
+bool is_rdmacm_netdev(const char *ifa_name) {
+    struct dirent *entry;
     char path[PATH_MAX];
+    char dev_name[16];
+    char guid_buf[32];
     DIR *dir;
 
     snprintf(path, PATH_MAX, "/sys/class/net/%s/device/infiniband", ifa_name);
-
     dir = opendir(path);
     if (dir == NULL) {
         return false;
-    } else {
-        closedir(dir);
-        return true;
     }
+
+    /* read IB device name */
+    for (;;) {
+        entry = readdir(dir);
+        if (entry == NULL) {
+            closedir(dir);
+            return false;
+        } else if (entry->d_name[0] != '.') {
+            ucs_strncpy_zero(dev_name, entry->d_name, sizeof(dev_name));
+            break;
+        }
+    }
+    closedir(dir);
+
+    /* read node guid */
+    memset(guid_buf, 0, sizeof(guid_buf));
+    ssize_t nread = ucs_read_file(guid_buf, sizeof(guid_buf), 1,
+                                  "/sys/class/infiniband/%s/node_guid", dev_name);
+    if (nread < 0) {
+        return false;
+    }
+
+    /* use the device if node_guid != 0 */
+    return strstr(guid_buf, "0000:0000:0000:0000") == NULL;
 }
 
 uint16_t get_port() {
@@ -120,9 +144,10 @@ uint16_t get_port() {
 
     do {
         addr_in.sin_port        = htons(0);
-        /* Ports below 1024 are considered "privileged" (can be used only by user root).
-         * Ports above and including 1024 can be used by anyone */
-        ret = bind(sock_fd, (struct sockaddr*)&addr_in, sizeof(struct sockaddr_in));
+        /* Ports below 1024 are considered "privileged" (can be used only by
+         * user root). Ports above and including 1024 can be used by anyone */
+        ret = bind(sock_fd, (struct sockaddr*)&addr_in,
+                   sizeof(struct sockaddr_in));
     } while (ret);
 
     ret = getsockname(sock_fd, (struct sockaddr*)&ret_addr, &len);
@@ -157,59 +182,3 @@ message_stream::~message_stream() {
 } // detail
 
 } // ucs
-
-namespace ucp {
-
-
-data_type_desc_t &
-data_type_desc_t::make(ucp_datatype_t datatype, const void *buf, size_t length,
-                       size_t iov_cnt)
-{
-    EXPECT_FALSE(is_valid());
-
-    if (m_length == 0) {
-        m_length = length;
-    }
-
-    if (m_origin == uintptr_t(NULL)) {
-        m_origin = uintptr_t(buf);
-    }
-
-    m_dt = datatype;
-    memset(m_iov, 0, sizeof(m_iov));
-
-    switch (m_dt & UCP_DATATYPE_CLASS_MASK) {
-    case UCP_DATATYPE_CONTIG:
-        m_buf   = buf;
-        m_count = length / ucp_contig_dt_elem_size(datatype);
-        break;
-    case UCP_DATATYPE_IOV:
-    {
-        const size_t iov_length = (length > iov_cnt) ?
-            ucs::rand() % (length / iov_cnt) : 0;
-        size_t iov_length_it = 0;
-        for (size_t iov_it = 0; iov_it < iov_cnt - 1; ++iov_it) {
-            m_iov[iov_it].buffer = (char *)(buf) + iov_length_it;
-            m_iov[iov_it].length = iov_length;
-            iov_length_it += iov_length;
-        }
-
-        /* Last entry */
-        m_iov[iov_cnt - 1].buffer = (char *)(buf) + iov_length_it;
-        m_iov[iov_cnt - 1].length = length - iov_length_it;
-
-        m_buf   = m_iov;
-        m_count = iov_cnt;
-        break;
-    }
-    default:
-        m_buf   = NULL;
-        m_count = 0;
-        EXPECT_TRUE(false) << "Unsupported datatype";
-        break;
-    }
-
-    return *this;
-}
-
-} // ucp
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.h
index 8a0ad6efc..afcfa24c0 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_helpers.h
@@ -10,12 +10,6 @@
 
 #include "gtest.h"
 
-#include <ucp/api/ucp.h>
-extern "C" {
-#include <ucp/dt/dt_contig.h>
-#include <ucp/dt/dt_iov.h>
-}
-
 #include <ucs/sys/preprocessor.h>
 #include <ucs/sys/checker.h>
 #include <ucs/sys/string.h>
@@ -31,6 +25,138 @@ extern "C" {
 #include <stdint.h>
 
 
+#ifndef UINT16_MAX
+#define UINT16_MAX (65535)
+#endif /* UINT16_MAX */
+
+
+/* Test output */
+#define UCS_TEST_MESSAGE \
+    ucs::detail::message_stream("INFO")
+
+
+/* Skip test */
+#define UCS_TEST_SKIP \
+    do { \
+        throw ucs::test_skip_exception(); \
+    } while(0)
+
+
+#define UCS_TEST_SKIP_R(_reason) \
+    do { \
+        throw ucs::test_skip_exception(_reason); \
+    } while(0)
+
+
+/* Abort test */
+#define UCS_TEST_ABORT(_message) \
+    do { \
+        std::stringstream ss; \
+        ss << _message; \
+        GTEST_MESSAGE_(ss.str().c_str(), ::testing::TestPartResult::kFatalFailure); \
+        throw ucs::test_abort_exception(); \
+    } while(0)
+
+
+/* UCS error check */
+#define EXPECT_UCS_OK(_expr) \
+    do { \
+        ucs_status_t _status = (_expr); \
+        EXPECT_EQ(UCS_OK, _status) << "Error: " << ucs_status_string(_status); \
+    } while (0)
+
+
+#define ASSERT_UCS_OK(_expr, ...) \
+    do { \
+        ucs_status_t _status = (_expr); \
+        if ((_status) != UCS_OK) { \
+            UCS_TEST_ABORT("Error: " << ucs_status_string(_status)  __VA_ARGS__); \
+        } \
+    } while (0)
+
+
+#define ASSERT_UCS_OK_OR_INPROGRESS(_expr) \
+    do { \
+        ucs_status_t _status = (_expr); \
+        if (((_status) != UCS_OK) && ((_status) != UCS_INPROGRESS)) { \
+            UCS_TEST_ABORT("Error: " << ucs_status_string(_status)); \
+        } \
+    } while (0)
+
+
+#define ASSERT_UCS_PTR_OK(_expr) \
+    do { \
+        ucs_status_ptr_t _status = (_expr); \
+        if (UCS_PTR_IS_ERR(_status)) { \
+            UCS_TEST_ABORT("Error: " << ucs_status_string(UCS_PTR_STATUS(_status))); \
+        } \
+    } while (0)
+
+
+#define EXPECT_UD_CHECK(_val1, _val2, _exp_ud, _exp_non_ud) \
+    do { \
+        if ((GetParam()->tl_name == "ud") || (GetParam()->tl_name == "ud_mlx5")) { \
+            EXPECT_##_exp_ud(_val1, _val2); \
+        } else { \
+            EXPECT_##_exp_non_ud(_val1, _val2); \
+        } \
+    } while (0)
+
+
+/* Run code block with given time limit */
+#define UCS_TEST_TIME_LIMIT(_seconds) \
+    for (ucs_time_t _start_time = ucs_get_time(), _elapsed = 0; \
+         _start_time != 0; \
+         ((ucs_time_to_sec(_elapsed = ucs_get_time() - _start_time) >= \
+                         (_seconds) * ucs::test_time_multiplier()) && \
+          (ucs::perf_retry_count > 0)) \
+                         ? (GTEST_NONFATAL_FAILURE_("Time limit exceeded:") << \
+                                         "Expected time: " << ((_seconds) * ucs::test_time_multiplier()) << " seconds\n" << \
+                                         "Actual time: " << ucs_time_to_sec(_elapsed) << " seconds", 0) \
+                         : 0, \
+              _start_time = 0)
+
+
+/**
+ * Scoped exit for C++. Usage:
+ *
+ * UCS_TEST_SCOPE_EXIT() { <code> } UCS_TEST_SCOPE_EXIT_END
+ */
+#define _UCS_TEST_SCOPE_EXIT(_classname, ...) \
+    class _classname { \
+    public: \
+        _classname() {} \
+        ~_classname()
+#define UCS_TEST_SCOPE_EXIT(...) \
+    _UCS_TEST_SCOPE_EXIT(UCS_PP_APPEND_UNIQUE_ID(onexit), ## __VA_ARGS__)
+
+
+#define UCS_TEST_SCOPE_EXIT_END \
+    } UCS_PP_APPEND_UNIQUE_ID(onexit_var);
+
+
+/**
+ * Make uct_iov_t iov[iovcnt] array with pointer elements to original buffer
+ */
+#define UCS_TEST_GET_BUFFER_IOV(_name_iov, _name_iovcnt, _buffer_ptr, _buffer_length, _memh, _iovcnt) \
+        uct_iov_t _name_iov[_iovcnt]; \
+        const size_t _name_iovcnt = _iovcnt; \
+        const size_t _buffer_iov_length = _buffer_length / _name_iovcnt; \
+        size_t _buffer_iov_length_it = 0; \
+        for (size_t iov_it = 0; iov_it < _name_iovcnt; ++iov_it) { \
+            _name_iov[iov_it].buffer = (char *)(_buffer_ptr) + _buffer_iov_length_it; \
+            _name_iov[iov_it].count  = 1; \
+            _name_iov[iov_it].stride = 0; \
+            _name_iov[iov_it].memh   = _memh; \
+            if (iov_it == (_name_iovcnt - 1)) { /* Last iteration */ \
+                _name_iov[iov_it].length = _buffer_length - _buffer_iov_length_it; \
+            } else { \
+                _name_iov[iov_it].length = _buffer_iov_length; \
+                _buffer_iov_length_it += _buffer_iov_length; \
+            } \
+        }
+
+
 namespace ucs {
 
 class test_abort_exception : public std::exception {
@@ -51,8 +177,8 @@ public:
 
 private:
     const bool m_failed;
-}
-;
+};
+
 
 class test_skip_exception : public std::exception {
 public:
@@ -76,6 +202,11 @@ private:
 int test_time_multiplier();
 
 
+/**
+ * @return System limit on number of TCP connections.
+ */
+int max_tcp_connections();
+
 /**
  * Signal-safe sleep.
  */
@@ -90,9 +221,9 @@ bool is_inet_addr(const struct sockaddr* ifa_addr);
 
 
 /**
- * Check if the given interface is associated with a device.
+ * Check if the given network device is supported by rdmacm.
  */
-bool is_ib_netdev(const char *ifa_name);
+bool is_rdmacm_netdev(const char *ifa_name);
 
 
 /**
@@ -125,7 +256,8 @@ template <typename T>
 static std::ostream& operator<<(std::ostream& os, const std::vector<T>& vec) {
     static const size_t LIMIT = 2000;
     size_t i = 0;
-    for (std::vector<char>::const_iterator iter = vec.begin(); iter != vec.end(); ++iter) {
+    for (std::vector<char>::const_iterator iter = vec.begin();
+         iter != vec.end(); ++iter) {
         if (i >= LIMIT) {
             os << "...";
             break;
@@ -159,7 +291,8 @@ static void fill_random(C& c, size_t size) {
 
 template <typename T>
 static inline T random_upper() {
-  return static_cast<T>((rand() / static_cast<double>(RAND_MAX)) * std::numeric_limits<T>::max());
+  return static_cast<T>((rand() / static_cast<double>(RAND_MAX)) *
+                        std::numeric_limits<T>::max());
 }
 
 template <typename T>
@@ -319,15 +452,16 @@ public:
         EXPECT_TRUE(value != NULL);
     }
 
-    handle(const T& value, dtor2_t dtor, ArgT *arg) :
+    handle(const T& value, dtor2_t dtor, ArgT arg) :
         m_initialized(true), m_value(value), m_dtor(NULL),
         m_dtor_with_arg(dtor), m_dtor_arg(arg)
     {
         EXPECT_TRUE(value != NULL);
     }
 
-    handle(const handle& other) : m_initialized(false), m_value(NULL), m_dtor(NULL),
-                                  m_dtor_with_arg(NULL), m_dtor_arg(NULL) {
+    handle(const handle& other) : m_initialized(false), m_value(NULL),
+                                  m_dtor(NULL), m_dtor_with_arg(NULL),
+                                  m_dtor_arg(NULL) {
         *this = other;
     }
 
@@ -507,201 +641,4 @@ private:
 
 } // ucs
 
-
-namespace ucp {
-
-class data_type_desc_t {
-public: 
-    data_type_desc_t()
-        : m_origin(uintptr_t(NULL)), m_length(0), m_dt(0), m_buf(NULL),
-          m_count(0), m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
-        memset(m_iov, 0, sizeof(m_iov));
-    };
-
-    data_type_desc_t(ucp_datatype_t datatype, const void *buf, size_t length)
-        : m_origin(uintptr_t(buf)), m_length(length), m_buf(NULL),
-          m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
-        make(datatype, buf, length);
-    }
-
-    data_type_desc_t(ucp_datatype_t datatype, const void *buf, size_t length,
-                     size_t iov_count)
-        : m_origin(uintptr_t(buf)), m_length(length), m_buf(NULL),
-          m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
-        make(datatype, buf, length, iov_count);
-    };
-
-    data_type_desc_t &make(ucp_datatype_t datatype, const void *buf,
-                           size_t length, size_t iov_count);
-
-    data_type_desc_t &make(ucp_datatype_t datatype, const void *buf,
-                           size_t length) {
-        return make(datatype, buf, length, m_iov_cnt_limit);
-    };
-
-    data_type_desc_t &forward_to(size_t offset) {
-        EXPECT_LE(offset, m_length);
-        invalidate();
-        return make(m_dt, (const void *)(m_origin + offset), m_length - offset,
-                    m_iov_cnt_limit);
-    };
-
-    ucp_datatype_t dt() const {
-        EXPECT_TRUE(is_valid());
-        return m_dt;
-    };
-
-    void *buf() const {
-        EXPECT_TRUE(is_valid());
-        return const_cast<void *>(m_buf);
-    };
-
-    size_t count() const {
-        EXPECT_TRUE(is_valid());
-        return m_count;
-    };
-
-    bool is_valid() const {
-        return (m_buf != NULL) && (m_count != 0) &&
-               (UCP_DT_IS_IOV(m_dt) ? (m_count <= m_iov_cnt_limit) :
-                UCP_DT_IS_CONTIG(m_dt));
-    }
-
-private:
-    void invalidate() {
-        EXPECT_TRUE(is_valid());
-        m_buf   = NULL;
-        m_count = 0;
-    }
-
-    uintptr_t       m_origin;
-    size_t          m_length;
-
-    ucp_datatype_t  m_dt;
-    const void     *m_buf;
-    size_t          m_count;
-
-    const size_t    m_iov_cnt_limit;
-    ucp_dt_iov_t    m_iov[40];
-};
-
-} // ucp
-
-
-#ifndef UINT16_MAX
-#define UINT16_MAX (65535)
-#endif /* UINT16_MAX */
-
-
-/* Test output */
-#define UCS_TEST_MESSAGE \
-    ucs::detail::message_stream("INFO")
-
-
-/* Skip test */
-#define UCS_TEST_SKIP \
-    do { \
-        throw ucs::test_skip_exception(); \
-    } while(0)
-#define UCS_TEST_SKIP_R(_reason) \
-    do { \
-        throw ucs::test_skip_exception(_reason); \
-    } while(0)
-
-
-/* Abort test */
-#define UCS_TEST_ABORT(_message) \
-    do { \
-        std::stringstream ss; \
-        ss << _message; \
-        GTEST_MESSAGE_(ss.str().c_str(), ::testing::TestPartResult::kFatalFailure); \
-        throw ucs::test_abort_exception(); \
-    } while(0)
-
-
-/* UCS error check */
-#define EXPECT_UCS_OK(_expr) \
-    do { \
-        ucs_status_t _status = (_expr); \
-        EXPECT_EQ(UCS_OK, _status) << "Error: " << ucs_status_string(_status); \
-    } while (0)
-
-#define ASSERT_UCS_OK(_expr, ...) \
-    do { \
-        ucs_status_t _status = (_expr); \
-        if ((_status) != UCS_OK) { \
-            UCS_TEST_ABORT("Error: " << ucs_status_string(_status)  __VA_ARGS__); \
-        } \
-    } while (0)
-
-#define ASSERT_UCS_OK_OR_INPROGRESS(_expr) \
-    do { \
-        ucs_status_t _status = (_expr); \
-        if ((status) != UCS_OK && (_status) != UCS_INPROGRESS) { \
-            UCS_TEST_ABORT("Error: " << ucs_status_string(_status)); \
-        } \
-    } while (0)
-
-#define EXPECT_UD_CHECK(_val1, _val2, _exp_ud, _exp_non_ud) \
-    do { \
-        if ((GetParam()->tl_name == "ud") || (GetParam()->tl_name == "ud_mlx5")) { \
-            EXPECT_##_exp_ud(_val1, _val2); \
-        } else { \
-            EXPECT_##_exp_non_ud(_val1, _val2); \
-        } \
-    } while (0)
-
-
-/* Run code block with given time limit */
-#define UCS_TEST_TIME_LIMIT(_seconds) \
-    for (ucs_time_t _start_time = ucs_get_time(), _elapsed = 0; \
-         _start_time != 0; \
-         ((ucs_time_to_sec(_elapsed = ucs_get_time() - _start_time) >= \
-                         (_seconds) * ucs::test_time_multiplier()) && \
-          (ucs::perf_retry_count > 0)) \
-                         ? (GTEST_NONFATAL_FAILURE_("Time limit exceeded:") << \
-                                         "Expected time: " << ((_seconds) * ucs::test_time_multiplier()) << " seconds\n" << \
-                                         "Actual time: " << ucs_time_to_sec(_elapsed) << " seconds", 0) \
-                         : 0, \
-              _start_time = 0)
-
-
-/**
- * Scoped exit for C++. Usage:
- *
- * UCS_TEST_SCOPE_EXIT() { <code> } UCS_TEST_SCOPE_EXIT_END
- */
-#define _UCS_TEST_SCOPE_EXIT(_classname, ...) \
-    class _classname { \
-    public: \
-        _classname() {} \
-        ~_classname()
-#define UCS_TEST_SCOPE_EXIT(...) \
-    _UCS_TEST_SCOPE_EXIT(UCS_PP_APPEND_UNIQUE_ID(onexit), ## __VA_ARGS__)
-
-#define UCS_TEST_SCOPE_EXIT_END \
-    } UCS_PP_APPEND_UNIQUE_ID(onexit_var);
-
-/**
- * Make uct_iov_t iov[iovcnt] array with pointer elements to original buffer
- */
-#define UCS_TEST_GET_BUFFER_IOV(_name_iov, _name_iovcnt, _buffer_ptr, _buffer_length, _memh, _iovcnt) \
-        uct_iov_t _name_iov[_iovcnt]; \
-        const size_t _name_iovcnt = _iovcnt; \
-        const size_t _buffer_iov_length = _buffer_length / _name_iovcnt; \
-        size_t _buffer_iov_length_it = 0; \
-        for (size_t iov_it = 0; iov_it < _name_iovcnt; ++iov_it) { \
-            _name_iov[iov_it].buffer = (char *)(_buffer_ptr) + _buffer_iov_length_it; \
-            _name_iov[iov_it].count  = 1; \
-            _name_iov[iov_it].stride = 0; \
-            _name_iov[iov_it].memh   = _memh; \
-            if (iov_it == (_name_iovcnt - 1)) { /* Last iteration */ \
-                _name_iov[iov_it].length = _buffer_length - _buffer_iov_length_it; \
-            } else { \
-                _name_iov[iov_it].length = _buffer_iov_length; \
-                _buffer_iov_length_it += _buffer_iov_length; \
-            } \
-        }
-
-
-#endif
+#endif /* UCS_TEST_HELPERS_H */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_obj_size.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_obj_size.cc
index 4d7bd40e3..fd8a0512e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_obj_size.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_obj_size.cc
@@ -11,12 +11,13 @@
 #include <common/test.h>
 #include <ucs/type/cpu_set.h>
 
+extern "C" {
 #include <ucp/core/ucp_ep.h>
 #include <ucp/core/ucp_request.h>
 #include <ucp/core/ucp_types.h>
 #include <uct/api/tl.h>
 #include <uct/base/uct_iface.h>
-#include <uct/sm/self/self_ep.h>
+#include <uct/sm/self/self.h>
 #include <uct/tcp/tcp.h>
 #if HAVE_TL_RC
 #  include <uct/ib/rc/verbs/rc_verbs.h>
@@ -30,11 +31,12 @@
 #  include <uct/ib/ud/base/ud_ep.h>
 #  include <uct/ib/ud/verbs/ud_verbs.h>
 #endif
+}
 
 class test_obj_size : public ucs::test {
 };
 
-#define EXPECTED_SIZE(_obj, _size) EXPECT_EQ(sizeof(_obj), (size_t)_size)
+#define EXPECTED_SIZE(_obj, _size) EXPECT_EQ((size_t)_size, sizeof(_obj))
 
 UCS_TEST_F(test_obj_size, size) {
 
@@ -42,9 +44,12 @@ UCS_TEST_F(test_obj_size, size) {
    UCS_TEST_SKIP_R("Debug data");
 #elif ENABLE_STATS
    UCS_TEST_SKIP_R("Statistic enabled");
+#elif ENABLE_ASSERT
+   UCS_TEST_SKIP_R("Assert enabled");
 #else
-    EXPECTED_SIZE(ucp_ep_t, 104);
-    EXPECTED_SIZE(ucp_request_t, 224);
+    EXPECTED_SIZE(ucp_ep_t, 64);
+    EXPECTED_SIZE(ucp_request_t, 232);
+    EXPECTED_SIZE(ucp_recv_desc_t, 48);
     EXPECTED_SIZE(uct_ep_t, 8);
     EXPECTED_SIZE(uct_base_ep_t, 8);
     EXPECTED_SIZE(uct_rkey_bundle_t, 24);
@@ -64,8 +69,8 @@ UCS_TEST_F(test_obj_size, size) {
     EXPECTED_SIZE(uct_dc_verbs_ep_t, 40);
 #  endif
 #  if HAVE_TL_UD
-    EXPECTED_SIZE(uct_ud_ep_t, 248);
-    EXPECTED_SIZE(uct_ud_verbs_ep_t, 264);
+    EXPECTED_SIZE(uct_ud_ep_t, 240);
+    EXPECTED_SIZE(uct_ud_verbs_ep_t, 256);
 #  endif
 #endif
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.cc
index f46ba368c..ef27243df 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.cc
@@ -28,7 +28,8 @@ void test_perf::rte_comm::push(const void *data, size_t size) {
     pthread_mutex_unlock(&m_mutex);
 }
 
-void test_perf::rte_comm::pop(void *data, size_t size) {
+void test_perf::rte_comm::pop(void *data, size_t size,
+                              void (*progress)(void *arg), void *arg) {
     bool done = false;
     do {
         pthread_mutex_lock(&m_mutex);
@@ -38,6 +39,9 @@ void test_perf::rte_comm::pop(void *data, size_t size) {
             done = true;
         }
         pthread_mutex_unlock(&m_mutex);
+        if (!done) {
+            progress(arg);
+        }
     } while (!done);
 }
 
@@ -59,13 +63,14 @@ unsigned test_perf::rte::group_index(void *rte_group) {
     return self->index();
 }
 
-void test_perf::rte::barrier(void *rte_group) {
+void test_perf::rte::barrier(void *rte_group, void (*progress)(void *arg),
+                             void *arg) {
     static const uint32_t magic = 0xdeadbeed;
     rte *self = reinterpret_cast<rte*>(rte_group);
     uint32_t dummy = magic;
     self->m_send.push(&dummy, sizeof(dummy));
     dummy = 0;
-    self->m_recv.pop(&dummy, sizeof(dummy));
+    self->m_recv.pop(&dummy, sizeof(dummy), progress, arg);
     ucs_assert_always(dummy == magic);
 }
 
@@ -97,9 +102,9 @@ void test_perf::rte::recv(void *rte_group, unsigned src, void *buffer,
         return;
     }
 
-    self->m_recv.pop(&size, sizeof(size));
+    self->m_recv.pop(&size, sizeof(size), (void(*)(void*))ucs_empty_function, NULL);
     ucs_assert_always(size <= max);
-    self->m_recv.pop(buffer, size);
+    self->m_recv.pop(buffer, size, (void(*)(void*))ucs_empty_function, NULL);
 }
 
 void test_perf::rte::exchange_vec(void *rte_group, void * req)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.h
index 743d5cd08..48952b19f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/common/test_perf.h
@@ -45,7 +45,7 @@ private:
 
         void push(const void *data, size_t size);
 
-        void pop(void *data, size_t size);
+        void pop(void *data, size_t size, void (*progress)(void *arg), void *arg);
 
     private:
         pthread_mutex_t  m_mutex;
@@ -63,7 +63,8 @@ private:
 
         static unsigned group_index(void *rte_group);
 
-        static void barrier(void *rte_group);
+        static void barrier(void *rte_group, void (*progress)(void *arg),
+                            void *arg);
 
         static void post_vec(void *rte_group, const struct iovec *iovec,
                              int iovcnt, void **req);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/cuda_hooks.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/cuda_hooks.cc
index a3feec353..9e2ce5a19 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/cuda_hooks.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/cuda_hooks.cc
@@ -7,66 +7,229 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 
-class cuda_hook : public ucs::test {
-};
+static ucm_event_t alloc_event, free_event;
 
-static void  *free_ptr;
-static void cuda_mem_event_callback(ucm_event_type_t event_type,
+static void cuda_mem_alloc_callback(ucm_event_type_t event_type,
                                     ucm_event_t *event, void *arg)
 {
-    free_ptr = event->vm_unmapped.address;
+    alloc_event.mem_type.address  = event->mem_type.address;
+    alloc_event.mem_type.size     = event->mem_type.size;
+    alloc_event.mem_type.mem_type = event->mem_type.mem_type;
 }
 
-UCS_TEST_F(cuda_hook, cudafree) {
-    ucs_status_t result;
-    cudaError_t ret;
-    void *ptr, *ptr1;
+static void cuda_mem_free_callback(ucm_event_type_t event_type,
+                                   ucm_event_t *event, void *arg)
+{
+    free_event.mem_type.address  = event->mem_type.address;
+    free_event.mem_type.size     = event->mem_type.size;
+    free_event.mem_type.mem_type = event->mem_type.mem_type;
+}
+
+
+class cuda_hooks : public ucs::test {
+protected:
+
+    virtual void init() {
+        ucs_status_t result;
+        CUresult ret;
+        ucs::test::init();
+
+        /* intialize device context */
+        if (cudaSetDevice(0) != cudaSuccess) {
+            UCS_TEST_SKIP_R("can't set cuda device");
+        }
+
+        ret = cuInit(0);
+        if (ret != CUDA_SUCCESS) {
+            UCS_TEST_SKIP_R("can't init cuda device");
+        }
+
+        ret = cuDeviceGet(&device, 0);
+        if (ret != CUDA_SUCCESS) {
+            UCS_TEST_SKIP_R("can't get cuda device");
+        }
+
+        ret = cuCtxCreate(&context, 0, device);
+        if (ret != CUDA_SUCCESS) {
+            UCS_TEST_SKIP_R("can't create cuda context");
+        }
+
+        /* install memory hooks */
+        result = ucm_set_event_handler(UCM_EVENT_MEM_TYPE_ALLOC, 0, cuda_mem_alloc_callback,
+                                       reinterpret_cast<void*>(this));
+        ASSERT_UCS_OK(result);
+
+        result = ucm_set_event_handler(UCM_EVENT_MEM_TYPE_FREE, 0, cuda_mem_free_callback,
+                                       reinterpret_cast<void*>(this));
+        ASSERT_UCS_OK(result);
+    }
 
-    /* set cuda device */
-    if (cudaSetDevice(0) != cudaSuccess) {
-        UCS_TEST_SKIP_R("can't set cuda device");
+    virtual void cleanup() {
+        CUresult ret;
+
+        ucm_unset_event_handler(UCM_EVENT_MEM_TYPE_ALLOC, cuda_mem_alloc_callback,
+                                reinterpret_cast<void*>(this));
+        ucm_unset_event_handler(UCM_EVENT_MEM_TYPE_FREE, cuda_mem_free_callback,
+                                reinterpret_cast<void*>(this));
+
+        ret = cuCtxDestroy(context);
+        EXPECT_EQ(ret, CUDA_SUCCESS);
+
+        ucs::test::cleanup();
+    }
+
+
+    void check_mem_alloc_events(void *ptr, size_t size,
+                                int expect_mem_type = UCM_MEM_TYPE_CUDA)  {
+        ASSERT_EQ(ptr, alloc_event.mem_type.address);
+        ASSERT_EQ(size, alloc_event.mem_type.size);
+        ASSERT_EQ(expect_mem_type, alloc_event.mem_type.mem_type);
     }
 
-    /* Install memory hooks */
-    result = ucm_set_event_handler(UCM_EVENT_VM_UNMAPPED, 0, cuda_mem_event_callback,
-                                   reinterpret_cast<void*>(this));
-    ASSERT_UCS_OK(result);
+    void check_mem_free_events(void *ptr, size_t size,
+                               int expect_mem_type = UCM_MEM_TYPE_CUDA) {
+        ASSERT_EQ(ptr, free_event.mem_type.address);
+        ASSERT_EQ(expect_mem_type, free_event.mem_type.mem_type);
+    }
+
+    CUdevice   device;
+    CUcontext  context;
+};
+
+UCS_TEST_F(cuda_hooks, test_cuMem_Alloc_Free) {
+    CUresult ret;
+    CUdeviceptr dptr, dptr1;
+
+    /* small allocation */
+    ret = cuMemAlloc(&dptr, 64);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr, 64);
+
+    ret = cuMemFree(dptr);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr, 64);
+
+    /* large allocation */
+    ret = cuMemAlloc(&dptr, (256 * 1024 *1024));
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr, (256 * 1024 *1024));
+
+    ret = cuMemFree(dptr);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr, (256 * 1024 *1024));
+
+    /* multiple allocations, cudafree in reverse order */
+    ret = cuMemAlloc(&dptr, (1 * 1024 *1024));
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr, (1 * 1024 *1024));
+
+    ret = cuMemAlloc(&dptr1, (1 * 1024 *1024));
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr1, (1 * 1024 *1024));
+
+    ret = cuMemFree(dptr1);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr1, (1 * 1024 *1024));
+
+    ret = cuMemFree(dptr);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr, (1 * 1024 *1024));
+}
+
+UCS_TEST_F(cuda_hooks, test_cuMemAllocManaged) {
+    CUresult ret;
+    CUdeviceptr dptr;
+
+    ret = cuMemAllocManaged(&dptr, 64, CU_MEM_ATTACH_GLOBAL);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr, 64, UCM_MEM_TYPE_CUDA_MANAGED);
+
+    ret = cuMemFree(dptr);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr, 0);
+}
+
+UCS_TEST_F(cuda_hooks, test_cuMemAllocPitch) {
+    CUresult ret;
+    CUdeviceptr dptr;
+    size_t pitch;
+
+    ret = cuMemAllocPitch(&dptr, &pitch, 4, 8, 4);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_alloc_events((void *)dptr, (4 * 8));
+
+    ret = cuMemFree(dptr);
+    ASSERT_EQ(ret, CUDA_SUCCESS);
+    check_mem_free_events((void *)dptr, 0);
+}
+
+UCS_TEST_F(cuda_hooks, test_cuda_Malloc_Free) {
+    cudaError_t ret;
+    void *ptr, *ptr1;
 
     /* small allocation */
-    free_ptr = NULL;
     ret = cudaMalloc(&ptr, 64);
-    EXPECT_EQ(ret, cudaSuccess);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(ptr, 64);
 
     ret = cudaFree(ptr);
-    EXPECT_EQ(ret, cudaSuccess);
-    EXPECT_EQ(ptr, free_ptr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(ptr, 64);
 
     /* large allocation */
-    free_ptr = NULL;
     ret = cudaMalloc(&ptr, (256 * 1024 *1024));
-    EXPECT_EQ(ret, cudaSuccess);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(ptr, (256 * 1024 *1024));
 
     ret = cudaFree(ptr);
-    EXPECT_EQ(ret, cudaSuccess);
-    EXPECT_EQ(ptr, free_ptr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(ptr, (256 * 1024 *1024));
 
     /* multiple allocations, cudafree in reverse order */
-    free_ptr = NULL;
     ret = cudaMalloc(&ptr, (1 * 1024 *1024));
-    EXPECT_EQ(ret, cudaSuccess);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(ptr, (1 * 1024 *1024));
 
     ret = cudaMalloc(&ptr1, (1 * 1024 *1024));
-    EXPECT_EQ(ret, cudaSuccess);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(ptr1, (1 * 1024 *1024));
 
     ret = cudaFree(ptr1);
-    EXPECT_EQ(ret, cudaSuccess);
-    EXPECT_EQ(ptr1, free_ptr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(ptr1, (1 * 1024 *1024));
+
+    ret = cudaFree(ptr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(ptr, (1 * 1024 *1024));
+
+    /* cudaFree with NULL */
+    ret = cudaFree(NULL);
+    ASSERT_EQ(ret, cudaSuccess);
+}
+
+UCS_TEST_F(cuda_hooks, test_cudaMallocManaged) {
+    cudaError_t ret;
+    void *ptr;
+
+    ret = cudaMallocManaged(&ptr, 64, cudaMemAttachGlobal);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(ptr, 64, UCM_MEM_TYPE_CUDA_MANAGED);
 
-    free_ptr = NULL;
     ret = cudaFree(ptr);
-    EXPECT_EQ(ret, cudaSuccess);
-    EXPECT_EQ(ptr, free_ptr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(ptr, 0);
+}
+
+UCS_TEST_F(cuda_hooks, test_cudaMallocPitch) {
+    cudaError_t ret;
+    void *devPtr;
+    size_t pitch;
+
+    ret = cudaMallocPitch(&devPtr, &pitch, 4, 8);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_alloc_events(devPtr, (4 * 8));
 
-    ucm_unset_event_handler(UCM_EVENT_VM_UNMAPPED, cuda_mem_event_callback,
-                            reinterpret_cast<void*>(this));
+    ret = cudaFree(devPtr);
+    ASSERT_EQ(ret, cudaSuccess);
+    check_mem_free_events(devPtr, 0);
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/malloc_hook.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/malloc_hook.cc
index dbb767a07..5dd500579 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/malloc_hook.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/malloc_hook.cc
@@ -1,5 +1,5 @@
 /**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
  *
  * See file LICENSE for terms.
  */
@@ -15,11 +15,13 @@
 #include <pthread.h>
 #include <sstream>
 #include <stdint.h>
+#include <dlfcn.h>
+#include <libgen.h>
 
 extern "C" {
 #include <ucs/time/time.h>
 #include <ucm/malloc/malloc_hook.h>
-#include <ucm/util/ucm_config.h>
+#include <ucm/bistro/bistro.h>
 #include <ucs/sys/sys.h>
 #include <malloc.h>
 }
@@ -28,93 +30,166 @@ extern "C" {
 #  define HAVE_MALLOC_STATES 1
 #endif /* HAVE_MALLOC_SET_STATE && HAVE_MALLOC_GET_STATE */
 
+#define EXPECT_INCREASED(_value, _prev, _size, _name)  \
+    {                                                  \
+        EXPECT_GE(_value, (_prev) + (_size)) << _name; \
+        _prev = _value;                                \
+    }
+
+template <class T>
+class mhook_thread {
+public:
+    mhook_thread(T *test): m_test(test)
+    {
+        pthread_create(&m_thread, NULL, thread_func, reinterpret_cast<void*>(m_test));
+    }
+
+    ~mhook_thread() {
+        join();
+        delete m_test;
+    }
+
+    void join() {
+        void *retval;
+        pthread_join(m_thread, &retval);
+    }
+
+protected:
+    T         *m_test;
+    pthread_t m_thread;
+
+    static void *thread_func(void *arg) {
+        T *test = reinterpret_cast<T*>(arg);
+        test->test();
+        return NULL;
+    }
+};
+
+template <class T>
+class mmap_event {
+public:
+    mmap_event(T *test): m_test(test), m_events(0)
+    {
+    }
+
+    ~mmap_event()
+    {
+        unset();
+    }
+
+    ucs_status_t set(int events)
+    {
+        ucs_status_t status;
+
+        status = ucm_set_event_handler(events, 0, mem_event_callback,
+                                       reinterpret_cast<void*>(m_test));
+        ASSERT_UCS_OK(status);
+        m_events |= events;
+        return status;
+    }
+
+    void unset()
+    {
+        if (m_events) {
+            ucm_unset_event_handler(m_events, mem_event_callback,
+                                    reinterpret_cast<void*>(m_test));
+            m_events = 0;
+        }
+    }
+
+protected:
+    T   *m_test;
+    int m_events;
+
+    static void mem_event_callback(ucm_event_type_t event_type,
+                                   ucm_event_t *event,
+                                   void *arg)
+    {
+        T *test = reinterpret_cast<T*>(arg);
+        test->mem_event(event_type, event);
+    }
+};
+
 
 class malloc_hook : public ucs::test {
+    friend class mmap_event<malloc_hook>;
 protected:
+    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)
+    {
+        m_got_event = 1;
+    }
+
     virtual void init() {
-        size_t free_space, min_free_space, prev_free_space, alloc_size;
-        struct mallinfo mi;
+        ucs_status_t status;
+        mmap_event<malloc_hook> event(this);
 
+        m_got_event = 0;
         ucm_malloc_state_reset(128 * 1024, 128 * 1024);
         malloc_trim(0);
-
-        /* Take up free space so we would definitely get mmap events */
-        min_free_space  = SIZE_MAX;
-        alloc_size      = 0;
-        mi = mallinfo();
-        prev_free_space = mi.fsmblks + mi.fordblks;
-
-        while (alloc_size < (size_t)(prev_free_space * 0.90)) {
-            m_pts.push_back(malloc(small_alloc_size));
-            alloc_size += small_alloc_size;
-        }
+        status = event.set(UCM_EVENT_VM_MAPPED);
+        ASSERT_UCS_OK(status);
 
         for (;;) {
             void *ptr = malloc(small_alloc_size);
-            mi = mallinfo();
-            free_space = mi.fsmblks + mi.fordblks;
-            if (free_space > min_free_space) {
+            if (m_got_event) {
                 /* If the heap grew, the minimal size is the previous one */
                 free(ptr);
-                min_free_space = free_space;
                 break;
             } else {
                 m_pts.push_back(ptr);
-                alloc_size += small_alloc_size;
-                min_free_space = free_space;
             }
         }
+        event.unset();
+    }
 
-        UCS_TEST_MESSAGE << "Reduced heap free space from " << prev_free_space <<
-                            " to " << free_space << " after allocating " <<
-                            alloc_size << " bytes";
+    static int bistro_munmap_hook(void *addr, size_t length)
+    {
+        UCM_BISTRO_PROLOGUE;
+        bistro_call_counter++;
+        int res = (intptr_t)syscall(SYS_munmap, addr, length);
+        UCM_BISTRO_EPILOGUE;
+        return res;
+    }
+
+    void skip_on_bistro() {
+        /* BISTRO is disabled under valgrind, we may run tests */
+        if ((ucm_global_opts.mmap_hook_mode == UCM_MMAP_HOOK_BISTRO) &&
+             !RUNNING_ON_VALGRIND) {
+            UCS_TEST_SKIP_R("skipping on BISTRO hooks");
+        }
     }
 
 public:
     static int            small_alloc_count;
-    static const size_t   small_alloc_size  = 10000;
+    static const size_t   small_alloc_size = 10000;
     ucs::ptr_vector<void> m_pts;
+    int                   m_got_event;
+    static volatile int   bistro_call_counter;
 };
 
-int malloc_hook::small_alloc_count = 1000 / ucs::test_time_multiplier();
+int malloc_hook::small_alloc_count            = 1000 / ucs::test_time_multiplier();
+volatile int malloc_hook::bistro_call_counter = 0;
 
 class test_thread {
 public:
     test_thread(const std::string& name, int num_threads, pthread_barrier_t *barrier,
-                malloc_hook *test) :
+                malloc_hook *test, void (test_thread::*test_func)() = &test_thread::test) :
         m_name(name), m_num_threads(num_threads), m_barrier(barrier),
-        m_map_size(0), m_unmap_size(0), m_test(test)
+        m_map_size(0), m_unmap_size(0), m_test(test), m_event(this)
     {
         pthread_mutex_init(&m_stats_lock, NULL);
-        pthread_create(&m_thread, NULL, thread_func, reinterpret_cast<void*>(this));
     }
 
     ~test_thread() {
-        join();
         pthread_mutex_destroy(&m_stats_lock);
     }
 
-    void join() {
-        void *retval;
-        pthread_join(m_thread, &retval);
-    }
+    void test();
+    void mem_event(ucm_event_type_t event_type, ucm_event_t *event);
 
 private:
     typedef std::pair<void*, void*> range;
 
-    static void *thread_func(void *arg) {
-        test_thread *self = reinterpret_cast<test_thread*>(arg);
-        self->test();
-        return NULL;
-    }
-
-    static void mem_event_callback(ucm_event_type_t event_type, ucm_event_t *event,
-                                   void *arg)
-    {
-        test_thread *self = reinterpret_cast<test_thread*>(arg);
-        self->mem_event(event_type, event);
-    }
-
     bool is_ptr_in_range(void *ptr, size_t size, const std::vector<range> &ranges) {
         for (std::vector<range>::const_iterator iter = ranges.begin(); iter != ranges.end(); ++iter) {
             if ((ptr >= iter->first) && ((char*)ptr < iter->second)) {
@@ -124,16 +199,12 @@ private:
         return false;
     }
 
-    void test();
-    void mem_event(ucm_event_type_t event_type, ucm_event_t *event);
-
     static pthread_mutex_t   lock;
     static pthread_barrier_t barrier;
 
     std::string        m_name;
     int                m_num_threads;
     pthread_barrier_t  *m_barrier;
-    pthread_t          m_thread;
 
     pthread_mutex_t    m_stats_lock;
     size_t             m_map_size;
@@ -142,6 +213,7 @@ private:
     std::vector<range> m_unmap_ranges;
 
     malloc_hook        *m_test;
+    mmap_event<test_thread> m_event;
 };
 
 pthread_mutex_t test_thread::lock = PTHREAD_MUTEX_INITIALIZER;
@@ -192,9 +264,7 @@ void test_thread::test() {
     pthread_barrier_wait(m_barrier);
 
     /* Install memory hooks */
-    result = ucm_set_event_handler(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED,
-                                   0, mem_event_callback,
-                                   reinterpret_cast<void*>(this));
+    result = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);
     ASSERT_UCS_OK(result);
 
     /* Allocate small pointers with new heap manager */
@@ -316,32 +386,36 @@ void test_thread::test() {
     std::cout.flush();
     pthread_mutex_unlock(&lock);
 
-    ucm_unset_event_handler(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED,
-                            mem_event_callback,
-                            reinterpret_cast<void*>(this));
+    m_event.unset();
 }
 
 UCS_TEST_F(malloc_hook, single_thread) {
+    skip_on_bistro();
+
     pthread_barrier_t barrier;
     pthread_barrier_init(&barrier, NULL, 1);
     {
-        test_thread thread("single-thread", 1, &barrier, this);
+        mhook_thread<test_thread>(new test_thread("single-thread", 1, &barrier, this));
     }
     pthread_barrier_destroy(&barrier);
 }
 
 UCS_TEST_F(malloc_hook, multi_threads) {
+    typedef mhook_thread<test_thread> thread_t;
+
     static const int num_threads = 8;
-    ucs::ptr_vector<test_thread> threads;
+    ucs::ptr_vector<thread_t> threads;
     pthread_barrier_t barrier;
 
+    skip_on_bistro();
+
     malloc_trim(0);
 
     pthread_barrier_init(&barrier, NULL, num_threads);
     for (int i = 0; i < num_threads; ++i) {
         std::stringstream ss;
         ss << "thread " << i << "/" << num_threads;
-        threads.push_back(new test_thread(ss.str(), num_threads, &barrier, this));
+        threads.push_back(new thread_t(new test_thread(ss.str(), num_threads, &barrier, this)));
     }
 
     threads.clear();
@@ -384,44 +458,39 @@ public:
 
     malloc_hook_cplusplus() :
         m_mapped_size(0), m_unmapped_size(0),
-        m_dynamic_mmap_config(ucm_global_config.enable_dynamic_mmap_thresh) {
+        m_dynamic_mmap_config(ucm_global_opts.enable_dynamic_mmap_thresh),
+        m_event(this) {
     }
 
     ~malloc_hook_cplusplus() {
-        ucm_global_config.enable_dynamic_mmap_thresh = m_dynamic_mmap_config;
+        ucm_global_opts.enable_dynamic_mmap_thresh = m_dynamic_mmap_config;
     }
 
     void set() {
         ucs_status_t status;
-        status = ucm_set_event_handler(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED,
-                                       0, mem_event_callback,
-                                       reinterpret_cast<void*>(this));
+        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);
         ASSERT_UCS_OK(status);
     }
 
     void unset() {
-        ucm_unset_event_handler(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED,
-                                mem_event_callback, reinterpret_cast<void*>(this));
+        m_event.unset();
     }
 
-protected:
-    static void mem_event_callback(ucm_event_type_t event_type,
-                                   ucm_event_t *event, void *arg)
+    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)
     {
-        malloc_hook_cplusplus *self =
-                        reinterpret_cast<malloc_hook_cplusplus*>(arg);
         switch (event_type) {
         case UCM_EVENT_VM_MAPPED:
-            self->m_mapped_size   += event->vm_mapped.size;
+            m_mapped_size   += event->vm_mapped.size;
             break;
         case UCM_EVENT_VM_UNMAPPED:
-            self->m_unmapped_size += event->vm_unmapped.size;
+            m_unmapped_size += event->vm_unmapped.size;
             break;
         default:
             break;
         }
     }
 
+protected:
     double measure_alloc_time(size_t size, unsigned iters)
     {
         ucs_time_t start_time = ucs_get_time();
@@ -459,7 +528,7 @@ protected:
 
         m_unmapped_size = 0;
         strs.clear();
-        if (ucm_global_config.enable_dynamic_mmap_thresh) {
+        if (ucm_global_opts.enable_dynamic_mmap_thresh) {
             EXPECT_EQ(0ul, m_unmapped_size);
         } else {
             EXPECT_GE(m_unmapped_size, size);
@@ -471,6 +540,139 @@ protected:
     size_t m_mapped_size;
     size_t m_unmapped_size;
     int    m_dynamic_mmap_config;
+    mmap_event<malloc_hook_cplusplus> m_event;
+};
+
+
+class mmap_hooks {
+public:
+    mmap_hooks(const std::string& name, int num_threads, pthread_barrier_t *barrier):
+        m_num_threads(num_threads), m_mapped_size(0), m_unmapped_size(0),
+        m_name(name), m_barrier(barrier), m_event(this)
+    {
+    }
+
+    void mem_event(ucm_event_type_t event_type, ucm_event_t *event)
+    {
+        switch (event_type) {
+        case UCM_EVENT_VM_MAPPED:
+            m_mapped_size   += event->vm_mapped.size;
+            break;
+        case UCM_EVENT_VM_UNMAPPED:
+            m_unmapped_size += event->vm_unmapped.size;
+            break;
+        default:
+            break;
+        }
+    }
+
+    void test()
+    {
+        /*
+         * Test memory mapping functions which override an existing mapping
+         */
+        size_t size          = ucs_get_page_size() * 800;
+        size_t mapped_size   = 0;
+        size_t unmapped_size = 0;
+        void *buffer;
+        int shmid;
+        ucs_status_t status;
+
+        EXPECT_EQ(0u, m_mapped_size) << m_name;
+        EXPECT_EQ(0u, m_unmapped_size) << m_name;
+
+        status = m_event.set(UCM_EVENT_VM_MAPPED|UCM_EVENT_VM_UNMAPPED);
+        ASSERT_UCS_OK(status);
+
+        pthread_barrier_wait(m_barrier);
+
+        /* 1. Map a large buffer */
+        {
+            buffer = mmap(NULL, size, PROT_READ|PROT_WRITE,
+                                MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
+            ASSERT_NE(MAP_FAILED, buffer) << strerror(errno);
+
+            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);
+            EXPECT_INCREASED(m_unmapped_size, unmapped_size, 0, m_name);
+        }
+
+        /*
+         * 2. Map another buffer in the same place.
+         *    Expected behavior: unmap event on the old buffer
+         */
+        {
+            void *remap = mmap(buffer, size, PROT_READ|PROT_WRITE,
+                               MAP_PRIVATE|MAP_ANONYMOUS|MAP_FIXED, -1, 0);
+            ASSERT_EQ(buffer, remap);
+
+            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);
+            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);
+        }
+
+        /* 3. Create a shared memory segment */
+        {
+            shmid = shmget(IPC_PRIVATE, size, IPC_CREAT | SHM_R | SHM_W);
+            ASSERT_NE(-1, shmid) << strerror(errno) << m_name;
+        }
+
+        /*
+         * 4. Attach the segment at the same buffer address.
+         *    Expected behavior: unmap event on the old buffer
+         */
+        {
+            void *shmaddr = shmat(shmid, buffer, SHM_REMAP);
+            ASSERT_EQ(buffer, shmaddr) << m_name;
+
+            EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);
+            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);
+        }
+
+        /* 5. Detach the sysv segment */
+        {
+            shmdt(buffer);
+
+            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);
+        }
+
+        /* 6. Remove the shared memory segment */
+        {
+            int ret = shmctl(shmid, IPC_RMID, NULL);
+            ASSERT_NE(-1, ret) << strerror(errno);
+        }
+
+        /* 7. Unmap the buffer */
+        {
+            munmap(buffer, size);
+
+            EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);
+        }
+
+        /* 8. sbrk call - single thread only */
+        {
+            if (!RUNNING_ON_VALGRIND && m_num_threads < 2) {
+                /* valgrind failed when sbrk is called directly,
+                 * also sbrk is not thread safe */
+
+                /* sbrk call is used to extend/cut memory heap,
+                 * don't add any evaluations between calls sbrk+/sbrk- - it
+                 * may break heap */
+                sbrk(size);
+                sbrk(-size);
+
+                EXPECT_INCREASED(m_mapped_size, mapped_size, size, m_name);
+                EXPECT_INCREASED(m_unmapped_size, unmapped_size, size, m_name);
+            }
+        }
+        pthread_barrier_wait(m_barrier);
+    }
+
+protected:
+    int                     m_num_threads;
+    size_t                  m_mapped_size;
+    size_t                  m_unmapped_size;
+    std::string             m_name;
+    pthread_barrier_t       *m_barrier;
+    mmap_event<mmap_hooks>  m_event;
 };
 
 
@@ -503,12 +705,16 @@ UCS_TEST_F(malloc_hook_cplusplus, dynamic_mmap_enable) {
     if (RUNNING_ON_VALGRIND) {
         UCS_TEST_SKIP_R("skipping on valgrind");
     }
-    EXPECT_TRUE(ucm_global_config.enable_dynamic_mmap_thresh);
+    skip_on_bistro();
+    EXPECT_TRUE(ucm_global_opts.enable_dynamic_mmap_thresh);
     test_dynamic_mmap_thresh();
 }
 
 UCS_TEST_F(malloc_hook_cplusplus, dynamic_mmap_disable) {
-    ucm_global_config.enable_dynamic_mmap_thresh = 0;
+    skip_on_bistro();
+
+    ucm_global_opts.enable_dynamic_mmap_thresh = 0;
+
     test_dynamic_mmap_thresh();
 }
 
@@ -523,6 +729,8 @@ UCS_TEST_F(malloc_hook_cplusplus, mallopt) {
     char *p;
     size_t size;
 
+    skip_on_bistro();
+
     /* This test can not be run with the other
      * tests because it assumes that malloc hooks
      * are not initialized
@@ -585,7 +793,7 @@ UCS_TEST_F(malloc_hook_cplusplus, mmap_ptrs) {
         UCS_TEST_SKIP_R("skipping on valgrind");
     }
 
-    ucm_global_config.enable_dynamic_mmap_thresh = 0;
+    ucm_global_opts.enable_dynamic_mmap_thresh = 0;
     set();
 
     const size_t   size    = ucm_dlmallopt_get(M_MMAP_THRESHOLD) * 2;
@@ -647,3 +855,157 @@ UCS_TEST_F(malloc_hook_cplusplus, mmap_ptrs) {
     unset();
 
 }
+
+UCS_TEST_F(malloc_hook_cplusplus, remap_override_single_thread) {
+    pthread_barrier_t barrier;
+    pthread_barrier_init(&barrier, NULL, 1);
+    {
+        mhook_thread<mmap_hooks>(new mmap_hooks("single-thread", 1, &barrier));
+    }
+    pthread_barrier_destroy(&barrier);
+}
+
+UCS_TEST_F(malloc_hook_cplusplus, remap_override_multi_threads) {
+    typedef mhook_thread<mmap_hooks> thread_t;
+
+    static const int num_threads = 8;
+    ucs::ptr_vector<thread_t> threads;
+    pthread_barrier_t barrier;
+
+    pthread_barrier_init(&barrier, NULL, num_threads);
+    for (int i = 0; i < num_threads; ++i) {
+        std::stringstream ss;
+        ss << "thread " << i << "/" << num_threads;
+        threads.push_back(new thread_t(new mmap_hooks(ss.str(), num_threads, &barrier)));
+    }
+
+    threads.clear();
+    pthread_barrier_destroy(&barrier);
+}
+
+typedef int (munmap_f_t)(void *addr, size_t len);
+
+UCS_TEST_F(malloc_hook, bistro_patch) {
+    const char *symbol = "munmap";
+    ucm_bistro_restore_point_t *rp = NULL;
+    ucs_status_t status;
+    munmap_f_t *munmap_f;
+    void *ptr;
+    int res;
+    uint64_t UCS_V_UNUSED patched;
+    uint64_t UCS_V_UNUSED origin;
+
+    if (RUNNING_ON_VALGRIND) {
+        UCS_TEST_SKIP_R("skipping on valgrind");
+    }
+
+    /* set hook to mmap call */
+    status = ucm_bistro_patch(symbol, (void*)bistro_munmap_hook, &rp);
+    ASSERT_UCS_OK(status);
+    EXPECT_NE((intptr_t)rp, NULL);
+
+    munmap_f = (munmap_f_t*)ucm_bistro_restore_addr(rp);
+    EXPECT_NE((intptr_t)munmap_f, NULL);
+
+    /* save partial body of patched function */
+    patched = *(uint64_t*)munmap_f;
+
+    bistro_call_counter = 0;
+    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
+    EXPECT_NE(ptr, MAP_FAILED);
+
+    /* try to call munmap, we should jump into munmap_hook instead */
+    res = munmap_f(ptr, 4096);
+    EXPECT_EQ(res, 0);
+    /* due to cache coherency issues on ARM systems could be executed
+     * original function body, so, skip counter evaluation */
+    EXPECT_GT(bistro_call_counter, 0);
+
+    /* restore original mmap body */
+    status = ucm_bistro_restore(rp);
+    ASSERT_UCS_OK(status);
+
+    bistro_call_counter = 0;
+    /* now try to call mmap, we should NOT jump into mmap_hook */
+    ptr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
+    EXPECT_NE(ptr, MAP_FAILED);
+    res = munmap_f(ptr, 4096);
+    EXPECT_EQ(res, 0);
+    EXPECT_EQ(bistro_call_counter, 0);  /* hook is not called */
+    /* save partial body of restored function */
+    origin = *(uint64_t*)munmap_f;
+
+#if !defined (__powerpc64__)
+    EXPECT_NE(patched, origin);
+#endif
+}
+
+/* test for mmap events are fired from non-direct load modules
+ * we are trying to load lib1, from lib1 load lib2, and
+ * fire mmap event from lib2 */
+UCS_TEST_F(malloc_hook, dlopen) {
+#ifndef GTEST_UCM_HOOK_LIB_DIR
+#  error "Missing build configuration"
+#else
+    typedef void (fire_mmap_f)(void);
+    typedef void* (load_lib_f)(const char *path);
+
+    const char *libdlopen_load = "/libdlopen_test_do_load.so";
+    const char *libdlopen_mmap = "/libdlopen_test_do_mmap.so";
+    const char *load_lib       = "load_lib";
+    const char *fire_mmap      = "fire_mmap";
+
+    std::string lib_load;
+    std::string lib_mmap;
+    void *lib;
+    void *lib2;
+    load_lib_f *load;
+    fire_mmap_f *fire;
+    ucs_status_t status;
+    mmap_event<malloc_hook> event(this);
+
+    status = event.set(UCM_EVENT_VM_MAPPED);
+    ASSERT_UCS_OK(status);
+
+    lib_load = std::string(GTEST_UCM_HOOK_LIB_DIR) + libdlopen_load;
+    lib_mmap = std::string(GTEST_UCM_HOOK_LIB_DIR) + libdlopen_mmap;
+
+    UCS_TEST_MESSAGE << "Loading " << lib_load;
+    UCS_TEST_MESSAGE << "Loading " << lib_mmap;
+
+    lib = dlopen(lib_load.c_str(), RTLD_NOW);
+    EXPECT_NE((uintptr_t)lib, (uintptr_t)NULL);
+    if (!lib) {
+        goto no_lib;
+    }
+
+    load = (load_lib_f*)dlsym(lib, load_lib);
+    EXPECT_NE((uintptr_t)load, (uintptr_t)NULL);
+    if (!load) {
+        goto no_load;
+    }
+
+    lib2 = load(lib_mmap.c_str());
+    EXPECT_NE((uintptr_t)lib2, (uintptr_t)NULL);
+    if (!lib2) {
+        goto no_load;
+    }
+
+    fire = (fire_mmap_f*)dlsym(lib2, fire_mmap);
+    EXPECT_NE((uintptr_t)fire, (uintptr_t)NULL);
+    if (!fire) {
+        goto no_fire;
+    }
+
+    m_got_event = 0;
+    fire();
+    EXPECT_GT(m_got_event, 0);
+
+no_fire:
+    dlclose(lib2);
+no_load:
+    dlclose(lib);
+no_lib:
+    event.unset();
+#endif /* GTEST_UCM_HOOK_LIB_DIR */
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/Makefile.am b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/Makefile.am
new file mode 100644
index 000000000..5aaf5d0e4
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/Makefile.am
@@ -0,0 +1,16 @@
+#
+# Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+#
+# See file LICENSE for terms.
+#
+
+
+noinst_lib_LTLIBRARIES = \
+	libdlopen_test_do_mmap.la \
+	libdlopen_test_do_load.la
+
+libdlopen_test_do_mmap_la_SOURCES = dlopen_test_do_mmap.c
+libdlopen_test_do_load_la_SOURCES = dlopen_test_do_load.c
+noinst_libdir = ${PWD}/.noinst
+
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_load.c b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_load.c
new file mode 100644
index 000000000..a6dae7e5f
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_load.c
@@ -0,0 +1,12 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd.      2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include <dlfcn.h>
+
+void* load_lib(const char *path)
+{
+    return dlopen(path, RTLD_NOW);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_mmap.c b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_mmap.c
new file mode 100644
index 000000000..91bead84e
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucm/test_dlopen/dlopen_test_do_mmap.c
@@ -0,0 +1,15 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd.      2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include <sys/mman.h>
+
+void fire_mmap(void)
+{
+    void* map_ptr;
+
+    map_ptr = mmap(0, 4096, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);
+    munmap(map_ptr, 4096);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.cc
index 9d54c90a7..53322c652 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.cc
@@ -38,7 +38,7 @@ void test_ucp_atomic::init() {
 
 template <typename T>
 void test_ucp_atomic::blocking_add(entity *e,  size_t max_size, void *memheap_addr,
-                  ucp_rkey_h rkey, std::string& expected_data)
+                                   ucp_rkey_h rkey, std::string& expected_data)
 {
     ucs_status_t status;
     T add, prev;
@@ -63,126 +63,35 @@ void test_ucp_atomic::unaligned_blocking_add64(entity *e,  size_t max_size,
                                                void *memheap_addr, ucp_rkey_h rkey,
                                                std::string& expected_data)
 {
-    /* Test that unaligned addresses generate error */
     ucs_status_t status;
-    status = ucp_atomic_add64(e->ep(), 0, (uintptr_t)memheap_addr + 1, rkey);
+    {
+        /* Test that unaligned addresses generate error */
+        scoped_log_handler slh(hide_errors_logger);
+        status = ucp_atomic_add64(e->ep(), 0, (uintptr_t)memheap_addr + 1, rkey);
+    }
     EXPECT_EQ(UCS_ERR_INVALID_PARAM, status);
     expected_data.clear();
 }
 
-template <typename T>
-void test_ucp_atomic::blocking_fadd(entity *e,  size_t max_size,
-                                    void *memheap_addr, ucp_rkey_h rkey,
-                                    std::string& expected_data)
-{
-    ucs_status_t status;
-    T add, prev, result;
-
-    prev = *(T*)memheap_addr;
-    add  = (T)ucs::rand() * (T)ucs::rand();
-
-    if (sizeof(T) == sizeof(uint32_t)) {
-        status = ucp_atomic_fadd32(e->ep(), add, (uintptr_t)memheap_addr, rkey,
-                                   (uint32_t*)(void*)&result);
-    } else if (sizeof(T) == sizeof(uint64_t)) {
-        status = ucp_atomic_fadd64(e->ep(), add, (uintptr_t)memheap_addr, rkey,
-                                   (uint64_t*)(void*)&result);
-    } else {
-        status = UCS_ERR_UNSUPPORTED;
-    }
-    ASSERT_UCS_OK(status);
-
-    EXPECT_EQ(prev, result);
-
-    expected_data.resize(sizeof(T));
-    *(T*)&expected_data[0] = add + prev;
-}
-
-template <typename T>
-void test_ucp_atomic::blocking_swap(entity *e,  size_t max_size, void *memheap_addr,
-                                    ucp_rkey_h rkey, std::string& expected_data)
-{
-    ucs_status_t status;
-    T swap, prev, result;
-
-    prev = *(T*)memheap_addr;
-    swap = (T)ucs::rand() * (T)ucs::rand();
-
-    if (sizeof(T) == sizeof(uint32_t)) {
-        status = ucp_atomic_swap32(e->ep(), swap, (uintptr_t)memheap_addr,
-                                   rkey, (uint32_t*)(void*)&result);
-    } else if (sizeof(T) == sizeof(uint64_t)) {
-        status = ucp_atomic_swap64(e->ep(), swap, (uintptr_t)memheap_addr,
-                                   rkey, (uint64_t*)(void*)&result);
-    } else {
-        status = UCS_ERR_UNSUPPORTED;
-    }
-    ASSERT_UCS_OK(status);
-
-    EXPECT_EQ(prev, result);
-
-    expected_data.resize(sizeof(T));
-    *(T*)&expected_data[0] = swap;
-}
-
-template <typename T>
-void test_ucp_atomic::blocking_cswap(entity *e,  size_t max_size, void *memheap_addr,
-                    ucp_rkey_h rkey, std::string& expected_data)
-{
-    ucs_status_t status;
-    T compare, swap, prev, result;
-
-    prev = *(T*)memheap_addr;
-    if ((ucs::rand() % 2) == 0) {
-        compare = prev; /* success mode */
-    } else {
-        compare = ~prev; /* fail mode */
-    }
-    swap = (T)ucs::rand() * (T)ucs::rand();
-
-    if (sizeof(T) == sizeof(uint32_t)) {
-        status = ucp_atomic_cswap32(e->ep(), compare, swap,
-                                    (uintptr_t)memheap_addr, rkey,
-                                    (uint32_t*)(void*)&result);
-    } else if (sizeof(T) == sizeof(uint64_t)) {
-        status = ucp_atomic_cswap64(e->ep(), compare, swap,
-                                    (uintptr_t)memheap_addr, rkey,
-                                    (uint64_t*)(void*)&result);
-    } else {
-        status = UCS_ERR_UNSUPPORTED;
-    }
-    ASSERT_UCS_OK(status);
-
-    EXPECT_EQ(prev, result);
-
-    expected_data.resize(sizeof(T));
-    if (compare == prev) {
-        *(T*)&expected_data[0] = swap;
-    } else {
-        *(T*)&expected_data[0] = prev;
-    }
-}
-
 template <typename T>
 ucs_status_t test_ucp_atomic::ucp_atomic_post_nbi(ucp_ep_h ep, ucp_atomic_post_op_t opcode,
-                                              T value, void *remote_addr,
-                                              ucp_rkey_h rkey)
+                                                  T value, void *remote_addr,
+                                                  ucp_rkey_h rkey)
 {
     return ucp_atomic_post(ep, opcode, value, sizeof(T), (uintptr_t)remote_addr, rkey);
 }
 
-template <typename T>
-void test_ucp_atomic::nb_add(entity *e,  size_t max_size, void *memheap_addr,
-                  ucp_rkey_h rkey, std::string& expected_data)
+template <typename T, ucp_atomic_post_op_t OP>
+void test_ucp_atomic::nb_post(entity *e,  size_t max_size, void *memheap_addr,
+                              ucp_rkey_h rkey, std::string& expected_data)
 {
     ucs_status_t status;
-    T add, prev;
+    T val, prev;
 
-    prev = *(T*)memheap_addr;
-    add  = (T)ucs::rand() * (T)ucs::rand();
+    prev   = *(T*)memheap_addr;
+    val    = (T)ucs::rand() * (T)ucs::rand();
 
-    status = test_ucp_atomic::ucp_atomic_post_nbi<T>(e->ep(), UCP_ATOMIC_POST_OP_ADD, add,
-                                                 memheap_addr, rkey);
+    status = test_ucp_atomic::ucp_atomic_post_nbi<T>(e->ep(), OP, val, memheap_addr, rkey);
 
     if (status == UCS_INPROGRESS) {
         flush_worker(*e);
@@ -190,20 +99,21 @@ void test_ucp_atomic::nb_add(entity *e,  size_t max_size, void *memheap_addr,
         ASSERT_UCS_OK(status);
     }
     expected_data.resize(sizeof(T));
-    *(T*)&expected_data[0] = add + prev;
+    *(T*)&expected_data[0] = atomic_op_val<T, OP>(val, prev);
 }
 
-void test_ucp_atomic::unaligned_nb_add64(entity *e,  size_t max_size,
-                                         void *memheap_addr, ucp_rkey_h rkey,
-                                         std::string& expected_data)
+template <ucp_atomic_post_op_t OP>
+void test_ucp_atomic::unaligned_nb_post(entity *e,  size_t max_size,
+                                        void *memheap_addr, ucp_rkey_h rkey,
+                                        std::string& expected_data)
 {
-    /* Test that unaligned addresses generate error */
     ucs_status_t status;
-    
-    status = test_ucp_atomic::ucp_atomic_post_nbi<uint64_t>(e->ep(),
-                                                        UCP_ATOMIC_POST_OP_ADD, 0,
-                                                        (void *)((uintptr_t)memheap_addr + 1),
-                                                        rkey);
+    {
+        /* Test that unaligned addresses generate error */
+        scoped_log_handler slh(hide_errors_logger);
+        status = test_ucp_atomic::ucp_atomic_post_nbi<uint64_t>
+                (e->ep(), OP, 0, (void *)((uintptr_t)memheap_addr + 1), rkey);
+    }
     EXPECT_EQ(UCS_ERR_INVALID_PARAM, status);
     expected_data.clear();
 }
@@ -218,41 +128,19 @@ ucs_status_ptr_t test_ucp_atomic::ucp_atomic_fetch(ucp_ep_h ep,
                                (uintptr_t)remote_addr, rkey, send_completion);
 }
 
-template <typename T>
-void test_ucp_atomic::nb_fadd(entity *e,  size_t max_size,
-                              void *memheap_addr, ucp_rkey_h rkey,
-                              std::string& expected_data)
-{
-    void *amo_req;
-    T add, prev, result;
-
-    prev = *(T*)memheap_addr;
-    add  = (T)ucs::rand() * (T)ucs::rand();
-
-    amo_req = test_ucp_atomic::ucp_atomic_fetch<T>(e->ep(), UCP_ATOMIC_FETCH_OP_FADD,
-                                                   add, &result, memheap_addr, rkey);
-    if(UCS_PTR_IS_PTR(amo_req)){
-        wait(amo_req);
-    }
-
-    EXPECT_EQ(prev, result);
-
-    expected_data.resize(sizeof(T));
-    *(T*)&expected_data[0] = add + prev;
-}
-
-template <typename T>
-void test_ucp_atomic::nb_swap(entity *e,  size_t max_size, void *memheap_addr,
-                              ucp_rkey_h rkey, std::string& expected_data)
+template <typename T, ucp_atomic_fetch_op_t FOP>
+void test_ucp_atomic::nb_fetch(entity *e,  size_t max_size,
+                               void *memheap_addr, ucp_rkey_h rkey,
+                               std::string& expected_data)
 {
-    T swap, prev, result;
     void *amo_req;
+    T val, prev, result;
 
-    prev = *(T*)memheap_addr;
-    swap = (T)ucs::rand() * (T)ucs::rand();
+    prev    = *(T*)memheap_addr;
+    val     = (T)ucs::rand() * (T)ucs::rand();
 
-    amo_req = test_ucp_atomic::ucp_atomic_fetch<T>(e->ep(), UCP_ATOMIC_FETCH_OP_SWAP,
-                                                   swap, &result, memheap_addr, rkey);
+    amo_req = test_ucp_atomic::ucp_atomic_fetch<T>(e->ep(), FOP,
+                                                   val, &result, memheap_addr, rkey);
     if(UCS_PTR_IS_PTR(amo_req)){
         wait(amo_req);
     }
@@ -260,7 +148,7 @@ void test_ucp_atomic::nb_swap(entity *e,  size_t max_size, void *memheap_addr,
     EXPECT_EQ(prev, result);
 
     expected_data.resize(sizeof(T));
-    *(T*)&expected_data[0] = swap;
+    *(T*)&expected_data[0] = atomic_fop_val<T, FOP>(val, prev);
 }
 
 template <typename T>
@@ -319,33 +207,48 @@ UCS_TEST_P(test_ucp_atomic32, atomic_add) {
 }
 
 UCS_TEST_P(test_ucp_atomic32, atomic_add_nb) {
-    test<uint32_t>(&test_ucp_atomic32::nb_add<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::nb_add<uint32_t>, true);
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_ADD>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_ADD>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic32, atomic_fadd) {
-    test<uint32_t>(&test_ucp_atomic32::blocking_fadd<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::blocking_fadd<uint32_t>, true);
+UCS_TEST_P(test_ucp_atomic32, atomic_and_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_AND>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_AND>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic32, atomic_or_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_OR>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_OR>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic32, atomic_xor_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_XOR>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_post<uint32_t, UCP_ATOMIC_POST_OP_XOR>, true);
 }
 
 UCS_TEST_P(test_ucp_atomic32, atomic_fadd_nb) {
-    test<uint32_t>(&test_ucp_atomic32::nb_fadd<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::nb_fadd<uint32_t>, true);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FADD>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FADD>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic32, atomic_swap) {
-    test<uint32_t>(&test_ucp_atomic32::blocking_swap<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::blocking_swap<uint32_t>, true);
+UCS_TEST_P(test_ucp_atomic32, atomic_fand_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FAND>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FAND>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic32, atomic_swap_nb) {
-    test<uint32_t>(&test_ucp_atomic32::nb_swap<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::nb_swap<uint32_t>, true);
+UCS_TEST_P(test_ucp_atomic32, atomic_for_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FOR>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FOR>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic32, atomic_cswap) {
-    test<uint32_t>(&test_ucp_atomic32::blocking_cswap<uint32_t>, false);
-    test<uint32_t>(&test_ucp_atomic32::blocking_cswap<uint32_t>, true);
+UCS_TEST_P(test_ucp_atomic32, atomic_fxor_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FXOR>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_FXOR>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic32, atomic_swap_nb) {
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_SWAP>, false);
+    test<uint32_t>(&test_ucp_atomic32::nb_fetch<uint32_t, UCP_ATOMIC_FETCH_OP_SWAP>, true);
 }
 
 UCS_TEST_P(test_ucp_atomic32, atomic_cswap_nb) {
@@ -370,35 +273,49 @@ UCS_TEST_P(test_ucp_atomic64, atomic_add) {
 }
 
 UCS_TEST_P(test_ucp_atomic64, atomic_add_nb) {
-    test<uint64_t>(&test_ucp_atomic64::nb_add<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::nb_add<uint64_t>, true);
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_ADD>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_ADD>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic64, atomic_and_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_AND>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_AND>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic64, atomic_fadd) {
-    test<uint64_t>(&test_ucp_atomic64::blocking_fadd<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::blocking_fadd<uint64_t>, true);
+UCS_TEST_P(test_ucp_atomic64, atomic_or_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_OR>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_OR>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic64, atomic_xor_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_XOR>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_post<uint64_t, UCP_ATOMIC_POST_OP_XOR>, true);
 }
 
 UCS_TEST_P(test_ucp_atomic64, atomic_fadd_nb) {
-    test<uint64_t>(&test_ucp_atomic64::nb_fadd<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::nb_fadd<uint64_t>, true);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FADD>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FADD>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic64, atomic_swap) {
-    test<uint64_t>(&test_ucp_atomic64::blocking_swap<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::blocking_swap<uint64_t>, true);
+UCS_TEST_P(test_ucp_atomic64, atomic_fand_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FAND>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FAND>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic64, atomic_swap_nb) {
-    test<uint64_t>(&test_ucp_atomic64::nb_swap<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::nb_swap<uint64_t>, true);
+UCS_TEST_P(test_ucp_atomic64, atomic_for_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FOR>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FOR>, true);
 }
 
-UCS_TEST_P(test_ucp_atomic64, atomic_cswap) {
-    test<uint64_t>(&test_ucp_atomic64::blocking_cswap<uint64_t>, false);
-    test<uint64_t>(&test_ucp_atomic64::blocking_cswap<uint64_t>, true);
+UCS_TEST_P(test_ucp_atomic64, atomic_fxor_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FXOR>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_FXOR>, true);
 }
 
+UCS_TEST_P(test_ucp_atomic64, atomic_swap_nb) {
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_SWAP>, false);
+    test<uint64_t>(&test_ucp_atomic64::nb_fetch<uint64_t, UCP_ATOMIC_FETCH_OP_SWAP>, true);
+}
 
 UCS_TEST_P(test_ucp_atomic64, atomic_cswap_nb) {
     test<uint64_t>(&test_ucp_atomic64::nb_cswap<uint64_t>, false);
@@ -412,8 +329,23 @@ UCS_TEST_P(test_ucp_atomic64, unaligned_atomic_add) {
 }
 
 UCS_TEST_P(test_ucp_atomic64, unaligned_atomic_add_nb) {
-    test<uint64_t>(&test_ucp_atomic::unaligned_nb_add64, false);
-    test<uint64_t>(&test_ucp_atomic::unaligned_nb_add64, true);
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_ADD>, false);
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_ADD>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic64, unaligned_atomic_and_nb) {
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_AND>, false);
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_AND>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic64, unaligned_atomic_or_nb) {
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_OR>, false);
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_OR>, true);
+}
+
+UCS_TEST_P(test_ucp_atomic64, unaligned_atomic_xor_nb) {
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_XOR>, false);
+    test<uint64_t>(&test_ucp_atomic::unaligned_nb_post<UCP_ATOMIC_POST_OP_XOR>, true);
 }
 #endif
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.h
index 4370e3733..0c48efed8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_atomic.h
@@ -30,40 +30,63 @@ public:
     void unaligned_blocking_add64(entity *e,  size_t max_size, void *memheap_addr,
                                   ucp_rkey_h rkey, std::string& expected_data);
 
-    template <typename T>
-    void blocking_fadd(entity *e,  size_t max_size, void *memheap_addr,
-                       ucp_rkey_h rkey, std::string& expected_data);
-
-    template <typename T>
-    void blocking_swap(entity *e,  size_t max_size, void *memheap_addr,
-                       ucp_rkey_h rkey, std::string& expected_data);
-
-    template <typename T>
-    void blocking_cswap(entity *e,  size_t max_size, void *memheap_addr,
-                        ucp_rkey_h rkey, std::string& expected_data);
-
-    template <typename T>
-    void nb_add(entity *e,  size_t max_size, void *memheap_addr,
-                      ucp_rkey_h rkey, std::string& expected_data);
-
-    void unaligned_nb_add64(entity *e,  size_t max_size, void *memheap_addr,
-                            ucp_rkey_h rkey, std::string& expected_data);
-
-    template <typename T>
-    void nb_fadd(entity *e,  size_t max_size, void *memheap_addr,
-                 ucp_rkey_h rkey, std::string& expected_data);
-
-    template <typename T>
-    void nb_swap(entity *e,  size_t max_size, void *memheap_addr,
-                 ucp_rkey_h rkey, std::string& expected_data);
+    template <ucp_atomic_post_op_t OP>
+    void unaligned_nb_post(entity *e,  size_t max_size, void *memheap_addr,
+                           ucp_rkey_h rkey, std::string& expected_data);
 
     template <typename T>
     void nb_cswap(entity *e,  size_t max_size, void *memheap_addr,
-                        ucp_rkey_h rkey, std::string& expected_data);
+                  ucp_rkey_h rkey, std::string& expected_data);
     
     template <typename T, typename F>
     void test(F f, bool malloc_allocate);
 
+    template <typename T, ucp_atomic_post_op_t OP>
+    void nb_post(entity *e,  size_t max_size, void *memheap_addr,
+                 ucp_rkey_h rkey, std::string& expected_data);
+
+    template <typename T, ucp_atomic_fetch_op_t FOP>
+    void nb_fetch(entity *e,  size_t max_size, void *memheap_addr,
+                  ucp_rkey_h rkey, std::string& expected_data);
+
+    template <typename T, ucp_atomic_post_op_t OP>
+    T atomic_op_val(T v1, T v2)
+    {
+        /* coverity[switch_selector_expr_is_constant] */
+        switch (OP) {
+        case UCP_ATOMIC_POST_OP_ADD:
+            return v1 + v2;
+        case UCP_ATOMIC_POST_OP_AND:
+            return v1 & v2;
+        case UCP_ATOMIC_POST_OP_OR:
+            return v1 | v2;
+        case UCP_ATOMIC_POST_OP_XOR:
+            return v1 ^ v2;
+        default:
+            return 0;
+        }
+    }
+
+    template <typename T, ucp_atomic_fetch_op_t OP>
+    T atomic_fop_val(T v1, T v2)
+    {
+        /* coverity[switch_selector_expr_is_constant] */
+        switch (OP) {
+        case UCP_ATOMIC_FETCH_OP_FADD:
+            return v1 + v2;
+        case UCP_ATOMIC_FETCH_OP_FAND:
+            return v1 & v2;
+        case UCP_ATOMIC_FETCH_OP_FOR:
+            return v1 | v2;
+        case UCP_ATOMIC_FETCH_OP_FXOR:
+            return v1 ^ v2;
+        case UCP_ATOMIC_FETCH_OP_SWAP:
+            return v1;
+        default:
+            return 0;
+        }
+    }
+
 private:
     static void send_completion(void *request, ucs_status_t status){}
     template <typename T>
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_context.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_context.cc
index bbb7656de..964a2cd13 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_context.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_context.cc
@@ -78,14 +78,19 @@ UCS_TEST_P(test_ucp_version, wrong_api_version) {
     ucp_params_t params = get_ctx_params();
     ucp_context_h ucph;
     ucs_status_t status;
+    size_t warn_count;
     {
-        wrap_errors();
+        scoped_log_handler slh(hide_warns_logger);
+        warn_count = m_warnings.size();
         status = ucp_init_version(99, 99, &params, config.get(), &ucph);
-        restore_errors();
     }
-    if (status == UCS_OK) {
+    if (status != UCS_OK) {
+        ADD_FAILURE() << "Failed to create UCP with wrong version";
+    } else {
+        if (m_warnings.size() == warn_count) {
+            ADD_FAILURE() << "Missing wrong version warning";
+        }
         ucp_cleanup(ucph);
-        ADD_FAILURE() << "Created UCP with wrong version";
     }
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_fence.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_fence.cc
index b911afd08..4e5afc4ef 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_fence.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_fence.cc
@@ -5,6 +5,7 @@
 */
 
 #include "test_ucp_atomic.h"
+#include "common/gtest.h"
 
 class test_ucp_fence : public test_ucp_atomic {
 public:
@@ -12,19 +13,16 @@ public:
                                                  uint64_t *result_buf, void *memheap_addr,
                                                  ucp_rkey_h rkey);
 
+    static void send_cb(void *request, ucs_status_t status)
+    {
+    }
+
     template <typename T>
     void blocking_add(entity *e, uint64_t *initial_buf, uint64_t *result_buf,
                       void *memheap_addr, ucp_rkey_h rkey) {
-        ucs_status_t status;
-        if (sizeof(T) == sizeof(uint32_t)) {
-            status = ucp_atomic_add32(e->ep(), (uint32_t)(*initial_buf),
-                                      (uintptr_t)memheap_addr, rkey);
-        } else if (sizeof(T) == sizeof(uint64_t)) {
-            status = ucp_atomic_add64(e->ep(), (uint64_t)(*initial_buf),
-                                      (uintptr_t)memheap_addr, rkey);
-        } else {
-            status = UCS_ERR_UNSUPPORTED;
-        }
+        ucs_status_t status = ucp_atomic_post(e->ep(), UCP_ATOMIC_POST_OP_ADD,
+                                              *initial_buf, sizeof(T),
+                                              (uintptr_t)memheap_addr, rkey);
         ASSERT_UCS_OK(status);
     }
 
@@ -32,19 +30,10 @@ public:
     void blocking_fadd(entity *e, uint64_t *initial_buf, uint64_t *result_buf,
                        void *memheap_addr, ucp_rkey_h rkey)
     {
-        ucs_status_t status;
-        if (sizeof(T) == sizeof(uint32_t)) {
-            status = ucp_atomic_fadd32(e->ep(), (uint32_t)(*initial_buf),
-                                       (uintptr_t)memheap_addr,
-                                       rkey, (uint32_t*)(void*)result_buf);
-        } else if (sizeof(T) == sizeof(uint64_t)) {
-            status = ucp_atomic_fadd64(e->ep(), (uint64_t)(*initial_buf),
-                                       (uintptr_t)memheap_addr,
-                                       rkey, (uint64_t*)(void*)result_buf);
-        } else {
-            status = UCS_ERR_UNSUPPORTED;
-        }
-        ASSERT_UCS_OK(status);
+        void *request = ucp_atomic_fetch_nb(e->ep(), UCP_ATOMIC_FETCH_OP_FADD,
+                                            *initial_buf, (T*)result_buf, sizeof(T),
+                                            (uintptr_t)memheap_addr, rkey, send_cb);
+        wait(request);
     }
 
     template <typename T, typename F>
@@ -65,7 +54,7 @@ public:
         }
 
         ~worker() {
-            ucs_assert(!running);
+            assert(!running);
         }
 
         static void *run(void *arg) {
@@ -142,6 +131,7 @@ protected:
         uint32_t error = 0;
 
         sender().connect(&receiver(), get_ep_params());
+        flush_worker(sender()); /* avoid deadlock for blocking amo */
 
         params.field_mask = UCP_MEM_MAP_PARAM_FIELD_ADDRESS |
                             UCP_MEM_MAP_PARAM_FIELD_LENGTH |
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mem_type.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mem_type.cc
index afa947d92..6e99e2ab2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mem_type.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mem_type.cc
@@ -5,9 +5,11 @@
 */
 
 #include "ucp_test.h"
+extern "C" {
 #include "uct/api/uct.h"
 #include "ucp/core/ucp_context.h"
 #include "ucp/core/ucp_mm.h"
+}
 
 
 class test_ucp_mem_type : public ucp_test {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_memheap.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_memheap.cc
index cac3e8c1e..f1eacf890 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_memheap.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_memheap.cc
@@ -102,9 +102,8 @@ void test_ucp_memheap::test_nonblocking_implicit_stream_xfer(nonblocking_send_fu
 
         ucs_assert(size * i + alignment <= memheap_size);
 
-        (this->*send)(&sender(), size,
-                      (void*)((uintptr_t)memheap + alignment + i * size),
-                      rkey, expected_data[i]);
+        char *ptr = (char*)memheap + alignment + i * size;
+        (this->*send)(&sender(), size, (void*)ptr, rkey, expected_data[i]);
 
         ASSERT_UCS_OK(status);
 
@@ -117,9 +116,10 @@ void test_ucp_memheap::test_nonblocking_implicit_stream_xfer(nonblocking_send_fu
     }
 
     for (int i = 0; i < max_iter; ++i) {
-        EXPECT_EQ(expected_data[i],
-                  std::string((char *)((uintptr_t)memheap + alignment + i * size),
-                              expected_data[i].length()));
+        char *ptr = (char*)memheap + alignment + i * size;
+        EXPECT_EQ(expected_data[i].substr(0, 20),
+                  std::string(ptr, expected_data[i].length()).substr(0, 20)) <<
+                        ((void*)ptr);
     }
 
     ucp_rkey_destroy(rkey);
@@ -159,6 +159,9 @@ void test_ucp_memheap::test_blocking_xfer(blocking_send_func_t send,
 
     sender().connect(&receiver(), get_ep_params());
 
+    /* avoid deadlock for blocking rma/amo */
+    flush_worker(sender());
+
     ucp_mem_h memh;
     void *memheap = NULL;
 
@@ -223,7 +226,6 @@ void test_ucp_memheap::test_blocking_xfer(blocking_send_func_t send,
         expected_data.resize(size);
 
         ucs::fill_random(expected_data);
-
         (this->*send)(&sender(), size, (void*)((uintptr_t)memheap + offset),
                       rkey, expected_data);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mmap.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mmap.cc
index 192cefac2..965634b3d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mmap.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_mmap.cc
@@ -30,14 +30,19 @@ protected:
     bool resolve_rma(entity *e, ucp_rkey_h rkey);
     bool resolve_amo(entity *e, ucp_rkey_h rkey);
     bool resolve_rma_bw(entity *e, ucp_rkey_h rkey);
+    void test_length0(unsigned flags);
     void test_rkey_management(entity *e, ucp_mem_h memh, bool is_dummy);
 };
 
 bool test_ucp_mmap::resolve_rma(entity *e, ucp_rkey_h rkey)
 {
-    hide_errors();
-    ucs_status_t status = UCP_RKEY_RESOLVE(rkey, e->ep(), rma);
-    restore_errors();
+    ucs_status_t status;
+
+    {
+        scoped_log_handler slh(hide_errors_logger);
+        status = UCP_RKEY_RESOLVE(rkey, e->ep(), rma);
+    }
+
     if (status == UCS_OK) {
         EXPECT_NE(UCP_NULL_LANE, rkey->cache.rma_lane);
         return true;
@@ -51,9 +56,13 @@ bool test_ucp_mmap::resolve_rma(entity *e, ucp_rkey_h rkey)
 
 bool test_ucp_mmap::resolve_amo(entity *e, ucp_rkey_h rkey)
 {
-    hide_errors();
-    ucs_status_t status = UCP_RKEY_RESOLVE(rkey, e->ep(), amo);
-    restore_errors();
+    ucs_status_t status;
+
+    {
+        scoped_log_handler slh(hide_errors_logger);
+        status = UCP_RKEY_RESOLVE(rkey, e->ep(), amo);
+    }
+
     if (status == UCS_OK) {
         EXPECT_NE(UCP_NULL_LANE, rkey->cache.amo_lane);
         return true;
@@ -70,7 +79,7 @@ bool test_ucp_mmap::resolve_rma_bw(entity *e, ucp_rkey_h rkey)
     ucp_lane_index_t lane;
     uct_rkey_t uct_rkey;
 
-    lane = ucp_rkey_get_rma_bw_lane(rkey, e->ep(), &uct_rkey, 0);
+    lane = ucp_rkey_get_rma_bw_lane(rkey, e->ep(), UCT_MD_MEM_TYPE_HOST, &uct_rkey, 0);
     if (lane != UCP_NULL_LANE) {
         return true;
     } else {
@@ -203,8 +212,8 @@ UCS_TEST_P(test_ucp_mmap, reg) {
     }
 }
 
-UCS_TEST_P(test_ucp_mmap, dummy_mem) {
-
+void test_ucp_mmap::test_length0(unsigned flags)
+{
     ucs_status_t status;
     int buf_num = 2;
     ucp_mem_h memh[buf_num];
@@ -222,7 +231,7 @@ UCS_TEST_P(test_ucp_mmap, dummy_mem) {
                         UCP_MEM_MAP_PARAM_FIELD_FLAGS;
     params.address    = NULL;
     params.length     = 0;
-    params.flags      = rand_flags() | UCP_MEM_MAP_ALLOCATE;
+    params.flags      = rand_flags() | flags;
 
     status = ucp_mem_map(sender().ucph(), &params, &memh[0]);
     ASSERT_UCS_OK(status);
@@ -238,6 +247,14 @@ UCS_TEST_P(test_ucp_mmap, dummy_mem) {
     }
 }
 
+UCS_TEST_P(test_ucp_mmap, reg0) {
+    test_length0(0);
+}
+
+UCS_TEST_P(test_ucp_mmap, alloc0) {
+    test_length0(UCP_MEM_MAP_ALLOCATE);
+}
+
 UCS_TEST_P(test_ucp_mmap, alloc_advise) {
     ucs_status_t status;
     bool is_dummy;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_peer_failure.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_peer_failure.cc
index 9df647e6a..7b0a62c91 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_peer_failure.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_peer_failure.cc
@@ -5,655 +5,409 @@
 */
 
 #include "test_ucp_tag.h"
-#include "ucp/api/ucp_def.h"
+#include "ucp_datatype.h"
 
 extern "C" {
-#include <malloc.h>
+#include <ucp/core/ucp_worker.h> /* for testing memory consumption */
+#include <ucp/core/ucp_request.h> // for debug
 }
 
+class test_ucp_peer_failure : public ucp_test {
+public:
+    test_ucp_peer_failure();
+
+    static std::vector<ucp_test_param>
+    enum_test_params(const ucp_params_t& ctx_params, const std::string& name,
+                     const std::string& test_case_name, const std::string& tls);
+
+    ucp_ep_params_t get_ep_params();
 
-class test_ucp_peer_failure_base {
 protected:
     enum {
-        FAIL_AFTER_WIREUP = ucp_test::DEFAULT_PARAM_VARIANT,
-        FAIL_IMMEDIATELY
+        TEST_TAG = UCS_BIT(0),
+        TEST_RMA = UCS_BIT(1),
+        FAIL_IMM = UCS_BIT(2)
     };
 
-    test_ucp_peer_failure_base() {
-        /* Set small TL timeouts to reduce testing time */
-        m_env.push_back(new ucs::scoped_setenv("UCX_RC_TIMEOUT",     "10us"));
-        m_env.push_back(new ucs::scoped_setenv("UCX_RC_RETRY_COUNT", "2"));
-        std::string ud_timeout = ucs::to_string<int>(1 * ucs::test_time_multiplier()) + "s";
-        m_env.push_back(new ucs::scoped_setenv("UCX_UD_TIMEOUT", ud_timeout.c_str()));
-    }
+    enum {
+        STABLE_EP_INDEX,
+        FAILING_EP_INDEX
+    };
 
-    virtual ucp_ep_params_t get_ep_params() {
-        ucp_ep_params_t params;
-        memset(&params, 0, sizeof(params));
-        params.field_mask      = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE |
-                                 UCP_EP_PARAM_FIELD_ERR_HANDLER;
-        params.err_mode        = UCP_ERR_HANDLING_MODE_PEER;
-        params.err_handler.cb  = err_cb;
-        params.err_handler.arg = NULL;
-        return params;
-    }
+    typedef ucs::handle<ucp_mem_h, ucp_context_h> mem_handle_t;
+
+    void set_timeouts();
+    static void err_cb(void *arg, ucp_ep_h ep, ucs_status_t status);
+    ucp_ep_h stable_sender();
+    ucp_ep_h failing_sender();
+    entity& stable_receiver();
+    entity& failing_receiver();
+    void *send_nb(ucp_ep_h ep, ucp_rkey_h rkey);
+    void *recv_nb(entity& e);
+    void fail_receiver();
+    void smoke_test(bool stable_pair);
+    static void unmap_memh(ucp_mem_h memh, ucp_context_h context);
+    void get_rkey(ucp_ep_h ep, entity& dst, mem_handle_t& memh,
+                  ucs::handle<ucp_rkey_h>& rkey);
+    void set_rkeys();
+    static void send_cb(void *request, ucs_status_t status);
+    static void recv_cb(void *request, ucs_status_t status,
+                        ucp_tag_recv_info_t *info);
 
-    void init() {
-        m_err_cntr   = 0;
-        m_err_status = UCS_OK;
-    }
+    virtual void cleanup();
 
-    static void err_cb(void *arg, ucp_ep_h ep, ucs_status_t status) {
-        EXPECT_EQ(UCS_ERR_ENDPOINT_TIMEOUT, status);
-        m_err_status = status;
-        ++m_err_cntr;
-    }
+    void do_test(size_t msg_size, int pre_msg_count, bool force_close,
+                 bool request_must_fail);
 
-protected:
-    static size_t                       m_err_cntr;
-    static ucs_status_t                 m_err_status;
+    size_t                              m_err_count;
+    ucs_status_t                        m_err_status;
+    std::string                         m_sbuf, m_rbuf;
+    mem_handle_t                        m_stable_memh, m_failing_memh;
+    ucs::handle<ucp_rkey_h>             m_stable_rkey, m_failing_rkey;
     ucs::ptr_vector<ucs::scoped_setenv> m_env;
 };
 
-size_t       test_ucp_peer_failure_base::m_err_cntr   = 0;
-ucs_status_t test_ucp_peer_failure_base::m_err_status = UCS_OK;
-
-
-class test_ucp_peer_failure :
-                    public test_ucp_tag,
-                    protected test_ucp_peer_failure_base {
-public:
-    test_ucp_peer_failure() : m_msg_size(1024) {
-    }
-
-    static std::vector<ucp_test_param>
-    enum_test_params(const ucp_params_t& ctx_params,
-                     const std::string& name,
-                     const std::string& test_case_name,
-                     const std::string& tls)
-    {
-        std::vector<ucp_test_param> result =
-            test_ucp_tag::enum_test_params(ctx_params, name, test_case_name, tls);
-
-        generate_test_params_variant(ctx_params, name, test_case_name, tls,
-                                     FAIL_AFTER_WIREUP, result);
-        generate_test_params_variant(ctx_params, name, test_case_name, tls,
-                                     FAIL_IMMEDIATELY, result);
-        return result;
-    }
-
-    virtual void init();
-    virtual void cleanup();
-
-    void test_status_after(bool request_must_fail);
-    void test_force_close();
-
-protected:
-    virtual ucp_ep_params_t get_ep_params() {
-        return test_ucp_peer_failure_base::get_ep_params();
-    }
-
-    void fail_receiver() {
-        /* TODO: need to handle non-empty TX window in UD EP destructor",
-         *       see debug message (ud_ep.c:220)
-         *       ucs_debug("ep=%p id=%d conn_id=%d has %d unacked packets",
-         *                 self, self->ep_id, self->conn_id,
-         *                 (int)ucs_queue_length(&self->tx.window));
-         */
-        flush_worker(receiver());
-        m_entities.remove(&receiver());
-    }
+UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure)
 
-    void smoke_test() {
-        long buf = 0;
-        request *req = recv_nb(&buf, sizeof(buf), DATATYPE, 0, 0);
-        send_b(&buf, sizeof(buf), DATATYPE, 0, 0);
-        wait_and_validate(req);
-    }
 
-    void wait_err() {
-        while (!m_err_cntr) {
-            progress();
-        }
-    }
+test_ucp_peer_failure::test_ucp_peer_failure() : m_err_count(0), m_err_status(UCS_OK) {
+    ucs::fill_random(m_sbuf);
+    set_timeouts();
+}
 
-    static void err_cb_mod(void *arg, ucp_ep_h ep, ucs_status_t status) {
-        EXPECT_EQ(uintptr_t(MAGIC), uintptr_t(arg));
-        err_cb(arg, ep, status);
-        m_err_cb_mod = true;
-    }
+std::vector<ucp_test_param>
+test_ucp_peer_failure::enum_test_params(const ucp_params_t& ctx_params,
+                                        const std::string& name,
+                                        const std::string& test_case_name,
+                                        const std::string& tls)
+{
+    std::vector<ucp_test_param> result;
 
-protected:
-    const size_t m_msg_size;
-    static bool  m_err_cb_mod;
-};
+    ucp_params_t params = ucp_test::get_ctx_params();
 
-bool test_ucp_peer_failure::m_err_cb_mod = false;
+    params.field_mask  |= UCP_PARAM_FIELD_FEATURES;
 
-void test_ucp_peer_failure::init() {
-    m_err_cb_mod = false;
+    params.features = UCP_FEATURE_TAG;
+    generate_test_params_variant(params, name, test_case_name + "/tag", tls,
+                                 TEST_TAG, result);
+    generate_test_params_variant(params, name, test_case_name + "/tag_fail_imm",
+                                 tls, TEST_TAG | FAIL_IMM, result);
 
-    test_ucp_peer_failure_base::init();
-    test_ucp_tag::init();
-    if (GetParam().variant != FAIL_IMMEDIATELY) {
-        smoke_test();
-    }
+    params.features = UCP_FEATURE_RMA;
+    generate_test_params_variant(params, name, test_case_name + "/rma", tls,
+                                 TEST_RMA, result);
+    generate_test_params_variant(params, name, test_case_name + "/rma_fail_imm",
+                                 tls, TEST_RMA | FAIL_IMM, result);
 
-    /* Make second pair */
-    create_entity(true);
-    create_entity(false);
-    sender().connect(&receiver(), get_ep_params());
-    if (GetParam().variant != FAIL_IMMEDIATELY) {
-        smoke_test();
-    }
-    wrap_errors();
-
-    ucp_ep_params_t ep_params_mod = {0};
-    ep_params_mod.field_mask = UCP_EP_PARAM_FIELD_ERR_HANDLER;
-    ep_params_mod.err_handler.cb = err_cb_mod;
-    /* NOTE: using of ucp_ep_params_t::user_data field is more preferable but
-     *       need to test err_handler.arg as well */
-    ep_params_mod.err_handler.arg = reinterpret_cast<void *>(uintptr_t(MAGIC));
-
-    for (size_t i = 0; i < m_entities.size(); ++i) {
-        for (int widx = 0; widx < e(i).get_num_workers(); ++widx) {
-            for (int epidx = 0; epidx < e(i).get_num_eps(widx); ++epidx) {
-                void *req = e(i).modify_ep(ep_params_mod, widx, epidx);
-                ucp_test::wait(req, widx);
-            }
-        }
-    }
+    return result;
 }
 
-void test_ucp_peer_failure::cleanup() {
-    restore_errors();
-    test_ucp_tag::cleanup();
+ucp_ep_params_t test_ucp_peer_failure::get_ep_params() {
+    ucp_ep_params_t params;
+    memset(&params, 0, sizeof(params));
+    params.field_mask      = UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE |
+                             UCP_EP_PARAM_FIELD_ERR_HANDLER;
+    params.err_mode        = UCP_ERR_HANDLING_MODE_PEER;
+    params.err_handler.cb  = err_cb;
+    params.err_handler.arg = reinterpret_cast<void*>(this);
+    return params;
 }
 
-void test_ucp_peer_failure::test_status_after(bool request_must_fail)
-{
-    fail_receiver();
-
-    std::vector<uint8_t> buf(m_msg_size, 0);
-    request *req = send_nb(buf.data(), buf.size(), DATATYPE,
-                                         0x111337);
-    wait_err();
-    EXPECT_NE(UCS_OK, m_err_status);
-    EXPECT_TRUE(m_err_cb_mod);
-
-    if (UCS_PTR_IS_PTR(req)) {
-        /* The request may either succeed or fail, even though the data is not
-         * delivered - depends on when the error is detected on sender side and
-         * if zcopy/bcopy protocol is used. In any case, the request must
-         * complete, and all resources have to be released.
-         */
-        EXPECT_TRUE(req->completed);
-        if (request_must_fail) {
-            EXPECT_EQ(m_err_status, req->status);
-        } else {
-            EXPECT_TRUE((m_err_status == req->status) || (UCS_OK == req->status));
-        }
-        request_release(req);
-    }
-
-    ucs_status_ptr_t status_ptr = ucp_tag_send_nb(sender().ep(), NULL, 0, DATATYPE,
-                                                  0x111337, NULL);
-    EXPECT_FALSE(UCS_PTR_IS_PTR(status_ptr));
-    EXPECT_EQ(m_err_status, UCS_PTR_STATUS(status_ptr));
-
-    /* Destroy failed sender */
-    sender().destroy_worker();
-    m_entities.remove(&sender());
-
-    /* Check workability of second pair */
-    smoke_test();
+void test_ucp_peer_failure::set_timeouts() {
+    /* Set small TL timeouts to reduce testing time */
+    m_env.push_back(new ucs::scoped_setenv("UCX_RC_TIMEOUT",     "10ms"));
+    m_env.push_back(new ucs::scoped_setenv("UCX_RC_RNR_TIMEOUT", "10ms"));
+    m_env.push_back(new ucs::scoped_setenv("UCX_RC_RETRY_COUNT", "2"));
+    std::string ud_timeout = ucs::to_string<int>(3 * ucs::test_time_multiplier()) + "s";
+    m_env.push_back(new ucs::scoped_setenv("UCX_UD_TIMEOUT", ud_timeout.c_str()));
 }
 
-void test_ucp_peer_failure::test_force_close()
-{
-    const size_t            msg_size = 16000;
-    const size_t            iter     = 1000;
-    uint8_t                 *buf     = (uint8_t *)calloc(msg_size, iter);
-    struct mallinfo         mem_before, mem_after;
-    std::vector<request *>  reqs;
+void test_ucp_peer_failure::err_cb(void *arg, ucp_ep_h ep, ucs_status_t status) {
+    test_ucp_peer_failure *self = reinterpret_cast<test_ucp_peer_failure*>(arg);
+    EXPECT_EQ(UCS_ERR_ENDPOINT_TIMEOUT, status);
+    self->m_err_status = status;
+    ++self->m_err_count;
+}
 
-    reqs.reserve(iter);
-    for (size_t i = 0; i < iter; ++i) {
-        request *sreq = send_nb(&buf[i * msg_size], msg_size, DATATYPE, 17);
+ucp_ep_h test_ucp_peer_failure::stable_sender() {
+    return sender().ep(0, STABLE_EP_INDEX);
+}
 
-        if (UCS_PTR_IS_PTR(sreq)) {
-            reqs.push_back(sreq);
-        } else if (UCS_PTR_IS_ERR(sreq)) {
-            EXPECT_EQ(UCS_ERR_NO_RESOURCE, UCS_PTR_STATUS(sreq));
-            break;
-        }
-    }
+ucp_ep_h test_ucp_peer_failure::failing_sender() {
+    return sender().ep(0, FAILING_EP_INDEX);
+}
 
-    fail_receiver();
+ucp_test::entity& test_ucp_peer_failure::stable_receiver() {
+    return m_entities.at(m_entities.size() - 2);
+}
 
-    mem_before = mallinfo();
+ucp_test::entity& test_ucp_peer_failure::failing_receiver() {
+    return m_entities.at(m_entities.size() - 1);
+}
 
-    request *close_req = (request *)ucp_ep_close_nb(sender().ep(),
-                                                    UCP_EP_CLOSE_MODE_FORCE);
-    if (UCS_PTR_IS_PTR(close_req)) {
-        wait(close_req);
-        ucp_request_release(close_req);
+void *test_ucp_peer_failure::send_nb(ucp_ep_h ep, ucp_rkey_h rkey) {
+    if (GetParam().variant & TEST_TAG) {
+        return ucp_tag_send_nb(ep, &m_sbuf[0], m_sbuf.size(), DATATYPE, 0,
+                               send_cb);
+    } else if (GetParam().variant & TEST_RMA) {
+        return ucp_put_nb(ep, &m_sbuf[0], m_sbuf.size(), (uintptr_t)&m_rbuf[0],
+                          rkey, send_cb);
     } else {
-        EXPECT_FALSE(UCS_PTR_IS_ERR(close_req));
-    }
-
-    mem_after = mallinfo();
-    /* Too low chance to predict memory consumption on wire up for all TLS */
-    if (GetParam().variant != FAIL_IMMEDIATELY) {
-        EXPECT_GT(mem_before.uordblks, mem_after.uordblks);
-    }
-
-    /* The EP can't be used now */
-    sender().revoke_ep();
-
-    while (!reqs.empty()) {
-        EXPECT_NE(UCS_INPROGRESS, ucp_request_test(reqs.back(), NULL));
-        ucp_request_release(reqs.back());
-        reqs.pop_back();
+        ucs_fatal("invalid test case");
     }
-
-    /* Check that TX polling is working well */
-    while (sender().progress());
-
-    /* When all requests on sender are done we need to prevent LOCAL_FLUSH
-     * in test teardown. Receiver is killed and doesn't respond on FC requests
-     */
-    sender().destroy_worker();
-    free(buf);
 }
 
-UCS_TEST_P(test_ucp_peer_failure, disable_sync_send) {
-    /* 1GB memory markup takes too long time with valgrind, reduce to 1MB */
-    const size_t        max_size = RUNNING_ON_VALGRIND ? (1024 * 1024) :
-                                   (1024 * 1024 * 1024);
-    std::vector<char>   buf(max_size, 0);
-    request             *req;
-
-    /* Make sure API is disabled for any size and data type */
-    for (size_t size = 1; size <= max_size; size *= 2) {
-        req = send_sync_nb(buf.data(), size, DATATYPE, 0x111337);
-        EXPECT_FALSE(UCS_PTR_IS_PTR(req));
-        EXPECT_EQ(UCS_ERR_UNSUPPORTED, UCS_PTR_STATUS(req));
-
-        ucp::data_type_desc_t dt_desc(DATATYPE_IOV, buf.data(), size);
-        req = send_sync_nb(dt_desc.buf(), dt_desc.count(), dt_desc.dt(), 0x111337);
-        EXPECT_FALSE(UCS_PTR_IS_PTR(req));
-        EXPECT_EQ(UCS_ERR_UNSUPPORTED, UCS_PTR_STATUS(req));
+void *test_ucp_peer_failure::recv_nb(entity& e) {
+    ucs_assert(m_rbuf.size() >= m_sbuf.size());
+    if (GetParam().variant & TEST_TAG) {
+        return ucp_tag_recv_nb(e.worker(), &m_rbuf[0], m_rbuf.size(), DATATYPE, 0,
+                               0, recv_cb);
+    } else if (GetParam().variant & TEST_RMA) {
+        return NULL;
+    } else {
+        ucs_fatal("invalid test case");
     }
 }
 
-UCS_TEST_P(test_ucp_peer_failure, status_after_error) {
-    test_status_after(false);
+void test_ucp_peer_failure::fail_receiver() {
+    /* TODO: need to handle non-empty TX window in UD EP destructor",
+     *       see debug message (ud_ep.c:220)
+     *       ucs_debug("ep=%p id=%d conn_id=%d has %d unacked packets",
+     *                 self, self->ep_id, self->conn_id,
+     *                 (int)ucs_queue_length(&self->tx.window));
+     */
+    // TODO use force-close to close connections
+    flush_worker(failing_receiver());
+    m_failing_memh.reset();
+    failing_receiver().cleanup();
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure)
-
-
-class test_ucp_peer_failure_zcopy : public test_ucp_peer_failure
-{
-public:
-    virtual void init() {
-        modify_config("ZCOPY_THRESH", ucs::to_string(m_msg_size - 1));
-        test_ucp_peer_failure::init();
-    }
-};
-
-UCS_TEST_P(test_ucp_peer_failure_zcopy, status_after_error) {
-    test_status_after(true);
+void test_ucp_peer_failure::smoke_test(bool stable_pair) {
+    void *rreq = recv_nb(stable_pair ? stable_receiver() : failing_receiver());
+    void *sreq = send_nb(stable_pair ? stable_sender()   : failing_sender(),
+                         stable_pair ? m_stable_rkey     : m_failing_rkey);
+    wait(sreq);
+    wait(rreq);
+    EXPECT_EQ(m_sbuf, m_rbuf);
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure_zcopy)
-
-
-class test_ucp_peer_failure_zcopy_multi : public test_ucp_peer_failure_zcopy
+void test_ucp_peer_failure::unmap_memh(ucp_mem_h memh, ucp_context_h context)
 {
-public:
-    virtual void init() {
-        /* MAX BCOPY is internally used as fragment size */
-        m_env.push_back(new ucs::scoped_setenv("UCX_MAX_BCOPY",
-                                               (ucs::to_string(m_msg_size/2) + "b").c_str()));
-        /* HW TM does not support multiprotocols and eager protocol for messages
-         * bigger than UCT segment size */
-        m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_ENABLE", "n"));
-        test_ucp_peer_failure_zcopy::init();
+    ucs_status_t status = ucp_mem_unmap(context, memh);
+    if (status != UCS_OK) {
+        ucs_warn("failed to unmap memory: %s", ucs_status_string(status));
     }
-};
-
-UCS_TEST_P(test_ucp_peer_failure_zcopy_multi, status_after_error) {
-    test_status_after(true);
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure_zcopy_multi)
-
-
-class test_ucp_peer_failure_with_rma : public test_ucp_peer_failure {
-public:
-    enum {
-        FAIL_ON_RMA = FAIL_IMMEDIATELY + 1
-    };
-
-    static std::vector<ucp_test_param>
-    enum_test_params(const ucp_params_t& ctx_params,
-                     const std::string& name,
-                     const std::string& test_case_name,
-                     const std::string& tls)
-    {
-        std::vector<ucp_test_param> result =
-            test_ucp_peer_failure::enum_test_params(ctx_params, name,
-                                                    test_case_name, tls);
-
-        generate_test_params_variant(ctx_params, name, test_case_name, tls,
-                                     FAIL_ON_RMA, result);
-        return result;
-    }
-
-    static ucp_params_t get_ctx_params() {
-        ucp_params_t params = test_ucp_tag::get_ctx_params();
-        params.features |= UCP_FEATURE_RMA;
-        return params;
-    }
-};
-
-UCS_TEST_P(test_ucp_peer_failure_with_rma, status_after_error) {
-    unsigned buf = 0;
+void test_ucp_peer_failure::get_rkey(ucp_ep_h ep, entity& dst, mem_handle_t& memh,
+                                     ucs::handle<ucp_rkey_h>& rkey) {
     ucp_mem_map_params_t params;
+
     memset(&params, 0, sizeof(params));
     params.field_mask = UCP_MEM_MAP_PARAM_FIELD_ADDRESS |
                         UCP_MEM_MAP_PARAM_FIELD_LENGTH;
-    params.address = &buf;
-    params.length = sizeof(buf);
+    params.address    = &m_rbuf[0];
+    params.length     = m_rbuf.size();
 
-    ucp_mem_h memh;
-    ucs_status_t status = ucp_mem_map(receiver().ucph(), &params, &memh);
-    ASSERT_UCS_OK(status);
-    ucp_mem_attr_t mem_attr;
-    mem_attr.field_mask = UCP_MEM_ATTR_FIELD_ADDRESS;
-    status = ucp_mem_query(memh, &mem_attr);
+    ucp_mem_h ucp_memh;
+    ucs_status_t status = ucp_mem_map(dst.ucph(), &params, &ucp_memh);
     ASSERT_UCS_OK(status);
+    memh.reset(ucp_memh, unmap_memh, dst.ucph());
 
     void *rkey_buffer;
     size_t rkey_buffer_size;
-    status = ucp_rkey_pack(receiver().ucph(), memh, &rkey_buffer, &rkey_buffer_size);
-    ASSERT_UCS_OK(status);
-    ucp_rkey_h rkey;
-    status = ucp_ep_rkey_unpack(sender().ep(), rkey_buffer, &rkey);
+    status = ucp_rkey_pack(dst.ucph(), memh, &rkey_buffer, &rkey_buffer_size);
     ASSERT_UCS_OK(status);
-    ucp_rkey_buffer_release(rkey_buffer);
-    ucp_mem_unmap(receiver().ucph(), memh);
 
-    fail_receiver();
-    if (GetParam().variant == FAIL_ON_RMA) {
-        ucp_get_nbi(sender().ep(), mem_attr.address, 1, (uintptr_t)&buf, rkey);
-    } else {
-        request *req = send_nb(NULL, 0, DATATYPE, 0x111337);
-        if (UCS_PTR_IS_PTR(req)) {
-            request_release(req);
-        }
-    }
-
-    ucp_ep_flush(sender().ep());
-    wait_err();
+    ucp_rkey_h ucp_rkey;
+    status = ucp_ep_rkey_unpack(ep, rkey_buffer, &ucp_rkey);
+    ASSERT_UCS_OK(status);
+    rkey.reset(ucp_rkey, ucp_rkey_destroy);
 
-    EXPECT_NE(UCS_OK, m_err_status);
+    ucp_rkey_buffer_release(rkey_buffer);
+}
 
-    ucs_status_ptr_t status_ptr = ucp_tag_send_nb(sender().ep(), NULL, 0, DATATYPE,
-                                           0x111337, NULL);
-    EXPECT_FALSE(UCS_PTR_IS_PTR(status_ptr));
-    EXPECT_EQ(m_err_status, UCS_PTR_STATUS(status_ptr));
+void test_ucp_peer_failure::set_rkeys() {
 
-    status = ucp_put(sender().ep(), mem_attr.address, 1, (uintptr_t)&buf, rkey);
-    EXPECT_FALSE(UCS_PTR_IS_PTR(status));
-    EXPECT_EQ(m_err_status, status);
-    ucp_rkey_destroy(rkey);
+    if (GetParam().variant & TEST_RMA) {
+        get_rkey(failing_sender(), failing_receiver(), m_failing_memh,
+                 m_failing_rkey);
+        get_rkey(stable_sender(), stable_receiver(), m_stable_memh,
+                 m_stable_rkey);
+    }
+}
 
-    /* Destroy failed sender */
-    sender().destroy_worker();
-    m_entities.remove(&sender());
+void test_ucp_peer_failure::send_cb(void *request, ucs_status_t status)
+{
+}
 
-    /* Check workability of second pair */
-    smoke_test();
+void test_ucp_peer_failure::recv_cb(void *request, ucs_status_t status,
+                                    ucp_tag_recv_info_t *info)
+{
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure_with_rma)
+void test_ucp_peer_failure::cleanup() {
+    m_failing_rkey.reset();
+    m_stable_rkey.reset();
+    m_failing_memh.reset();
+    m_stable_memh.reset();
+    ucp_test::cleanup();
+}
 
-class test_ucp_peer_failure_2pairs :
-                    public ucp_test,
-                    protected test_ucp_peer_failure_base
+void test_ucp_peer_failure::do_test(size_t msg_size, int pre_msg_count,
+                                    bool force_close, bool request_must_fail)
 {
-public:
-    static ucp_params_t get_ctx_params() {
-        ucp_params_t params = ucp_test::get_ctx_params();
-        params.features     = UCP_FEATURE_TAG;
-        return params;
-    }
+    skip_loopback();
 
-protected:
-    virtual void init();
-    virtual void cleanup();
+    /* connect 2 ep's from sender() to 2 receiver entities */
+    create_entity();
+    sender().connect(&stable_receiver(),  get_ep_params(), STABLE_EP_INDEX);
+    sender().connect(&failing_receiver(), get_ep_params(), FAILING_EP_INDEX);
 
-    static void err_cb(void *arg, ucp_ep_h ep, ucs_status_t);
-    ucp_worker_h rworker(int i);
-    ucp_worker_h sworker();
-    void progress();
-    void wait_err();
-    ucs_status_t wait_req(void *req);
-    static void rcomplete_cb(void *req, ucs_status_t status,
-                             ucp_tag_recv_info_t *info);
-    static void scomplete_cb(void *req, ucs_status_t status);
-    void smoke_test(size_t idx);
-
-    static void ep_destructor(ucp_ep_h ep, test_ucp_peer_failure_2pairs* test) {
-        test->wait_req(ucp_disconnect_nb(ep));
-    }
+    m_sbuf.resize(msg_size);
+    m_rbuf.resize(msg_size);
 
-    virtual ucp_ep_params_t get_ep_params() {
-        return test_ucp_peer_failure_base::get_ep_params();
-    }
+    set_rkeys();
 
-    ucs::handle<ucp_context_h>               m_ucph;
-    std::vector<ucs::handle<ucp_worker_h> >  m_workers;
-    std::vector<ucs::handle<ucp_ep_h, test_ucp_peer_failure_2pairs*> > m_eps;
-    ucs::ptr_vector<ucs::scoped_setenv>      m_env;
-};
+    if (!(GetParam().variant & FAIL_IMM)) {
+        /* if not faill immediately, run traffic on failing pair to connect it */
+        smoke_test(false);
+    }
 
-void test_ucp_peer_failure_2pairs::init()
-{
-    test_base::init(); /* skip entities creation */
-    test_ucp_peer_failure_base::init();
-
-    set_ucp_config(m_ucp_config);
-    ucp_params_t cparams = get_ctx_params();
-    UCS_TEST_CREATE_HANDLE(ucp_context_h, m_ucph, ucp_cleanup,
-                           ucp_init, &cparams, m_ucp_config);
-
-    m_workers.resize(3);
-    for (int i = 0; i < 3; ++i) {
-        ucp_worker_params_t wparams = get_worker_params();
-        UCS_TEST_CREATE_HANDLE(ucp_worker_h, m_workers[i], ucp_worker_destroy,
-                               ucp_worker_create, m_ucph, &wparams);
+    /* put some sends on the failing pair */
+    std::vector<void*> sreqs_pre;
+    for (int i = 0; i < pre_msg_count; ++i) {
+        progress();
+        void *req = send_nb(failing_sender(), m_failing_rkey);
+        ASSERT_FALSE(UCS_PTR_IS_ERR(req));
+        if (UCS_PTR_IS_PTR(req)) {
+            sreqs_pre.push_back(req);
+        }
     }
 
-    m_eps.resize(2);
-    for (int i = 0; i < 2; ++i) {
-        ucp_address_t *address;
-        size_t address_length;
-        ucs_status_t status;
-        ucp_ep_h ep;
+    EXPECT_EQ(UCS_OK, m_err_status);
 
-        status = ucp_worker_get_address(rworker(i), &address, &address_length);
-        ASSERT_UCS_OK(status);
+    {
+        scoped_log_handler slh(wrap_errors_logger);
 
-        ucp_ep_params ep_params = get_ep_params();
-        ep_params.field_mask |= UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
-        ep_params.address     = address;
+        fail_receiver();
 
-        hide_errors();
-        status = ucp_ep_create(sworker(), &ep_params, &ep);
-        restore_errors();
+        void *sreq = send_nb(failing_sender(), m_failing_rkey);
 
-        ucp_worker_release_address(rworker(i), address);
+        while (!m_err_count) {
+            progress();
+        }
+        EXPECT_NE(UCS_OK, m_err_status);
 
-        if (status == UCS_ERR_UNREACHABLE) {
-            UCS_TEST_SKIP_R(m_errors.empty() ? "" : m_errors.back());
+        if (UCS_PTR_IS_PTR(sreq)) {
+            /* The request may either succeed or fail, even though the data is
+             * not * delivered - depends on when the error is detected on sender
+             * side and if zcopy/bcopy protocol is used. In any case, the
+             * request must complete, and all resources have to be released.
+             */
+            ucs_status_t status = ucp_request_check_status(sreq);
+            EXPECT_NE(UCS_INPROGRESS, status);
+            if (request_must_fail) {
+                EXPECT_EQ(m_err_status, status);
+            } else {
+                EXPECT_TRUE((m_err_status == status) || (UCS_OK == status));
+            }
+            ucp_request_release(sreq);
         }
 
-        m_eps[i].reset(ep, ep_destructor, this);
-    }
+        /* Additional sends must fail */
+        void *sreq2 = send_nb(failing_sender(), m_failing_rkey);
+        EXPECT_FALSE(UCS_PTR_IS_PTR(sreq2));
+        EXPECT_EQ(m_err_status, UCS_PTR_STATUS(sreq2));
 
-    /* Make sure wire up is done*/
-    smoke_test(0);
-    smoke_test(1);
+        if (force_close) {
+            unsigned allocd_eps_before =
+                    ucs_strided_alloc_inuse_count(&sender().worker()->ep_alloc);
 
-    wrap_errors();
-}
+            ucp_ep_h ep = sender().revoke_ep(0, FAILING_EP_INDEX);
 
-void test_ucp_peer_failure_2pairs::cleanup()
-{
-    restore_errors();
-    m_eps.clear();
-    m_workers.clear();
-    test_base::cleanup();
-}
+            void *creq = ucp_ep_close_nb(ep, UCP_EP_CLOSE_MODE_FORCE);
+            wait(creq);
 
-void test_ucp_peer_failure_2pairs::err_cb(void *arg, ucp_ep_h ep, ucs_status_t) {
-    test_ucp_peer_failure_2pairs *self;
-    self = *reinterpret_cast<test_ucp_peer_failure_2pairs**>(arg);
-    self->m_err_cntr++;
-}
+            unsigned allocd_eps_after =
+                    ucs_strided_alloc_inuse_count(&sender().worker()->ep_alloc);
 
-ucp_worker_h test_ucp_peer_failure_2pairs::rworker(int i)
-{
-    return m_workers[i];
-}
-
-ucp_worker_h test_ucp_peer_failure_2pairs::sworker()
-{
-    return m_workers[2];
-}
-
-void test_ucp_peer_failure_2pairs::progress()
-{
-    for (std::vector<ucs::handle<ucp_worker_h> >::iterator iter = m_workers.begin();
-         iter != m_workers.end(); ++iter)
-    {
-        if (*iter) {
-            ucp_worker_progress(*iter);
+            if (!(GetParam().variant & FAIL_IMM)) {
+                EXPECT_LT(allocd_eps_after, allocd_eps_before);
+            }
         }
-    }
-}
 
-void test_ucp_peer_failure_2pairs::wait_err()
-{
-    while (!m_err_cntr) {
-        progress();
+        /* release requests */
+        while (!sreqs_pre.empty()) {
+            void *req = sreqs_pre.back();
+            sreqs_pre.pop_back();
+            EXPECT_NE(UCS_INPROGRESS, ucp_request_test(req, NULL));
+            ucp_request_release(req);
+        }
     }
-}
 
-ucs_status_t test_ucp_peer_failure_2pairs::wait_req(void *req)
-{
-    if (req == NULL) {
-        return UCS_OK;
-    }
+    /* Check workability of stable pair */
+    smoke_test(true);
 
-    ucs_assert(!!req);
-    if (UCS_PTR_IS_ERR(req)) {
-        return UCS_PTR_STATUS(req);
-    }
+    /* Check that TX polling is working well */
+    while (sender().progress());
 
-    ucs_status_t status;
-    do {
-        progress();
-        status = ucp_request_check_status(req);
-    } while (status == UCS_INPROGRESS);
-    ucp_request_release(req);
-    return status;
+    /* When all requests on sender are done we need to prevent LOCAL_FLUSH
+     * in test teardown. Receiver is killed and doesn't respond on FC requests
+     */
+    sender().destroy_worker();
 }
 
-void test_ucp_peer_failure_2pairs::rcomplete_cb(void *req, ucs_status_t status,
-                                                ucp_tag_recv_info_t *info)
-{
+UCS_TEST_P(test_ucp_peer_failure, basic) {
+    do_test(1024, /* msg_size */
+            0, /* pre_msg_cnt */
+            false, /* force_close */
+            false /* must_fail */);
 }
 
-void test_ucp_peer_failure_2pairs::scomplete_cb(void *req, ucs_status_t status)
-{
+UCS_TEST_P(test_ucp_peer_failure, zcopy, "ZCOPY_THRESH=1023") {
+    do_test(1024, /* msg_size */
+            0, /* pre_msg_cnt */
+            false, /* force_close */
+            true /* must_fail */);
 }
 
-void test_ucp_peer_failure_2pairs::smoke_test(size_t idx)
-{
-    long buf = 0;
-    void *rreq = ucp_tag_recv_nb(rworker(idx), &buf, 1,
-                                 ucp_dt_make_contig(1), 0, 0,
-                                 rcomplete_cb);
-    void *sreq = ucp_tag_send_nb(m_eps[idx], &buf, 1,
-                                 ucp_dt_make_contig(1), 0,
-                                 scomplete_cb);
-    wait_req(sreq);
-    wait_req(rreq);
+UCS_TEST_P(test_ucp_peer_failure, bcopy_multi, "MAX_BCOPY?=512", "RC_TM_ENABLE?=n") {
+    do_test(1024, /* msg_size */
+            0, /* pre_msg_cnt */
+            false, /* force_close */
+            false /* must_fail */);
 }
 
-UCS_TEST_P(test_ucp_peer_failure_2pairs, status_after_error) {
-
-    m_workers[0].reset();
-
-    ucs_status_t status;
-    void *sreq;
-    unsigned buf = 0;
-
-    do {
-        sreq = ucp_tag_send_nb(m_eps[0], &buf, 1, ucp_dt_make_contig(1),
-                               0x111337, scomplete_cb);
-        status = wait_req(sreq);
-    } while ((status == UCS_OK) || !m_err_cntr);
-
-    wait_err();
-
-    EXPECT_NE(UCS_OK, m_err_status);
-
-    sreq = ucp_tag_send_nb(m_eps[0], NULL, 0, ucp_dt_make_contig(1), 0x111337,
-                           scomplete_cb);
-    EXPECT_FALSE(UCS_PTR_IS_PTR(sreq));
-    EXPECT_EQ(m_err_status, UCS_PTR_STATUS(sreq));
+UCS_TEST_P(test_ucp_peer_failure, force_close, "RC_FC_ENABLE?=n") {
+    do_test(16000, /* msg_size */
+            1000, /* pre_msg_cnt */
+            true, /* force_close */
+            false /* must_fail */);
+}
 
-    /* Destroy failed sender */
-    m_eps[0].reset();
+UCS_TEST_P(test_ucp_peer_failure, disable_sync_send) {
+    const size_t        max_size = UCS_MBYTE;
+    std::vector<char>   buf(max_size, 0);
+    void                *req;
 
-    /* Check workability of second pair */
-    smoke_test(1);
-}
+    if (!(GetParam().variant & TEST_TAG)) {
+        UCS_TEST_SKIP_R("Skip non-tagged variant");
+    }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_peer_failure_2pairs)
+    sender().connect(&receiver(), get_ep_params());
 
-class test_ucp_ep_force_disconnect : public test_ucp_peer_failure
-{
-public:
-    virtual void init() {
-        const std::vector<std::string> &tls = GetParam().transports;
-        std::vector<std::string>       skip_tls;
-
-        skip_tls.push_back("rc_x");
-        skip_tls.push_back("ib");
-
-        if (std::find_first_of(tls.begin(),      tls.end(),
-                               skip_tls.begin(), skip_tls.end()) != tls.end()) {
-            UCS_TEST_SKIP_R("TEMPORATY DISABLED. There is a known issue with "
-                            "rc_mlx5 transport: it can't successfully clean up "
-                            "iface resources if it's receiving on closure. "
-                            "The issue is hardly reproducible with running 4+ "
-                            "gtest instances on the same node");
-        }
+    /* Make sure API is disabled for any size and data type */
+    for (size_t size = 1; size <= max_size; size *= 2) {
+        req = ucp_tag_send_sync_nb(sender().ep(), buf.data(), size, DATATYPE,
+                                   0x111337, NULL);
+        EXPECT_FALSE(UCS_PTR_IS_PTR(req));
+        EXPECT_EQ(UCS_ERR_UNSUPPORTED, UCS_PTR_STATUS(req));
 
-        m_env.clear(); /* restore default timeouts. */
-        test_ucp_peer_failure::init();
+        ucp::data_type_desc_t dt_desc(DATATYPE_IOV, buf.data(), size);
+        req = ucp_tag_send_sync_nb(sender().ep(), dt_desc.buf(), dt_desc.count(),
+                                   dt_desc.dt(), 0x111337, NULL);
+        EXPECT_FALSE(UCS_PTR_IS_PTR(req));
+        EXPECT_EQ(UCS_ERR_UNSUPPORTED, UCS_PTR_STATUS(req));
     }
-};
-
-UCS_TEST_P(test_ucp_ep_force_disconnect, test) {
-    test_force_close();
 }
-
-UCP_INSTANTIATE_TEST_CASE(test_ucp_ep_force_disconnect)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_perf.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_perf.cc
index 50376c307..0109bca29 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_perf.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_perf.cc
@@ -66,7 +66,7 @@ test_perf::test_spec test_ucp_perf::tests[] =
   { "tag sync mr", "Mpps",
     UCX_PERF_API_UCP, UCX_PERF_CMD_TAG_SYNC, UCX_PERF_TEST_TYPE_STREAM_UNI,
     UCP_PERF_DATATYPE_CONTIG, 0, 1, { 8 }, 1, 2000000l,
-    ucs_offsetof(ucx_perf_result_t, msgrate.total_average), 1e-6, 0.1, 100.0, 0},
+    ucs_offsetof(ucx_perf_result_t, msgrate.total_average), 1e-6, 0.05, 100.0, 0},
 
   { "tag wild mr", "Mpps",
     UCX_PERF_API_UCP, UCX_PERF_CMD_TAG, UCX_PERF_TEST_TYPE_STREAM_UNI,
@@ -139,7 +139,7 @@ test_perf::test_spec test_ucp_perf::tests[] =
   { "atomic add rate", "Mpps",
     UCX_PERF_API_UCP, UCX_PERF_CMD_ADD, UCX_PERF_TEST_TYPE_STREAM_UNI,
     UCP_PERF_DATATYPE_CONTIG, 0, 1, { 8 }, 1, 1000000l,
-    ucs_offsetof(ucx_perf_result_t, msgrate.total_average), 1e-6, 0.5, 100.0,
+    ucs_offsetof(ucx_perf_result_t, msgrate.total_average), 1e-6, 0.1, 500.0,
     0 },
 
   { "atomic fadd latency", "usec",
@@ -175,14 +175,14 @@ UCS_TEST_P(test_ucp_perf, envelope) {
 
     /* coverity[tainted_string_argument] */
     ucs::scoped_setenv tls("UCX_TLS", ss.str().c_str());
+    ucs::scoped_setenv warn_invalid("UCX_WARN_INVALID_CONFIG", "no");
+
     for (test_spec *test = tests; test->title != NULL; ++test) {
-        unsigned flags = (test->command == UCX_PERF_CMD_TAG) ? 0 :
-                                 UCX_PERF_TEST_FLAG_ONE_SIDED;
         if (ucs_arch_get_cpu_model() == UCS_CPU_MODEL_ARM_AARCH64) {
             test->max *= UCP_ARM_PERF_TEST_MULTIPLIER;
             test->min /= UCP_ARM_PERF_TEST_MULTIPLIER;
         }
-        run_test(*test, flags, check_perf, "", "");
+        run_test(*test, 0, check_perf, "", "");
     }
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma.cc
index 690de2025..6bb31e2f8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma.cc
@@ -10,6 +10,8 @@
 
 
 class test_ucp_rma : public test_ucp_memheap {
+private:
+    static void send_completion(void *request, ucs_status_t status){}
 public:
     static ucp_params_t get_ctx_params() {
         ucp_params_t params = ucp_test::get_ctx_params();
@@ -42,15 +44,19 @@ public:
         ASSERT_UCS_OK_OR_INPROGRESS(status);
     }
 
-    void blocking_put(entity *e, size_t max_size,
-                      void *memheap_addr,
-                      ucp_rkey_h rkey,
-                      std::string& expected_data)
+    void nonblocking_put_nb(entity *e, size_t max_size,
+                            void *memheap_addr,
+                            ucp_rkey_h rkey,
+                            std::string& expected_data)
     {
-        ucs_status_t status;
-        status = ucp_put(e->ep(), &expected_data[0], expected_data.length(),
-                         (uintptr_t)memheap_addr, rkey);
-        ASSERT_UCS_OK(status);
+        void *status;
+
+        status = ucp_put_nb(e->ep(), &expected_data[0], expected_data.length(),
+                            (uintptr_t)memheap_addr, rkey, send_completion);
+        ASSERT_UCS_PTR_OK(status);
+        if (UCS_PTR_IS_PTR(status)) {
+            wait(status);
+        }
     }
 
     void nonblocking_get_nbi(entity *e, size_t max_size,
@@ -66,17 +72,20 @@ public:
         ASSERT_UCS_OK_OR_INPROGRESS(status);
     }
 
-    void blocking_get(entity *e, size_t max_size,
-                      void *memheap_addr,
-                      ucp_rkey_h rkey,
-                      std::string& expected_data)
+    void nonblocking_get_nb(entity *e, size_t max_size,
+                            void *memheap_addr,
+                            ucp_rkey_h rkey,
+                            std::string& expected_data)
     {
-        ucs_status_t status;
+        void *status;
 
         ucs::fill_random(memheap_addr, ucs_min(max_size, 16384U));
-        status = ucp_get(e->ep(), (void *)&expected_data[0], expected_data.length(),
-                         (uintptr_t)memheap_addr, rkey);
-        ASSERT_UCS_OK(status);
+        status = ucp_get_nb(e->ep(), &expected_data[0], expected_data.length(),
+                            (uintptr_t)memheap_addr, rkey, send_completion);
+        ASSERT_UCS_PTR_OK(status);
+        if (UCS_PTR_IS_PTR(status)) {
+            wait(status);
+        }
     }
 
     void test_message_sizes(blocking_send_func_t func, size_t *msizes, int iters, int is_nbi);
@@ -96,38 +105,8 @@ void test_ucp_rma::test_message_sizes(blocking_send_func_t func, size_t *msizes,
    }
 }
 
-UCS_TEST_P(test_ucp_rma, blocking_small) {
-    size_t sizes[] = { 8, 24, 96, 120, 250, 0};
-
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_put),
-                       sizes, 1000, 0);
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_get), 
-                       sizes, 1000, 0);
-}
-
-UCS_TEST_P(test_ucp_rma, blocking_med) {
-    size_t sizes[] = { 1000, 3000, 9000, 17300, 31000, 99000, 130000, 0};
-
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_put),
-                       sizes, 100, 0);
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_get), 
-                       sizes, 100, 0);
-}
-
 static const size_t MEG = 1024 * 1024ULL;
 
-UCS_TEST_P(test_ucp_rma, blocking_large) {
-    size_t sizes[] = { 1 * MEG, 3 * MEG, 9 * MEG, 17 * MEG, 32 * MEG, 0};
-
-    if (RUNNING_ON_VALGRIND) {
-        UCS_TEST_SKIP_R("skipping on valgrind");
-    }
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_put),
-                       sizes, 3, 0);
-    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_get), 
-                       sizes, 3, 0);
-}
-
 UCS_TEST_P(test_ucp_rma, nbi_small) {
     size_t sizes[] = { 8, 24, 96, 120, 250, 0};
 
@@ -159,16 +138,35 @@ UCS_TEST_P(test_ucp_rma, nbi_large) {
                        sizes, 3, 1);
 }
 
-UCS_TEST_P(test_ucp_rma, blocking_put_allocated) {
-    test_blocking_xfer(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_put),
-                       DEFAULT_SIZE, DEFAULT_ITERS,
-                       1, false, false);
+UCS_TEST_P(test_ucp_rma, nb_small) {
+    size_t sizes[] = { 8, 24, 96, 120, 250, 0};
+
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       sizes, 1000, 1);
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       sizes, 1000, 1);
 }
 
-UCS_TEST_P(test_ucp_rma, blocking_put_registered) {
-    test_blocking_xfer(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_put),
-                       DEFAULT_SIZE, DEFAULT_ITERS,
-                       1, true, false);
+UCS_TEST_P(test_ucp_rma, nb_med) {
+    size_t sizes[] = { 1000, 3000, 9000, 17300, 31000, 99000, 130000, 0};
+
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       sizes, 100, 1);
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       sizes, 100, 1);
+}
+
+UCS_TEST_P(test_ucp_rma, nb_large) {
+    size_t sizes[] = { 1 * MEG, 3 * MEG, 9 * MEG, 17 * MEG, 32 * MEG, 0};
+
+    if (RUNNING_ON_VALGRIND) {
+        UCS_TEST_SKIP_R("skipping on valgrind");
+    }
+
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       sizes, 3, 1);
+    test_message_sizes(static_cast<blocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       sizes, 3, 1);
 }
 
 UCS_TEST_P(test_ucp_rma, nonblocking_put_nbi_flush_worker) {
@@ -207,15 +205,42 @@ UCS_TEST_P(test_ucp_rma, nonblocking_stream_put_nbi_flush_ep) {
                        1, true, true);
 }
 
-UCS_TEST_P(test_ucp_rma, blocking_get) {
-    test_blocking_xfer(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_get),
+UCS_TEST_P(test_ucp_rma, nonblocking_put_nb_flush_worker) {
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
                        DEFAULT_SIZE, DEFAULT_ITERS,
                        1, false, false);
-    test_blocking_xfer(static_cast<blocking_send_func_t>(&test_ucp_rma::blocking_get),
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
                        DEFAULT_SIZE, DEFAULT_ITERS,
                        1, true, false);
 }
 
+UCS_TEST_P(test_ucp_rma, nonblocking_put_nb_flush_ep) {
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, true);
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, true);
+}
+
+UCS_TEST_P(test_ucp_rma, nonblocking_stream_put_nb_flush_worker) {
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, false);
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, false);
+}
+
+UCS_TEST_P(test_ucp_rma, nonblocking_stream_put_nb_flush_ep) {
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, true);
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_put_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, true);
+}
+
 UCS_TEST_P(test_ucp_rma, nonblocking_get_nbi_flush_worker) {
     test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nbi),
                        DEFAULT_SIZE, DEFAULT_ITERS,
@@ -252,4 +277,40 @@ UCS_TEST_P(test_ucp_rma, nonblocking_stream_get_nbi_flush_ep) {
                        1, true, true);
 }
 
+UCS_TEST_P(test_ucp_rma, nonblocking_get_nb_flush_worker) {
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, false);
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, false);
+}
+
+UCS_TEST_P(test_ucp_rma, nonblocking_get_nb_flush_ep) {
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, true);
+    test_blocking_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, true);
+}
+
+UCS_TEST_P(test_ucp_rma, nonblocking_stream_get_nb_flush_worker) {
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, false);
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, false);
+}
+
+UCS_TEST_P(test_ucp_rma, nonblocking_stream_get_nb_flush_ep) {
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, false, true);
+    test_nonblocking_implicit_stream_xfer(static_cast<nonblocking_send_func_t>(&test_ucp_rma::nonblocking_get_nb),
+                       DEFAULT_SIZE, DEFAULT_ITERS,
+                       1, true, true);
+}
+
 UCP_INSTANTIATE_TEST_CASE(test_ucp_rma)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma_mt.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma_mt.cc
index 9f9b97c72..8ca56c85a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma_mt.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_rma_mt.cc
@@ -27,6 +27,14 @@ public:
     {
         ucp_test::init();
         sender().connect(&receiver(), get_ep_params());
+        for (int i = 0; i < sender().get_num_workers(); i++) {
+            /* avoid deadlock for blocking rma */
+            flush_worker(sender(), i);
+        }
+    }
+
+    static void send_cb(void *req, ucs_status_t status)
+    {
     }
 
     static std::vector<ucp_test_param> enum_test_params(const ucp_params_t& ctx_params,
@@ -90,15 +98,16 @@ UCS_TEST_P(test_ucp_rma_mt, put_get) {
 #if _OPENMP && ENABLE_MT
 #pragma omp parallel for
     for (i = 0; i < MT_TEST_NUM_THREADS; i++) {
-        ucs_status_t status;
         int worker_index = 0;
 
-        if (GetParam().thread_type == MULTI_THREAD_CONTEXT)
+        if (GetParam().thread_type == MULTI_THREAD_CONTEXT) {
             worker_index = i;
+        }
 
-        status = ucp_put(sender().ep(worker_index), &orig_data[i], sizeof(uint64_t),
-                         (uintptr_t)((uint64_t *)memheap + i), rkey[worker_index]);
-        ASSERT_UCS_OK(status);
+        void* req = ucp_put_nb(sender().ep(worker_index), &orig_data[i],
+                               sizeof(uint64_t), (uintptr_t)((uint64_t*)memheap + i),
+                               rkey[worker_index], send_cb);
+        wait(req, worker_index);
 
         flush_worker(sender(), worker_index);
 
@@ -123,7 +132,7 @@ UCS_TEST_P(test_ucp_rma_mt, put_get) {
             worker_index = i;
 
         status = ucp_put_nbi(sender().ep(worker_index), &orig_data[i], sizeof(uint64_t),
-                             (uintptr_t)((uint64_t *)memheap + i), rkey[worker_index]);
+                             (uintptr_t)((uint64_t*)memheap + i), rkey[worker_index]);
         ASSERT_UCS_OK_OR_INPROGRESS(status);
 
         flush_worker(sender(), worker_index);
@@ -142,15 +151,16 @@ UCS_TEST_P(test_ucp_rma_mt, put_get) {
 #if _OPENMP && ENABLE_MT
 #pragma omp parallel for
     for (i = 0; i < MT_TEST_NUM_THREADS; i++) {
-        ucs_status_t status;
         int worker_index = 0;
 
-        if (GetParam().thread_type == MULTI_THREAD_CONTEXT)
+        if (GetParam().thread_type == MULTI_THREAD_CONTEXT) {
             worker_index = i;
+        }
 
-        status = ucp_get(sender().ep(worker_index), &orig_data[i], sizeof(uint64_t),
-                         (uintptr_t)((uint64_t *)memheap + i), rkey[worker_index]);
-        ASSERT_UCS_OK(status);
+        void *req = ucp_get_nb(sender().ep(worker_index), &orig_data[i],
+                               sizeof(uint64_t), (uintptr_t)((uint64_t*)memheap + i),
+                               rkey[worker_index], send_cb);
+        wait(req, worker_index);
 
         flush_worker(sender(), worker_index);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_sockaddr.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_sockaddr.cc
index 8fb00b5b3..0de1d7534 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_sockaddr.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_sockaddr.cc
@@ -9,16 +9,79 @@
 #include <common/test_helpers.h>
 #include <ucs/sys/sys.h>
 #include <ifaddrs.h>
+#include <sys/poll.h>
+
+#define UCP_INSTANTIATE_ALL_TEST_CASE(_test_case) \
+        UCP_INSTANTIATE_TEST_CASE (_test_case) \
+        UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, shm, "shm") \
+        UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, dc_ud, "dc_x,dc,ud,ud_x,mm") \
+        UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, no_ud_ud_x, "dc_x,dc,mm") \
+        /* dc_ud case is for testing handling of a large worker address on
+         * UCT_IFACE_FLAG_CONNECT_TO_IFACE transports (dc, dc_x) */
+        /* no_ud_ud_x case is for testing handling a large worker address
+         * but with the lack of ud/ud_x transports, which would return an error
+         * and skipped */
 
 class test_ucp_sockaddr : public ucp_test {
 public:
     static ucp_params_t get_ctx_params() {
         ucp_params_t params = ucp_test::get_ctx_params();
         params.field_mask  |= UCP_PARAM_FIELD_FEATURES;
-        params.features     = UCP_FEATURE_TAG;
+        params.features     = UCP_FEATURE_TAG | UCP_FEATURE_STREAM;
         return params;
     }
 
+    enum {
+        MT_PARAM_VARIANT = DEFAULT_PARAM_VARIANT + 1, /* Enabled worker level
+                                                         multi-threading */
+        CONN_REQ_TAG,                                 /* Accepting by ucp_conn_request_h,
+                                                         send/recv by TAG API */
+        CONN_REQ_STREAM                               /* Accepting by ucp_conn_request_h,
+                                                         send/recv by STREAM API */
+    };
+
+    typedef enum {
+        SEND_RECV_TAG,
+        SEND_RECV_STREAM
+    } send_recv_type_t;
+
+    static std::vector<ucp_test_param>
+    enum_test_params(const ucp_params_t& ctx_params,
+                     const std::string& name,
+                     const std::string& test_case_name,
+                     const std::string& tls)
+    {
+        std::vector<ucp_test_param> result =
+            ucp_test::enum_test_params(ctx_params, name, test_case_name, tls);
+
+        generate_test_params_variant(ctx_params, name, test_case_name, tls,
+                                     MT_PARAM_VARIANT, result,
+                                     MULTI_THREAD_WORKER);
+        generate_test_params_variant(ctx_params, name, test_case_name, tls,
+                                     CONN_REQ_TAG, result);
+        generate_test_params_variant(ctx_params, name, test_case_name, tls,
+                                     CONN_REQ_STREAM, result);
+        return result;
+    }
+
+    static ucs_log_func_rc_t
+    detect_error_logger(const char *file, unsigned line, const char *function,
+                        ucs_log_level_t level, const char *message, va_list ap)
+    {
+        if (level == UCS_LOG_LEVEL_ERROR) {
+            std::string err_str = format_message(message, ap);
+            if ((strstr(err_str.c_str(), "no supported sockaddr auxiliary transports found for")) ||
+                (strstr(err_str.c_str(), "sockaddr aux resources addresses")) ||
+                (strstr(err_str.c_str(), "no peer failure handler")) ||
+                /* when the "peer failure" error happens, it is followed by: */
+                (strstr(err_str.c_str(), "received event RDMA_CM_EVENT_UNREACHABLE"))) {
+                UCS_TEST_MESSAGE << err_str;
+                return UCS_LOG_FUNC_RC_STOP;
+            }
+        }
+        return UCS_LOG_FUNC_RC_CONTINUE;
+    }
+
     void get_listen_addr(struct sockaddr_in *listen_addr) {
         struct ifaddrs* ifaddrs;
         int ret = getifaddrs(&ifaddrs);
@@ -27,7 +90,7 @@ public:
         for (struct ifaddrs *ifa = ifaddrs; ifa != NULL; ifa = ifa->ifa_next) {
             if (ucs_netif_is_active(ifa->ifa_name) &&
                 ucs::is_inet_addr(ifa->ifa_addr)   &&
-                ucs::is_ib_netdev(ifa->ifa_name))
+                ucs::is_rdmacm_netdev(ifa->ifa_name))
             {
                 *listen_addr = *(struct sockaddr_in*)(void*)ifa->ifa_addr;
                 listen_addr->sin_port = ucs::get_port();
@@ -39,41 +102,172 @@ public:
         UCS_TEST_SKIP_R("No interface for testing");
     }
 
+    void inaddr_any_addr(struct sockaddr_in *addr, in_port_t port)
+    {
+        memset(addr, 0, sizeof(struct sockaddr_in));
+        addr->sin_family      = AF_INET;
+        addr->sin_addr.s_addr = INADDR_ANY;
+        addr->sin_port        = port;
+    }
+
+    void start_listener(ucp_test_base::entity::listen_cb_type_t cb_type,
+                        const struct sockaddr* addr)
+    {
+        ucs_status_t status = receiver().listen(cb_type, addr, sizeof(*addr));
+        if (status == UCS_ERR_UNREACHABLE) {
+            UCS_TEST_SKIP_R("cannot listen to " + ucs::sockaddr_to_str(addr));
+        }
+    }
+
     static void scomplete_cb(void *req, ucs_status_t status)
     {
-        ASSERT_UCS_OK(status);
+        if ((status == UCS_OK)              ||
+            (status == UCS_ERR_UNREACHABLE) ||
+            (status == UCS_ERR_REJECTED)) {
+            return;
+        }
+        UCS_TEST_ABORT("Error: " << ucs_status_string(status));
+    }
+
+    static void rtag_complete_cb(void *req, ucs_status_t status,
+                                 ucp_tag_recv_info_t *info)
+    {
+        EXPECT_UCS_OK(status);
     }
 
-    static void rcomplete_cb(void *req, ucs_status_t status,
-                             ucp_tag_recv_info_t *info)
+    static void rstream_complete_cb(void *req, ucs_status_t status,
+                                    size_t length)
     {
+        EXPECT_UCS_OK(status);
+    }
+
+    static void wait_for_wakeup(ucp_worker_h send_worker, ucp_worker_h recv_worker)
+    {
+        int ret, send_efd, recv_efd;
+        ucs_status_t status;
+
+        ASSERT_UCS_OK(ucp_worker_get_efd(send_worker, &send_efd));
+        ASSERT_UCS_OK(ucp_worker_get_efd(recv_worker, &recv_efd));
+
+        status = ucp_worker_arm(recv_worker);
+        if (status == UCS_ERR_BUSY) {
+            return;
+        }
         ASSERT_UCS_OK(status);
+
+        status = ucp_worker_arm(send_worker);
+        if (status == UCS_ERR_BUSY) {
+            return;
+        }
+        ASSERT_UCS_OK(status);
+
+        do {
+            struct pollfd pfd[2];
+            pfd[0].fd     = send_efd;
+            pfd[1].fd     = recv_efd;
+            pfd[0].events = POLLIN;
+            pfd[1].events = POLLIN;
+            ret = poll(pfd, 2, -1);
+        } while ((ret < 0) && (errno == EINTR));
+        if (ret < 0) {
+            UCS_TEST_MESSAGE << "poll() failed: " << strerror(errno);
+        }
+
+        EXPECT_GE(ret, 1);
     }
 
-    void tag_send(entity& from, entity& to) {
-        uint64_t send_data = ucs_generate_uuid(0);
-        void *send_req = ucp_tag_send_nb(from.ep(), &send_data, 1,
-                                         ucp_dt_make_contig(sizeof(send_data)),
-                                         1, scomplete_cb);
+    void check_events(ucp_worker_h send_worker, ucp_worker_h recv_worker,
+                      bool wakeup, void *req)
+    {
+        if (progress()) {
+            return;
+        }
+
+        if ((req != NULL) && (ucp_request_check_status(req) == UCS_ERR_UNREACHABLE)) {
+            return;
+        }
+
+        if (wakeup) {
+            wait_for_wakeup(send_worker, recv_worker);
+        }
+    }
+
+    void send_recv(entity& from, entity& to, send_recv_type_t send_recv_type,
+                   bool wakeup, ucp_test_base::entity::listen_cb_type_t cb_type)
+    {
+        const uint64_t send_data = ucs_generate_uuid(0);
+        void *send_req = NULL;
+        if (send_recv_type == SEND_RECV_TAG) {
+            send_req = ucp_tag_send_nb(from.ep(), &send_data, 1,
+                                       ucp_dt_make_contig(sizeof(send_data)), 1,
+                                       scomplete_cb);
+        } else if (send_recv_type == SEND_RECV_STREAM) {
+            send_req = ucp_stream_send_nb(from.ep(), &send_data, 1,
+                                          ucp_dt_make_contig(sizeof(send_data)),
+                                          scomplete_cb, 0);
+        } else {
+            ASSERT_TRUE(false) << "unsupported communication type";
+        }
+
+        ucs_status_t send_status;
         if (send_req == NULL) {
+            send_status = UCS_OK;
         } else if (UCS_PTR_IS_ERR(send_req)) {
-            ASSERT_UCS_OK(UCS_PTR_STATUS(send_req));
+            send_status = UCS_PTR_STATUS(send_req);
+            ASSERT_UCS_OK(send_status);
         } else {
             while (!ucp_request_is_completed(send_req)) {
-                progress();
+                check_events(from.worker(), to.worker(), wakeup, send_req);
             }
+            send_status = ucp_request_check_status(send_req);
             ucp_request_free(send_req);
         }
 
+        if (send_status == UCS_ERR_UNREACHABLE) {
+            /* Check if the error was completed due to the error handling flow.
+             * If so, skip the test since a valid error occurred - the one expected
+             * from the error handling flow - cases of failure to handle long worker
+             * address or transport doesn't support the error handling requirement */
+            UCS_TEST_SKIP_R("Skipping due an unreachable destination (unsupported "
+                            "feature or too long worker address or no "
+                            "supported transport to send partial worker "
+                            "address)");
+        } else if ((send_status == UCS_ERR_REJECTED) &&
+                   (cb_type == ucp_test_base::entity::LISTEN_CB_REJECT)) {
+            return;
+        } else {
+            ASSERT_UCS_OK(send_status);
+        }
+
         uint64_t recv_data = 0;
-        void *recv_req = ucp_tag_recv_nb(to.worker(), &recv_data, 1,
-                                         ucp_dt_make_contig(sizeof(recv_data)),
-                                         1, 0, rcomplete_cb);
-        if (UCS_PTR_IS_ERR(recv_req)) {
-            ASSERT_UCS_OK(UCS_PTR_STATUS(recv_req));
+        void *recv_req;
+        if (send_recv_type == SEND_RECV_TAG) {
+            recv_req = ucp_tag_recv_nb(to.worker(), &recv_data, 1,
+                                       ucp_dt_make_contig(sizeof(recv_data)),
+                                       1, 0, rtag_complete_cb);
         } else {
-            while (!ucp_request_is_completed(recv_req)) {
+            ASSERT_TRUE(send_recv_type == SEND_RECV_STREAM);
+            ucp_stream_poll_ep_t poll_eps;
+            ssize_t              ep_count;
+            size_t               recv_length;
+            do {
                 progress();
+                ep_count = ucp_stream_worker_poll(to.worker(), &poll_eps, 1, 0);
+            } while (ep_count == 0);
+            ASSERT_EQ(1,                  ep_count);
+            EXPECT_EQ(to.ep(),            poll_eps.ep);
+            EXPECT_EQ((void *)0xdeadbeef, poll_eps.user_data);
+
+            recv_req = ucp_stream_recv_nb(to.ep(), &recv_data, 1,
+                                          ucp_dt_make_contig(sizeof(recv_data)),
+                                          rstream_complete_cb, &recv_length,
+                                          UCP_STREAM_RECV_FLAG_WAITALL);
+        }
+
+        if (recv_req != NULL) {
+            ASSERT_TRUE(UCS_PTR_IS_PTR(recv_req));
+            while (!ucp_request_is_completed(recv_req)) {
+                check_events(from.worker(), to.worker(), wakeup, recv_req);
             }
             ucp_request_free(recv_req);
         }
@@ -81,81 +275,266 @@ public:
         EXPECT_EQ(send_data, recv_data);
     }
 
+    bool wait_for_server_ep(bool wakeup)
+    {
+        ucs_time_t time_limit = ucs_get_time() + ucs_time_from_sec(UCP_TEST_TIMEOUT_IN_SEC);
+
+        while ((receiver().get_num_eps() == 0) && (m_err_handler_count == 0) &&
+               (ucs_get_time() < time_limit)) {
+            check_events(sender().worker(), receiver().worker(), wakeup, NULL);
+        }
+        return (m_err_handler_count == 0) && (receiver().get_num_eps() > 0);
+    }
+
+    void wait_for_reject(entity &e, bool wakeup)
+    {
+        ucs_time_t time_limit = ucs_get_time() +
+                                ucs_time_from_sec(UCP_TEST_TIMEOUT_IN_SEC);
+
+        while ((e.get_rejected_cntr() == 0) &&
+               (ucs_get_time() < time_limit)) {
+            check_events(sender().worker(), receiver().worker(), wakeup, NULL);
+        }
+        EXPECT_GT(time_limit, ucs_get_time());
+        EXPECT_EQ(1ul, e.get_rejected_cntr());
+    }
+
+    virtual ucp_ep_params_t get_ep_params()
+    {
+        ucp_ep_params_t ep_params = ucp_test::get_ep_params();
+        ep_params.field_mask      |= UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE |
+                                     UCP_EP_PARAM_FIELD_ERR_HANDLER |
+                                     UCP_EP_PARAM_FIELD_USER_DATA;
+        /* The error handling requirement is needed since we need to take
+         * care of a case where the client gets an error. In case ucp needs to
+         * handle a large worker address but neither ud nor ud_x are present */
+        ep_params.err_mode         = UCP_ERR_HANDLING_MODE_PEER;
+        ep_params.err_handler.cb   = err_handler_cb;
+        ep_params.err_handler.arg  = NULL;
+        ep_params.user_data        = reinterpret_cast<void*>(this);
+        return ep_params;
+    }
+
+    void client_ep_connect(struct sockaddr *connect_addr)
+    {
+        ucp_ep_params_t ep_params = get_ep_params();
+        ep_params.field_mask      |= UCP_EP_PARAM_FIELD_FLAGS |
+                                     UCP_EP_PARAM_FIELD_SOCK_ADDR;
+        ep_params.flags            = UCP_EP_PARAMS_FLAGS_CLIENT_SERVER;
+        ep_params.sockaddr.addr    = connect_addr;
+        ep_params.sockaddr.addrlen = sizeof(*connect_addr);
+        sender().connect(&receiver(), ep_params);
+    }
+
+    void connect_and_send_recv(struct sockaddr *connect_addr, bool wakeup)
+    {
+        {
+            scoped_log_handler slh(detect_error_logger);
+            client_ep_connect(connect_addr);
+            if (!wait_for_server_ep(wakeup)) {
+                UCS_TEST_SKIP_R("cannot connect to server");
+            }
+        }
+
+        send_recv(sender(), receiver(),
+                  (GetParam().variant == CONN_REQ_STREAM) ? SEND_RECV_STREAM :
+                  SEND_RECV_TAG, wakeup, cb_type());
+    }
+
+    void connect_and_reject(struct sockaddr *connect_addr, bool wakeup)
+    {
+        {
+            scoped_log_handler slh(detect_error_logger);
+            client_ep_connect(connect_addr);
+            /* Check reachability with tagged send */
+            send_recv(sender(), receiver(), SEND_RECV_TAG, wakeup,
+                      ucp_test_base::entity::LISTEN_CB_REJECT);
+        }
+        wait_for_reject(receiver(), wakeup);
+        wait_for_reject(sender(),   wakeup);
+    }
+
+    void listen_and_communicate(ucp_test_base::entity::listen_cb_type_t cb_type,
+                                bool wakeup)
+    {
+        struct sockaddr_in connect_addr;
+        get_listen_addr(&connect_addr);
+
+        UCS_TEST_MESSAGE << "Testing "
+                         << ucs::sockaddr_to_str(
+                                (const struct sockaddr*)&connect_addr);
+
+        start_listener(cb_type, (const struct sockaddr*)&connect_addr);
+        connect_and_send_recv((struct sockaddr*)&connect_addr, wakeup);
+    }
+
+    void listen_and_reject(ucp_test_base::entity::listen_cb_type_t cb_type,
+                           bool wakeup)
+    {
+        struct sockaddr_in connect_addr;
+        get_listen_addr(&connect_addr);
+
+        UCS_TEST_MESSAGE << "Testing "
+                         << ucs::sockaddr_to_str(
+                                (const struct sockaddr*)&connect_addr);
+        start_listener(cb_type, (const struct sockaddr*)&connect_addr);
+        connect_and_reject((struct sockaddr*)&connect_addr, wakeup);
+    }
+
+
     static void err_handler_cb(void *arg, ucp_ep_h ep, ucs_status_t status) {
         test_ucp_sockaddr *self = reinterpret_cast<test_ucp_sockaddr*>(arg);
-        self->err_handler_count++;
+        ucp_test::err_handler_cb(static_cast<ucp_test *>(self), ep, status);
+
+        if (status == UCS_ERR_REJECTED) {
+            entity *e = self->get_entity_by_ep(ep);
+            if (e != NULL) {
+                e->inc_rejected_cntr();
+                return;
+            }
+        }
+
+        /* The current expected errors are only from the err_handle test
+         * and from transports where the worker address is too long but ud/ud_x
+         * are not present, or ud/ud_x are present but their addresses are too
+         * long as well */
+        if (status != UCS_ERR_UNREACHABLE) {
+            UCS_TEST_ABORT("Error: " << ucs_status_string(status));
+        }
     }
 
 protected:
-    volatile int err_handler_count;
+    ucp_test_base::entity::listen_cb_type_t cb_type() const {
+        if ((GetParam().variant == CONN_REQ_TAG) ||
+            (GetParam().variant == CONN_REQ_STREAM)) {
+            return ucp_test_base::entity::LISTEN_CB_CONN;
+        }
+        return ucp_test_base::entity::LISTEN_CB_EP;
+    }
 };
 
 UCS_TEST_P(test_ucp_sockaddr, listen) {
+    listen_and_communicate(cb_type(), false);
+}
 
-    struct sockaddr_in listen_addr;
-    get_listen_addr(&listen_addr);
+UCS_TEST_P(test_ucp_sockaddr, listen_inaddr_any) {
 
-    ucs_status_t status = receiver().listen((const struct sockaddr*)&listen_addr,
-                                            sizeof(listen_addr));
-    if (status == UCS_ERR_INVALID_ADDR) {
-        UCS_TEST_SKIP_R("cannot listen to " + ucs::sockaddr_to_str(&listen_addr));
-    }
+    struct sockaddr_in connect_addr, inaddr_any_listen_addr;
+    get_listen_addr(&connect_addr);
+    inaddr_any_addr(&inaddr_any_listen_addr, connect_addr.sin_port);
 
-    ucp_ep_params_t ep_params = ucp_test::get_ep_params();
-    ep_params.field_mask      |= UCP_EP_PARAM_FIELD_FLAGS |
-                                 UCP_EP_PARAM_FIELD_SOCK_ADDR |
-                                 UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE;
-    ep_params.err_mode         = UCP_ERR_HANDLING_MODE_PEER;
-    ep_params.flags            = UCP_EP_PARAMS_FLAGS_CLIENT_SERVER;
-    ep_params.sockaddr.addr    = (struct sockaddr*)&listen_addr;
-    ep_params.sockaddr.addrlen = sizeof(listen_addr);
-    sender().connect(&receiver(), ep_params);
+    UCS_TEST_MESSAGE << "Testing "
+                     << ucs::sockaddr_to_str(
+                        (const struct sockaddr*)&inaddr_any_listen_addr);
 
-    tag_send(sender(), receiver());
+    start_listener(cb_type(), (const struct sockaddr*)&inaddr_any_listen_addr);
+    connect_and_send_recv((struct sockaddr*)&connect_addr, false);
+}
 
-    /* wait for reverse ep to appear */
-    while (receiver().get_num_eps() == 0) {
-        progress();
+UCS_TEST_P(test_ucp_sockaddr, reject) {
+    if (GetParam().variant > 0) {
+        UCS_TEST_SKIP_R("Not parameterized test");
     }
-    tag_send(receiver(), sender());
+
+    listen_and_reject(ucp_test_base::entity::LISTEN_CB_REJECT, false);
 }
 
 UCS_TEST_P(test_ucp_sockaddr, err_handle) {
 
     struct sockaddr_in listen_addr;
-    err_handler_count = 0;
 
     get_listen_addr(&listen_addr);
 
-    ucs_status_t status = receiver().listen((const struct sockaddr*)&listen_addr,
+    ucs_status_t status = receiver().listen(cb_type(),
+                                            (const struct sockaddr*)&listen_addr,
                                             sizeof(listen_addr));
-    if (status == UCS_ERR_INVALID_ADDR) {
+    if (status == UCS_ERR_UNREACHABLE) {
         UCS_TEST_SKIP_R("cannot listen to " + ucs::sockaddr_to_str(&listen_addr));
     }
 
     /* make the client try to connect to a non-existing port on the server side */
     listen_addr.sin_port = 1;
 
-    ucp_ep_params_t ep_params = ucp_test::get_ep_params();
-    ep_params.field_mask      |= UCP_EP_PARAM_FIELD_FLAGS |
-                                 UCP_EP_PARAM_FIELD_SOCK_ADDR |
-                                 UCP_EP_PARAM_FIELD_ERR_HANDLING_MODE |
-                                 UCP_EP_PARAM_FIELD_ERR_HANDLER |
-                                 UCP_EP_PARAM_FIELD_USER_DATA;
-    ep_params.err_mode         = UCP_ERR_HANDLING_MODE_PEER;
-    ep_params.err_handler.cb   = err_handler_cb;
-    ep_params.err_handler.arg  = NULL;
-    ep_params.user_data        = reinterpret_cast<void*>(this);
-    ep_params.flags            = UCP_EP_PARAMS_FLAGS_CLIENT_SERVER;
-    ep_params.sockaddr.addr    = (struct sockaddr*)&listen_addr;
-    ep_params.sockaddr.addrlen = sizeof(listen_addr);
-    wrap_errors();
-    sender().connect(&receiver(), ep_params);
-    /* allow for the unreachable event to arrive before restoring errors */
-    wait_for_flag(&err_handler_count);
-    restore_errors();
-
-    EXPECT_EQ(1, err_handler_count);
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+        client_ep_connect((struct sockaddr*)&listen_addr);
+        /* allow for the unreachable event to arrive before restoring errors */
+        wait_for_flag(&m_err_handler_count);
+    }
+
+    EXPECT_EQ(1, m_err_handler_count);
+}
+
+UCP_INSTANTIATE_ALL_TEST_CASE(test_ucp_sockaddr)
+
+
+class test_ucp_sockaddr_with_wakeup : public test_ucp_sockaddr {
+public:
+
+    static ucp_params_t get_ctx_params() {
+        ucp_params_t params = test_ucp_sockaddr::get_ctx_params();
+        params.features    |= UCP_FEATURE_WAKEUP;
+        return params;
+    }
+};
+
+UCS_TEST_P(test_ucp_sockaddr_with_wakeup, wakeup) {
+    listen_and_communicate(cb_type(), true);
+}
+
+UCS_TEST_P(test_ucp_sockaddr_with_wakeup, reject) {
+    if (GetParam().variant > 0) {
+        UCS_TEST_SKIP_R("Invalid test parameter");
+    }
+
+    listen_and_reject(ucp_test_base::entity::LISTEN_CB_REJECT, true);
+}
+
+UCP_INSTANTIATE_ALL_TEST_CASE(test_ucp_sockaddr_with_wakeup)
+
+
+class test_ucp_sockaddr_with_rma_atomic : public test_ucp_sockaddr {
+public:
+
+    static ucp_params_t get_ctx_params() {
+        ucp_params_t params = test_ucp_sockaddr::get_ctx_params();
+        params.field_mask  |= UCP_PARAM_FIELD_FEATURES;
+        params.features    |= UCP_FEATURE_RMA   |
+                              UCP_FEATURE_AMO32 |
+                              UCP_FEATURE_AMO64;
+        return params;
+    }
+};
+
+UCS_TEST_P(test_ucp_sockaddr_with_rma_atomic, wireup) {
+
+    /* This test makes sure that the client-server flow works when the required
+     * features are RMA/ATOMIC. With these features, need to make sure that
+     * there is a lane for ucp-wireup (an am_lane should be created and used) */
+    struct sockaddr_in connect_addr;
+    get_listen_addr(&connect_addr);
+
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str((const struct sockaddr*)&connect_addr);
+
+    start_listener(cb_type(), (const struct sockaddr*)&connect_addr);
+
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+
+        client_ep_connect((struct sockaddr*)&connect_addr);
+
+        /* allow the err_handler callback to be invoked if needed */
+        if (!wait_for_server_ep(false)) {
+            EXPECT_EQ(1, m_err_handler_count);
+            UCS_TEST_SKIP_R("cannot connect to server");
+        }
+
+        EXPECT_EQ(0, m_err_handler_count);
+        /* even if server EP is created, in case of long address, wireup will be
+         * done later, need to communicate */
+        send_recv(sender(), receiver(), (GetParam().variant == CONN_REQ_STREAM) ?
+                  SEND_RECV_STREAM : SEND_RECV_TAG, false, cb_type());
+    }
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_sockaddr)
+UCP_INSTANTIATE_ALL_TEST_CASE(test_ucp_sockaddr_with_rma_atomic)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_stream.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_stream.cc
index 377614fc4..e7c99d286 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_stream.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_stream.cc
@@ -9,8 +9,8 @@
 #include <set>
 #include <vector>
 
+#include "ucp_datatype.h"
 #include "ucp_test.h"
-#include <common/test_helpers.h>
 
 
 class test_ucp_stream_base : public ucp_test {
@@ -40,7 +40,7 @@ size_t test_ucp_stream_base::wait_stream_recv(void *request)
         status = ucp_stream_recv_request_test(request, &length);
     } while (status == UCS_INPROGRESS);
     ASSERT_UCS_OK(status);
-    ucp_request_release(request);
+    ucp_request_free(request);
 
     return length;
 }
@@ -52,6 +52,67 @@ test_ucp_stream_base::stream_send_nb(const ucp::data_type_desc_t& dt_desc)
                               dt_desc.dt(), ucp_send_cb, 0);
 }
 
+class test_ucp_stream_onesided : public test_ucp_stream_base {
+public:
+    ucp_ep_params_t get_ep_params() {
+        ucp_ep_params_t params = test_ucp_stream_base::get_ep_params();
+        params.field_mask |= UCP_EP_PARAM_FIELD_FLAGS;
+        params.flags      |= UCP_EP_PARAMS_FLAGS_NO_LOOPBACK;
+        return params;
+    }
+};
+
+UCS_TEST_P(test_ucp_stream_onesided, send_recv_no_ep) {
+
+    /* connect from sender side only and send */
+    sender().connect(&receiver(), get_ep_params());
+    uint64_t send_data = ucs::rand();
+    ucp::data_type_desc_t dt_desc(ucp_dt_make_contig(sizeof(uint64_t)),
+                                  &send_data, sizeof(send_data));
+    void *sreq = stream_send_nb(dt_desc);
+    wait(sreq);
+
+    /* must not receive data before ep is created on receiver side */
+    static const size_t max_eps = 10;
+    ucp_stream_poll_ep_t poll_eps[max_eps];
+    ssize_t count = ucp_stream_worker_poll(receiver().worker(), poll_eps,
+                                           max_eps, 0);
+    EXPECT_EQ(0l, count) << "ucp_stream_worker_poll returned ep too early";
+
+    /* create receiver side ep */
+    ucp_ep_params_t recv_ep_param = get_ep_params();
+    recv_ep_param.field_mask |= UCP_EP_PARAM_FIELD_USER_DATA;
+    recv_ep_param.user_data   = reinterpret_cast<void*>(static_cast<uintptr_t>(ucs::rand()));
+    receiver().connect(&sender(), recv_ep_param);
+
+    /* expect ep to be ready */
+    ucs_time_t deadline = ucs_get_time() +
+                          (ucs_time_from_sec(10.0) * ucs::test_time_multiplier());
+    do {
+        progress();
+        count = ucp_stream_worker_poll(receiver().worker(), poll_eps, max_eps, 0);
+    } while ((count == 0) && (ucs_get_time() < deadline));
+    EXPECT_EQ(1l, count);
+    EXPECT_EQ(recv_ep_param.user_data, poll_eps[0].user_data);
+    EXPECT_EQ(receiver().ep(0), poll_eps[0].ep);
+
+    /* expect data to be received */
+    uint64_t recv_data = 0;
+    size_t recv_length = 0;
+    void *rreq = ucp_stream_recv_nb(receiver().ep(), &recv_data, 1,
+                                    ucp_dt_make_contig(sizeof(uint64_t)),
+                                    ucp_recv_cb, &recv_length, 0);
+    ASSERT_UCS_PTR_OK(rreq);
+    if (rreq != NULL) {
+        recv_length = wait_stream_recv(rreq);
+    }
+
+    EXPECT_EQ(sizeof(uint64_t), recv_length);
+    EXPECT_EQ(send_data, recv_data);
+}
+
+UCP_INSTANTIATE_TEST_CASE(test_ucp_stream_onesided)
+
 class test_ucp_stream : public test_ucp_stream_base
 {
 public:
@@ -66,11 +127,16 @@ public:
 
 protected:
     void do_send_recv_data_test(ucp_datatype_t datatype);
-    template <typename T>
+    template <typename T, unsigned recv_flags>
     void do_send_recv_test(ucp_datatype_t datatype);
-    template <typename T>
+    template <typename T, unsigned recv_flags>
     void do_send_exp_recv_test(ucp_datatype_t datatype);
     void do_send_recv_data_recv_test(ucp_datatype_t datatype);
+
+    /* for self-validation of generic datatype
+     * NOTE: it's tested only with byte array data since it's recv completion
+     *       granularity without UCP_RECV_FLAG_WAITALL flag */
+    std::vector<uint8_t> context;
 };
 
 void test_ucp_stream::do_send_recv_data_test(ucp_datatype_t datatype)
@@ -82,10 +148,17 @@ void test_ucp_stream::do_send_recv_data_test(ucp_datatype_t datatype)
 
     /* send all msg sizes*/
     for (size_t i = 3; i < sbuf.size(); i *= 2) {
-        ucs::fill_random(sbuf, i);
-        check_pattern.insert(check_pattern.end(), sbuf.begin(),
-                             sbuf.begin() + i);
-        sstatus = stream_send_nb(ucp::data_type_desc_t(datatype, sbuf.data(), i));
+        if (UCP_DT_IS_GENERIC(datatype)) {
+            for (size_t j = 0; j < i; ++j) {
+                check_pattern.push_back(char(j));
+            }
+        } else {
+            ucs::fill_random(sbuf, i);
+            check_pattern.insert(check_pattern.end(), sbuf.begin(),
+                                 sbuf.begin() + i);
+        }
+        ucp::data_type_desc_t dt_desc(datatype, sbuf.data(), i);
+        sstatus = stream_send_nb(dt_desc);
         EXPECT_FALSE(UCS_PTR_IS_ERR(sstatus));
         wait(sstatus);
         ssize += i;
@@ -111,7 +184,7 @@ void test_ucp_stream::do_send_recv_data_test(ucp_datatype_t datatype)
     EXPECT_EQ(check_pattern, rbuf);
 }
 
-template <typename T>
+template <typename T, unsigned recv_flags>
 void test_ucp_stream::do_send_recv_test(ucp_datatype_t datatype)
 {
     const size_t      dt_elem_size = UCP_DT_IS_CONTIG(datatype) ?
@@ -123,20 +196,33 @@ void test_ucp_stream::do_send_recv_test(ucp_datatype_t datatype)
 
     /* send all msg sizes in bytes*/
     for (size_t i = 3; i < sbuf.size(); i *= 2) {
-        ucs::fill_random(sbuf, i);
-        check_pattern.insert(check_pattern.end(), sbuf.begin(), sbuf.begin() + i);
-        sstatus = stream_send_nb(ucp::data_type_desc_t(DATATYPE, sbuf.data(), i));
+        ucp_datatype_t dt;
+        if (UCP_DT_IS_GENERIC(datatype)) {
+            dt = datatype;
+            for (size_t j = 0; j < i; ++j) {
+                context.push_back(uint8_t(j));
+            }
+        } else {
+            dt = DATATYPE;
+            ucs::fill_random(sbuf, i);
+            check_pattern.insert(check_pattern.end(), sbuf.begin(),
+                                 sbuf.begin() + i);
+        }
+        ucp::data_type_desc_t dt_desc(dt, sbuf.data(), i);
+        sstatus = stream_send_nb(dt_desc);
         EXPECT_FALSE(UCS_PTR_IS_ERR(sstatus));
         wait(sstatus);
         ssize += i;
     }
 
-    size_t align_tail = dt_elem_size - ssize % dt_elem_size;
+    size_t align_tail = UCP_DT_IS_GENERIC(datatype) ? 0 :
+                        (dt_elem_size - ssize % dt_elem_size);
     if (align_tail != 0) {
         ucs::fill_random(sbuf, align_tail);
         check_pattern.insert(check_pattern.end(), sbuf.begin(), sbuf.begin() + align_tail);
-        sstatus = stream_send_nb(ucp::data_type_desc_t(ucp_dt_make_contig(align_tail),
-                                                       sbuf.data(), align_tail));
+        ucp::data_type_desc_t dt_desc(ucp_dt_make_contig(align_tail),
+                                      sbuf.data(), align_tail);
+        sstatus = stream_send_nb(dt_desc);
         EXPECT_FALSE(UCS_PTR_IS_ERR(sstatus));
         wait(sstatus);
         ssize += align_tail;
@@ -146,6 +232,7 @@ void test_ucp_stream::do_send_recv_test(ucp_datatype_t datatype)
 
     std::vector<T> rbuf(ssize / dt_elem_size, 'r');
     size_t         roffset = 0;
+    size_t         counter = 0;
     do {
         ucp::data_type_desc_t dt_desc(datatype, &rbuf[roffset / dt_elem_size],
                                       ssize - roffset);
@@ -153,22 +240,30 @@ void test_ucp_stream::do_send_recv_test(ucp_datatype_t datatype)
         size_t length;
         void   *rreq = ucp_stream_recv_nb(receiver().ep(), dt_desc.buf(),
                                           dt_desc.count(), dt_desc.dt(),
-                                          ucp_recv_cb, &length, 0);
+                                          ucp_recv_cb, &length, recv_flags);
         ASSERT_TRUE(!UCS_PTR_IS_ERR(rreq));
         if (UCS_PTR_IS_PTR(rreq)) {
             length = wait_stream_recv(rreq);
         }
         EXPECT_EQ(size_t(0), length % dt_elem_size);
         roffset += length;
+        counter++;
     } while (roffset < ssize);
 
+    /* waitall flag requires completion by single request */
+    if (recv_flags & UCP_STREAM_RECV_FLAG_WAITALL) {
+        EXPECT_EQ(size_t(1), counter);
+    }
+
     EXPECT_EQ(roffset, ssize);
-    const T     *check_ptr  = reinterpret_cast<const T *>(check_pattern.data());
-    const size_t check_size = check_pattern.size() / dt_elem_size;
-    EXPECT_EQ(std::vector<T>(check_ptr, check_ptr + check_size), rbuf);
+    if (!UCP_DT_IS_GENERIC(datatype)) {
+        const T     *check_ptr  = reinterpret_cast<const T *>(check_pattern.data());
+        const size_t check_size = check_pattern.size() / dt_elem_size;
+        EXPECT_EQ(std::vector<T>(check_ptr, check_ptr + check_size), rbuf);
+    }
 }
 
-template <typename T>
+template <typename T, unsigned recv_flags>
 void test_ucp_stream::do_send_exp_recv_test(ucp_datatype_t datatype)
 {
     const size_t dt_elem_size = UCP_DT_IS_CONTIG(datatype) ?
@@ -189,7 +284,7 @@ void test_ucp_stream::do_send_exp_recv_test(ucp_datatype_t datatype)
 
         void *rreq = ucp_stream_recv_nb(receiver().ep(), rdesc.buf(),
                                         rdesc.count(), rdesc.dt(), ucp_recv_cb,
-                                        &length, 0);
+                                        &length, recv_flags);
         EXPECT_TRUE(UCS_PTR_IS_PTR(rreq));
         rreqs.push_back(rreq);
     }
@@ -213,6 +308,7 @@ void test_ucp_stream::do_send_exp_recv_test(ucp_datatype_t datatype)
         rcount += length;
     }
 
+    size_t counter = 0;
     while (rcount < scount) {
         size_t           length = std::numeric_limits<size_t>::max();
         ucs_status_ptr_t rreq;
@@ -226,9 +322,15 @@ void test_ucp_stream::do_send_exp_recv_test(ucp_datatype_t datatype)
         ASSERT_LE(length, msg_size);
         EXPECT_EQ(size_t(0), length % dt_elem_size);
         rcount += length;
+        counter++;
     }
     EXPECT_EQ(scount, rcount);
 
+    /* waitall flag requires completion by single request */
+    if (recv_flags & UCP_STREAM_RECV_FLAG_WAITALL) {
+        EXPECT_EQ(size_t(0), counter);
+    }
+
     /* double check, no data should be here */
     while (progress());
 
@@ -263,8 +365,8 @@ void test_ucp_stream::do_send_recv_data_recv_test(ucp_datatype_t datatype)
             ucs::fill_random(sbuf, send_i);
             check_pattern.insert(check_pattern.end(), sbuf.begin(),
                                  sbuf.begin() + send_i);
-            sstatus = stream_send_nb(ucp::data_type_desc_t(datatype, sbuf.data(),
-                                                           send_i));
+            ucp::data_type_desc_t dt_desc(datatype, sbuf.data(), send_i);
+            sstatus = stream_send_nb(dt_desc);
             EXPECT_FALSE(UCS_PTR_IS_ERR(sstatus));
             wait(sstatus);
             ssize += send_i;
@@ -306,44 +408,91 @@ UCS_TEST_P(test_ucp_stream, send_iov_recv_data) {
     do_send_recv_data_test(DATATYPE_IOV);
 }
 
+UCS_TEST_P(test_ucp_stream, send_generic_recv_data) {
+    ucp_datatype_t dt;
+    ucs_status_t status;
+
+    status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, NULL, &dt);
+    ASSERT_UCS_OK(status);
+    do_send_recv_data_test(dt);
+    ucp_dt_destroy(dt);
+}
+
 UCS_TEST_P(test_ucp_stream, send_recv_8) {
-    do_send_recv_test<uint8_t>(ucp_dt_make_contig(sizeof(uint8_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint8_t));
+
+    do_send_recv_test<uint8_t, 0>(datatype);
+    do_send_recv_test<uint8_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_recv_16) {
-    do_send_recv_test<uint16_t>(ucp_dt_make_contig(sizeof(uint16_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint16_t));
+
+    do_send_recv_test<uint16_t, 0>(datatype);
+    do_send_recv_test<uint16_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_recv_32) {
-    do_send_recv_test<uint32_t>(ucp_dt_make_contig(sizeof(uint32_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint32_t));
+
+    do_send_recv_test<uint32_t, 0>(datatype);
+    do_send_recv_test<uint32_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_recv_64) {
-    do_send_recv_test<uint64_t>(ucp_dt_make_contig(sizeof(uint64_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint64_t));
+
+    do_send_recv_test<uint64_t, 0>(datatype);
+    do_send_recv_test<uint64_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_recv_iov) {
-    do_send_recv_test<uint8_t>(DATATYPE_IOV);
+    do_send_recv_test<uint8_t, 0>(DATATYPE_IOV);
+    do_send_recv_test<uint8_t, UCP_STREAM_RECV_FLAG_WAITALL>(DATATYPE_IOV);
+}
+
+UCS_TEST_P(test_ucp_stream, send_recv_generic) {
+    ucp_datatype_t dt;
+    ucs_status_t status;
+
+    status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, &context, &dt);
+    ASSERT_UCS_OK(status);
+    do_send_recv_test<uint8_t, UCP_STREAM_RECV_FLAG_WAITALL>(dt);
+    ucp_dt_destroy(dt);
+
 }
 
 UCS_TEST_P(test_ucp_stream, send_exp_recv_8) {
-    do_send_exp_recv_test<uint8_t>(ucp_dt_make_contig(sizeof(uint8_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint8_t));
+
+    do_send_exp_recv_test<uint8_t, 0>(datatype);
+    do_send_exp_recv_test<uint8_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_exp_recv_16) {
-    do_send_exp_recv_test<uint16_t>(ucp_dt_make_contig(sizeof(uint16_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint16_t));
+
+    do_send_exp_recv_test<uint16_t, 0>(datatype);
+    do_send_exp_recv_test<uint16_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_exp_recv_32) {
-    do_send_exp_recv_test<uint32_t>(ucp_dt_make_contig(sizeof(uint32_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint32_t));
+
+    do_send_exp_recv_test<uint32_t, 0>(datatype);
+    do_send_exp_recv_test<uint32_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_exp_recv_64) {
-    do_send_exp_recv_test<uint64_t>(ucp_dt_make_contig(sizeof(uint64_t)));
+    ucp_datatype_t datatype = ucp_dt_make_contig(sizeof(uint64_t));
+
+    do_send_exp_recv_test<uint64_t, 0>(datatype);
+    do_send_exp_recv_test<uint64_t, UCP_STREAM_RECV_FLAG_WAITALL>(datatype);
 }
 
 UCS_TEST_P(test_ucp_stream, send_exp_recv_iov) {
-    do_send_exp_recv_test<uint8_t>(DATATYPE_IOV);
+    do_send_exp_recv_test<uint8_t, 0>(DATATYPE_IOV);
+    do_send_exp_recv_test<uint8_t, UCP_STREAM_RECV_FLAG_WAITALL>(DATATYPE_IOV);
 }
 
 UCS_TEST_P(test_ucp_stream, send_recv_data_recv_8) {
@@ -366,6 +515,52 @@ UCS_TEST_P(test_ucp_stream, send_recv_data_recv_iov) {
     do_send_recv_data_recv_test(DATATYPE_IOV);
 }
 
+UCS_TEST_P(test_ucp_stream, send_zero_ending_iov_recv_data) {
+    const size_t min_size         = 1024;
+    const size_t max_size         = min_size * 64;
+    const size_t iov_num          = 8; /* must be divisible by 4 without a
+                                        * remainder, caught on mlx5 based TLs
+                                        * where max_iov = 3 for zcopy multi
+                                        * protocol, where every posting includes:
+                                        * 1 header + 2 nonempty IOVs */
+    const size_t iov_num_nonempty = iov_num / 2;
+
+    std::vector<uint8_t> buf(max_size * 2);
+    ucs::fill_random(buf, buf.size());
+    std::vector<ucp_dt_iov_t> v(iov_num);
+
+    for (size_t size = min_size; size < max_size; ++size) {
+        size_t slen = 0;
+        for (size_t j = 0; j < iov_num; ++j) {
+            if ((j % 2) == 0) {
+                uint8_t *ptr = buf.data();
+                v[j].buffer = &(ptr[j * size / iov_num_nonempty]);
+                v[j].length = size / iov_num_nonempty;
+                slen       += v[j].length;
+            } else {
+                v[j].buffer = NULL;
+                v[j].length = 0;
+            }
+        }
+
+        void *sreq = ucp_stream_send_nb(sender().ep(), &v[0], iov_num,
+                                        DATATYPE_IOV, ucp_send_cb, 0);
+
+        size_t rlen = 0;
+        while (rlen < slen) {
+            progress();
+            size_t length;
+            void *rdata = ucp_stream_recv_data_nb(receiver().ep(), &length);
+            EXPECT_FALSE(UCS_PTR_IS_ERR(rdata));
+            if (rdata != NULL) {
+                rlen += length;
+                ucp_stream_data_release(receiver().ep(), rdata);
+            }
+        }
+        wait(sreq);
+    }
+}
+
 UCP_INSTANTIATE_TEST_CASE(test_ucp_stream)
 
 class test_ucp_stream_many2one : public test_ucp_stream_base {
@@ -403,7 +598,7 @@ protected:
     size_t send_all(ucp_datatype_t datatype, size_t n_iter);
     void check_no_data();
     std::set<ucp_ep_h> check_no_data(entity &e);
-    void check_recv_data(size_t n_iter);
+    void check_recv_data(size_t n_iter, ucp_datatype_t dt);
 
     std::vector<std::string>        m_msgs;
     std::vector<std::vector<char> > m_recv_data;
@@ -476,7 +671,7 @@ void test_ucp_stream_many2one::do_send_worker_poll_test(ucp_datatype_t dt)
     } while (!sreqs.empty() || (total_len != 0));
 
     check_no_data();
-    check_recv_data(niter);
+    check_recv_data(niter, dt);
 }
 
 void test_ucp_stream_many2one::do_send_recv_test(ucp_datatype_t dt)
@@ -495,7 +690,7 @@ void test_ucp_stream_many2one::do_send_recv_test(ucp_datatype_t dt)
         m_recv_data[i].resize(m_msgs[i].length() * niter + 1);
         ucp::data_type_desc_t &rdesc = dt_rdescs[i].make(dt,
                                                          &m_recv_data[i][roffsets[i]],
-                                             m_recv_data[i].size());
+                                                         m_recv_data[i].size());
         size_t length;
         void *rreq = ucp_stream_recv_nb(e(m_receiver_idx).ep(0, i),
                                         rdesc.buf(), rdesc.count(), rdesc.dt(),
@@ -514,35 +709,44 @@ void test_ucp_stream_many2one::do_send_recv_test(ucp_datatype_t dt)
             roffsets[rreqs[i].first] += wait_stream_recv(rreqs[i].second.m_req);
         }
         rreqs.clear();
+        progress();
 
-        do {
-            const size_t max_eps = 10;
-            ucp_stream_poll_ep_t poll_eps[max_eps];
-            progress();
-            count = ucp_stream_worker_poll(e(m_receiver_idx).worker(),
-                                           poll_eps, max_eps, 0);
-            EXPECT_LE(0, count);
+        const size_t max_eps = 10;
+        ucp_stream_poll_ep_t poll_eps[max_eps];
+        count = ucp_stream_worker_poll(e(m_receiver_idx).worker(),
+                                       poll_eps, max_eps, 0);
+        EXPECT_LE(0, count);
+        EXPECT_LE(size_t(count), m_nsenders);
 
-            for (ssize_t i = 0; i < count; ++i) {
+        for (ssize_t i = 0; i < count; ++i) {
+            bool again = true;
+            while (again) {
                 size_t sender_idx = uintptr_t(poll_eps[i].user_data);
                 size_t &roffset   = roffsets[sender_idx];
                 ucp::data_type_desc_t &dt_desc =
                     dt_rdescs[sender_idx].forward_to(roffset);
                 EXPECT_TRUE(dt_desc.is_valid());
                 size_t length;
-                void *rreq = ucp_stream_recv_nb(poll_eps[i].ep, dt_desc.buf(),
-                                                dt_desc.count(), dt_desc.dt(),
+                void *rreq = ucp_stream_recv_nb(poll_eps[i].ep,
+                                                dt_desc.buf(),
+                                                dt_desc.count(),
+                                                dt_desc.dt(),
                                                 ucp_recv_cb, &length, 0);
-                if (UCS_PTR_STATUS(rreq) == UCS_OK) {
+                EXPECT_FALSE(UCS_PTR_IS_ERR(rreq));
+                if (rreq == NULL) {
+                    EXPECT_LT(size_t(0), length);
                     roffset += length;
+                    if (ssize_t(length) < dt_desc.buf_length()) {
+                        continue; /* Need to drain the EP */
+                    }
                 } else {
                     rreqs.push_back(std::make_pair(sender_idx,
                                                    request_wrapper_t(rreq,
                                                                      &dt_desc)));
                 }
-                EXPECT_FALSE(UCS_PTR_IS_ERR(rreq));
+                again = false;
             }
-        } while (count > 0);
+        }
 
         erase_completed_reqs(sreqs);
     } while (!rreqs.empty() || !sreqs.empty() ||
@@ -552,7 +756,7 @@ void test_ucp_stream_many2one::do_send_recv_test(ucp_datatype_t dt)
     EXPECT_EQ(total_sdata, std::accumulate(roffsets.begin(),
                                            roffsets.end(), 0ul));
     check_no_data();
-    check_recv_data(niter);
+    check_recv_data(niter, dt);
 }
 
 ucs_status_ptr_t
@@ -653,16 +857,25 @@ std::set<ucp_ep_h> test_ucp_stream_many2one::check_no_data(entity &e)
     return ret;
 }
 
-void test_ucp_stream_many2one::check_recv_data(size_t n_iter)
+void test_ucp_stream_many2one::check_recv_data(size_t n_iter, ucp_datatype_t dt)
 {
     for (size_t i = 0; i < m_nsenders; ++i) {
-        const std::string test = std::string("sender_") + ucs::to_string(i);
+        std::string test = std::string("sender_") + ucs::to_string(i);
         const std::string str(&m_recv_data[i].front());
+        if (UCP_DT_IS_GENERIC(dt)) {
+            std::vector<char> test_gen;
+            for (size_t j = 0; j < test.length(); ++j) {
+                test_gen.push_back(char(j));
+            }
+            test_gen.push_back('\0');
+            test = std::string(test_gen.data());
+        }
+
         size_t            next = 0;
         for (size_t j = 0; j < n_iter; ++j) {
             size_t match = str.find(test, next);
-            EXPECT_NE(std::string::npos, match) << "failed on " << j
-                                                << " iteration";
+            EXPECT_NE(std::string::npos, match) << "failed on sender " << i
+                                                << " iteration " << j;
             if (match == std::string::npos) {
                 break;
             }
@@ -682,7 +895,7 @@ test_ucp_stream_many2one::erase_completed_reqs(std::vector<request_wrapper_t> &r
         ucs_status_t status = ucp_request_check_status(i->m_req);
         if (status != UCS_INPROGRESS) {
             EXPECT_EQ(UCS_OK, status);
-            ucp_request_release(i->m_req);
+            ucp_request_free(i->m_req);
             delete i->m_dt_desc;
             i = reqs.erase(i);
         } else {
@@ -707,9 +920,23 @@ UCS_TEST_P(test_ucp_stream_many2one, drop_data) {
     m_entities.at(0).revoke_ep();
     m_entities.at(m_receiver_idx).revoke_ep(0, 0);
 
+    /* wait for 1-st byte on the last EP to be sure the network packets have
+       been arrived */
+    uint8_t check;
+    size_t  check_length;
+    ucp_ep_h last_ep = m_entities.at(m_receiver_idx).ep(0, m_nsenders - 1);
+    void *check_req  = ucp_stream_recv_nb(last_ep, &check, 1, DATATYPE,
+                                          ucp_recv_cb, &check_length, 0);
+    EXPECT_FALSE(UCS_PTR_IS_ERR(check_req));
+    if (UCS_PTR_IS_PTR(check_req)) {
+        wait_stream_recv(check_req);
+    }
+
     /* data from disconnected EP should be dropped */
     std::set<ucp_ep_h> others = check_no_data(m_entities.at(0));
-    EXPECT_EQ(m_nsenders - 1, others.size());
+    /* since ordering between EPs is not guaranteed, some data may be still in
+     * the network or buffered by transport */
+    EXPECT_LE(others.size(), m_nsenders - 1);
 
     /* reconnect */
     m_entities.at(0).connect(&m_entities.at(m_receiver_idx), get_ep_params(), 0);
@@ -737,6 +964,16 @@ UCS_TEST_P(test_ucp_stream_many2one, send_worker_poll_iov) {
     do_send_worker_poll_test(DATATYPE_IOV);
 }
 
+UCS_TEST_P(test_ucp_stream_many2one, send_worker_poll_generic) {
+    ucp_datatype_t dt;
+    ucs_status_t status;
+
+    status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, NULL, &dt);
+    ASSERT_UCS_OK(status);
+    do_send_worker_poll_test(dt);
+    ucp_dt_destroy(dt);
+}
+
 UCS_TEST_P(test_ucp_stream_many2one, send_recv_nb) {
     do_send_recv_test(DATATYPE);
 }
@@ -745,4 +982,14 @@ UCS_TEST_P(test_ucp_stream_many2one, send_recv_nb_iov) {
     do_send_recv_test(DATATYPE_IOV);
 }
 
+UCS_TEST_P(test_ucp_stream_many2one, send_recv_nb_generic) {
+    ucp_datatype_t dt;
+    ucs_status_t status;
+
+    status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, NULL, &dt);
+    ASSERT_UCS_OK(status);
+    do_send_recv_test(dt);
+    ucp_dt_destroy(dt);
+}
+
 UCP_INSTANTIATE_TEST_CASE(test_ucp_stream_many2one)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.cc
index ec391895b..732e14175 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.cc
@@ -7,9 +7,12 @@
 
 #include "test_ucp_tag.h"
 
-#include <common/test_helpers.h>
+#include "ucp_datatype.h"
+
 extern "C" {
 #include <ucp/core/ucp_worker.h>
+#include <ucp/core/ucp_ep.h>
+#include <ucp/core/ucp_ep.inl>
 }
 
 
@@ -34,8 +37,8 @@ void test_ucp_tag::init()
     ctx_attr.field_mask |= UCP_ATTR_FIELD_THREAD_MODE;
     ucp_context_query(receiver().ucph(), &ctx_attr);
 
-    dt_gen_start_count  = 0;
-    dt_gen_finish_count = 0;
+    ucp::dt_gen_start_count  = 0;
+    ucp::dt_gen_finish_count = 0;
 }
 
 void test_ucp_tag::request_init(void *request)
@@ -145,6 +148,16 @@ void test_ucp_tag::wait_for_unexpected_msg(ucp_worker_h worker, double sec)
     } while (ucp_tag_unexp_is_empty(&worker->tm) && (ucs_get_time() < timeout));
 }
 
+void test_ucp_tag::check_offload_support(bool offload_required)
+{
+    bool offload_supported = ucp_ep_is_tag_offload_enabled(ucp_ep_config(sender().ep()));
+    if (offload_supported != offload_required) {
+        cleanup();
+        std::string reason = offload_supported ? "tag offload" : "no tag offload";
+        UCS_TEST_SKIP_R(reason);
+    }
+}
+
 int test_ucp_tag::get_worker_index(int buf_index)
 {
     int worker_index = 0;
@@ -306,131 +319,9 @@ ucs_status_t test_ucp_tag::recv_req_b(void *buffer, size_t count, ucp_datatype_t
     return status;
 }
 
-void* test_ucp_tag::dt_common_start(size_t count)
-{
-    dt_gen_state *dt_state = new dt_gen_state;
-    dt_state->count   = count;
-    dt_state->started = 1;
-    dt_state->magic   = MAGIC;
-    dt_gen_start_count++;
-    return dt_state;
-}
-
-void* test_ucp_tag::dt_common_start_pack(void *context, const void *buffer, size_t count)
-{
-    return dt_common_start(count);
-}
-
-void* test_ucp_tag::dt_common_start_unpack(void *context, void *buffer, size_t count)
-{
-    return dt_common_start(count);
-}
-
-template <typename T>
-size_t test_ucp_tag::dt_packed_size(void *state)
-{
-    dt_gen_state *dt_state = (dt_gen_state*)state;
-    return dt_state->count * sizeof(T);
-}
-
-template <typename T>
-size_t test_ucp_tag::dt_pack(void *state, size_t offset, void *dest, size_t max_length)
-{
-    dt_gen_state *dt_state = (dt_gen_state*)state;
-    T *p = reinterpret_cast<T*> (dest);
-    uint32_t count;
-
-    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
-    EXPECT_EQ(1, dt_state->started);
-    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
-
-    ucs_assert((offset % sizeof(T)) == 0);
-
-    count = ucs_min(max_length / sizeof(T),
-                    dt_state->count - (offset / sizeof(T)));
-    for (unsigned i = 0; i < count; ++i) {
-        p[i] = (offset / sizeof(T)) + i;
-    }
-    return count * sizeof(T);
-}
-
-template <typename T>
-ucs_status_t test_ucp_tag::dt_unpack(void *state, size_t offset, const void *src,
-                                     size_t length)
-{
-    dt_gen_state *dt_state = (dt_gen_state*)state;
-    uint32_t count;
-
-    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
-    EXPECT_EQ(1, dt_state->started);
-    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
-
-    count = length / sizeof(T);
-    for (unsigned i = 0; i < count; ++i) {
-        T expected = (offset / sizeof(T)) + i;
-        T actual   = ((T*)src)[i];
-        if (actual != expected) {
-            UCS_TEST_ABORT("Invalid data at index " << i << ". expected: " <<
-                           expected << " actual: " << actual << " offset: " <<
-                           offset << ".");
-        }
-    }
-    return UCS_OK;
-}
-
-ucs_status_t test_ucp_tag::dt_err_unpack(void *state, size_t offset, const void *src,
-                                         size_t length)
-{
-    dt_gen_state *dt_state = (dt_gen_state*)state;
-
-    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
-    EXPECT_EQ(1, dt_state->started);
-    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
-
-    return UCS_ERR_NO_MEMORY;
-}
-
-void test_ucp_tag::dt_common_finish(void *state)
-{
-    dt_gen_state *dt_state = (dt_gen_state*)state;
-    --dt_state->started;
-    EXPECT_EQ(0, dt_state->started);
-    dt_gen_finish_count++;
-    delete dt_state;
-}
-
 bool test_ucp_tag::is_external_request()
 {
     return false;
 }
 
-ucp_generic_dt_ops test_ucp_tag::test_dt_uint32_ops = {
-    test_ucp_tag::dt_common_start_pack,
-    test_ucp_tag::dt_common_start_unpack,
-    test_ucp_tag::dt_packed_size<uint32_t>,
-    test_ucp_tag::dt_pack<uint32_t>,
-    test_ucp_tag::dt_unpack<uint32_t>,
-    test_ucp_tag::dt_common_finish
-};
-
-ucp_generic_dt_ops test_ucp_tag::test_dt_uint8_ops = {
-    test_ucp_tag::dt_common_start_pack,
-    test_ucp_tag::dt_common_start_unpack,
-    test_ucp_tag::dt_packed_size<uint8_t>,
-    test_ucp_tag::dt_pack<uint8_t>,
-    test_ucp_tag::dt_unpack<uint8_t>,
-    test_ucp_tag::dt_common_finish
-};
-
-ucp_generic_dt_ops test_ucp_tag::test_dt_uint32_err_ops = {
-    test_ucp_tag::dt_common_start_pack,
-    test_ucp_tag::dt_common_start_unpack,
-    test_ucp_tag::dt_packed_size<uint32_t>,
-    test_ucp_tag::dt_pack<uint32_t>,
-    test_ucp_tag::dt_err_unpack,
-    test_ucp_tag::dt_common_finish
-};
-
-int test_ucp_tag::dt_gen_start_count = 0;
-int test_ucp_tag::dt_gen_finish_count = 0;
 ucp_context_attr_t test_ucp_tag::ctx_attr;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.h
index db2ea2a6f..e7331d80b 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag.h
@@ -30,12 +30,6 @@ protected:
         ucp_tag_recv_info_t info;
     };
 
-    struct dt_gen_state {
-        size_t              count;
-        int                 started;
-        uint32_t            magic;
-    };
-
     virtual void init();
 
     static void request_init(void *request);
@@ -90,35 +84,10 @@ protected:
 
     void wait_for_unexpected_msg(ucp_worker_h worker, double sec);
 
-    static void* dt_common_start(size_t count);
-
-    static void* dt_common_start_pack(void *context, const void *buffer, size_t count);
-
-    static void* dt_common_start_unpack(void *context, void *buffer, size_t count);
-
-    template <typename T>
-    static size_t dt_packed_size(void *state);
-
-    template <typename T>
-    static size_t dt_pack(void *state, size_t offset, void *dest, size_t max_length);
-
-    template <typename T>
-    static ucs_status_t dt_unpack(void *state, size_t offset, const void *src,
-                                  size_t length);
-
-    static ucs_status_t dt_err_unpack(void *state, size_t offset, const void *src,
-                                      size_t length);
-
-    static void dt_common_finish(void *state);
+    void check_offload_support(bool offload_required);
 
     virtual bool is_external_request();
 
-    static const uint32_t MAGIC = 0xd7d7d7d7U;
-    static ucp_generic_dt_ops test_dt_uint32_ops;
-    static ucp_generic_dt_ops test_dt_uint32_err_ops;
-    static ucp_generic_dt_ops test_dt_uint8_ops;
-    static int dt_gen_start_count;
-    static int dt_gen_finish_count;
     static ucp_context_attr_t ctx_attr;
 private:
     int get_worker_index(int buf_index);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_match.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_match.cc
index d4279ef7f..9d0919c96 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_match.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_match.cc
@@ -17,7 +17,11 @@ public:
     virtual void init()
     {
         m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_ENABLE", "y"));
+        if (RUNNING_ON_VALGRIND) {
+            m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_MAX_BCOPY", "8k"));
+        }
         modify_config("TM_THRESH",  "1");
+
         test_ucp_tag::init();
         ucp_test_param param = GetParam();
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_offload.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_offload.cc
index a30b9367b..7ea54ddd6 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_offload.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_offload.cc
@@ -6,10 +6,9 @@
 
 #include "test_ucp_tag.h"
 
-#include <common/test_helpers.h>
+#include "ucp_datatype.h"
 
 extern "C" {
-#include <ucp/core/ucp_ep.h>
 #include <ucp/core/ucp_worker.h>
 #include <ucp/tag/tag_match.h>
 }
@@ -20,12 +19,8 @@ public:
     void init()
     {
         m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_ENABLE", "y"));
-
         test_ucp_tag::init();
-        if (!(sender().ep()->flags & UCP_EP_FLAG_TAG_OFFLOAD_ENABLED)) {
-            test_ucp_tag::cleanup();
-            UCS_TEST_SKIP_R("no tag offload");
-        }
+        check_offload_support(true);
     }
 
     request* recv_nb_and_check(void *buffer, size_t count, ucp_datatype_t dt,
@@ -37,30 +32,31 @@ public:
         return req;
     }
 
-    void send_b_from(entity &se, const void *buffer, size_t count,
-                     ucp_datatype_t datatype, ucp_tag_t tag)
+    void send_recv(entity &se, ucp_tag_t tag, size_t length)
     {
+        std::vector<uint8_t> sendbuf(length);
+        std::vector<uint8_t> recvbuf(length);
 
-        request *req = (request*)ucp_tag_send_nb(se.ep(), buffer, count,
-                                                 datatype, tag, send_callback);
-        if (UCS_PTR_IS_ERR(req)) {
-            ASSERT_UCS_OK(UCS_PTR_STATUS(req));
-        } else if (req != NULL) {
-            wait(req);
-            request_free(req);
+        request *rreq = recv_nb_and_check(&recvbuf[0], length, DATATYPE, tag,
+                                          UCP_TAG_MASK_FULL);
+        request *sreq = (request*)ucp_tag_send_nb(se.ep(), &sendbuf[0], length,
+                                                  DATATYPE, tag, send_callback);
+        if (UCS_PTR_IS_ERR(sreq)) {
+            ASSERT_UCS_OK(UCS_PTR_STATUS(sreq));
+        } else if (sreq != NULL) {
+            wait(sreq);
+            request_free(sreq);
         }
+
+        wait(rreq);
+        request_free(rreq);
     }
 
     void activate_offload(entity &se, ucp_tag_t tag = 0x11)
     {
         uint64_t small_val = 0xFAFA;
 
-        request *req = recv_nb_and_check(&small_val, sizeof(small_val),
-                                         DATATYPE, tag, UCP_TAG_MASK_FULL);
-
-        send_b_from(se, &small_val, sizeof(small_val), DATATYPE, tag);
-        wait(req);
-        request_free(req);
+        send_recv(se, tag, sizeof(small_val));
     }
 
     void req_cancel(entity &e, request *req)
@@ -293,18 +289,19 @@ public:
 
         // TODO: add more tls which support tag offloading
         std::vector<std::string> tls;
-        tls.push_back("rc");
+        tls.push_back("dc_x");
         tls.push_back("dc");
         tls.push_back("rc_x");
-        tls.push_back("dc_x");
         ucp_test_param params = GetParam();
 
+        // Create new entity and add to to the end of vector
+        // (thus it will be receiver without any connections)
+        create_entity(false);
         for (std::vector<std::string>::const_iterator i = tls.begin();
              i != tls.end(); ++i) {
             params.transports.clear();
             params.transports.push_back(*i);
             create_entity(true, params);
-            e(0).connect(&receiver(), get_ep_params());
         }
     }
 
@@ -320,6 +317,15 @@ public:
         return (i << 48) | t;
     }
 
+    void activate_offload_hashing(entity &se, ucp_tag_t tag)
+    {
+        se.connect(&receiver(), get_ep_params());
+        // Need to send twice, since the first message may not enable hashing
+        // (num_active_iface on worker is increased after unexpected offload handler)
+        send_recv(se, tag, 2048);
+        send_recv(se, tag, 2048);
+    }
+
     void post_recv_and_check(entity &e, unsigned sw_count, ucp_tag_t tag,
                              ucp_tag_t tag_mask)
     {
@@ -342,19 +348,20 @@ UCS_TEST_P(test_ucp_tag_offload_multi, recv_from_multi)
     ucp_tag_t tag = 0x11;
 
     // Activate first offload iface. Tag hashing is not done yet, since we
-    // have only one iface so far.
-    activate_offload(e(0), make_tag(e(0), tag));
+    // have only one active iface so far.
+    activate_offload_hashing(e(0), make_tag(e(0), tag));
     EXPECT_EQ(0u, kh_size(&receiver().worker()->tm.offload.tag_hash));
 
     // Activate second offload iface. The tag has been added to the hash.
     // From now requests will be offloaded only for those tags which are
     // in the hash.
-    activate_offload(e(1), make_tag(e(1), tag));
+    activate_offload_hashing(e(1), make_tag(e(1), tag));
     EXPECT_EQ(1u, kh_size(&receiver().worker()->tm.offload.tag_hash));
 
     // Need to send a message on the first iface again, for its 'tag_sender'
     // part of the tag to be added to the hash.
-    activate_offload(e(0), make_tag(e(0), tag));
+    send_recv(e(0), make_tag(e(0), tag), 2048);
+    EXPECT_EQ(2u, kh_size(&receiver().worker()->tm.offload.tag_hash));
 
     // Now requests from first two senders should be always offloaded regardless
     // of the tag value. Tag does not matter, because hashing is done with
@@ -367,11 +374,15 @@ UCS_TEST_P(test_ucp_tag_offload_multi, recv_from_multi)
     // sender and its 'tag_sender_mask' is not added to the hash yet.
     post_recv_and_check(e(2), 1u, tag, UCP_TAG_MASK_FULL);
 
-    activate_offload(e(2), make_tag(e(2), tag));
+    activate_offload_hashing(e(2), make_tag(e(2), tag));
+    EXPECT_EQ(3u, kh_size(&receiver().worker()->tm.offload.tag_hash));
+
     // Check that this sender was added as well
     post_recv_and_check(e(2), 0u, tag + 1, UCP_TAG_MASK_FULL);
 }
 
+// Do not include SM transports, because they would be selected for tag matching.
+// And since they do not support TM offload, this test would be skipped.
 UCP_INSTANTIATE_TEST_CASE_TLS(test_ucp_tag_offload_multi, all_rcdc,
                               "\\rc,\\dc,\\ud,rc_x,dc_x")
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_perf.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_perf.cc
index 8701be5a1..60fcf6e62 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_perf.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_perf.cc
@@ -6,8 +6,6 @@
 
 #include "test_ucp_tag.h"
 
-#include <common/test_helpers.h>
-
 
 class test_ucp_tag_perf : public test_ucp_tag {
 public:
@@ -36,7 +34,7 @@ double test_ucp_tag_perf::check_perf(size_t count, bool is_exp)
 
         for (size_t i = 0; i < count; ++i) {
             request *rreq = recv_nb(NULL, 0, DATATYPE, i, TAG_MASK);
-            ucs_assert(!UCS_PTR_IS_ERR(rreq));
+            assert(!UCS_PTR_IS_ERR(rreq));
             EXPECT_FALSE(rreq->completed);
             rreqs.push_back(rreq);
         }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_xfer.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_xfer.cc
index e5a5249d3..068da4563 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_xfer.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_tag_xfer.cc
@@ -7,12 +7,13 @@
 
 #include "test_ucp_tag.h"
 
+#include "ucp_datatype.h"
+
 extern "C" {
 #include <ucp/core/ucp_ep.inl>
 #include <ucs/datastruct/queue.h>
 }
 
-#include <common/test_helpers.h>
 #include <iostream>
 
 
@@ -32,6 +33,12 @@ public:
         } else if (GetParam().variant == VARIANT_RNDV_AUTO) {
             modify_config("RNDV_SCHEME", "auto");
         }
+        modify_config("MAX_EAGER_LANES", "2");
+        modify_config("MAX_RNDV_LANES", "2");
+        m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_ENABLE", "y"));
+        if (RUNNING_ON_VALGRIND) {
+            m_env.push_back(new ucs::scoped_setenv("UCX_RC_TM_MAX_BCOPY", "8k"));
+        }
         test_ucp_tag::init();
     }
 
@@ -87,6 +94,10 @@ protected:
     typedef void (test_ucp_tag_xfer::* xfer_func_t)(size_t size, bool expected,
                                                     bool sync, bool truncated);
 
+    size_t do_xfer(const void *sendbuf, void *recvbuf, size_t count,
+                   ucp_datatype_t send_dt, ucp_datatype_t recv_dt,
+                   bool expected, bool sync, bool truncated);
+
     void test_xfer(xfer_func_t func, bool expected, bool sync, bool truncated);
     void test_run_xfer(bool send_contig, bool recv_contig,
                        bool expected, bool sync, bool truncated);
@@ -97,16 +108,15 @@ protected:
     void test_xfer_probe(bool send_contig, bool recv_contig,
                          bool expected, bool sync);
 
-private:
-    size_t do_xfer(const void *sendbuf, void *recvbuf, size_t count,
-                   ucp_datatype_t send_dt, ucp_datatype_t recv_dt,
-                   bool expected, bool sync, bool truncated);
+    void test_xfer_len_offset();
 
+private:
     request* do_send(const void *sendbuf, size_t count, ucp_datatype_t dt, bool sync);
 
     static const uint64_t SENDER_TAG = 0x111337;
     static const uint64_t RECV_MASK  = 0xffff;
     static const uint64_t RECV_TAG   = 0x1337;
+    ucs::ptr_vector<ucs::scoped_setenv> m_env;
 
 };
 
@@ -179,7 +189,7 @@ void test_ucp_tag_xfer::test_xfer_prepare_bufs(uint8_t *sendbuf, uint8_t *recvbu
         *send_dt = DATATYPE;
     } else {
         /* the sender has a generic datatype */
-        status = ucp_dt_create_generic(&test_dt_uint8_ops, NULL, send_dt);
+        status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, NULL, send_dt);
         ASSERT_UCS_OK(status);
     }
 
@@ -188,7 +198,7 @@ void test_ucp_tag_xfer::test_xfer_prepare_bufs(uint8_t *sendbuf, uint8_t *recvbu
         *recv_dt = DATATYPE;
     } else {
         /* the receiver has a generic datatype */
-        status = ucp_dt_create_generic(&test_dt_uint8_ops, NULL, recv_dt);
+        status = ucp_dt_create_generic(&ucp::test_dt_uint8_ops, NULL, recv_dt);
         /* the recvbuf can be NULL because we only validate the received data in the
         * unpack function - we don't copy it to the recvbuf */
         ASSERT_UCS_OK(status);
@@ -207,8 +217,8 @@ void test_ucp_tag_xfer::test_run_xfer(bool send_contig, bool recv_contig,
         skip_err_handling();
     }
 
-    dt_gen_start_count  = 0;
-    dt_gen_finish_count = 0;
+    ucp::dt_gen_start_count  = 0;
+    ucp::dt_gen_finish_count = 0;
 
     if (send_contig) {
         /* the sender has a contig datatype for the data buffer */
@@ -258,8 +268,8 @@ void test_ucp_tag_xfer::test_xfer_probe(bool send_contig, bool recv_contig,
         UCS_TEST_SKIP_R("loop-back unsupported");
     }
 
-    dt_gen_start_count  = 0;
-    dt_gen_finish_count = 0;
+    ucp::dt_gen_start_count  = 0;
+    ucp::dt_gen_finish_count = 0;
 
     sendbuf = (uint8_t*) malloc(count * sizeof(*sendbuf));
     recvbuf = (uint8_t*) malloc(count * sizeof(*recvbuf));
@@ -335,23 +345,23 @@ void test_ucp_tag_xfer::test_xfer_generic(size_t size, bool expected, bool sync,
     ucs_status_t status;
     size_t recvd;
 
-    dt_gen_start_count  = 0;
-    dt_gen_finish_count = 0;
+    ucp::dt_gen_start_count  = 0;
+    ucp::dt_gen_finish_count = 0;
 
     /* if count is zero, truncation has no effect */
     if ((truncated) && (!count)) {
         truncated = false;
     }
 
-    status = ucp_dt_create_generic(&test_dt_uint32_ops, this, &dt);
+    status = ucp_dt_create_generic(&ucp::test_dt_uint32_ops, NULL, &dt);
     ASSERT_UCS_OK(status);
 
     recvd = do_xfer(NULL, NULL, count, dt, dt, expected, sync, truncated);
     if (!truncated) {
         EXPECT_EQ(count * sizeof(uint32_t), recvd);
     }
-    EXPECT_EQ(2, dt_gen_start_count);
-    EXPECT_EQ(2, dt_gen_finish_count);
+    EXPECT_EQ(2, ucp::dt_gen_start_count);
+    EXPECT_EQ(2, ucp::dt_gen_finish_count);
 
     ucp_dt_destroy(dt);
 }
@@ -389,10 +399,10 @@ void test_ucp_tag_xfer::test_xfer_generic_err(size_t size, bool expected,
     ucs_status_t status;
     request *rreq, *sreq;
 
-    dt_gen_start_count  = 0;
-    dt_gen_finish_count = 0;
+    ucp::dt_gen_start_count  = 0;
+    ucp::dt_gen_finish_count = 0;
 
-    status = ucp_dt_create_generic(&test_dt_uint32_err_ops, this, &dt);
+    status = ucp_dt_create_generic(&ucp::test_dt_uint32_err_ops, this, &dt);
     ASSERT_UCS_OK(status);
 
     if (expected) {
@@ -417,8 +427,8 @@ void test_ucp_tag_xfer::test_xfer_generic_err(size_t size, bool expected,
     /* the generic unpack function is expected to fail */
     EXPECT_EQ(UCS_ERR_NO_MEMORY, rreq->status);
     request_release(rreq);
-    EXPECT_EQ(2, dt_gen_start_count);
-    EXPECT_EQ(2, dt_gen_finish_count);
+    EXPECT_EQ(2, ucp::dt_gen_start_count);
+    EXPECT_EQ(2, ucp::dt_gen_finish_count);
     ucp_dt_destroy(dt);
 }
 
@@ -472,7 +482,7 @@ size_t test_ucp_tag_xfer::do_xfer(const void *sendbuf, void *recvbuf,
 
     recvd = rreq->info.length;
     if (!truncated) {
-        ASSERT_UCS_OK(rreq->status);
+        EXPECT_UCS_OK(rreq->status);
         EXPECT_EQ((ucp_tag_t)SENDER_TAG, rreq->info.sender_tag);
     } else {
         EXPECT_EQ(UCS_ERR_MESSAGE_TRUNCATED, rreq->status);
@@ -482,11 +492,66 @@ size_t test_ucp_tag_xfer::do_xfer(const void *sendbuf, void *recvbuf,
     return recvd;
 }
 
+void test_ucp_tag_xfer::test_xfer_len_offset()
+{
+    const size_t max_offset  = 128;
+    const size_t max_length  = 64 * 1024;
+    const size_t min_length  = 1024;
+    const size_t offset_step = 16;
+    const size_t length_step = 16;
+    const size_t buf_size    = max_length + max_offset + 2;
+    ucp_datatype_t type      = ucp_dt_make_contig(1);
+    void *send_buf           = 0;
+    void *recv_buf           = 0;;
+    size_t offset;
+    size_t length;
+    ucs::detail::message_stream *ms;
+
+    skip_err_handling();
+    if (RUNNING_ON_VALGRIND) {
+        UCS_TEST_SKIP_R("valgrind");
+    }
+
+    EXPECT_EQ(posix_memalign(&send_buf, 8192, buf_size), 0);
+    EXPECT_EQ(posix_memalign(&recv_buf, 8192, buf_size), 0);
+
+    memset(send_buf, 0, buf_size);
+    memset(recv_buf, 0, buf_size);
+
+    for (offset = 0; offset <= max_offset; offset += offset_step) {
+        if (!offset || ucs_is_pow2(offset)) {
+            ms = new ucs::detail::message_stream("INFO");
+            *ms << "offset: " << offset << ": ";
+        } else {
+            ms = NULL;
+        }
+        for (length = min_length; length <= max_length; length += length_step) {
+            if (ms && ucs_is_pow2(length)) {
+                *ms << length << " ";
+                fflush(stdout);
+            }
+
+            do_xfer((char*)send_buf + offset, (char*)recv_buf + offset,
+                    length, type, type, true, true, false);
+            do_xfer((char*)send_buf + max_offset - offset,
+                    (char*)recv_buf + max_offset - offset,
+                    length, type, type, true, true, false);
+        }
+        if (ms) {
+            delete(ms);
+        }
+    }
+
+    free(recv_buf);
+    free(send_buf);
+}
+
 UCS_TEST_P(test_ucp_tag_xfer, contig_exp) {
     test_xfer(&test_ucp_tag_xfer::test_xfer_contig, true, false, false);
 }
 
-UCS_TEST_P(test_ucp_tag_xfer, contig_exp_truncated, "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_xfer, contig_exp_truncated) {
+    check_offload_support(false);
     test_xfer(&test_ucp_tag_xfer::test_xfer_contig, true, false, true);
 }
 
@@ -523,6 +588,11 @@ UCS_TEST_P(test_ucp_tag_xfer, generic_err_exp) {
 }
 
 UCS_TEST_P(test_ucp_tag_xfer, generic_err_unexp) {
+#if HAVE_DC_DV
+    if (GetParam().transports.front().compare("dc_x") == 0) {
+        UCS_TEST_SKIP_R("DCI stuck bug");
+    }
+#endif
     test_xfer(&test_ucp_tag_xfer::test_xfer_generic_err, false, false, false);
 }
 
@@ -544,6 +614,11 @@ UCS_TEST_P(test_ucp_tag_xfer, contig_exp_sync) {
     test_xfer(&test_ucp_tag_xfer::test_xfer_contig, true, true, false);
 }
 
+UCS_TEST_P(test_ucp_tag_xfer, contig_exp_sync_zcopy, "ZCOPY_THRESH=1000") {
+    skip_loopback();
+    test_xfer(&test_ucp_tag_xfer::test_xfer_contig, true, true, false);
+}
+
 UCS_TEST_P(test_ucp_tag_xfer, contig_unexp_sync) {
     test_xfer(&test_ucp_tag_xfer::test_xfer_contig, false, true, false);
 }
@@ -634,8 +709,8 @@ UCS_TEST_P(test_ucp_tag_xfer, send_contig_recv_contig_exp_rndv, "RNDV_THRESH=100
 }
 
 UCS_TEST_P(test_ucp_tag_xfer, send_contig_recv_contig_exp_rndv_truncated, "RNDV_THRESH=1000",
-                                                                          "ZCOPY_THRESH=1248576",
-                                                                          "RC_TM_ENABLE?=n") {
+                                                                          "ZCOPY_THRESH=1248576") {
+    check_offload_support(false);
     test_run_xfer(true, true, true, false, true);
 }
 
@@ -878,6 +953,46 @@ UCS_TEST_P(test_ucp_tag_xfer, send_contig_recv_generic_exp_rndv_probe_zcopy, "RN
     test_xfer_probe(true, false, true, false);
 }
 
+UCS_TEST_P(test_ucp_tag_xfer, test_xfer_len_offset, "RNDV_THRESH=1000") {
+    test_xfer_len_offset();
+}
+
+UCS_TEST_P(test_ucp_tag_xfer, iov_with_empty_buffers, "ZCOPY_THRESH=512") {
+    const size_t iovcnt    = ucp::data_type_desc_t::MAX_IOV;
+    const size_t size      = 1024;
+    const int    expected  = 1;
+    const int    sync      = 0;
+    const int    truncated = 0;
+
+    std::vector<char> sendbuf(size, 0);
+    std::vector<char> recvbuf(size, 0);
+    ucp_dt_iov_t iovec[iovcnt];
+
+    ucs::fill_random(sendbuf);
+
+    /* initialize iovec with MAX_IOV-1 empty buffers and one non-empty */
+    for (size_t i = 0; i < iovcnt - 1; ++i) {
+        iovec[i].buffer = NULL;
+        iovec[i].length = 0;
+    }
+
+    /* coverity[escape] */
+    iovec[iovcnt - 1].buffer = &sendbuf[0];
+    iovec[iovcnt - 1].length = size;
+
+    ucp::data_type_desc_t recv_dt_desc(DATATYPE_IOV, recvbuf.data(),
+                                       recvbuf.size(), iovcnt);
+
+    size_t recvd = do_xfer(iovec, recv_dt_desc.buf(), iovcnt,
+                           DATATYPE_IOV, DATATYPE_IOV, expected, 0,
+                           truncated);
+
+    ASSERT_EQ(sendbuf.size(), recvd);
+    EXPECT_TRUE(!check_buffers(sendbuf, recvbuf, recvd, iovcnt,
+                               recv_dt_desc.count(), size, expected, sync,
+                               "IOV"));
+}
+
 UCP_INSTANTIATE_TEST_CASE(test_ucp_tag_xfer)
 
 
@@ -924,8 +1039,8 @@ public:
 };
 
 
-UCS_TEST_P(test_ucp_tag_stats, eager_expected, "RNDV_THRESH=1248576",
-                                               "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, eager_expected, "RNDV_THRESH=1248576") {
+    check_offload_support(false);
     test_run_xfer(true, true, true, false, false);
     validate_counters(UCP_EP_STAT_TAG_TX_EAGER,
                       UCP_WORKER_STAT_TAG_RX_EAGER_MSG);
@@ -936,8 +1051,8 @@ UCS_TEST_P(test_ucp_tag_stats, eager_expected, "RNDV_THRESH=1248576",
     EXPECT_EQ(cnt, 0ul);
 }
 
-UCS_TEST_P(test_ucp_tag_stats, eager_unexpected, "RNDV_THRESH=1248576",
-                                                 "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, eager_unexpected, "RNDV_THRESH=1248576") {
+    check_offload_support(false);
     test_run_xfer(true, true, false, false, false);
     validate_counters(UCP_EP_STAT_TAG_TX_EAGER,
                       UCP_WORKER_STAT_TAG_RX_EAGER_MSG);
@@ -947,8 +1062,8 @@ UCS_TEST_P(test_ucp_tag_stats, eager_unexpected, "RNDV_THRESH=1248576",
     EXPECT_GT(cnt, 0ul);
 }
 
-UCS_TEST_P(test_ucp_tag_stats, sync_expected, "RNDV_THRESH=1248576",
-                                              "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, sync_expected, "RNDV_THRESH=1248576") {
+    check_offload_support(false);
     skip_loopback();
     test_run_xfer(true, true, true, true, false);
     validate_counters(UCP_EP_STAT_TAG_TX_EAGER_SYNC,
@@ -960,8 +1075,8 @@ UCS_TEST_P(test_ucp_tag_stats, sync_expected, "RNDV_THRESH=1248576",
     EXPECT_EQ(cnt, 0ul);
 }
 
-UCS_TEST_P(test_ucp_tag_stats, sync_unexpected, "RNDV_THRESH=1248576",
-                                                "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, sync_unexpected, "RNDV_THRESH=1248576") {
+    check_offload_support(false);
     skip_loopback();
     test_run_xfer(true, true, false, true, false);
     validate_counters(UCP_EP_STAT_TAG_TX_EAGER_SYNC,
@@ -972,15 +1087,15 @@ UCS_TEST_P(test_ucp_tag_stats, sync_unexpected, "RNDV_THRESH=1248576",
     EXPECT_GT(cnt, 0ul);
 }
 
-UCS_TEST_P(test_ucp_tag_stats, rndv_expected, "RNDV_THRESH=1000",
-                                              "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, rndv_expected, "RNDV_THRESH=1000") {
+    check_offload_support(false);
     test_run_xfer(true, true, true, false, false);
     validate_counters(UCP_EP_STAT_TAG_TX_RNDV,
                       UCP_WORKER_STAT_TAG_RX_RNDV_EXP);
 }
 
-UCS_TEST_P(test_ucp_tag_stats, rndv_unexpected, "RNDV_THRESH=1000",
-                                                "RC_TM_ENABLE?=n") {
+UCS_TEST_P(test_ucp_tag_stats, rndv_unexpected, "RNDV_THRESH=1000") {
+    check_offload_support(false);
     test_run_xfer(true, true, false, false, false);
     validate_counters(UCP_EP_STAT_TAG_TX_RNDV,
                       UCP_WORKER_STAT_TAG_RX_RNDV_UNEXP);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_wireup.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_wireup.cc
index 055c940d3..dfed76d13 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_wireup.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/test_ucp_wireup.cc
@@ -14,20 +14,23 @@
 extern "C" {
 #include <ucp/wireup/address.h>
 #include <ucp/proto/proto.h>
+#include <ucp/core/ucp_ep.inl>
 }
 
 class test_ucp_wireup : public ucp_test {
 public:
     static std::vector<ucp_test_param>
-    enum_test_params(const ucp_params_t& ctx_params,
-                     const std::string& name,
-                     const std::string& test_case_name,
-                     const std::string& tls);
+    enum_test_params_features(const ucp_params_t& ctx_params,
+                              const std::string& name,
+                              const std::string& test_case_name,
+                              const std::string& tls,
+                              uint64_t features);
 
 protected:
     enum {
         TEST_RMA,
-        TEST_TAG
+        TEST_TAG,
+        TEST_STREAM
     };
 
     typedef uint64_t               elem_type;
@@ -41,13 +44,17 @@ protected:
     virtual void init();
     virtual void cleanup();
 
-    void send_nb(ucp_ep_h ep, size_t length, int repeat, std::vector<void*>& reqs);
+    void send_nb(ucp_ep_h ep, size_t length, int repeat, std::vector<void*>& reqs,
+                 uint64_t send_data = SEND_DATA);
 
-    void send_b(ucp_ep_h ep,         size_t length, int repeat);
+    void send_b(ucp_ep_h ep, size_t length, int repeat,
+                uint64_t send_data = SEND_DATA);
 
-    void recv_b(ucp_worker_h worker, size_t length, int repeat);
+    void recv_b(ucp_worker_h worker, ucp_ep_h ep, size_t length, int repeat,
+                uint64_t recv_data = SEND_DATA);
 
-    void send_recv(ucp_ep_h ep, ucp_worker_h worker, size_t vecsize, int repeat);
+    void send_recv(ucp_ep_h send_ep, ucp_worker_h recv_worker, ucp_ep_h recv_ep,
+                   size_t vecsize, int repeat);
 
     void waitall(std::vector<void*> reqs);
 
@@ -56,50 +63,76 @@ protected:
     void disconnect(ucp_test::entity &e);
 
 private:
-    vec_type   m_send_data;
-    vec_type   m_recv_data;
-    ucp_mem_h  m_memh1, m_memh2;
-    ucp_rkey_h m_rkey1, m_rkey2;
+    vec_type                               m_send_data;
+    vec_type                               m_recv_data;
+    ucs::handle<ucp_mem_h, ucp_context_h>  m_memh_sender;
+    ucs::handle<ucp_mem_h, ucp_context_h>  m_memh_receiver;
+    std::vector< ucs::handle<ucp_rkey_h> > m_rkeys;
 
     void clear_recv_data();
 
-    ucp_rkey_h get_rkey(ucp_mem_h memh);
+    ucp_rkey_h get_rkey(ucp_ep_h ep, ucp_mem_h memh);
 
     static void send_completion(void *request, ucs_status_t status);
 
-    static void recv_completion(void *request, ucs_status_t status,
-                                ucp_tag_recv_info_t *info);
+    static void tag_recv_completion(void *request, ucs_status_t status,
+                                    ucp_tag_recv_info_t *info);
+
+    static void stream_recv_completion(void *request, ucs_status_t status,
+                                       size_t length);
+
+    static void unmap_memh(ucp_mem_h memh, ucp_context_h context);
 };
 
 std::vector<ucp_test_param>
-test_ucp_wireup::enum_test_params(const ucp_params_t& ctx_params,
-                                  const std::string& name,
-                                  const std::string& test_case_name,
-                                  const std::string& tls)
+test_ucp_wireup::enum_test_params_features(const ucp_params_t& ctx_params,
+                                           const std::string& name,
+                                           const std::string& test_case_name,
+                                           const std::string& tls,
+                                           uint64_t features)
 {
     std::vector<ucp_test_param> result;
     ucp_params_t tmp_ctx_params = ctx_params;
 
-    tmp_ctx_params.features = UCP_FEATURE_RMA;
-    generate_test_params_variant(tmp_ctx_params, name, test_case_name + "/rma",
-                                 tls, TEST_RMA, result);
+    if (features & UCP_FEATURE_RMA) {
+        tmp_ctx_params.features = UCP_FEATURE_RMA;
+        generate_test_params_variant(tmp_ctx_params, name, test_case_name + "/rma",
+                                     tls, TEST_RMA, result);
+    }
+
+    if (features & UCP_FEATURE_TAG) {
+        tmp_ctx_params.features = UCP_FEATURE_TAG;
+        generate_test_params_variant(tmp_ctx_params, name, test_case_name + "/tag",
+                                     tls, TEST_TAG, result);
+    }
 
-    tmp_ctx_params.features = UCP_FEATURE_TAG;
-    generate_test_params_variant(tmp_ctx_params, name, test_case_name + "/tag",
-                                 tls, TEST_TAG, result);
+    if (features & UCP_FEATURE_STREAM) {
+        tmp_ctx_params.features = UCP_FEATURE_STREAM;
+        generate_test_params_variant(tmp_ctx_params, name, test_case_name + "/stream",
+                                     tls, TEST_STREAM, result);
+    }
 
     return result;
 }
 
+void test_ucp_wireup::unmap_memh(ucp_mem_h memh, ucp_context_h context)
+{
+    ucs_status_t status = ucp_mem_unmap(context, memh);
+    if (status != UCS_OK) {
+        ucs_warn("failed to unmap memory: %s", ucs_status_string(status));
+    }
+}
+
 void test_ucp_wireup::init() {
     ucp_test::init();
 
-    m_send_data.resize(BUFFER_LENGTH, elem_type(SEND_DATA));
+    m_send_data.resize(BUFFER_LENGTH, 0);
     m_recv_data.resize(BUFFER_LENGTH, 0);
 
-    if (GetParam().variant == UCP_FEATURE_RMA) {
+    if (GetParam().variant == TEST_RMA) {
         ucs_status_t status;
         ucp_mem_map_params_t params;
+        ucp_mem_h memh;
 
         params.field_mask = UCP_MEM_MAP_PARAM_FIELD_ADDRESS |
                             UCP_MEM_MAP_PARAM_FIELD_LENGTH |
@@ -108,28 +141,33 @@ void test_ucp_wireup::init() {
         params.length     = m_recv_data.size() * sizeof(m_recv_data[0]);
         params.flags      = 0;
 
-        status = ucp_mem_map(receiver().ucph(), &params, &m_memh1);
+        status = ucp_mem_map(sender().ucph(), &params, &memh);
         ASSERT_UCS_OK(status);
+        m_memh_sender.reset(memh, unmap_memh, sender().ucph());
 
-        status = ucp_mem_map(sender().ucph(), &params, &m_memh2);
+        status = ucp_mem_map(receiver().ucph(), &params, &memh);
         ASSERT_UCS_OK(status);
-
-        m_rkey1 = get_rkey(m_memh1);
-        m_rkey2 = get_rkey(m_memh2);
+        m_memh_receiver.reset(memh, unmap_memh, receiver().ucph());
     }
 }
 
-ucp_rkey_h test_ucp_wireup::get_rkey(ucp_mem_h memh)
+ucp_rkey_h test_ucp_wireup::get_rkey(ucp_ep_h ep, ucp_mem_h memh)
 {
     void *rkey_buffer;
     size_t rkey_size;
     ucs_status_t status;
     ucp_rkey_h rkey;
 
-    status = ucp_rkey_pack(receiver().ucph(), memh, &rkey_buffer, &rkey_size);
+    if (memh == m_memh_receiver) {
+        status = ucp_rkey_pack(receiver().ucph(), memh, &rkey_buffer, &rkey_size);
+    } else if (memh == m_memh_sender) {
+        status = ucp_rkey_pack(sender().ucph(), memh, &rkey_buffer, &rkey_size);
+    } else {
+        status = UCS_ERR_INVALID_PARAM;
+    }
     ASSERT_UCS_OK(status);
 
-    status = ucp_ep_rkey_unpack(sender().ep(), rkey_buffer, &rkey);
+    status = ucp_ep_rkey_unpack(ep, rkey_buffer, &rkey);
     ASSERT_UCS_OK(status);
 
     ucp_rkey_buffer_release(rkey_buffer);
@@ -138,12 +176,9 @@ ucp_rkey_h test_ucp_wireup::get_rkey(ucp_mem_h memh)
 }
 
 void test_ucp_wireup::cleanup() {
-    if (GetParam().variant == UCP_FEATURE_RMA) {
-        ucp_rkey_destroy(m_rkey1);
-        ucp_mem_unmap(receiver().ucph(), m_memh1);
-        ucp_rkey_destroy(m_rkey2);
-        ucp_mem_unmap(sender().ucph(), m_memh2);
-    }
+    m_rkeys.clear();
+    m_memh_sender.reset();
+    m_memh_receiver.reset();
     ucp_test::cleanup();
 }
 
@@ -152,9 +187,10 @@ void test_ucp_wireup::clear_recv_data() {
 }
 
 void test_ucp_wireup::send_nb(ucp_ep_h ep, size_t length, int repeat,
-                              std::vector<void*>& reqs)
+                              std::vector<void*>& reqs, uint64_t send_data)
 {
-    if (GetParam().variant == UCP_FEATURE_TAG) {
+    if (GetParam().variant == TEST_TAG) {
+        std::fill(m_send_data.begin(), m_send_data.end(), send_data);
         for (int i = 0; i < repeat; ++i) {
             void *req = ucp_tag_send_nb(ep, &m_send_data[0], length,
                                         DT_U64, TAG, send_completion);
@@ -164,47 +200,86 @@ void test_ucp_wireup::send_nb(ucp_ep_h ep, size_t length, int repeat,
                 ASSERT_UCS_OK(UCS_PTR_STATUS(req));
             }
         }
-    } else if (GetParam().variant == UCP_FEATURE_RMA) {
+    } else if (GetParam().variant == TEST_STREAM) {
+        std::fill(m_send_data.begin(), m_send_data.end(), send_data);
+        for (int i = 0; i < repeat; ++i) {
+            void *req = ucp_stream_send_nb(ep, &m_send_data[0], length, DT_U64,
+                                           send_completion, 0);
+            if (UCS_PTR_IS_PTR(req)) {
+                reqs.push_back(req);
+            } else {
+                ASSERT_UCS_OK(UCS_PTR_STATUS(req));
+            }
+        }
+    } else if (GetParam().variant == TEST_RMA) {
         clear_recv_data();
+
+        ucp_mem_h memh  = (sender().ucph() == ep->worker->context) ?
+                            m_memh_receiver : m_memh_sender;
+        ucp_rkey_h rkey = get_rkey(ep, memh);
+
+        m_rkeys.push_back(ucs::handle<ucp_rkey_h>(rkey, ucp_rkey_destroy));
+
         for (int i = 0; i < repeat; ++i) {
-            std::fill(m_send_data.begin(), m_send_data.end(), SEND_DATA + i);
-            ucs_status_t status;
-            status = ucp_put(ep, &m_send_data[0],
-                             m_send_data.size() * sizeof(m_send_data[0]),
-                             (uintptr_t)&m_recv_data[0],
-                             (sender().ep() == ep) ? m_rkey1 : m_rkey2);
-            ASSERT_UCS_OK(status);
+            std::fill(m_send_data.begin(), m_send_data.end(), send_data + i);
+            void *req = ucp_put_nb(ep, &m_send_data[0],
+                                   m_send_data.size() * sizeof(m_send_data[0]),
+                                   (uintptr_t)&m_recv_data[0], rkey,
+                                   send_completion);
+            if (UCS_PTR_IS_PTR(req)) {
+                reqs.push_back(req);
+            } else {
+                ASSERT_UCS_OK(UCS_PTR_STATUS(req));
+            }
         }
     }
 }
 
-void test_ucp_wireup::send_b(ucp_ep_h ep, size_t length, int repeat)
+void test_ucp_wireup::send_b(ucp_ep_h ep, size_t length, int repeat,
+                             uint64_t send_data)
 {
     std::vector<void*> reqs;
-    send_nb(ep, length, repeat, reqs);
+    send_nb(ep, length, repeat, reqs, send_data);
     waitall(reqs);
 }
 
-void test_ucp_wireup::recv_b(ucp_worker_h worker, size_t length, int repeat)
+void test_ucp_wireup::recv_b(ucp_worker_h worker, ucp_ep_h ep, size_t length,
+                             int repeat, uint64_t recv_data)
 {
-    if (GetParam().variant == UCP_FEATURE_TAG) {
+    if ((GetParam().variant == TEST_TAG) || (GetParam().variant == TEST_STREAM))
+    {
         for (int i = 0; i < repeat; ++i) {
+            size_t recv_length;
+            void *req;
+
             clear_recv_data();
-            void *req = ucp_tag_recv_nb(worker, &m_recv_data[0], length,
-                                        DT_U64, TAG, (ucp_tag_t)-1,
-                                        recv_completion);
+            if (GetParam().variant == TEST_TAG) {
+                req = ucp_tag_recv_nb(worker, &m_recv_data[0], length, DT_U64,
+                                      TAG, (ucp_tag_t)-1, tag_recv_completion);
+            } else if (GetParam().variant == TEST_STREAM) {
+                req = ucp_stream_recv_nb(ep, &m_recv_data[0], length, DT_U64,
+                                         stream_recv_completion, &recv_length,
+                                         UCP_STREAM_RECV_FLAG_WAITALL);
+            } else {
+                req = NULL;
+            }
             if (UCS_PTR_IS_PTR(req)) {
                 wait(req);
             } else {
                 ASSERT_UCS_OK(UCS_PTR_STATUS(req));
             }
+            EXPECT_EQ(recv_data, m_recv_data[0])
+                      << "repeat " << i << "/" << repeat;
             EXPECT_EQ(length,
-                      (size_t)std::count(m_recv_data.begin(), m_recv_data.begin() + length,
-                                         elem_type(SEND_DATA)));
+                      (size_t)std::count(m_recv_data.begin(),
+                                         m_recv_data.begin() + length,
+                                         recv_data));
         }
-    } else if (GetParam().variant == UCP_FEATURE_RMA) {
+    } else if (GetParam().variant == TEST_RMA) {
         for (size_t i = 0; i < length; ++i) {
-            while (m_recv_data[i] != SEND_DATA + repeat);
+            while (m_recv_data[i] != recv_data + repeat - 1) {
+                progress();
+            }
         }
     }
 }
@@ -213,18 +288,27 @@ void test_ucp_wireup::send_completion(void *request, ucs_status_t status)
 {
 }
 
-void test_ucp_wireup::recv_completion(void *request, ucs_status_t status,
-                                      ucp_tag_recv_info_t *info)
+void test_ucp_wireup::tag_recv_completion(void *request, ucs_status_t status,
+                                          ucp_tag_recv_info_t *info)
+{
+}
+
+void test_ucp_wireup::stream_recv_completion(void *request, ucs_status_t status,
+                                             size_t length)
 {
 }
 
-void test_ucp_wireup::send_recv(ucp_ep_h ep, ucp_worker_h worker,
-                                size_t length, int repeat)
+void test_ucp_wireup::send_recv(ucp_ep_h send_ep, ucp_worker_h recv_worker,
+                                ucp_ep_h recv_ep, size_t length, int repeat)
 {
     std::vector<void*> send_reqs;
-    send_nb(ep,     length, repeat, send_reqs);
-    recv_b (worker, length, repeat);
+    static uint64_t next_send_data = 0;
+    uint64_t send_data = next_send_data++;
+
+    send_nb(send_ep, length, repeat, send_reqs, send_data);
+    recv_b (recv_worker, recv_ep, length, repeat, send_data);
     waitall(send_reqs);
+    m_rkeys.clear();
 }
 
 void test_ucp_wireup::disconnect(ucp_ep_h ep) {
@@ -247,12 +331,22 @@ void test_ucp_wireup::waitall(std::vector<void*> reqs)
     }
 }
 
-UCS_TEST_P(test_ucp_wireup, address) {
+class test_ucp_wireup_1sided : public test_ucp_wireup {
+public:
+    static std::vector<ucp_test_param>
+    enum_test_params(const ucp_params_t& ctx_params, const std::string& name,
+                     const std::string& test_case_name, const std::string& tls)
+    {
+        return enum_test_params_features(ctx_params, name, test_case_name, tls,
+                                         UCP_FEATURE_RMA | UCP_FEATURE_TAG);
+    }
+};
+
+UCS_TEST_P(test_ucp_wireup_1sided, address) {
     ucs_status_t status;
     size_t size;
     void *buffer;
     unsigned order[UCP_MAX_RESOURCES];
-    const ucp_address_entry_t *ae;
     std::set<uint8_t> packed_dev_priorities, unpacked_dev_priorities;
     int tl;
 
@@ -268,30 +362,35 @@ UCS_TEST_P(test_ucp_wireup, address) {
         packed_dev_priorities.insert(sender().worker()->ifaces[tl].attr.priority);
     }
 
-    char name[UCP_WORKER_NAME_MAX];
-    uint64_t uuid;
-    unsigned address_count;
-    ucp_address_entry_t *address_list;
+    ucp_unpacked_address unpacked_address;
 
-    ucp_address_unpack(buffer, &uuid, name, sizeof(name), &address_count,
-                       &address_list);
-    EXPECT_EQ(sender().worker()->uuid, uuid);
-    EXPECT_EQ(std::string(ucp_worker_get_name(sender().worker())), std::string(name));
-    EXPECT_LE(address_count, static_cast<unsigned>(sender().ucph()->num_tls));
-    for (ae = address_list; ae < address_list + address_count; ++ae) {
+    status = ucp_address_unpack(buffer, &unpacked_address);
+    ASSERT_UCS_OK(status);
+
+    EXPECT_EQ(sender().worker()->uuid, unpacked_address.uuid);
+#if ENABLE_DEBUG_DATA
+    EXPECT_EQ(std::string(ucp_worker_get_name(sender().worker())),
+              std::string(unpacked_address.name));
+#endif
+    EXPECT_LE(unpacked_address.address_count,
+              static_cast<unsigned>(sender().ucph()->num_tls));
+
+    for (const ucp_address_entry_t *ae = unpacked_address.address_list;
+         ae < unpacked_address.address_list + unpacked_address.address_count;
+         ++ae) {
         unpacked_dev_priorities.insert(ae->iface_attr.priority);
     }
 
     /* TODO test addresses */
 
-    ucs_free(address_list);
+    ucs_free(unpacked_address.address_list);
     ucs_free(buffer);
     /* Make sure that the packed device priorities are equal to the unpacked
      * device priorities */
     ASSERT_TRUE(packed_dev_priorities == unpacked_dev_priorities);
 }
 
-UCS_TEST_P(test_ucp_wireup, empty_address) {
+UCS_TEST_P(test_ucp_wireup_1sided, empty_address) {
     ucs_status_t status;
     size_t size;
     void *buffer;
@@ -302,41 +401,40 @@ UCS_TEST_P(test_ucp_wireup, empty_address) {
     ASSERT_TRUE(buffer != NULL);
     ASSERT_GT(size, 0ul);
 
-    char name[UCP_WORKER_NAME_MAX];
-    uint64_t uuid;
-    unsigned address_count;
-    ucp_address_entry_t *address_list;
+    ucp_unpacked_address unpacked_address;
+
+    status = ucp_address_unpack(buffer, &unpacked_address);
+    ASSERT_UCS_OK(status);
 
-    ucp_address_unpack(buffer, &uuid, name, sizeof(name), &address_count,
-                       &address_list);
-    EXPECT_EQ(sender().worker()->uuid, uuid);
-    EXPECT_EQ(std::string(ucp_worker_get_name(sender().worker())), std::string(name));
-    EXPECT_LE(address_count, sender().ucph()->num_tls);
-    EXPECT_EQ(0u, address_count);
+    EXPECT_EQ(sender().worker()->uuid, unpacked_address.uuid);
+#if ENABLE_DEBUG_DATA
+    EXPECT_EQ(std::string(ucp_worker_get_name(sender().worker())),
+              std::string(unpacked_address.name));
+#endif
+    EXPECT_EQ(0u, unpacked_address.address_count);
 
-    ucs_free(address_list);
+    ucs_free(unpacked_address.address_list);
     ucs_free(buffer);
 }
 
-UCS_TEST_P(test_ucp_wireup, one_sided_wireup) {
+UCS_TEST_P(test_ucp_wireup_1sided, one_sided_wireup) {
     sender().connect(&receiver(), get_ep_params());
-    send_recv(sender().ep(), receiver().worker(), 1, 1);
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1, 1);
     flush_worker(sender());
 }
 
-UCS_TEST_P(test_ucp_wireup, two_sided_wireup) {
+UCS_TEST_P(test_ucp_wireup_1sided, one_sided_wireup_rndv, "RNDV_THRESH=1") {
     sender().connect(&receiver(), get_ep_params());
-    if (&sender() != &receiver()) {
-        receiver().connect(&sender(), get_ep_params());
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), BUFFER_LENGTH, 1);
+    if (is_loopback() && (GetParam().variant == TEST_TAG)) {
+        /* expect the endpoint to be connected to itself */
+        ucp_ep_h ep = sender().ep();
+        EXPECT_EQ((uintptr_t)ep, ucp_ep_dest_ep_ptr(ep));
     }
-
-    send_recv(sender().ep(), receiver().worker(), 1, 1);
     flush_worker(sender());
-    send_recv(receiver().ep(), sender().worker(), 1, 1);
-    flush_worker(receiver());
 }
 
-UCS_TEST_P(test_ucp_wireup, multi_wireup) {
+UCS_TEST_P(test_ucp_wireup_1sided, multi_wireup) {
     skip_loopback();
 
     const size_t count = 10;
@@ -346,53 +444,14 @@ UCS_TEST_P(test_ucp_wireup, multi_wireup) {
 
     /* connect from sender() to all the rest */
     for (size_t i = 0; i < count; ++i) {
-        sender().connect(&entities().at(i), get_ep_params());
-    }
-}
-
-UCS_TEST_P(test_ucp_wireup, reply_ep_send_before) {
-    skip_loopback();
-
-    sender().connect(&receiver(), get_ep_params());
-
-    if (GetParam().variant == TEST_TAG) {
-        /* Send a reply */
-        ucp_ep_connect_remote(sender().ep());
-        ucp_ep_h ep = ucp_worker_get_reply_ep(receiver().worker(),
-                                              sender().worker()->uuid);
-        send_recv(ep, sender().worker(), 1, 1);
-        flush_worker(sender());
-
-        disconnect(ep);
+        sender().connect(&entities().at(i), get_ep_params(), i);
     }
 }
 
-UCS_TEST_P(test_ucp_wireup, reply_ep_send_after) {
-    skip_loopback();
-
-    sender().connect(&receiver(), get_ep_params());
-
-    if (GetParam().variant == TEST_TAG) {
-        ucp_ep_connect_remote(sender().ep());
-
-        /* Make sure the wireup message arrives before sending a reply */
-        send_recv(sender().ep(), receiver().worker(), 1, 1);
-        flush_worker(sender());
-
-        /* Send a reply */
-        ucp_ep_h ep = ucp_worker_get_reply_ep(receiver().worker(), sender().worker()->uuid);
-        send_recv(ep, sender().worker(), 1, 1);
-
-        flush_worker(sender());
-
-        disconnect(ep);
-    }
-}
-
-UCS_TEST_P(test_ucp_wireup, stress_connect) {
+UCS_TEST_P(test_ucp_wireup_1sided, stress_connect) {
     for (int i = 0; i < 30; ++i) {
         sender().connect(&receiver(), get_ep_params());
-        send_recv(sender().ep(), receiver().worker(), 1,
+        send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1,
                   10000 / ucs::test_time_multiplier());
         if (!is_loopback()) {
             receiver().connect(&sender(), get_ep_params());
@@ -405,10 +464,11 @@ UCS_TEST_P(test_ucp_wireup, stress_connect) {
     }
 }
 
-UCS_TEST_P(test_ucp_wireup, stress_connect2) {
-    for (int i = 0; i < 1000 / ucs::test_time_multiplier(); ++i) {
+UCS_TEST_P(test_ucp_wireup_1sided, stress_connect2) {
+    int count = ucs_min(1000 / ucs::test_time_multiplier(), max_connections() / 2);
+    for (int i = 0; i < count; ++i) {
         sender().connect(&receiver(), get_ep_params());
-        send_recv(sender().ep(), receiver().worker(), 1, 1);
+        send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1, 1);
         if (&sender() != &receiver()) {
             receiver().connect(&sender(), get_ep_params());
         }
@@ -420,18 +480,7 @@ UCS_TEST_P(test_ucp_wireup, stress_connect2) {
     }
 }
 
-UCS_TEST_P(test_ucp_wireup, connect_disconnect) {
-    sender().connect(&receiver(), get_ep_params());
-    if (!is_loopback()) {
-        receiver().connect(&sender(), get_ep_params());
-    }
-    disconnect(sender());
-    if (!is_loopback()) {
-        disconnect(receiver());
-    }
-}
-
-UCS_TEST_P(test_ucp_wireup, disconnect_nonexistent) {
+UCS_TEST_P(test_ucp_wireup_1sided, disconnect_nonexistent) {
     skip_loopback();
     sender().connect(&receiver(), get_ep_params());
     disconnect(sender());
@@ -439,33 +488,41 @@ UCS_TEST_P(test_ucp_wireup, disconnect_nonexistent) {
     sender().destroy_worker();
 }
 
-UCS_TEST_P(test_ucp_wireup, disconnect_reconnect) {
+UCS_TEST_P(test_ucp_wireup_1sided, disconnect_reconnect) {
     sender().connect(&receiver(), get_ep_params());
     send_b(sender().ep(), 1000, 1);
     disconnect(sender());
-    recv_b(receiver().worker(), 1000, 1);
+    recv_b(receiver().worker(), receiver().ep(), 1000, 1);
 
     sender().connect(&receiver(), get_ep_params());
     send_b(sender().ep(), 1000, 1);
     disconnect(sender());
-    recv_b(receiver().worker(), 1000, 1);
+    recv_b(receiver().worker(), receiver().ep(), 1000, 1);
 }
 
-UCS_TEST_P(test_ucp_wireup, send_disconnect_onesided) {
+UCS_TEST_P(test_ucp_wireup_1sided, send_disconnect_onesided) {
     sender().connect(&receiver(), get_ep_params());
     send_b(sender().ep(), 1000, 100);
     disconnect(sender());
-    recv_b(receiver().worker(), 1000, 100);
+    recv_b(receiver().worker(), receiver().ep(), 1000, 100);
 }
 
-UCS_TEST_P(test_ucp_wireup, send_disconnect_onesided_nozcopy, "ZCOPY_THRESH=-1") {
+UCS_TEST_P(test_ucp_wireup_1sided, send_disconnect_onesided_nozcopy, "ZCOPY_THRESH=-1") {
     sender().connect(&receiver(), get_ep_params());
     send_b(sender().ep(), 1000, 100);
     disconnect(sender());
-    recv_b(receiver().worker(), 1000, 100);
+    recv_b(receiver().worker(), receiver().ep(), 1000, 100);
+}
+
+UCS_TEST_P(test_ucp_wireup_1sided, send_disconnect_onesided_wait) {
+    sender().connect(&receiver(), get_ep_params());
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), 8, 1);
+    send_b(sender().ep(), 1000, 200);
+    disconnect(sender());
+    recv_b(receiver().worker(), receiver().ep(), 1000, 200);
 }
 
-UCS_TEST_P(test_ucp_wireup, send_disconnect_reply1) {
+UCS_TEST_P(test_ucp_wireup_1sided, send_disconnect_reply1) {
     sender().connect(&receiver(), get_ep_params());
     if (!is_loopback()) {
         receiver().connect(&sender(), get_ep_params());
@@ -476,20 +533,20 @@ UCS_TEST_P(test_ucp_wireup, send_disconnect_reply1) {
         disconnect(sender());
     }
 
-    recv_b(receiver().worker(), 8, 1);
+    recv_b(receiver().worker(), receiver().ep(), 8, 1);
     send_b(receiver().ep(), 8, 1);
     disconnect(receiver());
-    recv_b(sender().worker(), 8, 1);
+    recv_b(sender().worker(), sender().ep(), 8, 1);
 }
 
-UCS_TEST_P(test_ucp_wireup, send_disconnect_reply2) {
+UCS_TEST_P(test_ucp_wireup_1sided, send_disconnect_reply2) {
     sender().connect(&receiver(), get_ep_params());
 
     send_b(sender().ep(), 8, 1);
     if (!is_loopback()) {
         disconnect(sender());
     }
-    recv_b(receiver().worker(), 8, 1);
+    recv_b(receiver().worker(), receiver().ep(),  8, 1);
 
     if (!is_loopback()) {
         receiver().connect(&sender(), get_ep_params());
@@ -497,18 +554,10 @@ UCS_TEST_P(test_ucp_wireup, send_disconnect_reply2) {
 
     send_b(receiver().ep(), 8, 1);
     disconnect(receiver());
-    recv_b(sender().worker(), 8, 1);
-}
-
-UCS_TEST_P(test_ucp_wireup, send_disconnect_onesided_wait) {
-    sender().connect(&receiver(), get_ep_params());
-    send_recv(sender().ep(), receiver().worker(), 8, 1);
-    send_b(sender().ep(), 1000, 200);
-    disconnect(sender());
-    recv_b(receiver().worker(), 1000, 200);
+    recv_b(sender().worker(), receiver().ep(), 8, 1);
 }
 
-UCS_TEST_P(test_ucp_wireup, disconnect_nb_onesided) {
+UCS_TEST_P(test_ucp_wireup_1sided, disconnect_nb_onesided) {
     sender().connect(&receiver(), get_ep_params());
 
     std::vector<void*> sreqs;
@@ -520,15 +569,155 @@ UCS_TEST_P(test_ucp_wireup, disconnect_nb_onesided) {
     }
 
     wait(dreq);
-    recv_b(receiver().worker(), 1000, 1000);
+    recv_b(receiver().worker(), receiver().ep(), 1000, 1000);
 
     waitall(sreqs);
+}
+
+UCS_TEST_P(test_ucp_wireup_1sided, multi_ep_1sided) {
+    const unsigned count = 10;
+
+    for (unsigned i = 0; i < count; ++i) {
+        sender().connect(&receiver(), get_ep_params(), i);
+    }
+
+    for (unsigned i = 0; i < count; ++i) {
+        send_recv(sender().ep(0, i), receiver().worker(), receiver().ep(), 8, 1);
+    }
+}
+
+UCP_INSTANTIATE_TEST_CASE(test_ucp_wireup_1sided)
+
+class test_ucp_wireup_2sided : public test_ucp_wireup {
+public:
+    static std::vector<ucp_test_param>
+    enum_test_params(const ucp_params_t& ctx_params, const std::string& name,
+                     const std::string& test_case_name, const std::string& tls)
+    {
+        return enum_test_params_features(ctx_params, name, test_case_name, tls,
+                                         UCP_FEATURE_RMA | UCP_FEATURE_TAG |
+                                         UCP_FEATURE_STREAM);
+    }
+
+protected:
+    void test_connect_loopback(bool delay_before_connect, bool enable_loopback);
+};
+
+UCS_TEST_P(test_ucp_wireup_2sided, two_sided_wireup) {
+    sender().connect(&receiver(), get_ep_params());
+    if (!is_loopback()) {
+        receiver().connect(&sender(), get_ep_params());
+    }
+
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1, 1);
+    flush_worker(sender());
+    send_recv(receiver().ep(), sender().worker(), sender().ep(), 1, 1);
+    flush_worker(receiver());
+}
+
+void test_ucp_wireup_2sided::test_connect_loopback(bool delay_before_connect,
+                                                   bool enable_loopback) {
+    ucp_ep_params_t params = test_ucp_wireup::get_ep_params();
+    if (!enable_loopback) {
+        params.field_mask |= UCP_EP_PARAM_FIELD_FLAGS;
+        params.flags      |= UCP_EP_PARAMS_FLAGS_NO_LOOPBACK;
+    }
+
+    for (int i = 0; i < 5; ++i) {
+        int base_index = i * 2;
+        sender().connect(&sender(), params, base_index);
+        ucp_ep_h ep1 = sender().ep(0, base_index);
+
+        if (delay_before_connect) {
+            /* let one side create ep */
+            short_progress_loop(0);
+        }
+
+        sender().connect(&sender(), params, base_index + 1);
+        ucp_ep_h ep2 = sender().ep(0, base_index + 1);
+
+        EXPECT_NE(ep1, ep2);
+
+        if (GetParam().variant == TEST_STREAM) {
+            uint64_t data1 = (base_index * 10) + 1;
+            uint64_t data2 = (base_index * 10) + 2;
+
+            send_b(ep1, 1, 1, data1);
+            send_b(ep2, 1, 1, data2);
+
+            if (enable_loopback) {
+                /* self-send - each ep receives what was sent on it */
+                recv_b(sender().worker(), ep1, 1, 1, data1);
+                recv_b(sender().worker(), ep2, 1, 1, data2);
+            } else {
+                /* cross-send - each ep receives what was sent on the other ep */
+                recv_b(sender().worker(), ep1, 1, 1, data2);
+                recv_b(sender().worker(), ep2, 1, 1, data1);
+            }
+        }
+    }
+    flush_worker(sender());
+}
+
+UCS_TEST_P(test_ucp_wireup_2sided, loopback) {
+    test_connect_loopback(false, true);
+}
+
+UCS_TEST_P(test_ucp_wireup_2sided, loopback_with_delay) {
+    test_connect_loopback(true, true);
+}
+
+UCS_TEST_P(test_ucp_wireup_2sided, no_loopback) {
+    test_connect_loopback(false, false);
+}
+
+UCS_TEST_P(test_ucp_wireup_2sided, no_loopback_with_delay) {
+    test_connect_loopback(true, false);
+}
 
+UCS_TEST_P(test_ucp_wireup_2sided, connect_disconnect) {
+    sender().connect(&receiver(), get_ep_params());
+    if (!is_loopback()) {
+        receiver().connect(&sender(), get_ep_params());
+    }
+    disconnect(sender());
+    if (!is_loopback()) {
+        disconnect(receiver());
+    }
+}
+
+UCS_TEST_P(test_ucp_wireup_2sided, multi_ep_2sided) {
+    const unsigned count = 10;
+
+    for (unsigned j = 0; j < 4; ++j) {
+
+        unsigned offset = j * count;
+
+        for (unsigned i = 0; i < count; ++i) {
+            unsigned ep_idx = offset + i;
+            sender().connect(&receiver(), get_ep_params(), ep_idx);
+            if (!is_loopback()) {
+                receiver().connect(&sender(), get_ep_params(), ep_idx);
+            }
+            UCS_TEST_MESSAGE << "iteration " << j << " pair " << i << ": " <<
+                            sender().ep(0, ep_idx) << " <--> " << receiver().ep(0, ep_idx);
+        }
+
+        for (unsigned i = 0; i < count; ++i) {
+            unsigned ep_idx = offset + i;
+            send_recv(sender().ep(0, ep_idx), receiver().worker(),
+                      receiver().ep(0, ep_idx), 8, 1);
+            send_recv(receiver().ep(0, ep_idx), sender().worker(),
+                      sender().ep(0, ep_idx), 8, 1);
+        }
+
+        short_progress_loop(0);
+    }
 }
 
-UCP_INSTANTIATE_TEST_CASE(test_ucp_wireup)
+UCP_INSTANTIATE_TEST_CASE(test_ucp_wireup_2sided)
 
-class test_ucp_wireup_errh_peer : public test_ucp_wireup
+class test_ucp_wireup_errh_peer : public test_ucp_wireup_1sided
 {
 public:
     virtual ucp_ep_params_t get_ep_params() {
@@ -553,19 +742,19 @@ UCS_TEST_P(test_ucp_wireup_errh_peer, msg_after_ep_create) {
     receiver().connect(&sender(), get_ep_params());
 
     sender().connect(&receiver(), get_ep_params());
-    send_recv(sender().ep(), receiver().worker(), 1, 1);
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1, 1);
     flush_worker(sender());
 }
 
 UCS_TEST_P(test_ucp_wireup_errh_peer, msg_before_ep_create) {
 
     sender().connect(&receiver(), get_ep_params());
-    send_recv(sender().ep(), receiver().worker(), 1, 1);
+    send_recv(sender().ep(), receiver().worker(), receiver().ep(), 1, 1);
     flush_worker(sender());
 
     receiver().connect(&sender(), get_ep_params());
 
-    send_recv(receiver().ep(), sender().worker(), 1, 1);
+    send_recv(receiver().ep(), sender().worker(), receiver().ep(), 1, 1);
     flush_worker(receiver());
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.cc
new file mode 100644
index 000000000..5c1cce737
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.cc
@@ -0,0 +1,201 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+* See file LICENSE for terms.
+*/
+
+#include "ucp_datatype.h"
+#include "ucp_test.h"
+
+#include <common/test_helpers.h>
+
+namespace ucp {
+
+
+data_type_desc_t &
+data_type_desc_t::make(ucp_datatype_t datatype, const void *buf, size_t length,
+                       size_t iov_cnt)
+{
+    EXPECT_FALSE(is_valid());
+
+    if (m_length == 0) {
+        m_length = length;
+    }
+
+    if (m_origin == uintptr_t(NULL)) {
+        m_origin = uintptr_t(buf);
+    }
+
+    m_dt = datatype;
+    memset(m_iov, 0, sizeof(m_iov));
+
+    switch (m_dt & UCP_DATATYPE_CLASS_MASK) {
+    case UCP_DATATYPE_CONTIG:
+        m_buf   = buf;
+        m_count = length / ucp_contig_dt_elem_size(datatype);
+        break;
+    case UCP_DATATYPE_IOV:
+    {
+        const size_t iov_length = (length > iov_cnt) ?
+            ucs::rand() % (length / iov_cnt) : 0;
+        size_t iov_length_it = 0;
+        for (size_t iov_it = 0; iov_it < iov_cnt - 1; ++iov_it) {
+            m_iov[iov_it].buffer = (char *)(buf) + iov_length_it;
+            m_iov[iov_it].length = iov_length;
+            iov_length_it += iov_length;
+        }
+
+        /* Last entry */
+        m_iov[iov_cnt - 1].buffer = (char *)(buf) + iov_length_it;
+        m_iov[iov_cnt - 1].length = length - iov_length_it;
+
+        m_buf   = m_iov;
+        m_count = iov_cnt;
+        break;
+    }
+    case UCP_DATATYPE_GENERIC:
+        m_buf   = buf;
+        m_count = length;
+        break;
+    default:
+        m_buf   = NULL;
+        m_count = 0;
+        EXPECT_TRUE(false) << "Unsupported datatype";
+        break;
+    }
+
+    return *this;
+}
+
+int dt_gen_start_count  = 0;
+int dt_gen_finish_count = 0;
+
+static void* dt_common_start(void *context, size_t count)
+{
+    dt_gen_state *dt_state = new dt_gen_state;
+
+    dt_state->count   = count;
+    dt_state->started = 1;
+    dt_state->magic   = ucp::MAGIC;
+    dt_state->context = context;
+    dt_gen_start_count++;
+
+    return dt_state;
+}
+
+static void* dt_common_start_pack(void *context, const void *buffer,
+                                  size_t count)
+{
+    return dt_common_start(NULL, count);
+}
+
+static void* dt_common_start_unpack(void *context, void *buffer, size_t count)
+{
+    return dt_common_start(context, count);
+}
+
+template <typename T>
+size_t dt_packed_size(void *state)
+{
+    dt_gen_state *dt_state = (dt_gen_state*)state;
+
+    return dt_state->count * sizeof(T);
+}
+
+template <typename T>
+size_t dt_pack(void *state, size_t offset, void *dest, size_t max_length)
+{
+    dt_gen_state *dt_state = (dt_gen_state*)state;
+    T *p = reinterpret_cast<T*> (dest);
+    uint32_t count;
+
+    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
+    EXPECT_EQ(1, dt_state->started);
+    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
+
+    ucs_assert((offset % sizeof(T)) == 0);
+
+    count = std::min(max_length / sizeof(T),
+                     dt_state->count - (offset / sizeof(T)));
+    for (unsigned i = 0; i < count; ++i) {
+        p[i] = (offset / sizeof(T)) + i;
+    }
+    return count * sizeof(T);
+}
+
+template <typename T>
+ucs_status_t dt_unpack(void *state, size_t offset, const void *src,
+                       size_t length)
+{
+    dt_gen_state *dt_state = (dt_gen_state*)state;
+    std::vector<T> *ctx;
+    uint32_t count;
+
+    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
+    EXPECT_EQ(1, dt_state->started);
+    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
+
+    ctx = reinterpret_cast<std::vector<T>*>(dt_state->context);
+    count = length / sizeof(T);
+    for (unsigned i = 0; i < count; ++i) {
+        T expected = ctx ? (*ctx)[offset / sizeof(T) + i] :
+                     (offset / sizeof(T)) + i;
+        T actual   = ((T*)src)[i];
+        if (actual != expected) {
+            UCS_TEST_ABORT("Invalid data at index " << i << ". expected: " <<
+                           expected << " actual: " << actual << " offset: " <<
+                           offset << ".");
+        }
+    }
+    return UCS_OK;
+}
+
+static ucs_status_t dt_err_unpack(void *state, size_t offset, const void *src,
+                                  size_t length)
+{
+    dt_gen_state *dt_state = (dt_gen_state*)state;
+
+    EXPECT_GT(dt_gen_start_count, dt_gen_finish_count);
+    EXPECT_EQ(1, dt_state->started);
+    EXPECT_EQ(uint32_t(MAGIC), dt_state->magic);
+
+    return UCS_ERR_NO_MEMORY;
+}
+
+static void dt_common_finish(void *state)
+{
+    dt_gen_state *dt_state = (dt_gen_state*)state;
+
+    --dt_state->started;
+    EXPECT_EQ(0, dt_state->started);
+    dt_gen_finish_count++;
+    delete dt_state;
+}
+
+ucp_generic_dt_ops test_dt_uint32_ops = {
+    dt_common_start_pack,
+    dt_common_start_unpack,
+    dt_packed_size<uint32_t>,
+    dt_pack<uint32_t>,
+    dt_unpack<uint32_t>,
+    dt_common_finish
+};
+
+ucp_generic_dt_ops test_dt_uint8_ops = {
+    dt_common_start_pack,
+    dt_common_start_unpack,
+    dt_packed_size<uint8_t>,
+    dt_pack<uint8_t>,
+    dt_unpack<uint8_t>,
+    dt_common_finish
+};
+
+ucp_generic_dt_ops test_dt_uint32_err_ops = {
+    dt_common_start_pack,
+    dt_common_start_unpack,
+    dt_packed_size<uint32_t>,
+    dt_pack<uint32_t>,
+    dt_err_unpack,
+    dt_common_finish
+};
+
+} // ucp
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.h
new file mode 100644
index 000000000..545c86036
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_datatype.h
@@ -0,0 +1,132 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+* See file LICENSE for terms.
+*/
+
+#ifndef TEST_UCP_DATATYPE_H_
+#define TEST_UCP_DATATYPE_H_
+
+#include <common/gtest.h>
+
+#include <ucp/api/ucp.h>
+extern "C" {
+#include <ucp/dt/dt_contig.h>
+#include <ucp/dt/dt_generic.h>
+#include <ucp/dt/dt_iov.h>
+}
+
+#include <string.h>
+
+namespace ucp {
+
+/* Can't be destroyed before related UCP request is completed */
+class data_type_desc_t {
+public: 
+    enum {
+        MAX_IOV = 40
+    };
+
+    data_type_desc_t()
+        : m_origin(uintptr_t(NULL)), m_length(0), m_dt(0), m_buf(NULL),
+          m_count(0), m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
+        memset(m_iov, 0, sizeof(m_iov));
+    };
+
+    data_type_desc_t(ucp_datatype_t datatype, const void *buf, size_t length)
+        : m_origin(uintptr_t(buf)), m_length(length), m_dt(0), m_buf(NULL),
+          m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
+        make(datatype, buf, length);
+    }
+
+    data_type_desc_t(ucp_datatype_t datatype, const void *buf, size_t length,
+                     size_t iov_count)
+        : m_origin(uintptr_t(buf)), m_length(length), m_dt(0), m_buf(NULL),
+          m_iov_cnt_limit(sizeof(m_iov) / sizeof(m_iov[0])) {
+        make(datatype, buf, length, iov_count);
+    };
+
+    data_type_desc_t &make(ucp_datatype_t datatype, const void *buf,
+                           size_t length) {
+        return make(datatype, buf, length, m_iov_cnt_limit);
+    };
+
+    data_type_desc_t &forward_to(size_t offset) {
+        EXPECT_LE(offset, m_length);
+        invalidate();
+        return make(m_dt, (const void *)(m_origin + offset), m_length - offset,
+                    m_iov_cnt_limit);
+    };
+
+    ucp_datatype_t dt() const {
+        EXPECT_TRUE(is_valid());
+        return m_dt;
+    };
+
+    void *buf() const {
+        EXPECT_TRUE(is_valid());
+        return const_cast<void *>(m_buf);
+    };
+
+    ssize_t buf_length() const {
+        EXPECT_TRUE(is_valid());
+        if (UCP_DT_IS_CONTIG(m_dt) || UCP_DT_IS_GENERIC(m_dt)) {
+            return m_length - (uintptr_t(m_buf) - m_origin);
+        } else if (UCP_DT_IS_IOV(m_dt)) {
+            size_t length = 0;
+            for (size_t i = 0; i < count(); ++i) {
+                length += m_iov[i].length;
+            }
+            return length;
+        }
+        ADD_FAILURE() << "Not supported datatype";
+        return -1;
+    }
+
+    size_t count() const {
+        EXPECT_TRUE(is_valid());
+        return m_count;
+    };
+
+    bool is_valid() const {
+        return (m_buf != NULL) && (m_count != 0) &&
+               (UCP_DT_IS_IOV(m_dt) ? (m_count <= m_iov_cnt_limit) :
+               (UCP_DT_IS_CONTIG(m_dt) || UCP_DT_IS_GENERIC(m_dt)));
+    }
+
+private:
+    data_type_desc_t &make(ucp_datatype_t datatype, const void *buf,
+                           size_t length, size_t iov_count);
+
+    void invalidate() {
+        EXPECT_TRUE(is_valid());
+        m_buf   = NULL;
+        m_count = 0;
+    }
+
+    uintptr_t       m_origin;
+    size_t          m_length;
+
+    ucp_datatype_t  m_dt;
+    const void     *m_buf;
+    size_t          m_count;
+
+    const size_t    m_iov_cnt_limit;
+    ucp_dt_iov_t    m_iov[MAX_IOV];
+};
+
+struct dt_gen_state {
+    size_t              count;
+    int                 started;
+    uint32_t            magic;
+    void                *context;
+};
+
+extern int dt_gen_start_count;
+extern int dt_gen_finish_count;
+extern ucp_generic_dt_ops test_dt_uint32_ops;
+extern ucp_generic_dt_ops test_dt_uint32_err_ops;
+extern ucp_generic_dt_ops test_dt_uint8_ops;
+
+} // ucp
+
+#endif /* TEST_UCP_DATATYPE_H_ */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.cc
index 84ce7867c..c54ebd282 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.cc
@@ -8,8 +8,13 @@
 #include <common/test_helpers.h>
 #include <ucs/arch/atomic.h>
 #include <ucs/stats/stats.h>
+#include <queue>
 
 
+namespace ucp {
+const uint32_t MAGIC = 0xd7d7d7d7U;
+}
+
 std::ostream& operator<<(std::ostream& os, const ucp_test_param& test_param)
 {
     std::vector<std::string>::const_iterator iter;
@@ -26,7 +31,7 @@ std::ostream& operator<<(std::ostream& os, const ucp_test_param& test_param)
 const ucp_datatype_t ucp_test::DATATYPE     = ucp_dt_make_contig(1);
 const ucp_datatype_t ucp_test::DATATYPE_IOV = ucp_dt_make_iov();
 
-ucp_test::ucp_test() {
+ucp_test::ucp_test() : m_err_handler_count(0) {
     ucs_status_t status;
     status = ucp_config_read(NULL, NULL, &m_ucp_config);
     ASSERT_UCS_OK(status);
@@ -87,6 +92,20 @@ ucp_test_base::entity* ucp_test::create_entity(bool add_in_front,
     return e;
 }
 
+ucp_test::entity* ucp_test::get_entity_by_ep(ucp_ep_h ep) {
+    ucs::ptr_vector<entity>::const_iterator e_it;
+    for (e_it = entities().begin(); e_it != entities().end(); ++e_it) {
+        for (int w_idx = 0; w_idx < (*e_it)->get_num_workers(); ++w_idx) {
+            for (int ep_idx = 0; ep_idx < (*e_it)->get_num_eps(w_idx); ++ep_idx) {
+                if (ep == (*e_it)->ep(w_idx, ep_idx)) {
+                    return *e_it;
+                }
+            }
+        }
+    }
+    return NULL;
+}
+
 ucp_params_t ucp_test::get_ctx_params() {
     ucp_params_t params;
     memset(&params, 0, sizeof(params));
@@ -114,6 +133,7 @@ unsigned ucp_test::progress(int worker_index) const {
          iter != entities().end(); ++iter)
     {
         count += (*iter)->progress(worker_index);
+        sched_yield();
     }
     return count;
 }
@@ -139,9 +159,14 @@ void ucp_test::flush_worker(const entity &e, int worker_index)
 
 void ucp_test::disconnect(const entity& entity) {
     for (int i = 0; i < entity.get_num_workers(); i++) {
-        flush_worker(entity, i);
+        if (m_err_handler_count == 0) {
+            flush_worker(entity, i);
+        }
+
         for (int j = 0; j < entity.get_num_eps(i); j++) {
-            void *dreq = entity.disconnect_nb(i, j);
+            void *dreq = entity.disconnect_nb(i, j, m_err_handler_count == 0 ?
+                                                    UCP_EP_CLOSE_MODE_FLUSH :
+                                                    UCP_EP_CLOSE_MODE_FORCE);
             if (!UCS_PTR_IS_PTR(dreq)) {
                 ASSERT_UCS_OK(UCS_PTR_STATUS(dreq));
             }
@@ -156,6 +181,12 @@ void ucp_test::wait(void *req, int worker_index)
         return;
     }
 
+    if (UCS_PTR_IS_ERR(req)) {
+        ucs_error("operation returned error: %s",
+                  ucs_status_string(UCS_PTR_STATUS(req)));
+        return;
+    }
+
     ucs_status_t status;
     do {
         progress(worker_index);
@@ -175,6 +206,15 @@ void ucp_test::set_ucp_config(ucp_config_t *config) {
     set_ucp_config(config, GetParam());
 }
 
+int ucp_test::max_connections() {
+    std::vector<std::string>::const_iterator end = GetParam().transports.end();
+    if (std::find(GetParam().transports.begin(), end, "tcp") != end) {
+        return ucs::max_tcp_connections();
+    } else {
+        return std::numeric_limits<int>::max();
+    }
+}
+
 std::vector<ucp_test_param>
 ucp_test::enum_test_params(const ucp_params_t& ctx_params,
                            const std::string& name,
@@ -228,6 +268,8 @@ void ucp_test::set_ucp_config(ucp_config_t *config,
     std::stringstream ss;
     ss << test_param;
     ucp_config_modify(config, "TLS", ss.str().c_str());
+    /* prevent configuration warnings in the UCP testing */
+    ucp_config_modify(config, "WARN_INVALID_CONFIG", "no");
 }
 
 void ucp_test::modify_config(const std::string& name, const std::string& value,
@@ -283,13 +325,13 @@ bool ucp_test::check_test_param(const std::string& name,
     UCS_TEST_CREATE_HANDLE(ucp_config_t*, config, ucp_config_release,
                            ucp_config_read, NULL, NULL);
     set_ucp_config(config, test_param);
+    ucp_config_modify(config.get(), "WARN_INVARIANT_TSC", "n");
 
     ucp_context_h ucph;
     ucs_status_t status;
     {
-        hide_errors();
+        scoped_log_handler slh(hide_errors_logger);
         status = ucp_init(&test_param.ctx_params, config, &ucph);
-        restore_errors();
     }
 
     bool result;
@@ -311,9 +353,11 @@ bool ucp_test::check_test_param(const std::string& name,
 ucp_test_base::entity::entity(const ucp_test_param& test_param,
                               ucp_config_t* ucp_config,
                               const ucp_worker_params_t& worker_params)
+    : m_rejected_cntr(0)
 {
     ucp_test_param entity_param = test_param;
     ucp_worker_params_t local_worker_params = worker_params;
+    int num_workers;
 
     if (test_param.thread_type == MULTI_THREAD_CONTEXT) {
         num_workers = MT_TEST_NUM_THREADS;
@@ -334,8 +378,11 @@ ucp_test_base::entity::entity(const ucp_test_param& test_param,
 
     ucp_test::set_ucp_config(ucp_config, entity_param);
 
-    UCS_TEST_CREATE_HANDLE(ucp_context_h, m_ucph, ucp_cleanup, ucp_init,
-                           &entity_param.ctx_params, ucp_config);
+    {
+        scoped_log_handler slh(hide_errors_logger);
+        UCS_TEST_CREATE_HANDLE(ucp_context_h, m_ucph, ucp_cleanup, ucp_init,
+                               &entity_param.ctx_params, ucp_config);
+    }
 
     m_workers.resize(num_workers);
     for (int i = 0; i < num_workers; i++) {
@@ -351,38 +398,63 @@ ucp_test_base::entity::~entity() {
 
 void ucp_test_base::entity::connect(const entity* other,
                                     const ucp_ep_params_t& ep_params,
-                                    int ep_idx) {
-    assert(num_workers == other->get_num_workers());
-    for (unsigned i = 0; i < unsigned(num_workers); i++) {
+                                    int ep_idx, int do_set_ep) {
+    assert(get_num_workers() == other->get_num_workers());
+    for (unsigned i = 0; i < unsigned(get_num_workers()); i++) {
         ucs_status_t status;
         ucp_address_t *address;
         size_t address_length;
         ucp_ep_h ep;
-        ucp_ep_params_t local_ep_params = ep_params;
 
         status = ucp_worker_get_address(other->worker(i), &address, &address_length);
         ASSERT_UCS_OK(status);
 
-        ucp_test::hide_errors();
-        local_ep_params.field_mask |= UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
-        local_ep_params.address     = address;
+        {
+            scoped_log_handler slh(hide_errors_logger);
 
-        status = ucp_ep_create(m_workers[i].first, &local_ep_params, &ep);
-        ucp_test::restore_errors();
+            ucp_ep_params_t local_ep_params = ep_params;
+            local_ep_params.field_mask |= UCP_EP_PARAM_FIELD_REMOTE_ADDRESS;
+            local_ep_params.address     = address;
+
+            status = ucp_ep_create(m_workers[i].first, &local_ep_params, &ep);
+        }
 
         if (status == UCS_ERR_UNREACHABLE) {
             ucp_worker_release_address(other->worker(i), address);
-            UCS_TEST_SKIP_R(m_errors.empty() ? "" : m_errors.back());
+            UCS_TEST_SKIP_R(m_errors.empty() ? "Unreachable" : m_errors.back());
         }
 
-        ASSERT_UCS_OK(status);
+        ASSERT_UCS_OK(status, << " (" << m_errors.back() << ")");
 
-        set_ep(ep, i, ep_idx);
+        if (do_set_ep) {
+            set_ep(ep, i, ep_idx);
+        }
 
         ucp_worker_release_address(other->worker(i), address);
     }
 }
 
+ucp_ep_h ucp_test_base::entity::accept(ucp_worker_h worker,
+                                       ucp_conn_request_h conn_request)
+{
+    ucp_ep_h        ep;
+    ucp_ep_params_t ep_params;
+    ep_params.field_mask   = UCP_EP_PARAM_FIELD_USER_DATA |
+                             UCP_EP_PARAM_FIELD_CONN_REQUEST;
+    ep_params.user_data    = (void *)0xdeadbeef;
+    ep_params.conn_request = conn_request;
+
+    ucs_status_t status    = ucp_ep_create(worker, &ep_params, &ep);
+    if (status == UCS_ERR_UNREACHABLE) {
+        UCS_TEST_SKIP_R("Skipping due an unreachable destination (unsupported "
+                        "feature or no supported transport to send partial "
+                        "worker address)");
+    }
+    ASSERT_UCS_OK(status);
+    return ep;
+}
+
+
 void* ucp_test_base::entity::modify_ep(const ucp_ep_params_t& ep_params,
                                       int worker_idx, int ep_idx) {
     return ucp_ep_modify_nb(ep(worker_idx, ep_idx), &ep_params);
@@ -402,12 +474,25 @@ void ucp_test_base::entity::set_ep(ucp_ep_h ep, int worker_index, int ep_index)
 void ucp_test_base::entity::empty_send_completion(void *r, ucs_status_t status) {
 }
 
-void ucp_test_base::entity::accept_cb(ucp_ep_h ep, void *arg) {
+void ucp_test_base::entity::accept_ep_cb(ucp_ep_h ep, void *arg) {
     entity *self = reinterpret_cast<entity*>(arg);
     int worker_index = 0; /* TODO pass worker index in arg */
     self->set_ep(ep, worker_index, self->get_num_eps(worker_index));
 }
 
+void ucp_test_base::entity::accept_conn_cb(ucp_conn_request_h conn_req, void* arg)
+{
+    entity *self = reinterpret_cast<entity*>(arg);
+    self->m_conn_reqs.push(conn_req);
+}
+
+void ucp_test_base::entity::reject_conn_cb(ucp_conn_request_h conn_req, void* arg)
+{
+    entity *self = reinterpret_cast<entity*>(arg);
+    ucp_listener_reject(self->m_listener, conn_req);
+    self->m_rejected_cntr++;
+}
+
 void* ucp_test_base::entity::flush_ep_nb(int worker_index, int ep_index) const {
     return ucp_ep_flush_nb(ep(worker_index, ep_index), 0, empty_send_completion);
 }
@@ -424,12 +509,13 @@ void ucp_test_base::entity::fence(int worker_index) const {
     ASSERT_UCS_OK(status);
 }
 
-void* ucp_test_base::entity::disconnect_nb(int worker_index, int ep_index) const {
+void* ucp_test_base::entity::disconnect_nb(int worker_index, int ep_index,
+                                           enum ucp_ep_close_mode mode) const {
     ucp_ep_h ep = revoke_ep(worker_index, ep_index);
     if (ep == NULL) {
         return NULL;
     }
-    return ucp_disconnect_nb(ep);
+    return ucp_ep_close_nb(ep, mode);
 }
 
 void ucp_test_base::entity::destroy_worker(int worker_index) {
@@ -458,33 +544,59 @@ ucp_ep_h ucp_test_base::entity::revoke_ep(int worker_index, int ep_index) const
     return ucp_ep;
 }
 
-ucs_status_t ucp_test_base::entity::listen(const struct sockaddr* saddr,
+ucs_status_t ucp_test_base::entity::listen(listen_cb_type_t cb_type,
+                                           const struct sockaddr* saddr,
                                            socklen_t addrlen, int worker_index)
 {
     ucp_listener_params_t params;
-    ucp_listener_h listener;
-
-    params.field_mask         = UCP_LISTENER_PARAM_FIELD_SOCK_ADDR |
-                                UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER;
-    params.sockaddr.addr      = saddr;
-    params.sockaddr.addrlen   = addrlen;
-    params.accept_handler.cb  = accept_cb;
-    params.accept_handler.arg = reinterpret_cast<void*>(this);
-
-    wrap_errors();
-    ucs_status_t status = ucp_listener_create(worker(worker_index), &params, &listener);
-    restore_errors();
+    ucp_listener_h        listener;
+
+    params.field_mask             = UCP_LISTENER_PARAM_FIELD_SOCK_ADDR;
+    params.sockaddr.addr          = saddr;
+    params.sockaddr.addrlen       = addrlen;
+
+    switch (cb_type) {
+    case LISTEN_CB_EP:
+        params.field_mask        |= UCP_LISTENER_PARAM_FIELD_ACCEPT_HANDLER;
+        params.accept_handler.cb  = accept_ep_cb;
+        params.accept_handler.arg = reinterpret_cast<void*>(this);
+        break;
+    case LISTEN_CB_CONN:
+        params.field_mask        |= UCP_LISTENER_PARAM_FIELD_CONN_HANDLER;
+        params.conn_handler.cb    = accept_conn_cb;
+        params.conn_handler.arg   = reinterpret_cast<void*>(this);
+        break;
+    case LISTEN_CB_REJECT:
+        params.field_mask        |= UCP_LISTENER_PARAM_FIELD_CONN_HANDLER;
+        params.conn_handler.cb    = reject_conn_cb;
+        params.conn_handler.arg   = reinterpret_cast<void*>(this);
+        break;
+    default:
+        UCS_TEST_ABORT("invalid test parameter");
+    }
+
+    ucs_status_t status;
+    {
+        scoped_log_handler wrap_err(wrap_errors_logger);
+        status = ucp_listener_create(worker(worker_index), &params, &listener);
+    }
+
     if (status == UCS_OK) {
         m_listener.reset(listener, ucp_listener_destroy);
-    } else if (status != UCS_ERR_INVALID_ADDR) {
-        /* throw error if status is not (UCS_OK or UCS_ERR_INVALID_ADDR) */
-        ASSERT_UCS_OK(status);
+    } else {
+        /* throw error if status is not (UCS_OK or UCS_ERR_UNREACHABLE).
+         * UCS_ERR_INVALID_PARAM may also return but then the test should fail */
+        EXPECT_EQ(UCS_ERR_UNREACHABLE, status);
     }
     return status;
 }
 
 ucp_worker_h ucp_test_base::entity::worker(int worker_index) const {
-    return m_workers[worker_index].first;
+    if (worker_index < get_num_workers()) {
+        return m_workers[worker_index].first;
+    } else {
+        return NULL;
+    }
 }
 
 ucp_context_h ucp_test_base::entity::ucph() const {
@@ -494,18 +606,40 @@ ucp_context_h ucp_test_base::entity::ucph() const {
 unsigned ucp_test_base::entity::progress(int worker_index)
 {
     ucp_worker_h ucp_worker = worker(worker_index);
-    return ucp_worker ? ucp_worker_progress(ucp_worker) : 0;
+
+    if (ucp_worker == NULL) {
+        return 0;
+    }
+
+    unsigned progress_count = 0;
+    if (!m_conn_reqs.empty()) {
+        ucp_conn_request_h conn_req = m_conn_reqs.back();
+        m_conn_reqs.pop();
+        ucp_ep_h ep = accept(ucp_worker, conn_req);
+        set_ep(ep, worker_index, std::numeric_limits<int>::max());
+        ++progress_count;
+    }
+
+    return progress_count + ucp_worker_progress(ucp_worker);
 }
 
 int ucp_test_base::entity::get_num_workers() const {
-    ucs_assert(m_workers.size() == size_t(num_workers));
-    return num_workers;
+    return m_workers.size();
 }
 
 int ucp_test_base::entity::get_num_eps(int worker_index) const {
     return m_workers[worker_index].second.size();
 }
 
+size_t ucp_test_base::entity::get_rejected_cntr() const {
+    return m_rejected_cntr;
+}
+
+void ucp_test_base::entity::inc_rejected_cntr() {
+    ++m_rejected_cntr;
+}
+
+
 void ucp_test_base::entity::warn_existing_eps() const {
     for (size_t worker_index = 0; worker_index < m_workers.size(); ++worker_index) {
         for (size_t ep_index = 0; ep_index < m_workers[worker_index].second.size();
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.h
index 361176f2f..bf0757801 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucp/ucp_test.h
@@ -16,9 +16,17 @@
 
 #include <common/test.h>
 
+#include <queue>
+
 #define MT_TEST_NUM_THREADS       4
 #define UCP_TEST_TIMEOUT_IN_SEC   10.0
 
+
+namespace ucp {
+extern const uint32_t MAGIC;
+}
+
+
 struct ucp_test_param {
     ucp_params_t              ctx_params;
     std::vector<std::string>  transports;
@@ -40,13 +48,21 @@ public:
                                       ep_vec_t> > worker_vec_t;
 
     public:
+        typedef enum {
+            LISTEN_CB_EP,       /* User's callback accepts ucp_ep_h */
+            LISTEN_CB_CONN,     /* User's callback accepts ucp_conn_request_h */
+            LISTEN_CB_REJECT    /* User's callback rejects ucp_conn_request_h */
+        } listen_cb_type_t;
+
         entity(const ucp_test_param& test_param, ucp_config_t* ucp_config,
                const ucp_worker_params_t& worker_params);
 
         ~entity();
 
         void connect(const entity* other, const ucp_ep_params_t& ep_params,
-                     int ep_idx = 0);
+                     int ep_idx = 0, int do_set_ep = 1);
+
+        ucp_ep_h accept(ucp_worker_h worker, ucp_conn_request_h conn_request);
 
         void* modify_ep(const ucp_ep_params_t& ep_params, int worker_idx = 0,
                        int ep_idx = 0);
@@ -57,11 +73,13 @@ public:
 
         void fence(int worker_index = 0) const;
 
-        void* disconnect_nb(int worker_index = 0, int ep_index = 0) const;
+        void* disconnect_nb(int worker_index = 0, int ep_index = 0,
+                            enum ucp_ep_close_mode mode = UCP_EP_CLOSE_MODE_FLUSH) const;
 
         void destroy_worker(int worker_index = 0);
 
-        ucs_status_t listen(const struct sockaddr *saddr, socklen_t addrlen,
+        ucs_status_t listen(listen_cb_type_t cb_type,
+                            const struct sockaddr *saddr, socklen_t addrlen,
                             int worker_index = 0);
 
         ucp_ep_h ep(int worker_index = 0, int ep_index = 0) const;
@@ -78,6 +96,10 @@ public:
 
         int get_num_eps(int worker_index = 0) const;
 
+        void inc_rejected_cntr();
+
+        size_t get_rejected_cntr() const;
+
         void warn_existing_eps() const;
 
         void cleanup();
@@ -85,15 +107,17 @@ public:
         static void ep_destructor(ucp_ep_h ep, entity *e);
 
     protected:
-        ucs::handle<ucp_context_h>  m_ucph;
-        worker_vec_t                m_workers;
-        ucs::handle<ucp_listener_h> m_listener;
-
-        int num_workers;
+        ucs::handle<ucp_context_h>      m_ucph;
+        worker_vec_t                    m_workers;
+        ucs::handle<ucp_listener_h>     m_listener;
+        std::queue<ucp_conn_request_h>  m_conn_reqs;
+        size_t                          m_rejected_cntr;
 
     private:
         static void empty_send_completion(void *r, ucs_status_t status);
-        static void accept_cb(ucp_ep_h ep, void *arg);
+        static void accept_ep_cb(ucp_ep_h ep, void *arg);
+        static void accept_conn_cb(ucp_conn_request_h conn_req, void *arg);
+        static void reject_conn_cb(ucp_conn_request_h conn_req, void *arg);
 
         void set_ep(ucp_ep_h ep, int worker_index, int ep_index);
     };
@@ -150,6 +174,7 @@ protected:
     virtual void cleanup();
     entity* create_entity(bool add_in_front = false);
     entity* create_entity(bool add_in_front, const ucp_test_param& test_param);
+    entity* get_entity_by_ep(ucp_ep_h ep);
     unsigned progress(int worker_index = 0) const;
     void short_progress_loop(int worker_index = 0) const;
     void flush_ep(const entity &e, int worker_index = 0, int ep_index = 0);
@@ -157,6 +182,12 @@ protected:
     void disconnect(const entity& entity);
     void wait(void *req, int worker_index = 0);
     void set_ucp_config(ucp_config_t *config);
+    int max_connections();
+
+    static void err_handler_cb(void *arg, ucp_ep_h ep, ucs_status_t status) {
+        ucp_test *self = reinterpret_cast<ucp_test*>(arg);
+        self->m_err_handler_count++;
+    }
 
     template <typename T>
     void wait_for_flag(volatile T *flag, double timeout = 10.0) {
@@ -174,6 +205,7 @@ private:
                                  const ucp_test_param& test_param);
 
 protected:
+    volatile int m_err_handler_count;
     static const ucp_datatype_t DATATYPE;
     static const ucp_datatype_t DATATYPE_IOV;
 };
@@ -181,7 +213,6 @@ protected:
 
 std::ostream& operator<<(std::ostream& os, const ucp_test_param& test_param);
 
-
 /**
  * Instantiate the parameterized test case a combination of transports.
  *
@@ -209,11 +240,9 @@ std::ostream& operator<<(std::ostream& os, const ucp_test_param& test_param);
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, udx,    "ud_x") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, rc,     "rc") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, rcx,    "rc_x") \
-    UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, rcx_cm, "\\rc_mlx5,cm:aux") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, shm_ib, "shm,ib") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, ugni,   "ugni") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, self,   "self") \
     UCP_INSTANTIATE_TEST_CASE_TLS(_test_case, tcp,    "tcp")
 
-
 #endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_async.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_async.cc
index cb74fdab5..8dae204c8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_async.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_async.cc
@@ -240,8 +240,10 @@ public:
     UCS_TEST_BASE_IMPL;
 
 protected:
-    static const int      COUNT       = 40;
-    static const unsigned SLEEP_USEC  = 1000;
+    static const int      COUNT           = 40;
+    static const unsigned SLEEP_USEC      = 1000;
+    static const int      TIMER_RETRIES   = 100;
+    static const int      TIMER_EXP_COUNT = COUNT / 4;
 
     void suspend(double scale = 1.0) {
         ucs::safe_usleep(ucs_max(scale * SLEEP_USEC, 0) *
@@ -414,6 +416,28 @@ UCS_TEST_P(test_async, max_events, "ASYNC_MAX_EVENTS=4") {
     ucs_async_context_cleanup(&async);
 }
 
+UCS_TEST_P(test_async, many_timers) {
+
+    for (int count = 0; count < 4010; ++count) {
+        std::vector<int> timers;
+        ucs_status_t status;
+        int timer_id;
+
+        for (int count2 = 0; count2 < 250; ++count2) {
+            status = ucs_async_add_timer(GetParam(), ucs_time_from_sec(1.0),
+                                         (ucs_async_event_cb_t)ucs_empty_function,
+                                         NULL, NULL, &timer_id);
+            ASSERT_UCS_OK(status);
+            timers.push_back(timer_id);
+        }
+
+        while (!timers.empty()) {
+            ucs_async_remove_handler(timers.back(), 0);
+            timers.pop_back();
+        }
+    }
+}
+
 UCS_TEST_P(test_async, ctx_event) {
     local_event le(GetParam());
     le.push_event();
@@ -423,16 +447,29 @@ UCS_TEST_P(test_async, ctx_event) {
 
 UCS_TEST_P(test_async, ctx_timer) {
     local_timer lt(GetParam());
-    suspend_and_poll(&lt, COUNT * 4);
-    EXPECT_GE(lt.count(), COUNT / 4);
+    for (int i = 0; i < TIMER_RETRIES; ++i) {
+        suspend_and_poll(&lt, COUNT * 4);
+        if (lt.count() >= TIMER_EXP_COUNT) {
+            break;
+        }
+        UCS_TEST_MESSAGE << "retry " << (i + 1);
+    }
+    EXPECT_GE(lt.count(), int(TIMER_EXP_COUNT));
 }
 
 UCS_TEST_P(test_async, two_timers) {
     local_timer lt1(GetParam());
     local_timer lt2(GetParam());
-    suspend_and_poll2(&lt1, &lt2, COUNT * 4);
-    EXPECT_GE(lt1.count(), COUNT / 4);
-    EXPECT_GE(lt2.count(), COUNT / 4);
+    for (int i = 0; i < TIMER_RETRIES; ++i) {
+        suspend_and_poll2(&lt1, &lt2, COUNT * 4);
+        if ((lt1.count() >= TIMER_EXP_COUNT) &&
+            (lt2.count() >= TIMER_EXP_COUNT)) {
+             break;
+        }
+        UCS_TEST_MESSAGE << "retry " << (i + 1);
+    }
+    EXPECT_GE(lt1.count(), int(TIMER_EXP_COUNT));
+    EXPECT_GE(lt2.count(), int(TIMER_EXP_COUNT));
 }
 
 UCS_TEST_P(test_async, ctx_event_block) {
@@ -484,13 +521,19 @@ UCS_TEST_P(test_async, ctx_event_block_two_miss) {
 UCS_TEST_P(test_async, ctx_timer_block) {
     local_timer lt(GetParam());
 
-    lt.block();
-    int count = lt.count();
-    suspend_and_poll(&lt, COUNT);
-    EXPECT_EQ(count, lt.count());
-    lt.unblock();
+    for (int i = 0; i < TIMER_RETRIES; ++i) {
+        lt.block();
+        int count = lt.count();
+        suspend_and_poll(&lt, COUNT);
+        EXPECT_EQ(count, lt.count());
+        lt.unblock();
 
-    lt.check_miss();
+        lt.check_miss();
+        if (lt.count() >= 1) {
+            break;
+        }
+        UCS_TEST_MESSAGE << "retry " << (i + 1);
+    }
     EXPECT_GE(lt.count(), 1); /* Timer could expire again after unblock */
 }
 
@@ -523,6 +566,25 @@ UCS_TEST_P(test_async, modify_event) {
     EXPECT_EQ(le.count(), count);
 }
 
+UCS_TEST_P(test_async, warn_block) {
+    {
+        scoped_log_handler slh(hide_warns_logger);
+        {
+            local_event le(GetParam());
+            le.block();
+        }
+    }
+
+    int warn_count = m_warnings.size();
+    for (int i = 0; i < warn_count; ++i) {
+        UCS_TEST_MESSAGE << "< " << m_warnings[i] << " >";
+    }
+
+    if (GetParam() != UCS_ASYNC_MODE_POLL) {
+        EXPECT_GE(warn_count, 1);
+    }
+}
+
 class local_timer_remove_handler : public local_timer {
 public:
     local_timer_remove_handler(ucs_async_mode_t mode) : local_timer(mode) {
@@ -649,20 +711,27 @@ UCS_TEST_P(test_async_event_mt, multithread) {
 
     for (unsigned i = 0; i < NUM_THREADS; ++i) {
         int count = thread_count(i);
-        EXPECT_GE(count, (int)(COUNT * 0.75));
+        EXPECT_GE(count, (int)(COUNT * 0.4));
     }
 }
 UCS_TEST_P(test_async_timer_mt, multithread) {
-    spawn();
-
-    suspend(2 * COUNT);
-
-    stop();
-
-    for (unsigned i = 0; i < NUM_THREADS; ++i) {
-        int count = thread_count(i);
-        EXPECT_GE(count, (int)(COUNT * 0.10));
+    const int exp_min_count = (int)(COUNT * 0.10);
+    int min_count = 0;
+    for (int r = 0; r < TIMER_RETRIES; ++r) {
+        spawn();
+        suspend(2 * COUNT);
+        stop();
+
+        min_count = std::numeric_limits<int>::max();
+        for (unsigned i = 0; i < NUM_THREADS; ++i) {
+            int count = thread_count(i);
+            min_count = ucs_min(count, min_count);
+        }
+        if (min_count >= exp_min_count) {
+            break;
+        }
     }
+    EXPECT_GE(min_count, exp_min_count);
 }
 
 INSTANTIATE_TEST_CASE_P(signal, test_async, ::testing::Values(UCS_ASYNC_MODE_SIGNAL));
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_config.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_config.cc
index e94f1c1aa..9b30c957f 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_config.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_config.cc
@@ -267,7 +267,7 @@ UCS_TEST_F(test_config, performance) {
     }
 
     /* Now test the time */
-    UCS_TEST_TIME_LIMIT(0.005) {
+    UCS_TEST_TIME_LIMIT(0.05) {
         car_opts opts(NULL, NULL);
     }
 }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_datatype.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_datatype.cc
index 5e4dd803f..2b1652e57 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_datatype.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_datatype.cc
@@ -147,27 +147,17 @@ UCS_TEST_F(test_datatype, queue) {
 
 UCS_TEST_F(test_datatype, queue_iter) {
 
+    const int num_elems = 4;
     ucs_queue_head_t head;
-    elem_t *elem1, *elem2, *elem3, *elem4;
+    std::vector<elem_t> elems(num_elems);
 
     ucs_queue_head_init(&head);
     EXPECT_TRUE(ucs_queue_is_empty(&head));
 
-    elem1 = (elem_t*)malloc(sizeof(elem_t));
-    elem2 = (elem_t*)malloc(sizeof(elem_t));
-    elem3 = (elem_t*)malloc(sizeof(elem_t));
-    elem4 = (elem_t*)malloc(sizeof(elem_t));
-
-    elem1->i = 1;
-    elem2->i = 2;
-    elem3->i = 3;
-    elem4->i = 4;
-
-    ucs_queue_push(&head, &elem1->queue);
-    ucs_queue_push(&head, &elem2->queue);
-    ucs_queue_push(&head, &elem3->queue);
-    ucs_queue_push(&head, &elem4->queue);
-
+    for (int i = 0; i < num_elems; ++i) {
+        elems[i].i = i + 1;
+        ucs_queue_push(&head, &elems[i].queue);
+    }
 
     {
         std::vector<int> vec;
@@ -176,7 +166,7 @@ UCS_TEST_F(test_datatype, queue_iter) {
         ucs_queue_for_each(elem, &head, queue) {
             vec.push_back(elem->i);
         }
-        ASSERT_EQ(4u, vec.size());
+        ASSERT_EQ(static_cast<size_t>(num_elems), vec.size());
         EXPECT_EQ(1, vec[0]);
         EXPECT_EQ(2, vec[1]);
         EXPECT_EQ(3, vec[2]);
@@ -192,7 +182,7 @@ UCS_TEST_F(test_datatype, queue_iter) {
         {
             if (elem->i == 3 || elem->i == 4) {
                 ucs_queue_del_iter(&head, iter);
-                free(elem);
+                memset(elem, 0xff, sizeof(*elem));
             }
         }
         ASSERT_EQ((unsigned long)2, ucs_queue_length(&head));
@@ -200,7 +190,7 @@ UCS_TEST_F(test_datatype, queue_iter) {
         ucs_queue_for_each_safe(elem, iter, &head, queue) {
             vec.push_back(elem->i);
             ucs_queue_del_iter(&head, iter);
-            free(elem);
+            memset(elem, 0xff, sizeof(*elem));
         }
         ASSERT_EQ(2u, vec.size());
         EXPECT_EQ(1, vec[0]);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_hash_perf.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_hash_perf.cc
deleted file mode 100644
index 49b0e767e..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_hash_perf.cc
+++ /dev/null
@@ -1,297 +0,0 @@
-/**
- * Copyright (C) Mellanox Technologies Ltd. 2001-2016.  ALL RIGHTS RESERVED.
- * See file LICENSE for terms.
- */
-
-#include <common/test.h>
-
-extern "C" {
-#include <ucp/core/ucp_ep.h>
-#include <ucs/datastruct/khash.h>
-#include <ucs/datastruct/sglib_wrapper.h>
-#include <ucs/time/time.h>
-#include <ucs/sys/sys.h>
-}
-
-#include <list>
-#include <map>
-#include <tr1/unordered_map>
-
-typedef uint64_t key_type;
-typedef ucp_ep_t hash_type;
-
-#define MAX_COUNT 4194304
-#define START_TIME ucs_time_t time = -ucs_get_time();
-#define END_TIME (ucs_time_to_nsec(time + ucs_get_time()) / num_elem)
-#define FIELD_ALIGN(_x_) ":" << std::right << std::setw(_x_)
-
-static key_type keys[MAX_COUNT]; /* the keys array */
-
-static void generate_keys()
-{
-    for (int i = 0; i < MAX_COUNT; ++i) {
-        keys[i] = ucs_generate_uuid((key_type)keys);
-    }
-}
-
-class perf_compare_base {
-public:
-    virtual ~perf_compare_base() {};
-
-    ucs_time_t initialize(const size_t num_elem) {
-        START_TIME
-        init_storage();
-        for (size_t i = 0; i < num_elem; ++i) {
-            add_elem(keys[i]);
-        }
-        return (ucs_time_t)END_TIME;
-    }
-
-    ucs_time_t lookup(const size_t num_elem) const {
-        START_TIME
-        for (size_t i = 0; i < num_elem; ++i) {
-            hash_type *ep = find_elem(keys[i]);
-            EXPECT_TRUE(ep->dest_uuid == keys[i]);
-        }
-        return (ucs_time_t)END_TIME;
-    }
-
-    ucs_time_t cleanup(const size_t num_elem) {
-        START_TIME
-        del_elems();
-        return (ucs_time_t)END_TIME;
-    }
-
-    virtual void        init_storage()                      = 0;
-    virtual void        add_elem(const uint64_t key)        = 0;
-    virtual hash_type  *find_elem(const uint64_t key) const = 0;
-    virtual void        del_elems()                         = 0;
-    virtual const char *get_name() const                    = 0;
-
-    template <typename _type_to_alloc>
-    _type_to_alloc *create_element(const uint64_t key) const {
-        _type_to_alloc *ep = (_type_to_alloc *) ucs_malloc(sizeof(_type_to_alloc), get_name());
-        EXPECT_TRUE(ep);
-        ep->dest_uuid = key;
-        return ep;
-    }
-};
-
-class perf_compare_map : public perf_compare_base {
-public:
-    const char *get_name() const {
-        return "map";
-    }
-
-    void init_storage() {}
-
-    void add_elem(const uint64_t key) {
-        obj[key] = create_element<hash_type>(key);
-    }
-
-    hash_type *find_elem(const uint64_t key) const {
-        std::map<key_type, hash_type *>::const_iterator ep_found = obj.find(key);
-        EXPECT_TRUE(ep_found != obj.end());
-        return ep_found->second;
-    }
-
-    void del_elems() {
-        for (std::map<key_type, hash_type *>::const_iterator it = obj.begin();
-             it != obj.end(); ++it) {
-            hash_type *ep = it->second;
-            free(ep);
-        }
-        obj.clear();
-    }
-
-    std::map<key_type, hash_type *> obj;
-};
-
-class perf_compare_unordered_map : public perf_compare_base {
-public:
-    const char *get_name() const {
-        return "unmap";
-    }
-
-    void init_storage() {}
-
-    void add_elem(const uint64_t key) {
-        obj[key] = create_element<hash_type>(key);
-    }
-
-    hash_type *find_elem(const uint64_t key) const {
-        std::tr1::unordered_map<key_type, hash_type *>::const_iterator ep_found = obj.find(key);
-        EXPECT_TRUE(ep_found != obj.end());
-        return ep_found->second;
-    }
-
-    void del_elems() {
-        for (std::tr1::unordered_map<key_type, hash_type *>::const_iterator it = obj.begin();
-             it != obj.end(); ++it) {
-            hash_type *ep = it->second;
-            free(ep);
-        }
-        obj.clear();
-    }
-
-    std::tr1::unordered_map<key_type, hash_type *> obj;
-};
-
-class perf_compare_khash : public perf_compare_base {
-public:
-    const char *get_name() const {
-        return "khash";
-    }
-
-    KHASH_MAP_INIT_INT64(khash_ep_hash, hash_type *);
-
-    void init_storage() {
-        kh_init_inplace(khash_ep_hash, &obj);
-    }
-
-    void add_elem(const uint64_t key) {
-        int hash_extra_status = 0;
-
-        khiter_t hash_it = kh_put(khash_ep_hash, &obj, key, &hash_extra_status);
-        EXPECT_TRUE(hash_it != kh_end(&obj));
-        kh_value(&obj, hash_it) = create_element<hash_type>(key);
-    }
-
-    hash_type *find_elem(const uint64_t key) const {
-        khiter_t ep_found = kh_get(khash_ep_hash, &obj, key);
-        EXPECT_TRUE(ep_found != kh_end(&obj));
-        return kh_value(&obj, ep_found);
-    }
-
-    void del_elems() {
-        for (khiter_t it = kh_begin(obj); it != kh_end(&obj); ++it) {
-            if (!kh_exist(&obj, it)) {
-                continue;
-            }
-            hash_type *ep = kh_value(&obj, it);
-            free(ep);
-        }
-        kh_destroy_inplace(khash_ep_hash, &obj);
-    }
-
-    khash_t(khash_ep_hash) obj;
-};
-
-#define SGLIB_HASH_SIZE                 32767
-#define test_sglib_compare(_ep1, _ep2)  ((int64_t)(_ep1)->dest_uuid - (int64_t)(_ep2)->dest_uuid)
-#define test_sglib_hash(_ep)            ((_ep)->dest_uuid)
-
-struct sglib_hash_type : public hash_type {
-    sglib_hash_type *next;
-};
-
-SGLIB_DEFINE_LIST_PROTOTYPES(sglib_hash_type, test_sglib_compare, next);
-SGLIB_DEFINE_LIST_FUNCTIONS(sglib_hash_type, test_sglib_compare, next);
-SGLIB_DEFINE_HASHED_CONTAINER_PROTOTYPES(sglib_hash_type, SGLIB_HASH_SIZE, test_sglib_hash);
-SGLIB_DEFINE_HASHED_CONTAINER_FUNCTIONS(sglib_hash_type, SGLIB_HASH_SIZE, test_sglib_hash);
-
-class perf_compare_sglib : public perf_compare_base {
-public:
-    const char *get_name() const {
-        return "sglib";
-    }
-
-    void init_storage() {
-        obj = (sglib_hash_type **) ucs_malloc(sizeof(*obj) * SGLIB_HASH_SIZE, "sglib_hash");
-        sglib_hashed_sglib_hash_type_init(obj);
-    }
-
-    void add_elem(const uint64_t key) {
-        sglib_hash_type *ep = create_element<sglib_hash_type>(key);
-        sglib_hashed_sglib_hash_type_add(obj, ep);
-    }
-
-    hash_type *find_elem(const uint64_t key) const {
-        sglib_hash_type ep, *ep_found = 0;
-        ep.dest_uuid = key;
-        ep_found = sglib_hashed_sglib_hash_type_find_member(obj, &ep);
-        EXPECT_TRUE(ep_found);
-        return ep_found;
-    }
-
-    void del_elems() {
-        struct sglib_hashed_sglib_hash_type_iterator it;
-        for (sglib_hash_type *local_it = sglib_hashed_sglib_hash_type_it_init(&it, obj);
-            local_it != NULL;
-            local_it = sglib_hashed_sglib_hash_type_it_next(&it)) {
-            sglib_hashed_sglib_hash_type_delete(obj, local_it);
-            free(local_it);
-        }
-        free(obj);
-    }
-
-    sglib_hash_type **obj;
-};
-
-class test_hash_perf : public ucs::test {
-protected:
-    void check_lookup_perf(perf_compare_base* hash, size_t num_elems);
-};
-
-void test_hash_perf::check_lookup_perf(perf_compare_base* hash, size_t num_elems) {
-    const ucs_time_t MAX_LOOKUP_NS_1024 = 400;
-    for (int i = 0; i < (ucs::perf_retry_count + 1); ++i) {
-        ucs_time_t lookup_ns = hash->lookup(num_elems);
-        if (!ucs::perf_retry_count) {
-            UCS_TEST_MESSAGE << "not validating performance";
-            return; /* Skip */
-        } else if (lookup_ns < MAX_LOOKUP_NS_1024) {
-            return; /* Success */
-        } else {
-            ucs::safe_sleep(ucs::perf_retry_interval);
-        }
-    }
-    ADD_FAILURE() << hash->get_name()  << " bad lookup performance";
-}
-
-UCS_TEST_F(test_hash_perf, perf_compare) {
-
-    size_t trip_counts[] = {1, 2, 8, 128, 1024, 32768, 262144, 1048576, 0};
-
-    if (ucs::test_time_multiplier() > 1) {
-        UCS_TEST_SKIP_R("Long run expected. Skipped.");
-    }
-    perf_compare_base *perf_compare_khash_ptr = new perf_compare_khash;
-    perf_compare_base *perf_compare_sglib_ptr = new perf_compare_sglib;
-    perf_compare_base *hashes[] = {
-        perf_compare_khash_ptr,
-        perf_compare_sglib_ptr,
-        new perf_compare_map,
-        new perf_compare_unordered_map,
-        NULL
-    };
-
-    generate_keys();
-
-    UCS_TEST_MESSAGE << ":    elements   :init  :lookup:remove";
-    for (int i = 0; hashes[i] != NULL; ++i) {
-        perf_compare_base *cur_hash = hashes[i];
-        for (int j = 0; trip_counts[j] > 0; ++j) {
-            size_t num_elems = trip_counts[j];
-
-            ucs_time_t insert_ns = cur_hash->initialize(num_elems);
-            ucs_time_t lookup_ns = cur_hash->lookup(num_elems);
-
-            if ((1024 == num_elems) &&
-                ((cur_hash == perf_compare_khash_ptr) ||
-                 (cur_hash == perf_compare_sglib_ptr)))
-            {
-                check_lookup_perf(cur_hash, num_elems);
-            }
-
-            ucs_time_t remove_ns = cur_hash->cleanup(num_elems);
-
-            UCS_TEST_MESSAGE << FIELD_ALIGN(6) << cur_hash->get_name()
-                             << FIELD_ALIGN(8) << num_elems
-                             << FIELD_ALIGN(6) << insert_ns
-                             << FIELD_ALIGN(6) << lookup_ns
-                             << FIELD_ALIGN(6) << remove_ns;
-        }
-        delete cur_hash;
-    }
-}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_log.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_log.cc
index 3778019ab..89ca33072 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_log.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_log.cc
@@ -41,6 +41,7 @@ public:
 
     virtual void cleanup() {
         ucs_log_cleanup();
+        m_num_log_handlers_before = 0;
         pop_config();
         check_log_file();
         unlink(logfile);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_math.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_math.cc
index b39feab9d..16cc44aec 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_math.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_math.cc
@@ -54,17 +54,26 @@ UCS_TEST_F(test_math, circular_compare) {
 }
 
 UCS_TEST_F(test_math, bitops) {
-    EXPECT_EQ((unsigned)0,  ucs_ffs64(0xfffff));
-    EXPECT_EQ((unsigned)16, ucs_ffs64(0xf0000));
-    EXPECT_EQ((unsigned)1,  ucs_ffs64(0x4002));
-    EXPECT_EQ((unsigned)41, ucs_ffs64(1ull<<41));
-
-    EXPECT_EQ((unsigned)0, ucs_ilog2(1));
-    EXPECT_EQ((unsigned)2, ucs_ilog2(4));
-    EXPECT_EQ((unsigned)2, ucs_ilog2(5));
-    EXPECT_EQ((unsigned)2, ucs_ilog2(7));
-    EXPECT_EQ((unsigned)14, ucs_ilog2(17000));
-    EXPECT_EQ((unsigned)40, ucs_ilog2(1ull<<40));
+    EXPECT_EQ(0u,  ucs_ffs64(0xfffff));
+    EXPECT_EQ(16u, ucs_ffs64(0xf0000));
+    EXPECT_EQ(1u,  ucs_ffs64(0x4002));
+    EXPECT_EQ(41u, ucs_ffs64(1ull<<41));
+
+    EXPECT_EQ(0u,  ucs_ilog2(1));
+    EXPECT_EQ(2u,  ucs_ilog2(4));
+    EXPECT_EQ(2u,  ucs_ilog2(5));
+    EXPECT_EQ(2u,  ucs_ilog2(7));
+    EXPECT_EQ(14u, ucs_ilog2(17000));
+    EXPECT_EQ(40u, ucs_ilog2(1ull<<40));
+
+    EXPECT_EQ(0,  ucs_popcount(0));
+    EXPECT_EQ(2,  ucs_popcount(5));
+    EXPECT_EQ(16, ucs_popcount(0xffff));
+    EXPECT_EQ(48, ucs_popcount(0xffffffffffffUL));
+
+    EXPECT_EQ(0, ucs_count_trailing_zero_bits(1));
+    EXPECT_EQ(28, ucs_count_trailing_zero_bits(0x10000000));
+    EXPECT_EQ(32, ucs_count_trailing_zero_bits(0x100000000UL));
 }
 
 #define TEST_ATOMIC_ADD(_bitsize) \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtrack.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtrack.cc
index 6978e0870..830890f72 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtrack.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtrack.cc
@@ -147,7 +147,7 @@ UCS_TEST_F(test_memtrack, sysv) {
     size = ALLOC_SIZE;
 
     status = ucs_sysv_alloc(&size, std::numeric_limits<size_t>::max(), &ptr, 0,
-                            &shmid, ALLOC_NAME);
+                            ALLOC_NAME, &shmid);
     ASSERT_UCS_OK(status);
     ASSERT_NE((void *)NULL, ptr);
 
@@ -188,16 +188,13 @@ UCS_TEST_F(test_memtrack, mmap) {
 
 UCS_TEST_F(test_memtrack, custom) {
     void *ptr, *initial_ptr;
-    size_t size;
 
-    size = ucs_memtrack_adjust_alloc_size(ALLOC_SIZE);
-    initial_ptr = ptr = malloc(size);
-    ucs_memtrack_allocated(&ptr, &size, ALLOC_NAME);
+    initial_ptr = ptr = malloc(ALLOC_SIZE);
+    ucs_memtrack_allocated(ptr, ALLOC_SIZE, ALLOC_NAME);
 
-    EXPECT_EQ(size_t(ALLOC_SIZE), size);
-    memset(ptr, 0, size);
+    memset(ptr, 0, ALLOC_SIZE);
 
-    ucs_memtrack_releasing(&ptr);
+    ucs_memtrack_releasing(ptr);
     ASSERT_EQ(initial_ptr, ptr);
     free(ptr);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtype_cache.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtype_cache.cc
new file mode 100644
index 000000000..32602b90d
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_memtype_cache.cc
@@ -0,0 +1,80 @@
+/**
+ * Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+ *
+ * See file LICENSE for terms.
+ */
+
+#include <common/test.h>
+#if HAVE_CUDA
+#include <cuda.h>
+#include <cuda_runtime.h>
+#endif
+extern "C" {
+#include <ucs/sys/memtype_cache.h>
+}
+
+
+class test_memtype_cache : public ucs::test {
+protected:
+
+    virtual void init() {
+        ucs_status_t status;
+
+        ucs::test::init();
+        status = ucs_memtype_cache_create(&m_memtype_cache);
+        ASSERT_UCS_OK(status);
+    }
+
+    virtual void cleanup() {
+        ucs_memtype_cache_destroy(m_memtype_cache);
+        ucs::test::cleanup();
+    }
+
+    ucs_memtype_cache_t *m_memtype_cache;
+};
+
+#if HAVE_CUDA
+UCS_TEST_F(test_memtype_cache, basic_cuda) {
+    cudaError_t cerr;
+    void *ptr;
+    ucm_mem_type_t ucm_mem_type;
+    ucs_status_t status;
+
+    /* set cuda device */
+    if (cudaSetDevice(0) != cudaSuccess) {
+        UCS_TEST_SKIP_R("can't set cuda device");
+    }
+
+    cerr = cudaMalloc(&ptr, 64);
+    EXPECT_EQ(cerr, cudaSuccess);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 64, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 32, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, (void *)((uintptr_t)ptr + 1), 7, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 1, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, (void *)((uintptr_t) ptr + 63), 1, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 0, &ucm_mem_type);
+    EXPECT_UCS_OK(status);
+    EXPECT_EQ(ucm_mem_type, UCM_MEM_TYPE_CUDA);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 65, &ucm_mem_type);
+    EXPECT_TRUE(status == UCS_ERR_NO_ELEM);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, (void *)((uintptr_t) ptr + 64), 1, &ucm_mem_type);
+    EXPECT_TRUE(status == UCS_ERR_NO_ELEM);
+
+    cerr = cudaFree(ptr);
+    EXPECT_EQ(cerr, cudaSuccess);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 64, &ucm_mem_type);
+    EXPECT_TRUE(status == UCS_ERR_NO_ELEM);
+    status = ucs_memtype_cache_lookup(m_memtype_cache, ptr, 1, &ucm_mem_type);
+    EXPECT_TRUE(status == UCS_ERR_NO_ELEM);
+}
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_pgtable.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_pgtable.cc
index 16895bd20..3444e290c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_pgtable.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_pgtable.cc
@@ -258,6 +258,17 @@ UCS_TEST_F(test_pgtable, nonexist_remove) {
     remove(&region2);
 }
 
+UCS_TEST_F(test_pgtable, search_large_region) {
+    ucs_pgt_region_t region = {0x3c03cb00, 0x3c03f600};
+    insert(&region, UCS_OK);
+
+    search_result_t result = search(0x36990000, 0x3c810000);
+    EXPECT_EQ(1u, result.size());
+    EXPECT_EQ(&region, result.front());
+
+    remove(&region);
+}
+
 class test_pgtable_perf : public test_pgtable {
 protected:
 
@@ -279,9 +290,10 @@ protected:
             return NULL;
         } else {
             ucs_pgt_region_t *region = *iter;
-            ucs_assertv(address < region->end,
-                        "address=0x%lx region 0x%lx..0x%lx", address,
-                        region->start, region->end);
+            EXPECT_LT(address, region->end) << std::hex << "address="
+                                            << address << " region "
+                                            << region->start << ".."
+                                            << region->end << std::dec;
             return (address >= region->start) ? region : NULL;
         }
     }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_profile.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_profile.cc
index db41b86de..11b9b9c7a 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_profile.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_profile.cc
@@ -7,7 +7,8 @@
 #include <common/test.h>
 extern "C" {
 #include <ucs/sys/sys.h>
-#include <ucs/debug/profile.h>
+#include <ucs/time/time.h>
+#include <ucs/profile/profile.h>
 }
 
 #include <fstream>
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_rcache.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_rcache.cc
index ff2bd3ca8..ffacc9ccc 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_rcache.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_rcache.cc
@@ -8,8 +8,11 @@
 extern "C" {
 #include <ucs/arch/atomic.h>
 #include <ucs/sys/math.h>
+#include <ucs/stats/stats.h>
 #include <ucs/sys/rcache.h>
+#include <ucs/sys/rcache_int.h>
 #include <ucs/sys/sys.h>
+#include <ucm/api/ucm.h>
 }
 
 
@@ -36,12 +39,13 @@ protected:
             sizeof(region),
             UCS_PGT_ADDR_ALIGN,
             ucs_get_page_size(),
+            UCM_EVENT_VM_UNMAPPED,
             1000,
             &ops,
             reinterpret_cast<void*>(this)
         };
         UCS_TEST_CREATE_HANDLE(ucs_rcache_t*, m_rcache, ucs_rcache_destroy,
-                               ucs_rcache_create, &params, "test", NULL);
+                               ucs_rcache_create, &params, "test", ucs_stats_get_root());
     }
 
     virtual void cleanup() {
@@ -133,7 +137,8 @@ protected:
 private:
 
     static ucs_status_t mem_reg_cb(void *context, ucs_rcache_t *rcache,
-                                   void *arg, ucs_rcache_region_t *r)
+                                   void *arg, ucs_rcache_region_t *r,
+                                   uint16_t rcache_mem_reg_flags)
     {
         return reinterpret_cast<test_rcache*>(context)->mem_reg(
                         ucs_derived_of(r, struct region));
@@ -602,3 +607,166 @@ UCS_MT_TEST_F(test_rcache_no_register, merge_invalid_prot_slow, 5)
 
     munmap(mem, size1+size2);
 }
+
+#if ENABLE_STATS
+class test_rcache_stats : public test_rcache {
+protected:
+
+    virtual void init() {
+        ucs_stats_cleanup();
+        push_config();
+        modify_config("STATS_DEST",    "file:/dev/null");
+        modify_config("STATS_TRIGGER", "exit");
+        ucs_stats_init();
+        ASSERT_TRUE(ucs_stats_is_active());
+        test_rcache::init();
+    }
+
+    virtual void cleanup() {
+        test_rcache::cleanup();
+        ucs_stats_cleanup();
+        pop_config();
+        ucs_stats_init();
+    }
+
+    int get_counter(int stat) {
+        return (int)UCS_STATS_GET_COUNTER(m_rcache.get()->stats, stat);
+    }
+
+    /* a helper function for stats tests debugging */
+    void dump_stats() {
+        printf("gets %d hf %d hs %d misses %d merges %d unmaps %d"
+               " unmaps_inv %d puts %d regs %d deregs %d\n",
+               get_counter(UCS_RCACHE_GETS),
+               get_counter(UCS_RCACHE_HITS_FAST),
+               get_counter(UCS_RCACHE_HITS_SLOW),
+               get_counter(UCS_RCACHE_MISSES),
+               get_counter(UCS_RCACHE_MERGES),
+               get_counter(UCS_RCACHE_UNMAPS),
+               get_counter(UCS_RCACHE_UNMAP_INVALIDATES),
+               get_counter(UCS_RCACHE_PUTS),
+               get_counter(UCS_RCACHE_REGS),
+               get_counter(UCS_RCACHE_DEREGS));
+    }
+};
+
+UCS_TEST_F(test_rcache_stats, basic) {
+    static const size_t size = 4096;
+    void *ptr = malloc(size);
+    region *r1, *r2;
+
+    r1 = get(ptr, size);
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_MISSES));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_REGS));
+
+    r2 = get(ptr, size);
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_HITS_FAST));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_MISSES));
+
+    put(r1);
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_PUTS));
+
+    put(r2);
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_PUTS));
+
+    free(ptr);
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_PUTS));
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_DEREGS));
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_UNMAPS));
+}
+
+UCS_TEST_F(test_rcache_stats, unmap_dereg) {
+    static const size_t size1 = 1024 * 1024;
+    void *mem = alloc_pages(size1, PROT_READ|PROT_WRITE);
+    region *r1;
+
+    r1 = get(mem, size1);
+    put(r1);
+
+    /* Should generate umap event but no dereg or unmap invalidation.
+     * We can have more unmap events if releasing the region structure triggers
+     * releasing memory back to the OS.
+     */
+    munmap(mem, size1);
+    EXPECT_GE(get_counter(UCS_RCACHE_UNMAPS), 1);
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_UNMAP_INVALIDATES));
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_DEREGS));
+
+    mem = alloc_pages(size1, PROT_READ|PROT_WRITE);
+
+    /*
+     * Adding a new region shall force a processing of invalidation queue and dereg
+     */
+    r1 = get(mem, size1);
+    EXPECT_GE(get_counter(UCS_RCACHE_UNMAPS), 1);
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_UNMAP_INVALIDATES));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_DEREGS));
+
+    /* cleanup */
+    put(r1);
+    munmap(mem, size1);
+}
+
+UCS_TEST_F(test_rcache_stats, merge) {
+    static const size_t size1 = 1024 * 1024;
+    void *mem = alloc_pages(size1, PROT_READ|PROT_WRITE);
+    region *r1, *r2;
+
+    r1 = get(mem, 8192);
+    /* should trigger merge of the two regions */
+    r2 = get((char *)mem + 4096, 8192);
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_MERGES));
+
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_MISSES));
+
+    put(r1);
+    put(r2);
+    munmap(mem, size1);
+}
+
+UCS_TEST_F(test_rcache_stats, hits_slow) {
+    static const size_t size1 = 1024 * 1024;
+    region *r1, *r2;
+    void *mem1, *mem2;
+
+    mem1 = alloc_pages(size1, PROT_READ|PROT_WRITE);
+    r1 = get(mem1, size1);
+    put(r1);
+
+    mem2 = alloc_pages(size1, PROT_READ|PROT_WRITE);
+    r1 = get(mem2, size1);
+
+    /* generate unmap event */
+    munmap(mem1, size1);
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_UNMAPS));
+
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_PUTS));
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_MISSES));
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_UNMAP_INVALIDATES));
+    EXPECT_EQ(0, get_counter(UCS_RCACHE_DEREGS));
+    /* it should produce a slow hit because there is
+     * a pending unmap event
+     */
+    r2 = get(mem2, size1);
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_HITS_SLOW));
+
+    EXPECT_EQ(3, get_counter(UCS_RCACHE_GETS));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_PUTS));
+    EXPECT_EQ(2, get_counter(UCS_RCACHE_MISSES));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_UNMAPS));
+    /* unmap event processed */
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_UNMAP_INVALIDATES));
+    EXPECT_EQ(1, get_counter(UCS_RCACHE_DEREGS));
+
+    put(r1);
+    put(r2);
+    munmap(mem2, size1);
+}
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats.cc
index a8a6f447c..d29dfa05c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats.cc
@@ -57,7 +57,7 @@ public:
     void prepare_nodes(ucs_stats_node_t **cat_node,
                        ucs_stats_node_t *data_nodes[NUM_DATA_NODES]) {
         static ucs_stats_class_t category_stats_class = {
-            "category", 0, {}
+            "category", 0
         };
 
         ucs_status_t status = UCS_STATS_NODE_ALLOC(cat_node,
@@ -149,7 +149,6 @@ public:
 
     void read_and_check_stats(ucs_stats_node_t *data_nodes[NUM_DATA_NODES]) {
         ucs_list_link_t *list = ucs_stats_server_get_stats(m_server);
-        ucs_assert(1ul == ucs_list_length(list));
         ASSERT_EQ(1ul, ucs_list_length(list));
         check_tree(ucs_list_head(list, ucs_stats_node_t, list), data_nodes);
         ucs_stats_server_purge_stats(m_server);
@@ -258,7 +257,7 @@ UCS_TEST_F(stats_on_demand_test, null_root) {
     ucs_stats_node_t       *cat_node;
 
     static ucs_stats_class_t category_stats_class = {
-        "category", 0, {}
+        "category", 0
     };
     ucs_status_t status = UCS_STATS_NODE_ALLOC(&cat_node, &category_stats_class,
                                                NULL);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats_filter.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats_filter.cc
index 6a5b21c91..9dcfa2002 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats_filter.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_stats_filter.cc
@@ -7,6 +7,7 @@
 #include <common/test.h>
 extern "C" {
 #include <ucs/stats/stats.h>
+#include <ucs/stats/libstats.h>
 #include <ucs/sys/sys.h>
 }
 
@@ -44,6 +45,7 @@ public:
         modify_config("STATS_DEST",    stats_dest_config().c_str());
         modify_config("STATS_TRIGGER", stats_trigger_config().c_str());
         modify_config("STATS_FORMAT", stats_format_config().c_str());
+        modify_config("WARN_INVARIANT_TSC", "n");
         ucs_stats_init();
         ASSERT_TRUE(ucs_stats_is_active());
     }
@@ -61,7 +63,7 @@ public:
 
     void prepare_nodes() {
         static ucs_stats_class_t category_stats_class = {
-            "category", 0, {}
+            "category", 0
         };
 
         ucs_status_t status = UCS_STATS_NODE_ALLOC(&cat_node, &category_stats_class,
@@ -192,8 +194,10 @@ UCS_TEST_F(stats_filter_report, report) {
         }
     }
 
-    std::string compared_string = std::string(ucs_get_host_name()) + ":" +
-                                  ucs::to_string(getpid()) + ":" +
+    std::string header = std::string(ucs_get_host_name()) + ":" +
+                                     ucs::to_string(getpid());
+
+    std::string compared_string = header.substr(0, UCS_STAT_NAME_MAX - 1) + ":" +
                                   "\n  category:\n" +
                                   "    data-0:\n" +
                                   "      counter0: 10\n" +
@@ -234,8 +238,11 @@ UCS_TEST_F(stats_filter_agg, report_agg) {
         }
     }
 
-    std::string compared_string = std::string(ucs_get_host_name()) + ":" +
-                                  ucs::to_string(getpid()) + ":" +
+    std::string header = std::string(ucs_get_host_name()) + ":" +
+                                     ucs::to_string(getpid());
+
+    std::string compared_string = header.substr(0, UCS_STAT_NAME_MAX - 1) +
+                                  ":" +
                                   "\n  category:\n"
                                   "    data*:\n"
                                   "      counter0: 30\n"
@@ -264,8 +271,11 @@ UCS_TEST_F(stats_filter_summary, summary) {
             break;
         }
     }
-    std::string compared_string = std::string(ucs_get_host_name()) + ":" +
-                                  ucs::to_string(getpid()) +
+
+    std::string node_name = std::string(ucs_get_host_name()) + ":" +
+                            ucs::to_string(getpid());
+    node_name.resize(std::min<size_t>(node_name.length(), UCS_STAT_NAME_MAX - 1));
+    std::string compared_string = node_name +
                                   ":data*:{counter0:30 counter1:60 " +
                                   "counter2:90 counter3:120} \n";
     EXPECT_EQ(compared_string, output);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_strided_alloc.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_strided_alloc.cc
new file mode 100644
index 000000000..e309b5a5a
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/ucs/test_strided_alloc.cc
@@ -0,0 +1,63 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include <common/test.h>
+extern "C" {
+#include <ucs/datastruct/strided_alloc.h>
+}
+
+#include <limits.h>
+#include <vector>
+#include <queue>
+
+class test_strided_alloc : public ucs::test {
+protected:
+    static const size_t area_size   = 64;
+    static const unsigned num_areas = 3;
+};
+
+
+UCS_TEST_F(test_strided_alloc, basic) {
+
+    ucs_strided_alloc_t sa;
+
+    ucs_strided_alloc_init(&sa, area_size, num_areas);
+
+    std::vector<void*> objs;
+    for (size_t i = 0; i < 2; ++i) {
+        /* allocate */
+        void *base = ucs_strided_alloc_get(&sa, "test");
+
+        for (unsigned j = 0; j < num_areas; ++j) {
+            void *area = ucs_strided_elem_get(base, 0, j);
+            memset(area, i*j, area_size);
+        }
+
+        /* save in a vector */
+        objs.push_back(base);
+    }
+
+    /* check data integrity */
+    char buf[area_size];
+    for (size_t i = 0; i < objs.size(); ++i) {
+        void *base = objs[i];
+
+        for (unsigned j = 0; j < num_areas; ++j) {
+            void *area = ucs_strided_elem_get(base, 0, j);
+            memset(buf, i*j, area_size);
+            EXPECT_EQ(0, memcmp(area, buf, area_size));
+        }
+    }
+
+    /* release */
+    while (!objs.empty()) {
+        void *base = objs.back();
+        objs.pop_back();
+        ucs_strided_alloc_put(&sa, base);
+    }
+
+    ucs_strided_alloc_cleanup(&sa);
+}
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_cq_moderation.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_cq_moderation.cc
index a3826e5f0..4aee357a3 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_cq_moderation.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_cq_moderation.cc
@@ -117,7 +117,7 @@ void test_uct_cq_moderation::run_test(uct_iface_h iface) {
     check_caps(UCT_IFACE_FLAG_EVENT_SEND_COMP);
     check_caps(UCT_IFACE_FLAG_EVENT_RECV);
 
-    uct_iface_set_am_handler(m_receiver->iface(), 0, am_cb, this, UCT_CB_FLAG_SYNC);
+    uct_iface_set_am_handler(m_receiver->iface(), 0, am_cb, this, 0);
 
     connect();
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_dc.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_dc.cc
index 442c59330..cb105dafd 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_dc.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_dc.cc
@@ -22,7 +22,7 @@ extern "C" {
     _UCT_INSTANTIATE_TEST_CASE(_test_case, dc_mlx5)
 
 
-class test_dc : public uct_test {
+class test_dc : public test_rc {
 public:
     virtual void init() {
         uct_test::init();
@@ -33,10 +33,8 @@ public:
         m_e2 = uct_test::create_entity(0);
         m_entities.push_back(m_e2);
 
-        uct_iface_set_am_handler(m_e1->iface(), 0, am_dummy_handler,
-                                 NULL, UCT_CB_FLAG_SYNC);
-        uct_iface_set_am_handler(m_e2->iface(), 0, am_dummy_handler,
-                                 NULL, UCT_CB_FLAG_SYNC);
+        uct_iface_set_am_handler(m_e1->iface(), 0, am_dummy_handler, NULL, 0);
+        uct_iface_set_am_handler(m_e2->iface(), 0, am_dummy_handler, NULL, 0);
     }
 
     static uct_dc_iface_t* dc_iface(entity *e) {
@@ -73,7 +71,6 @@ public:
     }
 
 protected:
-    entity *m_e1, *m_e2;
 
     struct dcs_comp {
         uct_completion_t uct_comp;
@@ -85,7 +82,7 @@ protected:
         struct dcs_comp *comp = (struct dcs_comp *)uct_comp;
         uct_dc_ep_t *ep;
 
-        ASSERT_UCS_OK(status);
+        EXPECT_UCS_OK(status);
 
         ep = dc_ep(comp->e, 0);
         /* dci must be released before completion cb is called */
@@ -304,9 +301,9 @@ UCS_TEST_P(test_dc, dcs_ep_flush_pending) {
     preq.is_done = 0;
     preq.e = m_e1;
     preq.uct_req.func = uct_pending_flush;
-    status = uct_ep_pending_add(m_e1->ep(0), &preq.uct_req);
+    status = uct_ep_pending_add(m_e1->ep(0), &preq.uct_req, 0);
     EXPECT_UCS_OK(status);
-    
+
     /* progress till ep is flushed */
     do {
         progress();
@@ -319,6 +316,48 @@ UCS_TEST_P(test_dc, dcs_ep_flush_pending) {
     EXPECT_EQ(0, iface->tx.stack_top);
 }
 
+/* Check that the following sequnce works ok:
+ * - Add some pending request to DCI wait queue
+ * - Try to send something from this ep. This will force ep to take free DCI
+ *   (the send will not succeed anyway)
+ * - Progress all pendings
+ * - Make sure that there is no any assertion and everyting is ok
+ *   (just send something).
+ * */
+UCS_TEST_P(test_dc, dcs_ep_am_pending) {
+
+    ucs_status_t status;
+    uct_dc_iface_t *iface;
+
+    m_e1->connect_to_iface(0, *m_e2);
+    m_e1->connect_to_iface(1, *m_e2);
+
+    /* use all iface resources */
+    iface = dc_iface(m_e1);
+    iface->super.tx.cq_available = 8;
+    do {
+        status = uct_ep_am_short(m_e1->ep(1), 0, 0, NULL, 0);
+    } while (status == UCS_OK);
+
+    EXPECT_EQ(UCS_ERR_NO_RESOURCE, status);
+
+    /* put AM op on pending */
+    preq.e            = m_e1;
+    preq.uct_req.func = uct_pending_flush;
+    status            = uct_ep_pending_add(m_e1->ep(0), &preq.uct_req, 0);
+    EXPECT_UCS_OK(status);
+
+    status = uct_ep_am_short(m_e1->ep(0), 0, 0, NULL, 0);
+    EXPECT_EQ(UCS_ERR_NO_RESOURCE, status);
+
+    flush();
+
+    status = uct_ep_am_short(m_e1->ep(0), 0, 0, NULL, 0);
+    EXPECT_EQ(UCS_OK, status);
+
+    flush();
+}
+
 /* check that ep does not hold dci after
  * purge
  */
@@ -350,7 +389,7 @@ UCS_TEST_P(test_dc, dcs_ep_purge_pending) {
     preq.is_done = 0;
     preq.e = m_e1;
     preq.uct_req.func = uct_pending_dummy;
-    status = uct_ep_pending_add(m_e1->ep(0), &preq.uct_req);
+    status = uct_ep_pending_add(m_e1->ep(0), &preq.uct_req, 0);
     EXPECT_UCS_OK(status);
 
     do {
@@ -364,6 +403,10 @@ UCS_TEST_P(test_dc, dcs_ep_purge_pending) {
     EXPECT_EQ(0, iface->tx.stack_top);
 }
 
+UCS_TEST_P(test_dc, stress_iface_ops) {
+    test_iface_ops();
+}
+
 UCT_DC_INSTANTIATE_TEST_CASE(test_dc)
 
 
@@ -418,7 +461,7 @@ UCS_TEST_P(test_dc_flow_control, fc_disabled_pending_no_dci) {
             get_fc_ptr(m_e1, ep_index)->fc_wnd = 0;
 
             /* Add to pending */
-            status = uct_ep_pending_add(m_e1->ep(ep_index), &pending_req.uct);
+            status = uct_ep_pending_add(m_e1->ep(ep_index), &pending_req.uct, 0);
             ASSERT_UCS_OK(status);
 
             wait_for_flag(&pending_req.cb_count);
@@ -448,10 +491,13 @@ UCS_TEST_P(test_dc_flow_control, soft_request)
     EXPECT_EQ(get_fc_ptr(m_e1)->fc_wnd, s_thresh - 1);
 }
 
-/* Check that flush returns UCS_OK even if there is an outgoing grant request */
+/* Check that:
+ * 1) flush returns UCS_OK even if there is an outgoing grant request
+ * 2) No crash when grant for destroyed ep arrives */
 UCS_TEST_P(test_dc_flow_control, flush_destroy)
 {
     int wnd = 5;
+    ucs_status_t status;
 
     disable_entity(m_e2);
 
@@ -461,11 +507,24 @@ UCS_TEST_P(test_dc_flow_control, flush_destroy)
 
     send_am_and_flush(m_e1, wnd);
 
-    EXPECT_UCS_OK(uct_ep_flush(m_e1->ep(0), 0, NULL));
+    /* At this point m_e1 sent grant request to m_e2, m_e2 received all
+     * messages and added grant to m_e1 to pending queue
+     * (because it does not have tx resources yet) */
+
+    /* Invoke flush in a loop, because some send completions may not be polled yet */
+    ucs_time_t timeout = ucs_get_time() + ucs_time_from_sec(DEFAULT_TIMEOUT_SEC);
+    do {
+        short_progress_loop();
+        status = uct_ep_flush(m_e1->ep(0), 0, NULL);
+    } while (((status == UCS_ERR_NO_RESOURCE) || (status == UCS_INPROGRESS)) &&
+             (ucs_get_time() < timeout));
+    ASSERT_UCS_OK(status);
+
     m_e1->destroy_eps();
 
-    /* Enable send capabilities of m_e2 and send AM message
-     * to force pending queue dispatch */
+    /* Enable send capabilities of m_e2 and send AM message to force pending queue
+     * dispatch. Thus, pending grant will be sent to m_e1. There should not be
+     * any warning/error and/or crash. */
     enable_entity(m_e2);
     set_tx_moderation(m_e2, 0);
     send_am_and_flush(m_e2, 1);
@@ -486,7 +545,7 @@ UCS_TEST_P(test_dc_flow_control, dci_leak)
     uct_pending_req_t req;
     req.func = reinterpret_cast<ucs_status_t (*)(uct_pending_req*)>
                                (ucs_empty_function_return_no_resource);
-    EXPECT_UCS_OK(uct_ep_pending_add(m_e1->ep(0), &req));
+    EXPECT_UCS_OK(uct_ep_pending_add(m_e1->ep(0), &req, 0));
 
     /* Make sure that ep does not hold dci when sends completed */
     uct_dc_iface_t *iface = ucs_derived_of(m_e1->iface(), uct_dc_iface_t);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib.cc
index f2df45f49..7066a7444 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib.cc
@@ -13,6 +13,7 @@ extern "C" {
 #include <ucs/time/time.h>
 #include <uct/ib/base/ib_device.h>
 #include <uct/ib/base/ib_iface.h>
+#include <uct/ib/base/ib_md.h>
 }
 
 
@@ -80,18 +81,35 @@ public:
     }
 
 #if HAVE_DECL_IBV_LINK_LAYER_ETHERNET
-    void test_eth_port(struct ibv_port_attr port_attr, struct ibv_context *ibctx,
-                       unsigned port_num, uct_ib_iface_config_t *ib_config,
+    void test_eth_port(struct ibv_device *device, struct ibv_port_attr port_attr,
+                       struct ibv_context *ibctx, unsigned port_num,
                        ib_port_desc_t *port_desc) {
 
         union ibv_gid gid;
+        uct_ib_md_config_t *md_config = ucs_derived_of(m_md_config, uct_ib_md_config_t);
+        char md_name[UCT_MD_NAME_MAX];
+        uct_md_h uct_md;
+        uct_ib_md_t *ib_md;
+        ucs_status_t status;
+        uint8_t gid_index;
 
         /* no pkeys for Ethernet */
         port_desc->have_pkey = 0;
 
+        uct_ib_make_md_name(md_name, device);
+
+        status = uct_ib_md_open(md_name, m_md_config, &uct_md);
+        ASSERT_UCS_OK(status);
+
+        ib_md = ucs_derived_of(uct_md, uct_ib_md_t);
+        status = uct_ib_device_select_gid_index(&ib_md->dev,
+                                                port_num, md_config->ext.gid_index,
+                                                &gid_index);
+        ASSERT_UCS_OK(status);
+
         /* check the gid index */
-        if (ibv_query_gid(ibctx, port_num, ib_config->gid_index, &gid) != 0) {
-            UCS_TEST_ABORT("Failed to query gid (index=" << ib_config->gid_index << ")");
+        if (ibv_query_gid(ibctx, port_num, gid_index, &gid) != 0) {
+            UCS_TEST_ABORT("Failed to query gid (index=" << gid_index << ")");
         }
         if (uct_ib_device_is_gid_raw_empty(gid.raw)) {
             port_desc->have_valid_gid_idx = 0;
@@ -99,6 +117,7 @@ public:
             port_desc->have_valid_gid_idx = 1;
         }
 
+        uct_ib_md_close(uct_md);
     }
 #endif
 
@@ -113,7 +132,6 @@ public:
         struct ibv_device **device_list;
         struct ibv_context *ibctx = NULL;
         struct ibv_port_attr port_attr;
-        uct_ib_iface_config_t *ib_config = ucs_derived_of(m_iface_config, uct_ib_iface_config_t);
         int num_devices, i, found = 0;
 
         /* get device list */
@@ -147,7 +165,7 @@ public:
         lmc_find(port_attr, port_desc);
 
         if (IBV_PORT_IS_LINK_LAYER_ETHERNET(&port_attr)) {
-            test_eth_port(port_attr, ibctx, port_num, ib_config, port_desc);
+            test_eth_port(device_list[i], port_attr, ibctx, port_num, port_desc);
             goto out;
         }
 
@@ -179,25 +197,28 @@ out:
         free(dev_name);
     }
 
-    void test_address_pack(uct_ib_address_type_t scope, uint64_t subnet_prefix) {
+    void test_address_pack(uint64_t subnet_prefix) {
+        uct_ib_iface_t *iface = ucs_derived_of(m_e1->iface(), uct_ib_iface_t);
         static const uint16_t lid_in = 0x1ee7;
         union ibv_gid gid_in, gid_out;
         uct_ib_address_t *ib_addr;
         uint16_t lid_out;
-        uint8_t is_global;
 
-        ib_addr = (uct_ib_address_t*)malloc(uct_ib_address_size(scope));
+        ib_addr = (uct_ib_address_t*)malloc(uct_ib_address_size(iface));
 
         gid_in.global.subnet_prefix = subnet_prefix;
         gid_in.global.interface_id  = 0xdeadbeef;
-        uct_ib_address_pack(ib_device(m_e1), scope, &gid_in, lid_in, ib_addr);
+        uct_ib_address_pack(iface, &gid_in, lid_in, ib_addr);
 
-        uct_ib_address_unpack(ib_addr, &lid_out, &is_global, &gid_out);
+        uct_ib_address_unpack(ib_addr, &lid_out, &gid_out);
 
-        EXPECT_EQ((scope != UCT_IB_ADDRESS_TYPE_LINK_LOCAL), is_global);
-        EXPECT_EQ(lid_in, lid_out);
+        if (IBV_PORT_IS_LINK_LAYER_ETHERNET(uct_ib_iface_port_attr(iface))) {
+            EXPECT_TRUE(iface->is_global_addr);
+        } else {
+            EXPECT_EQ(lid_in, lid_out);
+        }
 
-        if (is_global) {
+        if (iface->is_global_addr) {
             EXPECT_EQ(gid_in.global.subnet_prefix, gid_out.global.subnet_prefix);
             EXPECT_EQ(gid_in.global.interface_id,  gid_out.global.interface_id);
         }
@@ -217,7 +238,7 @@ out:
         recv_buffer->length = 0; /* Initialize length to 0 */
 
         /* set a callback for the uct to invoke for receiving the data */
-        uct_iface_set_am_handler(m_e2->iface(), 0, ib_am_handler , recv_buffer, UCT_CB_FLAG_SYNC);
+        uct_iface_set_am_handler(m_e2->iface(), 0, ib_am_handler , recv_buffer, 0);
 
         /* send the data */
         uct_ep_am_short(m_e1->ep(0), 0, test_ib_hdr, &send_data, sizeof(send_data));
@@ -281,7 +302,7 @@ UCS_TEST_P(test_uct_ib, non_default_lmc, "IB_LID_PATH_BITS=1")
 }
 
 #if HAVE_DECL_IBV_LINK_LAYER_ETHERNET
-UCS_TEST_P(test_uct_ib, non_default_gid_idx, "IB_GID_INDEX=1")
+UCS_TEST_P(test_uct_ib, non_default_gid_idx, "GID_INDEX=1")
 {
     ib_port_desc_t *port_desc;
 
@@ -303,9 +324,9 @@ UCS_TEST_P(test_uct_ib, non_default_gid_idx, "IB_GID_INDEX=1")
 
 UCS_TEST_P(test_uct_ib, address_pack) {
     initialize();
-    test_address_pack(UCT_IB_ADDRESS_TYPE_LINK_LOCAL, UCT_IB_LINK_LOCAL_PREFIX);
-    test_address_pack(UCT_IB_ADDRESS_TYPE_SITE_LOCAL, UCT_IB_SITE_LOCAL_PREFIX | htobe64(0x7200));
-    test_address_pack(UCT_IB_ADDRESS_TYPE_GLOBAL,     0xdeadfeedbeefa880ul);
+    test_address_pack(UCT_IB_LINK_LOCAL_PREFIX);
+    test_address_pack(UCT_IB_SITE_LOCAL_PREFIX | htobe64(0x7200));
+    test_address_pack(0xdeadfeedbeefa880ul);
 }
 
 
@@ -344,7 +365,7 @@ public:
 
         /* set a callback for the uct to invoke for receiving the data */
         uct_iface_set_am_handler(m_e1->iface(), 0, ib_am_handler, m_buf1->ptr(),
-                                 UCT_CB_FLAG_SYNC);
+                                 0);
 
         test_uct_event_ib::bcopy_pack_count = 0;
     }
@@ -377,12 +398,12 @@ public:
 
     void check_send_cq(uct_iface_t *iface, size_t val) {
         uct_ib_iface_t *ib_iface = ucs_derived_of(iface, uct_ib_iface_t);
-        struct ibv_cq  *send_cq = ib_iface->send_cq;
+        struct ibv_cq  *send_cq = ib_iface->cq[UCT_IB_DIR_TX];
 
         if (val != send_cq->comp_events_completed) {
             uint32_t completed_evt = send_cq->comp_events_completed;
             /* need this call to acknowledge the completion to prevent iface dtor hung*/
-            ibv_ack_cq_events(ib_iface->send_cq, 1);
+            ibv_ack_cq_events(ib_iface->cq[UCT_IB_DIR_TX], 1);
             UCS_TEST_ABORT("send_cq->comp_events_completed have to be 1 but the value "
                            << completed_evt);
         }
@@ -390,12 +411,12 @@ public:
 
     void check_recv_cq(uct_iface_t *iface, size_t val) {
         uct_ib_iface_t *ib_iface = ucs_derived_of(iface, uct_ib_iface_t);
-        struct ibv_cq  *recv_cq = ib_iface->recv_cq;
+        struct ibv_cq  *recv_cq = ib_iface->cq[UCT_IB_DIR_RX];
 
         if (val != recv_cq->comp_events_completed) {
             uint32_t completed_evt = recv_cq->comp_events_completed;
             /* need this call to acknowledge the completion to prevent iface dtor hung*/
-            ibv_ack_cq_events(ib_iface->recv_cq, 1);
+            ibv_ack_cq_events(ib_iface->cq[UCT_IB_DIR_RX], 1);
             UCS_TEST_ABORT("recv_cq->comp_events_completed have to be 1 but the value "
                            << completed_evt);
         }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_md.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_md.cc
index 901443f68..6a8105c24 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_md.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_md.cc
@@ -5,18 +5,20 @@
 * See file LICENSE for terms.
 */
 
-extern "C" {
 #include <uct/api/uct.h>
 #include <ucs/time/time.h>
 #include <uct/ib/base/ib_md.h>
-}
+
 #include <common/test.h>
 #include <uct/test_md.h>
 
 class test_ib_md : public test_md
 {
 protected:
-    void ib_md_umr_check(void *rkey_buffer, bool amo_access);
+    void ib_md_umr_check(void *rkey_buffer,
+                         bool amo_access,
+                         size_t size = 8192);
+    bool has_ksm() const;
 };
 
 
@@ -25,15 +27,29 @@ protected:
  * UCT_MD_MEM_ACCESS_REMOTE_ATOMIC is not set
  */
 
-void test_ib_md::ib_md_umr_check(void *rkey_buffer, bool amo_access) {
-
+void test_ib_md::ib_md_umr_check(void *rkey_buffer,
+                                 bool amo_access,
+                                 size_t size)
+{
     ucs_status_t status;
-    size_t size = 8192;
-    void *buffer = malloc(size);
-    ASSERT_TRUE(buffer != NULL);
+    size_t alloc_size;
+    void *buffer;
+
+    if (ucs_get_phys_mem_size() < size * 8) {
+        UCS_TEST_SKIP_R("not enough physical memory");
+    }
+    if (ucs_get_memfree_size() < size * 4) {
+        UCS_TEST_SKIP_R("not enough free memory");
+    }
+
+    buffer     = NULL;
+    alloc_size = size;
+    status     = ucs_mmap_alloc(&alloc_size, &buffer, 0
+                                UCS_MEMTRACK_NAME("test_umr"));
+    ASSERT_UCS_OK(status);
 
     uct_mem_h memh;
-    status = uct_md_mem_reg(pd(), buffer, size, 
+    status = uct_md_mem_reg(md(), buffer, size,
                             amo_access ? UCT_MD_MEM_ACCESS_REMOTE_ATOMIC :
                                          UCT_MD_MEM_ACCESS_RMA,
                             &memh);
@@ -41,7 +57,7 @@ void test_ib_md::ib_md_umr_check(void *rkey_buffer, bool amo_access) {
     ASSERT_TRUE(memh != UCT_MEM_HANDLE_NULL);
 
     uct_ib_mem_t *ib_memh = (uct_ib_mem_t *)memh;
-    uct_ib_md_t  *ib_md = (uct_ib_md_t *)pd();
+    uct_ib_md_t  *ib_md = (uct_ib_md_t *)md();
 
     if (amo_access) {
         EXPECT_TRUE(ib_memh->flags & UCT_IB_MEM_ACCESS_REMOTE_ATOMIC);
@@ -51,7 +67,7 @@ void test_ib_md::ib_md_umr_check(void *rkey_buffer, bool amo_access) {
         EXPECT_FALSE(ib_memh->flags & UCT_IB_MEM_FLAG_ATOMIC_MR);
     }
 
-    status = uct_md_mkey_pack(pd(), memh, rkey_buffer);
+    status = uct_md_mkey_pack(md(), memh, rkey_buffer);
     EXPECT_UCS_OK(status);
 
     if (amo_access) {
@@ -67,50 +83,56 @@ void test_ib_md::ib_md_umr_check(void *rkey_buffer, bool amo_access) {
         EXPECT_TRUE(ib_memh->atomic_mr == NULL);
     }
 
-    status = uct_md_mem_dereg(pd(), memh);
+    status = uct_md_mem_dereg(md(), memh);
     EXPECT_UCS_OK(status);
-    free(buffer);
-}
 
-UCS_TEST_P(test_ib_md, ib_md_umr_rcache, "REG_METHODS=rcache") {
+    ucs_mmap_free(buffer, alloc_size);
+}
 
-    ucs_status_t status;
-    uct_md_attr_t md_attr;
-    void *rkey_buffer;
+bool test_ib_md::has_ksm() const {
+#ifdef HAVE_EXP_UMR_KSM
+    return ucs_derived_of(md(), uct_ib_md_t)->dev.dev_attr.exp_device_cap_flags &
+           IBV_EXP_DEVICE_UMR_FIXED_SIZE;
+#else
+    return false;
+#endif
+}
 
-    status = uct_md_query(pd(), &md_attr);
-    ASSERT_UCS_OK(status);
-    rkey_buffer = malloc(md_attr.rkey_packed_size);
-    ASSERT_TRUE(rkey_buffer != NULL);
+UCS_TEST_P(test_ib_md, ib_md_umr_rcache, "REG_METHODS=rcache") {
+    std::string rkey_buffer(md_attr().rkey_packed_size, '\0');
 
     /* The order is important here because
      * of registration cache. A cached region will
      * be promoted to atomic access but it will never be demoted 
      */
-    ib_md_umr_check(rkey_buffer, false);
-    ib_md_umr_check(rkey_buffer, true);
-
-    free(rkey_buffer);
+    ib_md_umr_check(&rkey_buffer[0], false);
+    ib_md_umr_check(&rkey_buffer[0], true);
 }
 
 UCS_TEST_P(test_ib_md, ib_md_umr_direct, "REG_METHODS=direct") {
+    std::string rkey_buffer(md_attr().rkey_packed_size, '\0');
 
-    ucs_status_t status;
-    uct_md_attr_t md_attr;
-    void *rkey_buffer;
+    /* without rcache the order is not really important */
+    ib_md_umr_check(&rkey_buffer[0], true);
+    ib_md_umr_check(&rkey_buffer[0], false);
+    ib_md_umr_check(&rkey_buffer[0], true);
+    ib_md_umr_check(&rkey_buffer[0], false);
+}
 
-    status = uct_md_query(pd(), &md_attr);
-    ASSERT_UCS_OK(status);
-    rkey_buffer = malloc(md_attr.rkey_packed_size);
-    ASSERT_TRUE(rkey_buffer != NULL);
+UCS_TEST_P(test_ib_md, ib_md_umr_ksm) {
+    std::string rkey_buffer(md_attr().rkey_packed_size, '\0');
+    ib_md_umr_check(&rkey_buffer[0], has_ksm(), UCT_IB_MD_MAX_MR_SIZE + 0x1000);
+}
 
-    /* without rcache the order is not really important */
-    ib_md_umr_check(rkey_buffer, true);
-    ib_md_umr_check(rkey_buffer, false);
-    ib_md_umr_check(rkey_buffer, true);
-    ib_md_umr_check(rkey_buffer, false);
+#if HAVE_UMR_KSM
+UCS_TEST_P(test_ib_md, umr_noninline_klm, "MAX_INLINE_KLM_LIST=1") {
 
-    free(rkey_buffer);
+    /* KLM list size would be 2, and setting MAX_INLINE_KLM_LIST=1 would force
+     * using non-inline UMR post_send.
+     */
+    std::string rkey_buffer(md_attr().rkey_packed_size, '\0');
+    ib_md_umr_check(&rkey_buffer[0], has_ksm(), UCT_IB_MD_MAX_MR_SIZE + 0x1000);
 }
+#endif
 
 _UCT_MD_INSTANTIATE_TEST_CASE(test_ib_md, ib)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_xfer.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_xfer.cc
index 510094e64..c592b87a8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_xfer.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ib_xfer.cc
@@ -72,7 +72,7 @@ UCT_INSTANTIATE_IB_TEST_CASE(uct_p2p_rma_test_alloc_methods)
 class uct_p2p_mix_test_alloc_methods : public uct_p2p_mix_test {};
 
 UCS_TEST_P(uct_p2p_mix_test_alloc_methods, mix1000_odp,
-           "REG_METHODS=odp,direct")
+           "REG_METHODS=odp,direct", "DM_COUNT?=0")
 {
     run(1000);
 }
@@ -85,3 +85,14 @@ UCS_TEST_P(uct_p2p_mix_test_alloc_methods, mix1000_rcache,
 
 UCT_INSTANTIATE_IB_TEST_CASE(uct_p2p_mix_test_alloc_methods)
 
+
+class uct_p2p_mix_test_indirect_atomic : public uct_p2p_mix_test {};
+
+UCS_TEST_P(uct_p2p_mix_test_indirect_atomic, mix1000_indirect_atomic,
+        "INDIRECT_ATOMIC=n")
+{
+    run(1000);
+}
+
+UCT_INSTANTIATE_IB_TEST_CASE(uct_p2p_mix_test_indirect_atomic)
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.cc
index 552dcfb9e..8b336e720 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.cc
@@ -31,12 +31,59 @@ void test_rc::connect()
     m_e1->connect(0, *m_e2, 0);
     m_e2->connect(0, *m_e1, 0);
 
-    uct_iface_set_am_handler(m_e1->iface(), 0, am_dummy_handler,
-                             NULL, UCT_CB_FLAG_SYNC);
-    uct_iface_set_am_handler(m_e2->iface(), 0, am_dummy_handler,
-                             NULL, UCT_CB_FLAG_SYNC);
+    uct_iface_set_am_handler(m_e1->iface(), 0, am_dummy_handler, NULL, 0);
+    uct_iface_set_am_handler(m_e2->iface(), 0, am_dummy_handler, NULL, 0);
 }
 
+// Check that iface tx ops buffer and flush comp memory pool are moderated
+// properly when we have communication ops + lots of flushes
+void test_rc::test_iface_ops()
+{
+    check_caps(UCT_IFACE_FLAG_PUT_ZCOPY);
+    int cq_len = 16;
+
+    if (UCS_OK != uct_config_modify(m_iface_config, "RC_TX_CQ_LEN",
+                                    ucs::to_string(cq_len).c_str())) {
+        UCS_TEST_ABORT("Error: cannot enable random DCI policy");
+    }
+
+    entity *e = uct_test::create_entity(0);
+    m_entities.push_back(e);
+    e->connect(0, *m_e2, 0);
+
+    mapped_buffer sendbuf(1024, 0ul, *e);
+    mapped_buffer recvbuf(1024, 0ul, *m_e2);
+    uct_completion_t comp;
+    comp.count = cq_len * 512; // some big value to avoid func invocation
+    comp.func  = NULL;
+
+    UCS_TEST_GET_BUFFER_IOV(iov, iovcnt, sendbuf.ptr(), sendbuf.length(),
+                            sendbuf.memh(),
+                            m_e1->iface_attr().cap.am.max_iov);
+    // For _x transports several CQEs can be consumed per WQE, post less put zcopy
+    // ops, so that flush would be sucessfull (otherwise flush will return
+    // NO_RESOURCES and completion will not be added for it).
+    for (int i = 0; i < cq_len / 3; i++) {
+        ASSERT_UCS_OK_OR_INPROGRESS(uct_ep_put_zcopy(e->ep(0), iov, iovcnt,
+                                                     recvbuf.addr(),
+                                                     recvbuf.rkey(), &comp));
+
+        // Create some stress on iface (flush mp):
+        // post 10 flushes per every put.
+        for (int j = 0; j < 10; j++) {
+            ASSERT_UCS_OK_OR_INPROGRESS(uct_ep_flush(e->ep(0), 0, &comp));
+        }
+    }
+
+    flush();
+}
+
+UCS_TEST_P(test_rc, stress_iface_ops) {
+    test_iface_ops();
+}
+
+UCT_RC_INSTANTIATE_TEST_CASE(test_rc)
+
 
 class test_rc_max_wr : public test_rc {
 protected:
@@ -80,10 +127,8 @@ void test_rc_flow_control::init()
     ucs_assert(rc_iface(m_e1)->config.fc_enabled);
     ucs_assert(rc_iface(m_e2)->config.fc_enabled);
 
-    uct_iface_set_am_handler(m_e1->iface(), FLUSH_AM_ID, am_handler,
-                             NULL, UCT_CB_FLAG_SYNC);
-    uct_iface_set_am_handler(m_e2->iface(), FLUSH_AM_ID, am_handler,
-                             NULL, UCT_CB_FLAG_SYNC);
+    uct_iface_set_am_handler(m_e1->iface(), FLUSH_AM_ID, am_handler, NULL, 0);
+    uct_iface_set_am_handler(m_e2->iface(), FLUSH_AM_ID, am_handler, NULL, 0);
 
 }
 
@@ -192,7 +237,7 @@ void test_rc_flow_control::test_pending_purge(int wnd, int num_pend_sends)
     for (int i = 0; i < num_pend_sends; i++) {
         reqs[i].uct.func    = NULL; /* make valgrind happy */
         reqs[i].purge_count = 0;
-        EXPECT_EQ(uct_ep_pending_add(m_e2->ep(0), &reqs[i].uct), UCS_OK);
+        EXPECT_EQ(uct_ep_pending_add(m_e2->ep(0), &reqs[i].uct, 0), UCS_OK);
     }
     uct_ep_pending_purge(m_e2->ep(0), purge_cb, NULL);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.h
index d4437035f..c5cd27174 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_rc.h
@@ -41,6 +41,8 @@ public:
         uct_test::short_progress_loop(delta_ms);
     }
 
+    void test_iface_ops();
+
     static ucs_status_t am_dummy_handler(void *arg, void *data, size_t length,
                                          unsigned flags) {
         return UCS_OK;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_sockaddr.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_sockaddr.cc
index f01528778..3470fd678 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_sockaddr.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_sockaddr.cc
@@ -13,41 +13,71 @@ extern "C" {
 #include <ucs/sys/string.h>
 }
 
+#include <queue>
+
 class test_uct_sockaddr : public uct_test {
 public:
+    struct completion : public uct_completion_t {
+        volatile bool m_flag;
+
+        completion() : m_flag(false), m_status(UCS_INPROGRESS) {
+            count = 1;
+            func  = completion_cb;
+        }
+
+        ucs_status_t status() const {
+            return m_status;
+        }
+    private:
+        static void completion_cb(uct_completion_t *self, ucs_status_t status)
+        {
+            completion *c = static_cast<completion*>(self);
+            c->m_status   = status;
+            c->m_flag     = true;
+        }
+
+        ucs_status_t m_status;
+    };
+
+    test_uct_sockaddr() : server(NULL), client(NULL), err_count(0),
+                          server_recv_req(0), delay_conn_reply(false) {
+        memset(&listen_sock_addr,  0, sizeof(listen_sock_addr));
+        memset(&connect_sock_addr, 0, sizeof(connect_sock_addr));
+    }
+
     void init() {
         uct_test::init();
 
         uct_iface_params server_params, client_params;
-        struct sockaddr_in *addr_in;
+        struct sockaddr_in *listen_addr_in, *connect_addr_in;
 
         /* If we reached here, the interface is active, as it was tested at the
          * resource creation */
-        if (!ucs::is_inet_addr((struct sockaddr *)&(GetParam()->if_addr))) {
+        if (!ucs::is_inet_addr((struct sockaddr *)&(GetParam()->connect_if_addr))) {
             UCS_TEST_SKIP_R("There is no IP on the interface");
         }
 
-        /* If rdmacm is tested, make sure that this is an IPoIB or RoCE interface */
-        if (!strcmp(GetParam()->md_name.c_str(), "rdmacm") &&
-            (!ucs::is_ib_netdev(GetParam()->dev_name.c_str()))) {
-            UCS_TEST_SKIP_R("rdmacm - not an IPoIB or RoCE interface");
-        }
-
         /* This address is accessible, as it was tested at the resource creation */
-        sock_addr.addr = (struct sockaddr *)&(GetParam()->if_addr);
-        ASSERT_TRUE(sock_addr.addr != NULL);
+        listen_sock_addr.addr = (struct sockaddr *)&(GetParam()->listen_if_addr);
+        ASSERT_TRUE(listen_sock_addr.addr != NULL);
 
-        addr_in = (struct sockaddr_in *) (sock_addr.addr);
+        listen_addr_in = (struct sockaddr_in *) (listen_sock_addr.addr);
 
         /* Get a usable port on the host */
-        addr_in->sin_port = ucs::get_port();
+        listen_addr_in->sin_port = ucs::get_port();
+
+        connect_sock_addr.addr = (struct sockaddr *)&(GetParam()->connect_if_addr);
+        ASSERT_TRUE(connect_sock_addr.addr != NULL);
+        connect_addr_in = (struct sockaddr_in *)connect_sock_addr.addr;
+        connect_addr_in->sin_port = listen_addr_in->sin_port;
 
         /* open iface for the server side */
         memset(&server_params, 0, sizeof(server_params));
         server_params.open_mode                      = UCT_IFACE_OPEN_MODE_SOCKADDR_SERVER;
         server_params.err_handler                    = err_handler;
         server_params.err_handler_arg                = reinterpret_cast<void*>(this);
-        server_params.mode.sockaddr.listen_sockaddr  = sock_addr;
+        server_params.err_handler_flags              = 0;
+        server_params.mode.sockaddr.listen_sockaddr  = listen_sock_addr;
         server_params.mode.sockaddr.cb_flags         = UCT_CB_FLAG_ASYNC;
         server_params.mode.sockaddr.conn_request_cb  = conn_request_cb;
         server_params.mode.sockaddr.conn_request_arg = reinterpret_cast<void*>(this);
@@ -60,13 +90,18 @@ public:
         client_params.open_mode                      = UCT_IFACE_OPEN_MODE_SOCKADDR_CLIENT;
         client_params.err_handler                    = err_handler;
         client_params.err_handler_arg                = reinterpret_cast<void*>(this);
+        client_params.err_handler_flags              = 0;
 
         client = uct_test::create_entity(client_params);
         m_entities.push_back(client);
+
+        /* initiate the client's private data callback argument */
+        client->client_cb_arg = server->iface_attr().max_conn_priv;
     }
 
-    static ucs_status_t conn_request_cb(void *arg, const void *conn_priv_data,
-                                        size_t length)
+    static void conn_request_cb(uct_iface_h iface, void *arg,
+                                uct_conn_request_h conn_request,
+                                const void *conn_priv_data, size_t length)
     {
         test_uct_sockaddr *self = reinterpret_cast<test_uct_sockaddr*>(arg);
 
@@ -74,9 +109,13 @@ public:
                               (uct_test::entity::client_priv_data.c_str())),
                   std::string(reinterpret_cast<const char *>(conn_priv_data)));
 
-        EXPECT_EQ(uct_test::entity::client_priv_data.length(), length);
+        EXPECT_EQ(1 + uct_test::entity::client_priv_data.length(), length);
+        if (self->delay_conn_reply) {
+            self->delayed_conn_reqs.push(conn_request);
+        } else {
+            uct_iface_accept(iface, conn_request);
+        }
         self->server_recv_req++;
-        return UCS_OK;
     }
 
     static ucs_status_t err_handler(void *arg, uct_ep_h ep, ucs_status_t status)
@@ -88,21 +127,22 @@ public:
 
 protected:
     entity *server, *client;
-    ucs_sock_addr_t sock_addr;
+    ucs_sock_addr_t listen_sock_addr, connect_sock_addr;
     volatile int err_count, server_recv_req;
+    std::queue<uct_conn_request_h> delayed_conn_reqs;
+    bool delay_conn_reply;
 };
 
 UCS_TEST_P(test_uct_sockaddr, connect_client_to_server)
 {
-    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(sock_addr.addr);
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
 
-    server_recv_req = 0;
-    err_count = 0;
-    client->connect(0, *server, 0);
+    client->connect(0, *server, 0, &connect_sock_addr);
 
     /* wait for the server to connect */
     while (server_recv_req == 0) {
-        sched_yield();
+        progress();
     }
     ASSERT_TRUE(server_recv_req == 1);
     /* since the transport may support a graceful exit in case of an error,
@@ -115,11 +155,64 @@ UCS_TEST_P(test_uct_sockaddr, connect_client_to_server)
      * test ends and the client's ep was destroyed */
 }
 
+UCS_TEST_P(test_uct_sockaddr, connect_client_to_server_with_delay)
+{
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
+    delay_conn_reply = true;
+    client->connect(0, *server, 0, &connect_sock_addr);
+
+    /* wait for the server to connect */
+    while (server_recv_req == 0) {
+        progress();
+    }
+    ASSERT_EQ(1,   server_recv_req);
+    ASSERT_EQ(1ul, delayed_conn_reqs.size());
+    EXPECT_EQ(0,   err_count);
+    while (!delayed_conn_reqs.empty()) {
+        uct_iface_accept(server->iface(), delayed_conn_reqs.front());
+        delayed_conn_reqs.pop();
+    }
+
+    completion comp;
+    ucs_status_t status = uct_ep_flush(client->ep(0), 0, &comp);
+    if (status == UCS_INPROGRESS) {
+        wait_for_flag(&comp.m_flag);
+        EXPECT_EQ(UCS_OK, comp.status());
+    } else {
+        EXPECT_EQ(UCS_OK, status);
+    }
+    EXPECT_EQ(0, err_count);
+}
+
+UCS_TEST_P(test_uct_sockaddr, connect_client_to_server_reject_with_delay)
+{
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
+    delay_conn_reply = true;
+    client->connect(0, *server, 0, &connect_sock_addr);
+
+    /* wait for the server to connect */
+    while (server_recv_req == 0) {
+        progress();
+    }
+    ASSERT_EQ(1, server_recv_req);
+    ASSERT_EQ(1ul, delayed_conn_reqs.size());
+    EXPECT_EQ(0, err_count);
+    while (!delayed_conn_reqs.empty()) {
+        uct_iface_reject(server->iface(), delayed_conn_reqs.front());
+        delayed_conn_reqs.pop();
+    }
+    while (err_count == 0) {
+        progress();
+    }
+    EXPECT_EQ(1, err_count);
+}
+
 UCS_TEST_P(test_uct_sockaddr, many_clients_to_one_server)
 {
-    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(sock_addr.addr);
-    server_recv_req = 0;
-    err_count = 0;
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
 
     uct_iface_params client_params;
     entity *client_test;
@@ -129,18 +222,20 @@ UCS_TEST_P(test_uct_sockaddr, many_clients_to_one_server)
     for (i = 0; i < num_clients; ++i) {
         /* open iface for the client side */
         memset(&client_params, 0, sizeof(client_params));
-        client_params.open_mode       = UCT_IFACE_OPEN_MODE_SOCKADDR_CLIENT;
-        client_params.err_handler     = err_handler;
-        client_params.err_handler_arg = reinterpret_cast<void*>(this);
+        client_params.open_mode         = UCT_IFACE_OPEN_MODE_SOCKADDR_CLIENT;
+        client_params.err_handler       = err_handler;
+        client_params.err_handler_arg   = reinterpret_cast<void*>(this);
+        client_params.err_handler_flags = 0;
 
         client_test = uct_test::create_entity(client_params);
         m_entities.push_back(client_test);
 
-        client_test->connect(i, *server, 0);
+        client_test->client_cb_arg = server->iface_attr().max_conn_priv;
+        client_test->connect(i, *server, 0, &connect_sock_addr);
     }
 
     while (server_recv_req < num_clients){
-        sched_yield();
+        progress();
     }
     ASSERT_TRUE(server_recv_req == num_clients);
     EXPECT_EQ(0, err_count);
@@ -148,19 +243,18 @@ UCS_TEST_P(test_uct_sockaddr, many_clients_to_one_server)
 
 UCS_TEST_P(test_uct_sockaddr, many_conns_on_client)
 {
-    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(sock_addr.addr);
-    server_recv_req = 0;
-    err_count = 0;
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
 
     int i, num_conns_on_client = 100;
 
     /* multiple clients, on the same iface, connecting to the same server */
     for (i = 0; i < num_conns_on_client; ++i) {
-        client->connect(i, *server, 0);
+        client->connect(i, *server, 0, &connect_sock_addr);
     }
 
     while (server_recv_req < num_conns_on_client) {
-        sched_yield();
+        progress();
     }
     ASSERT_TRUE(server_recv_req == num_conns_on_client);
     EXPECT_EQ(0, err_count);
@@ -169,15 +263,13 @@ UCS_TEST_P(test_uct_sockaddr, many_conns_on_client)
 UCS_TEST_P(test_uct_sockaddr, err_handle)
 {
     check_caps(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE);
-    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(sock_addr.addr);
-
-    server_recv_req = 0;
-    err_count = 0;
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
 
-    client->connect(0, *server, 0);
+    client->connect(0, *server, 0, &connect_sock_addr);
 
+    scoped_log_handler slh(wrap_errors_logger);
     /* kill the server */
-    wrap_errors();
     m_entities.remove(server);
 
     /* If the server didn't receive a connection request from the client yet,
@@ -186,35 +278,40 @@ UCS_TEST_P(test_uct_sockaddr, err_handle)
         wait_for_flag(&err_count);
         EXPECT_EQ(1, err_count);
     }
-    restore_errors();
 }
 
 UCS_TEST_P(test_uct_sockaddr, conn_to_non_exist_server)
 {
     check_caps(UCT_IFACE_FLAG_ERRHANDLE_PEER_FAILURE);
 
-    struct sockaddr_in *addr_in;
-    addr_in = (struct sockaddr_in *) (sock_addr.addr);
-    in_port_t orig_port = addr_in->sin_port;
+    struct sockaddr_in *connect_addr_in;
+    connect_addr_in = (struct sockaddr_in *) (connect_sock_addr.addr);
+    in_port_t orig_port = connect_addr_in->sin_port;
 
-    addr_in->sin_port = 1;
-    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(sock_addr.addr);
+    connect_addr_in->sin_port = 1;
+    UCS_TEST_MESSAGE << "Testing " << ucs::sockaddr_to_str(listen_sock_addr.addr)
+                     << " Interface: " << GetParam()->dev_name.c_str();
 
     err_count = 0;
 
     /* wrap errors now since the client will try to connect to a non existing port */
-    wrap_errors();
-    /* client - try to connect to a non-existing port on the server side */
-    client->connect(0, *server, 0);
-
-    /* destroy the client's ep. this ep shouldn't be accessed anymore */
-    client->destroy_ep(0);
-    /* wait for the transport's events to arrive */
-    sleep(3);
-    restore_errors();
-
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+        /* client - try to connect to a non-existing port on the server side */
+        client->connect(0, *server, 0, &connect_sock_addr);
+        completion comp;
+        ucs_status_t status = uct_ep_flush(client->ep(0), 0, &comp);
+        if (status == UCS_INPROGRESS) {
+            wait_for_flag(&comp.m_flag);
+            EXPECT_EQ(UCS_ERR_UNREACHABLE, comp.status());
+        } else {
+            EXPECT_EQ(UCS_ERR_UNREACHABLE, status);
+        }
+        /* destroy the client's ep. this ep shouldn't be accessed anymore */
+        client->destroy_ep(0);
+    }
     /* restore the previous existing port */
-    addr_in->sin_port = orig_port;
+    connect_addr_in->sin_port = orig_port;
 }
 
 UCT_INSTANTIATE_SOCKADDR_TEST_CASE(test_uct_sockaddr)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud.cc
index 4841906b3..36a25cd64 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud.cc
@@ -434,13 +434,19 @@ UCS_TEST_P(test_ud, crep_ack_drop) {
                                       NULL, UCT_CB_FLAG_ASYNC);
     ASSERT_UCS_OK(status);
 
+    /* allow sending the active message, in case the congestion window is
+     * already reduced to minimum (=2) by the slow timer, since CREP ACK
+     * was not received.
+     */
+    set_tx_win(m_e1, 10);
+
     do {
         status = uct_ep_am_short(m_e1->ep(0), 0, 0, NULL, 0);
         progress();
     } while (status == UCS_ERR_NO_RESOURCE);
     ASSERT_UCS_OK(status);
 
-    validate_recv(ep(m_e2), 3U);
+    validate_recv(ep(m_e2), 3u - no_creq_cnt(ep(m_e1)));
 
     ep(m_e1, 0)->rx.rx_hook = uct_ud_ep_null_hook;
     ep(m_e2, 0)->rx.rx_hook = uct_ud_ep_null_hook;
@@ -543,6 +549,12 @@ UCS_TEST_P(test_ud, ca_md, "IB_TX_QUEUE_LEN=" UCS_PP_MAKE_STRING(UCT_UD_CA_MAX_W
     ep(m_e2, 0)->rx.rx_hook = drop_rx;
     for (i = 1; i < UCT_UD_CA_MAX_WINDOW; i++) {
         status = tx(m_e1);
+        if (status == UCS_ERR_NO_RESOURCE) {
+            // the congestion window can shrink by async timer if ACKs are
+            // not received fast enough
+            EXPECT_GT(i, 1); /* at least one packet should be sent */
+            break;
+        }
         EXPECT_UCS_OK(status);
         progress();
     }
@@ -590,7 +602,7 @@ UCS_TEST_P(test_ud, ca_resend) {
     ack_req_tx_cnt = 0;
     do {
         progress();
-    } while(ep(m_e1)->ca.cwnd != max_window/2);
+    } while(ep(m_e1)->ca.cwnd > max_window/2);
     /* expect that:
      * 4 packets will be retransmitted
      * first packet will have ack_req,
@@ -601,9 +613,9 @@ UCS_TEST_P(test_ud, ca_resend) {
     disable_async(m_e1);
     disable_async(m_e2);
     short_progress_loop(100);
-    EXPECT_LE(4, rx_drop_count);
+    EXPECT_LE(0, rx_drop_count);
     EXPECT_GE(4+2, rx_drop_count);
-    EXPECT_LE(2, ack_req_tx_cnt);
+    EXPECT_LE(0, ack_req_tx_cnt);
     EXPECT_GE(2+2, ack_req_tx_cnt);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_ds.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_ds.cc
index a1d12359b..3c3cef254 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_ds.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_ds.cc
@@ -65,9 +65,8 @@ unsigned test_ud_ds::N = 1000;
 UCS_TEST_P(test_ud_ds, if_addr) {
     union ibv_gid gid1, gid2;
     uint16_t lid1, lid2;
-    uint8_t is_global;
-    uct_ib_address_unpack(ib_adr1, &lid1, &is_global, &gid1);
-    uct_ib_address_unpack(ib_adr2, &lid2, &is_global, &gid2);
+    uct_ib_address_unpack(ib_adr1, &lid1, &gid1);
+    uct_ib_address_unpack(ib_adr2, &lid2, &gid2);
     EXPECT_EQ(lid1, lid2);
     EXPECT_EQ(gid1.global.subnet_prefix, gid2.global.subnet_prefix);
     EXPECT_EQ(gid1.global.interface_id, gid2.global.interface_id);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_pending.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_pending.cc
index 8e46398ca..10492d1e8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_pending.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_pending.cc
@@ -38,7 +38,7 @@ public:
         /* queuee some work */
         for(i = 0; i < N; i++) {
             m_r[i].func = pending_cb_dispatch;
-            EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &m_r[i]));
+            EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &m_r[i], 0));
         }
     }
 
@@ -102,7 +102,7 @@ UCS_TEST_P(test_ud_pending, async_progress) {
     EXPECT_UCS_OK(tx(m_e1));
 
     for(i = 0; i < N; i++) {
-        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i]));
+        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i], 0));
     }
     twait(300);
     /* requests must not be dispatched from async progress */
@@ -123,7 +123,7 @@ UCS_TEST_P(test_ud_pending, sync_progress) {
 
     for(i = 0; i < N; i++) {
         r[i].func = pending_cb;
-        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i]));
+        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i], 0));
     }
     wait_for_value(&req_count, N, true);
     /* requests must be dispatched from progress */
@@ -144,7 +144,7 @@ UCS_TEST_P(test_ud_pending, err_busy) {
 
     for(i = 0; i < N; i++) {
         r[i].func = pending_cb_busy;
-        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i]));
+        EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r[i], 0));
     }
     short_progress_loop();
     /* requests will not be dispatched from progress */
@@ -183,7 +183,7 @@ UCS_TEST_P(test_ud_pending, window)
     }
     EXPECT_EQ(UCS_ERR_NO_RESOURCE, tx(m_e1));
     r.func = pending_cb_dispatch;
-    EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r));
+    EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r, 0));
     wait_for_value(&req_count, 1, true);
     EXPECT_EQ(1, req_count);
     uct_ep_pending_purge(m_e1->ep(0), purge_cb, NULL);
@@ -208,7 +208,7 @@ UCS_TEST_P(test_ud_pending, tx_wqe)
     } while (status == UCS_OK);
 
     r.func = pending_cb_dispatch;
-    EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r));
+    EXPECT_EQ(UCS_OK, uct_ep_pending_add(m_e1->ep(0), &r, 0));
     wait_for_value(&req_count, 1, true);
     EXPECT_EQ(1, req_count);
     short_progress_loop();
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_slow_timer.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_slow_timer.cc
index bf29df44c..cf28a29ee 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_slow_timer.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/ib/test_ud_slow_timer.cc
@@ -11,6 +11,7 @@
 extern "C" {
 #include <ucs/time/time.h>
 #include <ucs/datastruct/queue.h>
+#include <ucs/datastruct/ptr_array.h>
 #include <uct/ib/ud/base/ud_ep.h>
 #include <uct/ib/ud/base/ud_iface.h>
 }
@@ -35,7 +36,7 @@ public:
     static ucs_status_t tick_counter(uct_ud_ep_t *ep, uct_ud_neth_t *neth)
     {
         uct_ud_iface_t *iface = ucs_derived_of(ep->super.super.iface,
-                uct_ud_iface_t);
+                                               uct_ud_iface_t);
 
         /* hack to disable retransmit */
         ep->tx.send_time = ucs_twheel_get_time(&iface->async.slow_timer);
@@ -56,6 +57,18 @@ public:
             usleep(1000);
         }
     }
+
+    void wait_for_ep_destroyed(uct_ud_iface_t *iface, uint32_t ep_idx)
+    {
+        ucs_time_t deadline = ucs_get_time() +
+                              ucs_time_from_sec(60) * ucs::test_time_multiplier();
+        void *ud_ep_tmp;
+
+        while ((ucs_get_time() < deadline) &&
+               ucs_ptr_array_lookup(&iface->eps, ep_idx, ud_ep_tmp)) {
+            usleep(1000);
+        }
+    }
 };
 
 int test_ud_slow_timer::rx_limit = 10;
@@ -67,7 +80,7 @@ int test_ud_slow_timer::tick_count = 0;
 UCS_TEST_P(test_ud_slow_timer, tx1) {
     connect();
     EXPECT_UCS_OK(tx(m_e1));
-    twait(200);
+    wait_for_rx_sn(1);
     EXPECT_EQ(2, ep(m_e1)->tx.psn);
     EXPECT_EQ(1, ucs_frag_list_sn(&ep(m_e2)->rx.ooo_pkts));
 }
@@ -85,6 +98,21 @@ UCS_TEST_P(test_ud_slow_timer, txn) {
     EXPECT_EQ(N, ucs_frag_list_sn(&ep(m_e2)->rx.ooo_pkts));
 }
 
+UCS_TEST_P(test_ud_slow_timer, ep_destroy, "UD_TIMEOUT=1s") {
+    void *ud_ep_tmp;
+    connect();
+
+    uct_ud_ep_t    *ud_ep = ep(m_e1);
+    uct_ud_iface_t *iface = ucs_derived_of(ud_ep->super.super.iface,
+                                           uct_ud_iface_t);
+    uint32_t       ep_idx = ud_ep->ep_id;
+    EXPECT_TRUE(ucs_ptr_array_lookup(&iface->eps, ep_idx, ud_ep_tmp));
+
+    m_e1->destroy_eps();
+    wait_for_ep_destroyed(iface, ep_idx);
+    EXPECT_FALSE(ucs_ptr_array_lookup(&iface->eps, ep_idx, ud_ep_tmp));
+}
+
 #ifdef UCT_UD_EP_DEBUG_HOOKS
 /* no traffic - no ticks */
 UCS_TEST_P(test_ud_slow_timer, tick1) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.cc
index b29b0ad3a..b898610d2 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.cc
@@ -196,4 +196,3 @@ void uct_amo_test::worker::join() {
     pthread_join(m_thread, &retval);
     running = false;
 }
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.h
index ba11f456d..9e58c8d73 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo.h
@@ -79,6 +79,83 @@ public:
         std::vector<completion> m_completions;
     };
 
+    ucs_status_t atomic_post(uct_ep_h ep, uct_atomic_op_t opcode,
+                             uint32_t value, uint64_t remote_addr,
+                             uct_rkey_t rkey) {
+        return uct_ep_atomic32_post(ep, opcode, value, remote_addr, rkey);
+    }
+
+    ucs_status_t atomic_post(uct_ep_h ep, uct_atomic_op_t opcode,
+                             uint64_t value, uint64_t remote_addr,
+                             uct_rkey_t rkey) {
+        return uct_ep_atomic64_post(ep, opcode, value, remote_addr, rkey);
+    }
+
+    ucs_status_t atomic_fetch_nb(uct_ep_h ep, uct_atomic_op_t opcode,
+                                 uint32_t value, uint32_t *result,
+                                 uint64_t remote_addr, uct_rkey_t rkey,
+                                 uct_completion_t *comp) {
+        return uct_ep_atomic32_fetch(ep, opcode, value, result, remote_addr, rkey, comp);
+    }
+
+    ucs_status_t atomic_fetch_nb(uct_ep_h ep, uct_atomic_op_t opcode,
+                                 uint64_t value, uint64_t *result,
+                                 uint64_t remote_addr, uct_rkey_t rkey,
+                                 uct_completion_t *comp) {
+        return uct_ep_atomic64_fetch(ep, opcode, value, result, remote_addr, rkey, comp);
+    }
+
+    template <typename T, uct_atomic_op_t opcode>
+    ucs_status_t atomic_op(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
+                           uint64_t *result, completion *comp) {
+        return atomic_post(ep, opcode, (T)worker.value, recvbuf.addr(), recvbuf.rkey());
+    }
+
+    template <typename T, uct_atomic_op_t opcode>
+    ucs_status_t atomic_fop(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
+                            uint64_t *result, completion *comp) {
+        comp->self     = this;
+        comp->uct.func = atomic_reply_cb;
+        return atomic_fetch_nb(ep, opcode, (T)worker.value,
+                               (T*)result, recvbuf.addr(), recvbuf.rkey(),
+                               &comp->uct);
+    }
+
+    template <typename T>
+    static T and_op(T v1, T v2)
+    {
+        return v1 & v2;
+    }
+
+    template <typename T>
+    static T or_op(T v1, T v2)
+    {
+        return v1 | v2;
+    }
+
+    template <typename T>
+    static T add_op(T v1, T v2)
+    {
+        return v1 + v2;
+    }
+
+    template <typename T>
+    static T xor_op(T v1, T v2)
+    {
+        return v1 ^ v2;
+    }
+
+    template <typename T>
+    static T and_val(unsigned i)
+    {
+        return ~(UCS_BIT(i * 2) | UCS_BIT(i + 16));
+    }
+
+    template <typename T>
+    static T or_val(unsigned i)
+    {
+        return UCS_BIT(i * 2) | UCS_BIT(i + 16);
+    }
 
 protected:
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add.cc
deleted file mode 100644
index e8f6b5a5a..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add.cc
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#include "test_amo.h"
-
-
-class uct_amo_add_test : public uct_amo_test {
-public:
-
-    ucs_status_t add32(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
-                       uint64_t *result, completion *comp) {
-        return uct_ep_atomic_add32(ep, worker.value, recvbuf.addr(), recvbuf.rkey());
-    }
-
-    ucs_status_t add64(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
-                       uint64_t *result, completion *comp) {
-        return uct_ep_atomic_add64(ep, worker.value, recvbuf.addr(), recvbuf.rkey());
-    }
-
-    template <typename T>
-    void test_add(send_func_t send) {
-        /*
-         * Method: Add may random values from multiple workers running at the same
-         * time. We expect the final result to be the sum of all these values.
-         */
-
-        mapped_buffer recvbuf(sizeof(T), 0, receiver());
-
-        T value = rand64();
-        *(T*)recvbuf.ptr() = value;
-
-        T exp_result = value;
-        std::vector<uint64_t> add_vec;
-        for (unsigned i = 0; i < num_senders(); ++i) {
-             value = rand64();
-             add_vec.push_back(value);
-
-             for (unsigned j = 0; j < count(); ++j) {
-                 exp_result += value;
-                 value = hash64(value);
-             }
-        }
-
-        run_workers(send, recvbuf, add_vec, true);
-
-        wait_for_remote();
-        EXPECT_EQ(exp_result, *(T*)recvbuf.ptr());
-    }
-};
-
-
-UCS_TEST_P(uct_amo_add_test, add32) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ADD32);
-    test_add<uint32_t>(static_cast<send_func_t>(&uct_amo_add_test::add32));
-}
-
-UCS_TEST_P(uct_amo_add_test, add64) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ADD64);
-    test_add<uint64_t>(static_cast<send_func_t>(&uct_amo_add_test::add64));
-}
-
-UCT_INSTANTIATE_TEST_CASE(uct_amo_add_test)
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add_xor.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add_xor.cc
new file mode 100644
index 000000000..f89477b19
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_add_xor.cc
@@ -0,0 +1,66 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "test_amo.h"
+
+
+class uct_amo_add_xor_test : public uct_amo_test {
+public:
+
+    template <typename T, uct_atomic_op_t opcode>
+    void test_op(T (*op)(T, T)) {
+        /*
+         * Method: Add/xor may random values from multiple workers running at the same
+         * time. We expect the final result to be the sum/xor of all these values.
+         */
+
+        mapped_buffer recvbuf(sizeof(T), 0, receiver());
+
+        T value = rand64();
+        *(T*)recvbuf.ptr() = value;
+
+        T exp_result = value;
+        std::vector<uint64_t> add_vec;
+        for (unsigned i = 0; i < num_senders(); ++i) {
+             value = rand64();
+             add_vec.push_back(value);
+
+             for (unsigned j = 0; j < count(); ++j) {
+                 exp_result = op(exp_result, value);
+                 value = hash64(value);
+             }
+        }
+
+        run_workers(static_cast<send_func_t>(&uct_amo_test::atomic_op<T, opcode>),
+                    recvbuf, add_vec, true);
+
+        wait_for_remote();
+        EXPECT_EQ(exp_result, *(T*)recvbuf.ptr());
+    }
+};
+
+UCS_TEST_P(uct_amo_add_xor_test, add32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), OP32);
+    test_op<uint32_t, UCT_ATOMIC_OP_ADD>(add_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_add_xor_test, add64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), OP64);
+    test_op<uint64_t, UCT_ATOMIC_OP_ADD>(add_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_add_xor_test, xor32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), OP32);
+    test_op<uint32_t, UCT_ATOMIC_OP_XOR>(xor_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_add_xor_test, xor64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), OP64);
+    test_op<uint64_t, UCT_ATOMIC_OP_XOR>(xor_op<uint64_t>);
+}
+
+UCT_INSTANTIATE_TEST_CASE(uct_amo_add_xor_test)
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_and_or.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_and_or.cc
new file mode 100644
index 000000000..1cf0edbe6
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_and_or.cc
@@ -0,0 +1,65 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "test_amo.h"
+
+
+class uct_amo_and_or_test : public uct_amo_test {
+public:
+
+    template <typename T, uct_atomic_op_t opcode>
+    void test_op(T (*op)(T, T), T (*val)(unsigned)) {
+        /*
+         * Method: Add may random values from multiple workers running at the same
+         * time. We expect the final result to be the and/or of all these values.
+         * This is simplified version of add/xor test: operated value is costant
+         * for every worker to eliminate result to 0 or MAX_INT
+         */
+
+        check_atomics(UCS_BIT(opcode), sizeof(T) == sizeof(uint64_t) ? OP64 : OP32);
+
+        mapped_buffer recvbuf(sizeof(T), 0, receiver());
+
+        T value = 0x0ff0f00f;
+        *(T*)recvbuf.ptr() = value;
+
+        T exp_result = value;
+        std::vector<uint64_t> op_vec;
+        for (unsigned i = 0; i < num_senders(); ++i) {
+             value = val(i);
+             op_vec.push_back(value);
+
+             for (unsigned j = 0; j < count(); ++j) {
+                 exp_result = op(exp_result, value);
+             }
+        }
+
+        run_workers(static_cast<send_func_t>(&uct_amo_test::atomic_op<T, opcode>),
+                    recvbuf, op_vec, false);
+
+        wait_for_remote();
+        EXPECT_EQ(exp_result, *(T*)recvbuf.ptr());
+    }
+};
+
+UCS_TEST_P(uct_amo_and_or_test, and32) {
+    test_op<uint32_t, UCT_ATOMIC_OP_AND>(and_op<uint32_t>, and_val<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_and_or_test, add64) {
+    test_op<uint64_t, UCT_ATOMIC_OP_AND>(and_op<uint64_t>, and_val<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_and_or_test, or32) {
+    test_op<uint32_t, UCT_ATOMIC_OP_OR>(or_op<uint32_t>, or_val<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_and_or_test, or64) {
+    test_op<uint64_t, UCT_ATOMIC_OP_OR>(or_op<uint64_t>, or_val<uint64_t>);
+}
+
+UCT_INSTANTIATE_TEST_CASE(uct_amo_and_or_test)
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_cswap.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_cswap.cc
index dfcd8e2fe..5bb7f8ede 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_cswap.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_cswap.cc
@@ -95,12 +95,12 @@ public:
 
 
 UCS_TEST_P(uct_amo_cswap_test, cswap32) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_CSWAP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_CSWAP), FOP32);
     test_cswap<uint32_t>(static_cast<send_func_t>(&uct_amo_cswap_test::cswap32));
 }
 
 UCS_TEST_P(uct_amo_cswap_test, cswap64) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_CSWAP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_CSWAP), FOP64);
     test_cswap<uint64_t>(static_cast<send_func_t>(&uct_amo_cswap_test::cswap64));
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd.cc
deleted file mode 100644
index 6672968a1..000000000
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd.cc
+++ /dev/null
@@ -1,89 +0,0 @@
-/**
-* Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
-*
-* See file LICENSE for terms.
-*/
-
-#include "test_amo.h"
-
-
-class uct_amo_fadd_test : public uct_amo_test {
-public:
-
-    ucs_status_t fadd32(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
-                        uint64_t *result, completion *comp) {
-        comp->self     = this;
-        comp->uct.func = atomic_reply_cb;
-        return uct_ep_atomic_fadd32(ep, worker.value, recvbuf.addr(), recvbuf.rkey(),
-                                    (uint32_t*)result, &comp->uct);
-    }
-
-    ucs_status_t fadd64(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
-                        uint64_t *result, completion *comp) {
-        comp->self     = this;
-        comp->uct.func = atomic_reply_cb;
-        return uct_ep_atomic_fadd64(ep, worker.value, recvbuf.addr(), recvbuf.rkey(),
-                                    result, &comp->uct);
-    }
-
-    template <typename T>
-    void test_fadd(send_func_t send) {
-        /*
-         * Method: Do concurrent atomic fetch-and-add of constant random value
-         * to a single atomic variable. Check that every sender gets a unique reply
-         * and the final value of atomic variable is the sum of all.
-         */
-
-        mapped_buffer recvbuf(sizeof(T), 0, receiver());
-
-        T value = rand64();
-        T add   = rand64();
-        *(T*)recvbuf.ptr() = value;
-
-        std::vector<uint64_t> exp_replies;
-        for (unsigned i = 0; i < count() * num_senders(); ++i) {
-            exp_replies.push_back(value);
-            value += add;
-        }
-
-        run_workers(send, recvbuf, std::vector<uint64_t>(num_senders(), add), false);
-
-        validate_replies(exp_replies);
-
-        wait_for_remote();
-        EXPECT_EQ(value, *(T*)recvbuf.ptr());
-    }
-};
-
-
-UCS_TEST_P(uct_amo_fadd_test, fadd32) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD32);
-    test_fadd<uint32_t>(static_cast<send_func_t>(&uct_amo_fadd_test::fadd32));
-}
-
-UCS_TEST_P(uct_amo_fadd_test, fadd64) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD64);
-    test_fadd<uint64_t>(static_cast<send_func_t>(&uct_amo_fadd_test::fadd64));
-}
-
-UCT_INSTANTIATE_TEST_CASE(uct_amo_fadd_test)
-
-class uct_amo_fadd_test_inlresp : public uct_amo_fadd_test {};
-
-UCS_TEST_P(uct_amo_fadd_test_inlresp, fadd64_inlresp0, "IB_TX_INLINE_RESP=0") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD64);
-    test_fadd<uint64_t>(static_cast<send_func_t>(&uct_amo_fadd_test::fadd64));
-}
-
-UCS_TEST_P(uct_amo_fadd_test_inlresp, fadd64_inlresp32, "IB_TX_INLINE_RESP=32") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD64);
-    test_fadd<uint64_t>(static_cast<send_func_t>(&uct_amo_fadd_test::fadd64));
-}
-
-UCS_TEST_P(uct_amo_fadd_test_inlresp, fadd64_inlresp64, "IB_TX_INLINE_RESP=64") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD64);
-    test_fadd<uint64_t>(static_cast<send_func_t>(&uct_amo_fadd_test::fadd64));
-}
-
-UCT_INSTANTIATE_IB_TEST_CASE(uct_amo_fadd_test_inlresp)
-
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd_fxor.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd_fxor.cc
new file mode 100644
index 000000000..b4ebfb518
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fadd_fxor.cc
@@ -0,0 +1,98 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "test_amo.h"
+
+
+class uct_amo_fadd_fxor_test : public uct_amo_test {
+public:
+
+    template <typename T, uct_atomic_op_t OP>
+    void test_fop(T (*op)(T, T)) {
+        /*
+         * Method: Do concurrent atomic fetch-and-add/xor of constant random value
+         * to a single atomic variable. Check that every sender gets a unique reply
+         * and the final value of atomic variable is the sum/xor of all.
+         */
+
+        mapped_buffer recvbuf(sizeof(T), 0, receiver());
+
+        T value = rand64();
+        T add   = rand64();
+        *(T*)recvbuf.ptr() = value;
+
+        std::vector<uint64_t> exp_replies;
+        for (unsigned i = 0; i < count() * num_senders(); ++i) {
+            exp_replies.push_back(value);
+            value = op(value, add);
+        }
+
+        run_workers(static_cast<send_func_t>(&uct_amo_test::atomic_fop<T, OP>),
+                    recvbuf, std::vector<uint64_t>(num_senders(), add), false);
+
+        validate_replies(exp_replies);
+
+        wait_for_remote();
+        EXPECT_EQ(value, *(T*)recvbuf.ptr());
+    }
+};
+
+UCS_TEST_P(uct_amo_fadd_fxor_test, fadd32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP32);
+    test_fop<uint32_t, UCT_ATOMIC_OP_ADD>(add_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test, fadd64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_ADD>(add_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test, fxor32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP32);
+    test_fop<uint32_t, UCT_ATOMIC_OP_XOR>(xor_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test, fxor64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_XOR>(xor_op<uint64_t>);
+}
+
+UCT_INSTANTIATE_TEST_CASE(uct_amo_fadd_fxor_test)
+
+class uct_amo_fadd_fxor_test_inlresp : public uct_amo_fadd_fxor_test {};
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fadd64_inlresp0, "IB_TX_INLINE_RESP=0") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_ADD>(add_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fadd64_inlresp32, "IB_TX_INLINE_RESP=32") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_ADD>(add_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fadd64_inlresp64, "IB_TX_INLINE_RESP=64") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_ADD>(add_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fxor64_inlresp0, "IB_TX_INLINE_RESP=0") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_XOR>(xor_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fxor64_inlresp32, "IB_TX_INLINE_RESP=32") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_XOR>(xor_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fadd_fxor_test_inlresp, fxor64_inlresp64, "IB_TX_INLINE_RESP=64") {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP64);
+    test_fop<uint64_t, UCT_ATOMIC_OP_XOR>(xor_op<uint64_t>);
+}
+
+UCT_INSTANTIATE_IB_TEST_CASE(uct_amo_fadd_fxor_test_inlresp)
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fand_for.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fand_for.cc
new file mode 100644
index 000000000..4c8dc2a21
--- /dev/null
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_fand_for.cc
@@ -0,0 +1,90 @@
+/**
+* Copyright (C) Mellanox Technologies Ltd. 2001-2018.  ALL RIGHTS RESERVED.
+*
+* See file LICENSE for terms.
+*/
+
+#include "test_amo.h"
+
+
+class uct_amo_fand_for_test : public uct_amo_test {
+public:
+
+    template <typename T, uct_atomic_op_t opcode>
+    void test_fop(T (*op)(T, T)) {
+        /*
+         * Method: Do concurrent atomic fetch-and-and/or of constant random value
+         * to a single atomic variable. Check that every sender gets a unique reply
+         * and the final value of atomic variable is the and/or of all.
+         */
+
+        check_atomics(UCS_BIT(opcode), sizeof(T) == sizeof(uint64_t) ? FOP64 : FOP32);
+
+        mapped_buffer recvbuf(sizeof(T), 0, receiver());
+
+        T value = rand64();
+        T add   = rand64();
+        *(T*)recvbuf.ptr() = value;
+
+        std::vector<uint64_t> exp_replies;
+        for (unsigned i = 0; i < count() * num_senders(); ++i) {
+            exp_replies.push_back(value);
+            value = op(value, add);
+        }
+
+        run_workers(static_cast<send_func_t>(&uct_amo_test::atomic_fop<T, opcode>),
+                    recvbuf, std::vector<uint64_t>(num_senders(), add), false);
+
+        validate_replies(exp_replies);
+
+        wait_for_remote();
+        EXPECT_EQ(value, *(T*)recvbuf.ptr());
+    }
+};
+
+UCS_TEST_P(uct_amo_fand_for_test, fand32) {
+    test_fop<uint32_t, UCT_ATOMIC_OP_AND>(and_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test, fand64) {
+    test_fop<uint64_t, UCT_ATOMIC_OP_AND>(and_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test, for32) {
+    test_fop<uint32_t, UCT_ATOMIC_OP_OR>(or_op<uint32_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test, for64) {
+    test_fop<uint64_t, UCT_ATOMIC_OP_OR>(or_op<uint64_t>);
+}
+
+UCT_INSTANTIATE_TEST_CASE(uct_amo_fand_for_test)
+
+class uct_amo_fand_for_test_inlresp : public uct_amo_fand_for_test {};
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, fand64_inlresp0, "IB_TX_INLINE_RESP=0") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_AND>(and_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, fand64_inlresp32, "IB_TX_INLINE_RESP=32") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_AND>(and_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, fand64_inlresp64, "IB_TX_INLINE_RESP=64") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_AND>(and_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, for64_inlresp0, "IB_TX_INLINE_RESP=0") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_OR>(or_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, for64_inlresp32, "IB_TX_INLINE_RESP=32") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_OR>(or_op<uint64_t>);
+}
+
+UCS_TEST_P(uct_amo_fand_for_test_inlresp, for64_inlresp64, "IB_TX_INLINE_RESP=64") {
+    test_fop<uint64_t, UCT_ATOMIC_OP_OR>(or_op<uint64_t>);
+}
+
+UCT_INSTANTIATE_IB_TEST_CASE(uct_amo_fand_for_test_inlresp)
+
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_swap.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_swap.cc
index dc6bcbf16..10f23341d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_swap.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_amo_swap.cc
@@ -14,16 +14,16 @@ public:
                         uint64_t *result, completion *comp) {
         comp->self     = this;
         comp->uct.func = atomic_reply_cb;
-        return uct_ep_atomic_swap32(ep, worker.value, recvbuf.addr(), recvbuf.rkey(),
-                                    (uint32_t*)result, &comp->uct);
+        return uct_ep_atomic32_fetch(ep, UCT_ATOMIC_OP_SWAP, worker.value, (uint32_t*)result,
+                                     recvbuf.addr(), recvbuf.rkey(), &comp->uct);
     }
 
     ucs_status_t swap64(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf,
                         uint64_t *result, completion *comp) {
         comp->self     = this;
         comp->uct.func = atomic_reply_cb;
-        return uct_ep_atomic_swap64(ep, worker.value, recvbuf.addr(), recvbuf.rkey(),
-                                    result, &comp->uct);
+        return uct_ep_atomic64_fetch(ep, UCT_ATOMIC_OP_SWAP, worker.value, (uint64_t*)result,
+                                     recvbuf.addr(), recvbuf.rkey(), &comp->uct);
     }
 
     template <typename T>
@@ -65,12 +65,12 @@ public:
 
 
 UCS_TEST_P(uct_amo_swap_test, swap32) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_SWAP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_SWAP), FOP32);
     test_swap<uint32_t>(static_cast<send_func_t>(&uct_amo_swap_test::swap32));
 }
 
 UCS_TEST_P(uct_amo_swap_test, swap64) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_SWAP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_SWAP), FOP64);
     test_swap<uint64_t>(static_cast<send_func_t>(&uct_amo_swap_test::swap64));
 }
 
@@ -79,17 +79,17 @@ UCT_INSTANTIATE_TEST_CASE(uct_amo_swap_test)
 class uct_amo_swap_test_inlresp : public uct_amo_swap_test {};
 
 UCS_TEST_P(uct_amo_swap_test_inlresp, swap32_inlresp0, "IB_TX_INLINE_RESP=0") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_SWAP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_SWAP), FOP32);
     test_swap<uint32_t>(static_cast<send_func_t>(&uct_amo_swap_test::swap32));
 }
 
 UCS_TEST_P(uct_amo_swap_test_inlresp, swap32_inlresp32, "IB_TX_INLINE_RESP=32") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_SWAP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_SWAP), FOP32);
     test_swap<uint32_t>(static_cast<send_func_t>(&uct_amo_swap_test::swap32));
 }
 
 UCS_TEST_P(uct_amo_swap_test_inlresp, swap32_inlresp64, "IB_TX_INLINE_RESP=64") {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_SWAP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_SWAP), FOP32);
     test_swap<uint32_t>(static_cast<send_func_t>(&uct_amo_swap_test::swap32));
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_event.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_event.cc
index 2a4b2827b..1b5d6ab39 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_event.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_event.cc
@@ -110,8 +110,7 @@ void test_uct_event_fd::test_recv_am(bool signaled)
     recv_buffer->length = 0; /* Initialize length to 0 */
 
     /* set a callback for the uct to invoke for receiving the data */
-    uct_iface_set_am_handler(m_e2->iface(), 0, am_handler, recv_buffer,
-                             UCT_CB_FLAG_SYNC);
+    uct_iface_set_am_handler(m_e2->iface(), 0, am_handler, recv_buffer, 0);
 
     /* create receiver wakeup */
     status = uct_iface_event_fd_get(m_e2->iface(), &wakeup_fd.fd);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_fence.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_fence.cc
index 090d0baea..b568cb186 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_fence.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_fence.cc
@@ -58,9 +58,9 @@ public:
     public:
         worker(uct_fence_test* test, send_func_t send, recv_func_t recv,
                const mapped_buffer& recvbuf,
-               const entity& entity, uint64_t initial_value, uint32_t* error) :
-            test(test), value(initial_value), result32(0), result64(0),
-            error(error), running(true), m_send(send), m_recv(recv),
+               const entity& entity, uct_atomic_op_t op, uint32_t* error) :
+            test(test), value(0), result32(0), result64(0),
+            error(error), running(true), op(op), m_send(send), m_recv(recv),
             m_recvbuf(recvbuf), m_entity(entity) {
             pthread_create(&m_thread, NULL, run, reinterpret_cast<void*>(this));
         }
@@ -81,19 +81,46 @@ public:
             running = false;
         }
 
+        uint64_t atomic_op_val(uct_atomic_op_t op, uint64_t v1, uint64_t v2)
+        {
+            switch (op) {
+            case UCT_ATOMIC_OP_ADD:
+                return v1 + v2;
+            case UCT_ATOMIC_OP_AND:
+                return v1 & v2;
+            case UCT_ATOMIC_OP_OR:
+                return v1 | v2;
+            case UCT_ATOMIC_OP_XOR:
+                return v1 ^ v2;
+            default:
+                return 0;
+            }
+        }
+
         uct_fence_test* const test;
         uint64_t value;
         uint32_t result32;
         uint64_t result64;
         uint32_t* error;
         bool running;
+        uct_atomic_op_t op;
 
     private:
         void run() {
             uct_completion_t uct_comp;
             uct_comp.func = completion_cb;
             for (unsigned i = 0; i < uct_fence_test::count(); i++) {
-                uct_comp.count = 1;
+                uint64_t local_val  = ucs::rand();
+                uint64_t remote_val = ucs::rand();
+                uct_comp.count      = 1;
+
+                if (m_recvbuf.length() == sizeof(uint32_t)) {
+                    *(uint32_t*)m_recvbuf.ptr() = remote_val;
+                } else {
+                    *(uint64_t*)m_recvbuf.ptr() = remote_val;
+                }
+                value = local_val;
+
                 (test->*m_send)(m_entity.ep(0), *this, m_recvbuf);
                 uct_ep_fence(m_entity.ep(0), 0);
                 (test->*m_recv)(m_entity.ep(0), *this,
@@ -103,7 +130,7 @@ public:
                 uint64_t result = (m_recvbuf.length() == sizeof(uint32_t)) ?
                                     result32 : result64;
 
-                if (result != (uint64_t)(i+1))
+                if (result != atomic_op_val(op, local_val, remote_val))
                     (*error)++;
 
                 // reset for next loop
@@ -119,39 +146,40 @@ public:
         pthread_t m_thread;
     };
 
+    template <uct_atomic_op_t OP>
     void run_workers(send_func_t send, recv_func_t recv,
-                     const mapped_buffer& recvbuf,
-                     uint64_t initial_value, uint32_t* error) {
+                     const mapped_buffer& recvbuf, uint32_t* error) {
         ucs::ptr_vector<worker> m_workers;
         m_workers.clear();
         m_workers.push_back(new worker(this, send, recv, recvbuf,
-                                       sender(), initial_value, error));
+                                       sender(), OP, error));
         m_workers.at(0).join();
         m_workers.clear();
     }
 
-    ucs_status_t add32(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf) {
-        return uct_ep_atomic_add32(ep, worker.value, recvbuf.addr(), recvbuf.rkey());
-    }
-
-    ucs_status_t add64(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf) {
-        return uct_ep_atomic_add64(ep, worker.value, recvbuf.addr(), recvbuf.rkey());
-    }
-
-    ucs_status_t fadd32(uct_ep_h ep, worker& worker,
-                        const mapped_buffer& recvbuf, uct_completion_t *comp) {
-        return uct_ep_atomic_fadd32(ep, 0, recvbuf.addr(), recvbuf.rkey(),
-                                    &worker.result32, comp);
+    template <typename T, uct_atomic_op_t OP>
+    ucs_status_t atomic_op(uct_ep_h ep, worker& worker, const mapped_buffer& recvbuf) {
+        if (sizeof(T) == sizeof(uint32_t)) {
+            return uct_ep_atomic32_post(ep, OP, worker.value, recvbuf.addr(), recvbuf.rkey());
+        } else {
+            return uct_ep_atomic64_post(ep, OP, worker.value, recvbuf.addr(), recvbuf.rkey());
+        }
     }
 
-    ucs_status_t fadd64(uct_ep_h ep, worker& worker,
-                        const mapped_buffer& recvbuf, uct_completion_t *comp) {
-        return uct_ep_atomic_fadd64(ep, 0, recvbuf.addr(), recvbuf.rkey(),
-                                    &worker.result64, comp);
+    template <typename T, uct_atomic_op_t OP>
+    ucs_status_t atomic_fop(uct_ep_h ep, worker& worker,
+                            const mapped_buffer& recvbuf, uct_completion_t *comp) {
+        if (sizeof(T) == sizeof(uint32_t)) {
+            return uct_ep_atomic32_fetch(ep, OP, 0, &worker.result32,
+                                         recvbuf.addr(), recvbuf.rkey(), comp);
+        } else {
+            return uct_ep_atomic64_fetch(ep, OP, 0, &worker.result64,
+                                         recvbuf.addr(), recvbuf.rkey(), comp);
+        }
     }
 
-    template <typename T>
-    void test_fence(send_func_t send, recv_func_t recv) {
+    template <typename T, uct_atomic_op_t OP>
+    void test_fence() {
 
         mapped_buffer recvbuf(sizeof(T), 0, receiver());
 
@@ -159,24 +187,60 @@ public:
 
         *(T*)recvbuf.ptr() = 0;
 
-        run_workers(send, recv, recvbuf, 1, &error);
+        run_workers<OP>(static_cast<send_func_t>(&uct_fence_test::atomic_op<T, OP>),
+                        static_cast<recv_func_t>(&uct_fence_test::atomic_fop<T, OP>),
+                        recvbuf, &error);
 
         EXPECT_EQ(error, (uint32_t)0);
     }
 };
 
 UCS_TEST_P(uct_fence_test, add32) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ADD32);
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD32);
-    test_fence<uint32_t>(static_cast<send_func_t>(&uct_fence_test::add32),
-                         static_cast<recv_func_t>(&uct_fence_test::fadd32));
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), OP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP32);
+    test_fence<uint32_t, UCT_ATOMIC_OP_ADD>();
 }
 
 UCS_TEST_P(uct_fence_test, add64) {
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ADD64);
-    check_caps(UCT_IFACE_FLAG_ATOMIC_FADD64);
-    test_fence<uint64_t>(static_cast<send_func_t>(&uct_fence_test::add64),
-                         static_cast<recv_func_t>(&uct_fence_test::fadd64));
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), OP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ADD), FOP64);
+    test_fence<uint64_t, UCT_ATOMIC_OP_ADD>();
+}
+
+UCS_TEST_P(uct_fence_test, and32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_AND), OP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_AND), FOP32);
+    test_fence<uint32_t, UCT_ATOMIC_OP_AND>();
+}
+
+UCS_TEST_P(uct_fence_test, and64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_AND), OP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_AND), FOP64);
+    test_fence<uint64_t, UCT_ATOMIC_OP_AND>();
+}
+
+UCS_TEST_P(uct_fence_test, or32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_OR), OP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_OR), FOP32);
+    test_fence<uint32_t, UCT_ATOMIC_OP_OR>();
+}
+
+UCS_TEST_P(uct_fence_test, or64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_OR), OP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_OR), FOP64);
+    test_fence<uint64_t, UCT_ATOMIC_OP_OR>();
+}
+
+UCS_TEST_P(uct_fence_test, xor32) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), OP32);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP32);
+    test_fence<uint32_t, UCT_ATOMIC_OP_XOR>();
+}
+
+UCS_TEST_P(uct_fence_test, xor64) {
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), OP64);
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_XOR), FOP64);
+    test_fence<uint64_t, UCT_ATOMIC_OP_XOR>();
 }
 
 UCT_INSTANTIATE_TEST_CASE(uct_fence_test)
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_flush.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_flush.cc
index 08627eace..f37eb7d3c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_flush.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_flush.cc
@@ -343,7 +343,7 @@ void uct_flush_test::test_flush_am_pending(flush_func_t flush, bool destroy_ep)
          it->uct.func   = am_progress;
          it->comp.count = 2;
          it->comp.func  = NULL;
-         status = uct_ep_pending_add(sender().ep(0), &it->uct);
+         status = uct_ep_pending_add(sender().ep(0), &it->uct, 0);
          if (UCS_ERR_BUSY == status) {
              /* User advised to retry the send. It means no requests added
               * to the queue
@@ -373,7 +373,7 @@ void uct_flush_test::test_flush_am_pending(flush_func_t flush, bool destroy_ep)
              /* If flush returned NO_RESOURCE, add to pending must succeed */
              flush_req.test      = this;
              flush_req.uct.func  = flush_progress;
-             status = uct_ep_pending_add(sender().ep(0), &flush_req.uct);
+             status = uct_ep_pending_add(sender().ep(0), &flush_req.uct, 0);
              if (status == UCS_ERR_BUSY) {
                  continue;
              }
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_many2one_am.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_many2one_am.cc
index 07f6d42cd..4d74aa122 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_many2one_am.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_many2one_am.cc
@@ -100,7 +100,7 @@ UCS_TEST_P(test_many2one_am, am_bcopy, "MAX_BCOPY=16384")
     m_am_count = 0;
 
     status = uct_iface_set_am_handler(receiver->iface(), AM_ID, am_handler,
-                                      (void*)this, UCT_CB_FLAG_SYNC);
+                                      (void*)this, 0);
     ASSERT_UCS_OK(status);
 
     for (unsigned i = 0; i < num_sends; ++i) {
@@ -129,8 +129,7 @@ UCS_TEST_P(test_many2one_am, am_bcopy, "MAX_BCOPY=16384")
         progress();
     }
 
-    status = uct_iface_set_am_handler(receiver->iface(), AM_ID, NULL, NULL,
-                                      UCT_CB_FLAG_SYNC);
+    status = uct_iface_set_am_handler(receiver->iface(), AM_ID, NULL, NULL, 0);
     ASSERT_UCS_OK(status);
 
     check_backlog();
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.cc
index 680ff2383..49dc72234 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.cc
@@ -24,7 +24,7 @@ extern "C" {
 #include <cuda_runtime.h>
 #endif
 
-std::string const test_md::mem_types[] = {"host", "cuda"};
+std::string const test_md::mem_types[] = {"host", "cuda", "cuda-managed"};
 
 void* test_md::alloc_thread(void *arg)
 {
@@ -75,18 +75,22 @@ test_md::test_md()
     UCS_TEST_CREATE_HANDLE(uct_md_config_t*, m_md_config,
                            (void (*)(uct_md_config_t*))uct_config_release,
                            uct_md_config_read, GetParam().c_str(), NULL, NULL);
+    memset(&m_md_attr, 0, sizeof(m_md_attr));
 }
 
 void test_md::init()
 {
     ucs::test_base::init();
-    UCS_TEST_CREATE_HANDLE(uct_md_h, m_pd, uct_md_close, uct_md_open,
+    UCS_TEST_CREATE_HANDLE(uct_md_h, m_md, uct_md_close, uct_md_open,
                            GetParam().c_str(), m_md_config);
+
+    ucs_status_t status = uct_md_query(m_md, &m_md_attr);
+    ASSERT_UCS_OK(status);
 }
 
 void test_md::cleanup()
 {
-    m_pd.reset();
+    m_md.reset();
     ucs::test_base::cleanup();
 }
 
@@ -104,7 +108,7 @@ void test_md::modify_config(const std::string& name, const std::string& value,
 void test_md::check_caps(uint64_t flags, const std::string& name)
 {
     uct_md_attr_t md_attr;
-    ucs_status_t status = uct_md_query(pd(), &md_attr);
+    ucs_status_t status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
     if (!ucs_test_all_flags(md_attr.cap.flags, flags)) {
         std::stringstream ss;
@@ -187,23 +191,23 @@ UCS_TEST_P(test_md, rkey_ptr) {
     check_caps(UCT_MD_FLAG_ALLOC|UCT_MD_FLAG_RKEY_PTR, "allocation+direct access");
     // alloc (should work with both sysv and xpmem
     size = 1024 * 1024 * sizeof(unsigned);
-    status = uct_md_mem_alloc(pd(), &size, (void **)&rva,
+    status = uct_md_mem_alloc(md(), &size, (void **)&rva,
                               UCT_MD_MEM_ACCESS_ALL,
                               "test", &memh);
     ASSERT_UCS_OK(status);
     EXPECT_LE(1024 * 1024 * sizeof(unsigned), size);
 
     // pack
-    status = uct_md_query(pd(), &md_attr);
+    status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
     rkey_buffer = malloc(md_attr.rkey_packed_size);
     if (rkey_buffer == NULL) {
         // make coverity happy
-        uct_md_mem_free(pd(), memh);
+        uct_md_mem_free(md(), memh);
         GTEST_FAIL();
     }
 
-    status = uct_md_mkey_pack(pd(), memh, rkey_buffer);
+    status = uct_md_mkey_pack(md(), memh, rkey_buffer);
 
     // unpack
     status = uct_rkey_unpack(rkey_buffer, &rkey_bundle);
@@ -234,7 +238,7 @@ UCS_TEST_P(test_md, rkey_ptr) {
     EXPECT_EQ(UCS_ERR_INVALID_ADDR, status);
 
     free(rkey_buffer);
-    uct_md_mem_free(pd(), memh);
+    uct_md_mem_free(md(), memh);
     uct_rkey_release(&rkey_bundle);
 }
 
@@ -252,7 +256,7 @@ UCS_TEST_P(test_md, alloc) {
             continue;
         }
 
-        status = uct_md_mem_alloc(pd(), &size, &address,
+        status = uct_md_mem_alloc(md(), &size, &address,
                                   UCT_MD_MEM_ACCESS_ALL, "test", &memh);
         EXPECT_GT(size, 0ul);
 
@@ -262,7 +266,7 @@ UCS_TEST_P(test_md, alloc) {
         EXPECT_TRUE(memh != UCT_MEM_HANDLE_NULL);
 
         memset(address, 0xBB, size);
-        uct_md_mem_free(pd(), memh);
+        uct_md_mem_free(md(), memh);
     }
 }
 
@@ -273,7 +277,7 @@ UCS_TEST_P(test_md, mem_type_owned) {
     int ret;
     void *address;
 
-    status = uct_md_query(pd(), &md_attr);
+    status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
 
     if (md_attr.cap.mem_type == UCT_MD_MEM_TYPE_HOST) {
@@ -282,7 +286,7 @@ UCS_TEST_P(test_md, mem_type_owned) {
 
     alloc_memory(&address, 1024, NULL, md_attr.cap.mem_type);
 
-    ret = uct_md_is_mem_type_owned(pd(), address, 1024);
+    ret = uct_md_is_mem_type_owned(md(), address, 1024);
     EXPECT_TRUE(ret > 0);
 }
 
@@ -295,7 +299,7 @@ UCS_TEST_P(test_md, reg) {
 
     check_caps(UCT_MD_FLAG_REG, "registration");
 
-    status = uct_md_query(pd(), &md_attr);
+    status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
     for (unsigned mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
         if (!(md_attr.cap.reg_mem_types & UCS_BIT(mem_type))) {
@@ -314,13 +318,13 @@ UCS_TEST_P(test_md, reg) {
 
             alloc_memory(&address, size, &fill_buffer[0], mem_type);
 
-            status = uct_md_mem_reg(pd(), address, size, UCT_MD_MEM_ACCESS_ALL, &memh);
+            status = uct_md_mem_reg(md(), address, size, UCT_MD_MEM_ACCESS_ALL, &memh);
 
             ASSERT_UCS_OK(status);
             ASSERT_TRUE(memh != UCT_MEM_HANDLE_NULL);
             check_memory(address, &fill_buffer[0], size, mem_type);
 
-            status = uct_md_mem_dereg(pd(), memh);
+            status = uct_md_mem_dereg(md(), memh);
             ASSERT_UCS_OK(status);
             check_memory(address, &fill_buffer[0], size, mem_type);
 
@@ -338,7 +342,7 @@ UCS_TEST_P(test_md, reg_perf) {
 
     check_caps(UCT_MD_FLAG_REG, "registration");
 
-    status = uct_md_query(pd(), &md_attr);
+    status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
     for (unsigned mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
         if (!(md_attr.cap.reg_mem_types & UCS_BIT(mem_type))) {
@@ -355,12 +359,12 @@ UCS_TEST_P(test_md, reg_perf) {
             unsigned n = 0;
             while (n < count) {
                 uct_mem_h memh;
-                status = uct_md_mem_reg(pd(), ptr, size, UCT_MD_MEM_ACCESS_ALL,
+                status = uct_md_mem_reg(md(), ptr, size, UCT_MD_MEM_ACCESS_ALL,
                         &memh);
                 ASSERT_UCS_OK(status);
                 ASSERT_TRUE(memh != UCT_MEM_HANDLE_NULL);
 
-                status = uct_md_mem_dereg(pd(), memh);
+                status = uct_md_mem_dereg(md(), memh);
                 ASSERT_UCS_OK(status);
 
                 ++n;
@@ -392,16 +396,16 @@ UCS_TEST_P(test_md, reg_advise) {
     address = malloc(size);
     ASSERT_TRUE(address != NULL);
 
-    status = uct_md_mem_reg(pd(), address, size,
+    status = uct_md_mem_reg(md(), address, size,
                             UCT_MD_MEM_FLAG_NONBLOCK|UCT_MD_MEM_ACCESS_ALL,
                             &memh);
     ASSERT_UCS_OK(status);
     ASSERT_TRUE(memh != UCT_MEM_HANDLE_NULL);
 
-    status = uct_md_mem_advise(pd(), memh, (char *)address + 7, 32*1024, UCT_MADV_WILLNEED);
+    status = uct_md_mem_advise(md(), memh, (char *)address + 7, 32*1024, UCT_MADV_WILLNEED);
     EXPECT_UCS_OK(status);
 
-    status = uct_md_mem_dereg(pd(), memh);
+    status = uct_md_mem_dereg(md(), memh);
     EXPECT_UCS_OK(status);
     free(address);
 }
@@ -416,7 +420,7 @@ UCS_TEST_P(test_md, alloc_advise) {
 
     orig_size = size = 128 * 1024 * 1024;
 
-    status = uct_md_mem_alloc(pd(), &size, &address,
+    status = uct_md_mem_alloc(md(), &size, &address,
                               UCT_MD_MEM_FLAG_NONBLOCK|
                               UCT_MD_MEM_ACCESS_ALL,
                               "test", &memh);
@@ -425,11 +429,11 @@ UCS_TEST_P(test_md, alloc_advise) {
     EXPECT_TRUE(address != NULL);
     EXPECT_TRUE(memh != UCT_MEM_HANDLE_NULL);
 
-    status = uct_md_mem_advise(pd(), memh, (char *)address + 7, 32*1024, UCT_MADV_WILLNEED);
+    status = uct_md_mem_advise(md(), memh, (char *)address + 7, 32*1024, UCT_MADV_WILLNEED);
     EXPECT_UCS_OK(status);
 
     memset(address, 0xBB, size);
-    uct_md_mem_free(pd(), memh);
+    uct_md_mem_free(md(), memh);
 }
 
 /*
@@ -442,7 +446,7 @@ UCS_TEST_P(test_md, reg_multi_thread) {
 
     check_caps(UCT_MD_FLAG_REG, "registration");
 
-    status = uct_md_query(pd(), &md_attr);
+    status = uct_md_query(md(), &md_attr);
     ASSERT_UCS_OK(status);
 
     if (!(md_attr.cap.reg_mem_types & UCS_BIT(UCT_MD_MEM_TYPE_HOST))) {
@@ -461,7 +465,7 @@ UCS_TEST_P(test_md, reg_multi_thread) {
         ASSERT_TRUE(buffer != NULL);
 
         uct_mem_h memh;
-        status = uct_md_mem_reg(pd(), buffer, size,
+        status = uct_md_mem_reg(md(), buffer, size,
                                 UCT_MD_MEM_FLAG_NONBLOCK|
                                 UCT_MD_MEM_ACCESS_ALL,
                                 &memh);
@@ -470,7 +474,7 @@ UCS_TEST_P(test_md, reg_multi_thread) {
 
         sched_yield();
 
-        status = uct_md_mem_dereg(pd(), memh);
+        status = uct_md_mem_dereg(md(), memh);
         EXPECT_UCS_OK(status);
         free(buffer);
     }
@@ -482,7 +486,7 @@ UCS_TEST_P(test_md, reg_multi_thread) {
 UCS_TEST_P(test_md, sockaddr_accessibility) {
     ucs_sock_addr_t sock_addr;
     struct ifaddrs *ifaddr, *ifa;
-    int rc_local, rc_remote, found_ipoib = 0;
+    int found_ipoib = 0;
 
     check_caps(UCT_MD_FLAG_SOCKADDR, "sockaddr");
 
@@ -493,22 +497,23 @@ UCS_TEST_P(test_md, sockaddr_accessibility) {
         if (ucs::is_inet_addr(ifa->ifa_addr) && ucs_netif_is_active(ifa->ifa_name)) {
             sock_addr.addr = ifa->ifa_addr;
 
-            rc_local  = uct_md_is_sockaddr_accessible(pd(), &sock_addr, UCT_SOCKADDR_ACC_LOCAL);
-            rc_remote = uct_md_is_sockaddr_accessible(pd(), &sock_addr, UCT_SOCKADDR_ACC_REMOTE);
-
             if (!strcmp(GetParam().c_str(), "rdmacm")) {
-                if (ucs::is_ib_netdev(ifa->ifa_name)) {
+                if (ucs::is_rdmacm_netdev(ifa->ifa_name)) {
                     UCS_TEST_MESSAGE << "Testing " << ifa->ifa_name << " with " <<
                                         ucs::sockaddr_to_str(ifa->ifa_addr);
-                    ASSERT_TRUE(rc_local);
-                    ASSERT_TRUE(rc_remote);
+                    ASSERT_TRUE(uct_md_is_sockaddr_accessible(md(), &sock_addr,
+                                                              UCT_SOCKADDR_ACC_LOCAL));
+                    ASSERT_TRUE(uct_md_is_sockaddr_accessible(md(), &sock_addr,
+                                                              UCT_SOCKADDR_ACC_REMOTE));
                     found_ipoib = 1;
                 }
             } else {
                 UCS_TEST_MESSAGE << "Testing " << ifa->ifa_name << " with " <<
                                     ucs::sockaddr_to_str(ifa->ifa_addr);
-                ASSERT_TRUE(rc_local);
-                ASSERT_TRUE(rc_remote);
+                ASSERT_TRUE(uct_md_is_sockaddr_accessible(md(), &sock_addr,
+                                                          UCT_SOCKADDR_ACC_LOCAL));
+                ASSERT_TRUE(uct_md_is_sockaddr_accessible(md(), &sock_addr,
+                                                          UCT_SOCKADDR_ACC_REMOTE));
             }
         }
     }
@@ -528,6 +533,7 @@ UCS_TEST_P(test_md, sockaddr_accessibility) {
                    sysv, \
                    xpmem, \
                    cuda_cpy, \
+                   cuda_ipc, \
                    rocm, \
                    ib, \
                    ugni, \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.h
index e12f243ca..7d9aa98bf 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_md.h
@@ -34,16 +34,22 @@ protected:
 
     void test_registration();
 
-    uct_md_h pd() {
-        return m_pd;
+    uct_md_h md() const {
+        return m_md;
     }
 
+    const uct_md_attr_t& md_attr() const {
+        return m_md_attr;
+    }
+
+
     static void* alloc_thread(void *arg);
     static std::string const mem_types[];
 
 private:
     ucs::handle<uct_md_config_t*> m_md_config;
-    ucs::handle<uct_md_h>         m_pd;
+    ucs::handle<uct_md_h>         m_md;
+    uct_md_attr_t                 m_md_attr;
 };
 
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_mm.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_mm.cc
index c59b7192b..af0f72153 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_mm.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_mm.cc
@@ -84,7 +84,7 @@ UCS_TEST_P(test_uct_mm, open_for_posix) {
 
         /* set a callback for the uct to invoke for receiving the data */
         uct_iface_set_am_handler(m_e2->iface(), 0, mm_am_handler , recv_buffer,
-                                 UCT_CB_FLAG_SYNC);
+                                 0);
 
         /* send the data */
         uct_ep_am_short(m_e1->ep(0), 0, test_mm_hdr, &send_data, sizeof(send_data));
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_am.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_am.cc
index f4314e952..b8ee988c7 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_am.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_am.cc
@@ -208,7 +208,7 @@ public:
                            uct_memory_type_t mem_type) {
 
         if (receiver().iface_attr().cap.flags & UCT_IFACE_FLAG_CB_SYNC) {
-            test_xfer_do(send, length, flags, UCT_CB_FLAG_SYNC, mem_type);
+            test_xfer_do(send, length, flags, 0, mem_type);
         }
         if (receiver().iface_attr().cap.flags & UCT_IFACE_FLAG_CB_ASYNC) {
             test_xfer_do(send, length, flags, UCT_CB_FLAG_ASYNC, mem_type);
@@ -263,7 +263,7 @@ UCS_TEST_P(uct_p2p_am_test, am_sync) {
     unsigned am_count = m_am_count = 0;
 
     status = uct_iface_set_am_handler(receiver().iface(), AM_ID, am_handler,
-                                      this, UCT_CB_FLAG_SYNC);
+                                      this, 0);
     ASSERT_UCS_OK(status);
 
     if (receiver().iface_attr().cap.flags & UCT_IFACE_FLAG_AM_SHORT) {
@@ -407,7 +407,7 @@ public:
 
 
 UCS_TEST_P(uct_p2p_am_test, am_short) {
-    check_caps(UCT_IFACE_FLAG_AM_SHORT);
+    check_caps(UCT_IFACE_FLAG_AM_SHORT, UCT_IFACE_FLAG_AM_DUP);
     test_xfer_multi(static_cast<send_func_t>(&uct_p2p_am_test::am_short),
                     sizeof(uint64_t),
                     sender().iface_attr().cap.am.max_short,
@@ -415,7 +415,7 @@ UCS_TEST_P(uct_p2p_am_test, am_short) {
 }
 
 UCS_TEST_P(uct_p2p_am_test, am_bcopy) {
-    check_caps(UCT_IFACE_FLAG_AM_BCOPY);
+    check_caps(UCT_IFACE_FLAG_AM_BCOPY, UCT_IFACE_FLAG_AM_DUP);
     test_xfer_multi(static_cast<send_func_t>(&uct_p2p_am_test::am_bcopy),
                     0ul,
                     sender().iface_attr().cap.am.max_bcopy,
@@ -432,7 +432,7 @@ UCS_TEST_P(uct_p2p_am_test, am_short_keep_data) {
 }
 
 UCS_TEST_P(uct_p2p_am_test, am_bcopy_keep_data) {
-    check_caps(UCT_IFACE_FLAG_AM_BCOPY);
+    check_caps(UCT_IFACE_FLAG_AM_BCOPY, UCT_IFACE_FLAG_AM_DUP);
     set_keep_data(true);
     test_xfer_multi(static_cast<send_func_t>(&uct_p2p_am_test::am_bcopy),
                     sizeof(uint64_t),
@@ -441,7 +441,7 @@ UCS_TEST_P(uct_p2p_am_test, am_bcopy_keep_data) {
 }
 
 UCS_TEST_P(uct_p2p_am_test, am_zcopy) {
-    check_caps(UCT_IFACE_FLAG_AM_ZCOPY);
+    check_caps(UCT_IFACE_FLAG_AM_ZCOPY, UCT_IFACE_FLAG_AM_DUP);
     test_xfer_multi(static_cast<send_func_t>(&uct_p2p_am_test::am_zcopy),
                     0ul,
                     sender().iface_attr().cap.am.max_zcopy,
@@ -476,7 +476,7 @@ UCS_TEST_P(uct_p2p_am_misc, no_rx_buffs) {
 
     /* set a callback for the uct to invoke for receiving the data */
     status = uct_iface_set_am_handler(receiver().iface(), AM_ID, am_handler,
-                                      (void*)this, UCT_CB_FLAG_SYNC);
+                                      (void*)this, 0);
     ASSERT_UCS_OK(status);
 
     /* send many messages and progress the receiver. the receiver will keep getting
@@ -527,11 +527,14 @@ UCS_TEST_P(uct_p2p_am_misc, am_max_short_multi) {
         ASSERT_UCS_OK(status);
     }
 
-    /* do some progress */
-    short_progress_loop(50);
-
-    /* should be able to send again */
-    status = uct_ep_am_short(sender_ep(), AM_ID, SEED1, NULL, 0);
+    /* should be able to send again after a while */
+    ucs_time_t deadline = ucs_get_time() +
+                    (ucs::test_time_multiplier() *
+                     ucs_time_from_sec(DEFAULT_TIMEOUT_SEC));
+    do {
+        progress();
+        status = uct_ep_am_short(sender_ep(), AM_ID, SEED1, NULL, 0);
+    } while ((status == UCS_ERR_NO_RESOURCE) && (ucs_get_time() < deadline));
     EXPECT_EQ(UCS_OK, status);
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_err.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_err.cc
index 602683656..12f0b1cab 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_err.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_err.cc
@@ -26,7 +26,7 @@ public:
     };
 
     uct_p2p_err_test() :
-        uct_p2p_test(0, uct_error_handler_t(ucs_empty_function_return_success)) {
+        uct_p2p_test(0, error_handler) {
     }
 
     static size_t pack_cb(void *dest, void *arg)
@@ -43,9 +43,7 @@ public:
     {
         pack_arg arg;
 
-        wrap_errors();
-
-        UCS_TEST_SCOPE_EXIT() { restore_errors(); } UCS_TEST_SCOPE_EXIT_END
+        scoped_log_handler slh(wrap_errors_logger);
 
         ucs_status_t status = UCS_OK;
         ssize_t packed_len;
@@ -130,6 +128,20 @@ public:
 
     static ucs_status_t last_error;
 
+private:
+    static ucs_status_t
+    error_handler(void *arg, uct_ep_h ep, ucs_status_t status) {
+        uct_p2p_err_test *self = static_cast<uct_p2p_err_test*>(arg);
+        const p2p_resource *r = dynamic_cast<const p2p_resource*>(self->GetParam());
+        ucs_assert_always(r != NULL);
+        if (r->loopback) {
+            /* In loop back IB TLs can generate QP flush error before remote
+             * access error. */
+            ucs_log(UCS_LOG_LEVEL_ERROR, "Error on ep %p with status %s is handled",
+                    ep, ucs_status_string(status));
+        }
+        return UCS_OK;
+    }
 };
 
 ucs_status_t uct_p2p_err_test::last_error = UCS_OK;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.cc
index 9f9e32435..c5ece1f6c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.cc
@@ -24,16 +24,21 @@ ucs_status_t uct_p2p_mix_test::am_callback(void *arg, void *data, size_t length,
 
 void uct_p2p_mix_test::completion_callback(uct_completion_t *comp, ucs_status_t status)
 {
-    ASSERT_UCS_OK(status);
+    EXPECT_UCS_OK(status);
 }
 
-ucs_status_t uct_p2p_mix_test::swap64(const mapped_buffer &sendbuf,
-                                      const mapped_buffer &recvbuf,
-                                      uct_completion_t *comp)
+template <typename T, uct_atomic_op_t OP>
+ucs_status_t uct_p2p_mix_test::uct_p2p_mix_test::atomic_fop(const mapped_buffer &sendbuf,
+                                                            const mapped_buffer &recvbuf,
+                                                            uct_completion_t *comp)
 {
-    return uct_ep_atomic_swap64(sender().ep(0), 1, recvbuf.addr(),
-                                recvbuf.rkey(), (uint64_t*)sendbuf.ptr(),
-                                comp);
+    if (sizeof(T) == sizeof(uint32_t)) {
+        return uct_ep_atomic32_fetch(sender().ep(0), OP, 1, (uint32_t*)sendbuf.ptr(),
+                                     recvbuf.addr(), recvbuf.rkey(), comp);
+    } else {
+        return uct_ep_atomic64_fetch(sender().ep(0), OP, 1, (uint64_t*)sendbuf.ptr(),
+                                     recvbuf.addr(), recvbuf.rkey(), comp);
+    }
 }
 
 ucs_status_t uct_p2p_mix_test::cswap64(const mapped_buffer &sendbuf,
@@ -45,22 +50,6 @@ ucs_status_t uct_p2p_mix_test::cswap64(const mapped_buffer &sendbuf,
                                  comp);
 }
 
-ucs_status_t uct_p2p_mix_test::uct_p2p_mix_test::fadd32(const mapped_buffer &sendbuf,
-                                                        const mapped_buffer &recvbuf,
-                                                        uct_completion_t *comp)
-{
-    return uct_ep_atomic_fadd32(sender().ep(0), 1, recvbuf.addr(),
-                                recvbuf.rkey(), (uint32_t*)sendbuf.ptr(), comp);
-}
-
-ucs_status_t uct_p2p_mix_test::swap32(const mapped_buffer &sendbuf,
-                                      const mapped_buffer &recvbuf,
-                                      uct_completion_t *comp)
-{
-    return uct_ep_atomic_swap32(sender().ep(0), 1, recvbuf.addr(),
-                                recvbuf.rkey(), (uint32_t*)sendbuf.ptr(), comp);
-}
-
 ucs_status_t uct_p2p_mix_test::put_short(const mapped_buffer &sendbuf,
                                          const mapped_buffer &recvbuf,
                                          uct_completion_t *comp)
@@ -166,6 +155,8 @@ void uct_p2p_mix_test::run(unsigned count) {
     for (unsigned i = 0; i < count; ++i) {
         random_op(sendbuf, recvbuf);
     }
+
+    sender().flush();
 }
 
 void uct_p2p_mix_test::init() {
@@ -192,17 +183,38 @@ void uct_p2p_mix_test::init() {
         m_avail_send_funcs.push_back(&uct_p2p_mix_test::put_bcopy);
         m_send_size = ucs_min(m_send_size, sender().iface_attr().cap.put.max_bcopy);
     }
-    if (sender().iface_attr().cap.flags & UCT_IFACE_FLAG_ATOMIC_SWAP64) {
-        m_avail_send_funcs.push_back(&uct_p2p_mix_test::swap64);
-    }
-    if (sender().iface_attr().cap.flags & UCT_IFACE_FLAG_ATOMIC_CSWAP64) {
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_CSWAP)) {
         m_avail_send_funcs.push_back(&uct_p2p_mix_test::cswap64);
     }
-    if (sender().iface_attr().cap.flags & UCT_IFACE_FLAG_ATOMIC_SWAP32) {
-        m_avail_send_funcs.push_back(&uct_p2p_mix_test::swap32);
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_ADD)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint64_t, UCT_ATOMIC_OP_ADD>);
+    }
+    if (sender().iface_attr().cap.atomic32.fop_flags & UCS_BIT(UCT_ATOMIC_OP_ADD)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint32_t, UCT_ATOMIC_OP_ADD>);
+    }
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_AND)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint64_t, UCT_ATOMIC_OP_AND>);
+    }
+    if (sender().iface_attr().cap.atomic32.fop_flags & UCS_BIT(UCT_ATOMIC_OP_AND)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint32_t, UCT_ATOMIC_OP_AND>);
+    }
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_OR)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint64_t, UCT_ATOMIC_OP_OR>);
+    }
+    if (sender().iface_attr().cap.atomic32.fop_flags & UCS_BIT(UCT_ATOMIC_OP_OR)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint32_t, UCT_ATOMIC_OP_OR>);
+    }
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_XOR)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint64_t, UCT_ATOMIC_OP_XOR>);
+    }
+    if (sender().iface_attr().cap.atomic32.fop_flags & UCS_BIT(UCT_ATOMIC_OP_XOR)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint32_t, UCT_ATOMIC_OP_XOR>);
+    }
+    if (sender().iface_attr().cap.atomic64.fop_flags & UCS_BIT(UCT_ATOMIC_OP_SWAP)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint64_t, UCT_ATOMIC_OP_SWAP>);
     }
-    if (sender().iface_attr().cap.flags & UCT_IFACE_FLAG_ATOMIC_FADD32) {
-        m_avail_send_funcs.push_back(&uct_p2p_mix_test::fadd32);
+    if (sender().iface_attr().cap.atomic32.fop_flags & UCS_BIT(UCT_ATOMIC_OP_SWAP)) {
+        m_avail_send_funcs.push_back(&uct_p2p_mix_test::atomic_fop<uint32_t, UCT_ATOMIC_OP_SWAP>);
     }
 }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.h
index d4dc51c77..458809525 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_mix.h
@@ -28,22 +28,15 @@ protected:
 
     static void completion_callback(uct_completion_t *comp, ucs_status_t status);
 
-    ucs_status_t swap64(const mapped_buffer &sendbuf,
-                        const mapped_buffer &recvbuf,
-                        uct_completion_t *comp);
+    template <typename T, uct_atomic_op_t OP>
+    ucs_status_t atomic_fop(const mapped_buffer &sendbuf,
+                            const mapped_buffer &recvbuf,
+                            uct_completion_t *comp);
 
     ucs_status_t cswap64(const mapped_buffer &sendbuf,
                          const mapped_buffer &recvbuf,
                          uct_completion_t *comp);
 
-    ucs_status_t fadd32(const mapped_buffer &sendbuf,
-                        const mapped_buffer &recvbuf,
-                        uct_completion_t *comp);
-
-    ucs_status_t swap32(const mapped_buffer &sendbuf,
-                        const mapped_buffer &recvbuf,
-                        uct_completion_t *comp);
-
     ucs_status_t put_short(const mapped_buffer &sendbuf,
                            const mapped_buffer &recvbuf,
                            uct_completion_t *comp);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.cc
index 574a778f6..832f02ef1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.cc
@@ -42,6 +42,13 @@ ucs_status_t uct_p2p_rma_test::put_zcopy(uct_ep_h ep, const mapped_buffer &sendb
     return uct_ep_put_zcopy(ep, iov, iovcnt, recvbuf.addr(), recvbuf.rkey(), comp());
 }
 
+ucs_status_t uct_p2p_rma_test::get_short(uct_ep_h ep, const mapped_buffer &sendbuf,
+                                         const mapped_buffer &recvbuf)
+{
+     return uct_ep_get_short(ep, sendbuf.ptr(), sendbuf.length(),
+                             recvbuf.addr(), recvbuf.rkey());
+}
+
 ucs_status_t uct_p2p_rma_test::get_bcopy(uct_ep_h ep, const mapped_buffer &sendbuf,
                                          const mapped_buffer &recvbuf)
 {
@@ -62,7 +69,13 @@ ucs_status_t uct_p2p_rma_test::get_zcopy(uct_ep_h ep, const mapped_buffer &sendb
 void uct_p2p_rma_test::test_xfer(send_func_t send, size_t length,
                                  unsigned flags, uct_memory_type_t mem_type)
 {
-    mapped_buffer sendbuf(length, SEED1, sender(), 1);
+    uct_memory_type_t src_mem_type = UCT_MD_MEM_TYPE_HOST;
+
+    if ((GetParam()->tl_name.compare("cuda_ipc") == 0)) {
+        src_mem_type = mem_type;
+    }
+
+    mapped_buffer sendbuf(length, SEED1, sender(), 1, src_mem_type);
     mapped_buffer recvbuf(length, SEED2, receiver(), 3, mem_type);
 
     blocking_send(send, sender_ep(), sendbuf, recvbuf, true);
@@ -98,6 +111,13 @@ UCS_TEST_P(uct_p2p_rma_test, put_zcopy) {
                     TEST_UCT_FLAG_SEND_ZCOPY);
 }
 
+UCS_TEST_P(uct_p2p_rma_test, get_short) {
+    check_caps(UCT_IFACE_FLAG_GET_SHORT);
+    test_xfer_multi(static_cast<send_func_t>(&uct_p2p_rma_test::get_short),
+                    0ul, sender().iface_attr().cap.get.max_short,
+                    TEST_UCT_FLAG_RECV_ZCOPY);
+}
+
 UCS_TEST_P(uct_p2p_rma_test, get_bcopy) {
     check_caps(UCT_IFACE_FLAG_GET_BCOPY);
     test_xfer_multi(static_cast<send_func_t>(&uct_p2p_rma_test::get_bcopy),
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.h
index d983593d2..599abe9d1 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_p2p_rma.h
@@ -26,6 +26,9 @@ public:
     ucs_status_t put_zcopy(uct_ep_h ep, const mapped_buffer &sendbuf,
                            const mapped_buffer &recvbuf);
 
+    ucs_status_t get_short(uct_ep_h ep, const mapped_buffer &sendbuf,
+                           const mapped_buffer &recvbuf);
+
     ucs_status_t get_bcopy(uct_ep_h ep, const mapped_buffer &sendbuf,
                            const mapped_buffer &recvbuf);
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_peer_failure.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_peer_failure.cc
index 40bff86ba..78f0b0821 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_peer_failure.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_peer_failure.cc
@@ -22,8 +22,7 @@ private:
         void operator() (test_uct_peer_failure::entity *e) {
             uct_iface_set_am_handler(e->iface(), 0,
                                      am_dummy_handler,
-                                     reinterpret_cast<void*>(m_test),
-                                     UCT_CB_FLAG_SYNC);
+                                     reinterpret_cast<void*>(m_test), 0);
         }
 
         test_uct_peer_failure* m_test;
@@ -40,8 +39,9 @@ public:
         static uct_iface_params_t params;
 
         memset(&params, 0, sizeof(params));
-        params.err_handler     = get_err_handler();
-        params.err_handler_arg = reinterpret_cast<void*>(this);
+        params.err_handler       = get_err_handler();
+        params.err_handler_arg   = reinterpret_cast<void*>(this);
+        params.err_handler_flags = 0;
         return params;
     }
 
@@ -187,9 +187,9 @@ void test_uct_peer_failure::init()
     if (GetParam()->tl_name == "rc" || GetParam()->tl_name == "rc_mlx5" ||
         GetParam()->tl_name == "dc" || GetParam()->tl_name == "dc_mlx5") {
         set_config("RC_TIMEOUT=100us"); /* 100 us should be enough */
-        set_config("RC_RETRY_COUNT=2");
+        set_config("RC_RETRY_COUNT=4");
     } else if (GetParam()->tl_name == "ud" || GetParam()->tl_name == "ud_mlx5") {
-        set_config("UD_TIMEOUT=1s");
+        set_config("UD_TIMEOUT=3s");
     }
 
     uct_iface_params_t p = entity_params();
@@ -210,14 +210,14 @@ UCS_TEST_P(test_uct_peer_failure, peer_failure)
 {
     check_caps(UCT_IFACE_FLAG_PUT_SHORT);
 
-    wrap_errors();
-
-    kill_receiver();
-    EXPECT_EQ(uct_ep_put_short(ep0(), NULL, 0, 0, 0), UCS_OK);
+    {
+        scoped_log_handler slh(wrap_errors_logger);
 
-    flush();
+        kill_receiver();
+        EXPECT_EQ(UCS_OK, uct_ep_put_short(ep0(), NULL, 0, 0, 0));
 
-    restore_errors();
+        flush();
+    }
 
     UCS_TEST_GET_BUFFER_IOV(iov, iovcnt, NULL, 0, NULL, 1);
 
@@ -234,15 +234,11 @@ UCS_TEST_P(test_uct_peer_failure, peer_failure)
               UCS_ERR_ENDPOINT_TIMEOUT);
     EXPECT_EQ(uct_ep_get_zcopy(ep0(), iov, iovcnt, 0, 0, NULL),
               UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_add64(ep0(), 0, 0, 0), UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_add32(ep0(), 0, 0, 0), UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_fadd64(ep0(), 0, 0, 0, NULL, NULL),
-              UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_fadd32(ep0(), 0, 0, 0, NULL, NULL),
-              UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_swap64(ep0(), 0, 0, 0, NULL, NULL),
+    EXPECT_EQ(uct_ep_atomic64_post(ep0(), UCT_ATOMIC_OP_ADD, 0, 0, 0), UCS_ERR_ENDPOINT_TIMEOUT);
+    EXPECT_EQ(uct_ep_atomic32_post(ep0(), UCT_ATOMIC_OP_ADD, 0, 0, 0), UCS_ERR_ENDPOINT_TIMEOUT);
+    EXPECT_EQ(uct_ep_atomic64_fetch(ep0(), UCT_ATOMIC_OP_ADD, 0, NULL, 0, 0, NULL),
               UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_atomic_swap32(ep0(), 0, 0, 0, NULL, NULL),
+    EXPECT_EQ(uct_ep_atomic32_fetch(ep0(), UCT_ATOMIC_OP_ADD, 0, NULL, 0, 0, NULL),
               UCS_ERR_ENDPOINT_TIMEOUT);
     EXPECT_EQ(uct_ep_atomic_cswap64(ep0(), 0, 0, 0, 0, NULL, NULL),
               UCS_ERR_ENDPOINT_TIMEOUT);
@@ -250,7 +246,7 @@ UCS_TEST_P(test_uct_peer_failure, peer_failure)
               UCS_ERR_ENDPOINT_TIMEOUT);
     EXPECT_EQ(uct_ep_flush(ep0(), 0, NULL), UCS_ERR_ENDPOINT_TIMEOUT);
     EXPECT_EQ(uct_ep_get_address(ep0(), NULL), UCS_ERR_ENDPOINT_TIMEOUT);
-    EXPECT_EQ(uct_ep_pending_add(ep0(), NULL), UCS_ERR_ENDPOINT_TIMEOUT);
+    EXPECT_EQ(uct_ep_pending_add(ep0(), NULL, 0), UCS_ERR_ENDPOINT_TIMEOUT);
     EXPECT_EQ(uct_ep_connect_to_ep(ep0(), NULL, NULL), UCS_ERR_ENDPOINT_TIMEOUT);
 
     EXPECT_GT(m_err_count, 0ul);
@@ -265,23 +261,25 @@ UCS_TEST_P(test_uct_peer_failure, purge_failed_peer)
     send_recv_am(0);
     send_recv_am(1);
 
-    wrap_errors();
-    kill_receiver();
-
-    ucs_status_t status;
-    do {
-        status = uct_ep_am_short(ep0(), 0, 0, NULL, 0);
-    } while (status == UCS_OK);
-
     const size_t num_pend_sends = 3ul;
     uct_pending_req_t reqs[num_pend_sends];
-    for (size_t i = 0; i < num_pend_sends; i ++) {
-        reqs[i].func = pending_cb;
-        EXPECT_EQ(uct_ep_pending_add(ep0(), &reqs[i]), UCS_OK);
-    }
+    {
+        scoped_log_handler slh(wrap_errors_logger);
 
-    flush();
-    restore_errors();
+        kill_receiver();
+
+        ucs_status_t status;
+        do {
+            status = uct_ep_am_short(ep0(), 0, 0, NULL, 0);
+        } while (status == UCS_OK);
+
+        for (size_t i = 0; i < num_pend_sends; i ++) {
+            reqs[i].func = pending_cb;
+            EXPECT_EQ(uct_ep_pending_add(ep0(), &reqs[i], 0), UCS_OK);
+        }
+
+        flush();
+    }
 
     EXPECT_EQ(uct_ep_am_short(ep0(), 0, 0, NULL, 0), UCS_ERR_ENDPOINT_TIMEOUT);
 
@@ -302,12 +300,13 @@ UCS_TEST_P(test_uct_peer_failure, two_pairs_send)
     }
 
     /* kill the 1st receiver while sending on 2nd pair */
-    wrap_errors();
-    kill_receiver();
-    send_am(0);
-    send_recv_am(1);
-    flush();
-    restore_errors();
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+        kill_receiver();
+        send_am(0);
+        send_recv_am(1);
+        flush();
+    }
 
     /* test flushing one operations */
     send_recv_am(0, UCS_ERR_ENDPOINT_TIMEOUT);
@@ -329,13 +328,14 @@ UCS_TEST_P(test_uct_peer_failure, two_pairs_send_after)
 
     set_am_handlers();
 
-    wrap_errors();
-    kill_receiver();
-    for (int i = 0; i < 100; ++i) {
-        send_am(0);
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+        kill_receiver();
+        for (int i = 0; i < 100; ++i) {
+            send_am(0);
+        }
+        flush();
     }
-    flush();
-    restore_errors();
 
     send_recv_am(0, UCS_ERR_ENDPOINT_TIMEOUT);
 
@@ -368,11 +368,10 @@ UCS_TEST_P(test_uct_peer_failure_cb, desproy_ep_cb)
 {
     check_caps(UCT_IFACE_FLAG_PUT_SHORT);
 
-    wrap_errors();
+    scoped_log_handler slh(wrap_errors_logger);
     kill_receiver();
     EXPECT_EQ(uct_ep_put_short(ep0(), NULL, 0, 0, 0), UCS_OK);
     flush();
-    restore_errors();
 }
 
 UCT_INSTANTIATE_TEST_CASE(test_uct_peer_failure_cb)
@@ -445,24 +444,25 @@ UCS_TEST_P(test_uct_peer_failure_multiple, test, "RC_TM_ENABLE?=n")
     ucs_time_t timeout  = ucs_get_time() +
                           ucs_time_from_sec(200 * ucs::test_time_multiplier());
 
-    wrap_errors();
-    for (size_t idx = 0; idx < m_nreceivers - 1; ++idx) {
-        for (size_t i = 0; i < m_tx_window; ++i) {
-            send_am(idx);
+    {
+        scoped_log_handler slh(wrap_errors_logger);
+        for (size_t idx = 0; idx < m_nreceivers - 1; ++idx) {
+            for (size_t i = 0; i < m_tx_window; ++i) {
+                send_am(idx);
+            }
+            kill_receiver();
         }
-        kill_receiver();
-    }
-    flush(timeout);
+        flush(timeout);
 
-    /* if EPs are not failed yet, these ops should trigger that */
-    for (size_t idx = 0; idx < m_nreceivers - 1; ++idx) {
-        for (size_t i = 0; i < m_tx_window; ++i) {
-            send_am(idx);
+        /* if EPs are not failed yet, these ops should trigger that */
+        for (size_t idx = 0; idx < m_nreceivers - 1; ++idx) {
+            for (size_t i = 0; i < m_tx_window; ++i) {
+                send_am(idx);
+            }
         }
-    }
 
-    flush(timeout);
-    restore_errors();
+        flush(timeout);
+    }
 
     for (size_t idx = 0; idx < m_nreceivers - 1; ++idx) {
         send_recv_am(idx, UCS_ERR_ENDPOINT_TIMEOUT);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_pending.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_pending.cc
index 6bbf74bff..389354782 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_pending.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_pending.cc
@@ -34,6 +34,7 @@ public:
         uct_pending_req_t uct;
         int               active;
         int               id;
+        mapped_buffer    *buf;
     } pending_send_request_t;
 
     static ucs_status_t am_handler(void *arg, void *data, size_t length,
@@ -70,7 +71,7 @@ public:
         status = uct_ep_am_short(req->ep, 0, test_pending_hdr, &req->data,
                                  sizeof(req->data));
         if (status == UCS_OK) {
-            delete req;
+            pending_delete(req);
         }
         return status;
     }
@@ -91,6 +92,29 @@ public:
         return status;
     }
 
+    static ucs_status_t pending_send_op_bcopy(uct_pending_req_t *self) {
+
+        pending_send_request_t *req = ucs_container_of(self, pending_send_request_t, uct);
+        ssize_t packed_len;
+
+        packed_len = uct_ep_am_bcopy(req->ep, 0, mapped_buffer::pack, req->buf, 0);
+        if (packed_len > 0) {
+            req->countdown ++;
+            n_pending--;
+            req->active = 0;
+            return UCS_OK;
+        }
+        return (ucs_status_t)packed_len;
+    }
+
+    static ucs_status_t pending_send_op_ok(uct_pending_req_t *self) {
+        pending_send_request_t *req = ucs_container_of(self, pending_send_request_t, uct);
+
+        pending_delete(req);
+        n_pending--;
+        return UCS_OK;
+    }
+
     pending_send_request_t* pending_alloc(uint64_t send_data) {
         pending_send_request_t *req =  new pending_send_request_t();
         req->ep        = m_e1->ep(0);
@@ -111,6 +135,20 @@ public:
         return req;
     }
 
+    pending_send_request_t* pending_alloc_simple(mapped_buffer *sbuf, int idx) {
+        pending_send_request_t *req =  new pending_send_request_t();
+        req->ep        = m_e1->ep(idx);
+        req->buf       = sbuf;
+        req->countdown = 0;
+        req->uct.func  = pending_send_op_bcopy;
+        req->active    = 0;
+        req->id        = idx;
+        return req;
+    }
+
+    static void pending_delete(pending_send_request_t *req) {
+        delete req;
+    }
 
 protected:
     static const uint64_t test_pending_hdr = 0xabcd;
@@ -129,8 +167,9 @@ void install_handler_sync_or_async(uct_iface_t *iface, uint8_t id, uct_am_callba
     ASSERT_UCS_OK(status);
 
     if (attr.cap.flags & UCT_IFACE_FLAG_CB_SYNC) {
-        uct_iface_set_am_handler(iface, id, cb, arg, UCT_CB_FLAG_SYNC);
+        uct_iface_set_am_handler(iface, id, cb, arg, 0);
     } else {
+        ASSERT_TRUE(attr.cap.flags & UCT_IFACE_FLAG_CB_ASYNC);
         uct_iface_set_am_handler(iface, id, cb, arg, UCT_CB_FLAG_ASYNC);
     }
 }
@@ -159,11 +198,11 @@ UCS_TEST_P(test_uct_pending, pending_op)
 
                 pending_send_request_t *req = pending_alloc(send_data);
 
-                status = uct_ep_pending_add(m_e1->ep(0), &req->uct);
+                status = uct_ep_pending_add(m_e1->ep(0), &req->uct, 0);
                 if (status != UCS_OK) {
                     /* the request wasn't added to the pending data structure
                      * since resources became available. retry sending this message */
-                    delete req;
+                    pending_delete(req);
                 } else {
                     /* the request was added to the pending data structure */
                     send_data += 1;
@@ -209,9 +248,9 @@ UCS_TEST_P(test_uct_pending, send_ooo_with_pending)
 
             pending_send_request_t *req = pending_alloc(send_data);
 
-            status_pend = uct_ep_pending_add(m_e1->ep(0), &req->uct);
+            status_pend = uct_ep_pending_add(m_e1->ep(0), &req->uct, 0);
             if (status_pend == UCS_ERR_BUSY) {
-                delete req;
+                pending_delete(req);
             } else {
                 /* coverity[leaked_storage] */
                 ++send_data;
@@ -254,6 +293,116 @@ UCS_TEST_P(test_uct_pending, send_ooo_with_pending)
     EXPECT_EQ(exp_counter, counter);
 }
 
+/*
+ * test that the pending op callback is only called from the progress()
+ */
+UCS_TEST_P(test_uct_pending, pending_async)
+{
+    pending_send_request_t *req = NULL;
+    ucs_status_t status;
+    ssize_t packed_len;
+
+    initialize();
+    check_caps(UCT_IFACE_FLAG_AM_BCOPY | UCT_IFACE_FLAG_PENDING);
+
+    mapped_buffer sbuf(ucs_min(64ul, m_e1->iface_attr().cap.am.max_bcopy), 0,
+                       *m_e1);
+
+    req = pending_alloc_simple(&sbuf, 0);
+
+    /* set a callback for the uct to invoke when receiving the data */
+    install_handler_sync_or_async(m_e2->iface(), 0, am_handler_simple, 0);
+
+    /* send while resources are available */
+    n_pending = 0;
+    do {
+        packed_len = uct_ep_am_bcopy(m_e1->ep(0), 0, mapped_buffer::pack,
+                                     &sbuf, 0);
+    } while (packed_len >= 0);
+
+    EXPECT_TRUE(packed_len == UCS_ERR_NO_RESOURCE);
+
+    status = uct_ep_pending_add(m_e1->ep(0), &req->uct, 0);
+    EXPECT_UCS_OK(status);
+    n_pending++;
+
+    /* pending op must not be called either asynchronously or from the
+     * uct_ep_am_bcopy() */
+    twait(300);
+    EXPECT_EQ(1, n_pending);
+
+    packed_len = uct_ep_am_bcopy(m_e1->ep(0), 0, mapped_buffer::pack, &sbuf, 0);
+    EXPECT_EQ(1, n_pending);
+    EXPECT_GT(0, packed_len);
+
+    wait_for_value(&n_pending, 0, true);
+    EXPECT_EQ(0, n_pending);
+    pending_delete(req);
+}
+
+/*
+ * test that arbiter does not block when ucs_ok is returned
+ * The issue is a dc transport specific but test may be also useful
+ * for other transports
+ */
+UCS_TEST_P(test_uct_pending, pending_ucs_ok_dc_arbiter_bug)
+{
+    ucs_status_t status;
+    ssize_t packed_len;
+    int N;
+    int i;
+
+    initialize();
+    check_caps(UCT_IFACE_FLAG_AM_BCOPY | UCT_IFACE_FLAG_PENDING);
+
+    mapped_buffer sbuf(ucs_min(64ul, m_e1->iface_attr().cap.am.max_bcopy), 0,
+                       *m_e1);
+
+    /* set a callback for the uct to invoke when receiving the data */
+    install_handler_sync_or_async(m_e2->iface(), 0, am_handler_simple, 0);
+
+    if (RUNNING_ON_VALGRIND) {
+        N = 64;
+    } else if (m_e1->iface_attr().cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {
+        N = 2048;
+    } else {
+        N = 128;
+    }
+
+    N = ucs_min(N, max_connections());
+
+    /* idx 0 is setup in initialize(). only need to alloc request */
+    for (i = 1; i < N; i++) {
+        m_e1->connect(i, *m_e2, i);
+    }
+    /* give a chance to finish connection for some transports (ud) */
+    short_progress_loop();
+
+    n_pending = 0;
+
+    /* try to exaust global resources and create a pending queue */
+    for (i = 0; i < N; i++) {
+        packed_len = uct_ep_am_bcopy(m_e1->ep(i), 0, mapped_buffer::pack,
+                                     &sbuf, 0);
+
+        if (packed_len == UCS_ERR_NO_RESOURCE) {
+            pending_send_request_t *req = pending_alloc(i);
+
+            req->uct.func = pending_send_op_ok;
+            status = uct_ep_pending_add(m_e1->ep(i), &req->uct, 0);
+            EXPECT_UCS_OK(status);
+            n_pending++;
+            /* coverity[leaked_storage] */
+        }
+    }
+
+    UCS_TEST_MESSAGE << "pending queue len: " << n_pending;
+
+    wait_for_value(&n_pending, 0, true);
+    EXPECT_EQ(0, n_pending);
+}
+
+
 UCS_TEST_P(test_uct_pending, pending_fairness)
 {
     int N=16;
@@ -296,7 +445,8 @@ UCS_TEST_P(test_uct_pending, pending_fairness)
                                              &send_data, sizeof(send_data));
                     if (status == UCS_ERR_NO_RESOURCE) {
                         /* schedule pending */
-                        status = uct_ep_pending_add(m_e1->ep(i), &reqs[i]->uct);
+                        status = uct_ep_pending_add(m_e1->ep(i), &reqs[i]->uct,
+                                                    0);
                         if (status == UCS_ERR_BUSY) {
                             continue; /* retry */
                         }
@@ -343,7 +493,7 @@ UCS_TEST_P(test_uct_pending, pending_fairness)
     flush();
 
     for (i = 0; i < N; i++) {
-        delete reqs[i];
+        pending_delete(reqs[i]);
     }
 
     /* there must be no starvation */
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_stats.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_stats.cc
index 571b4c667..77ce0b6ce 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_stats.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_stats.cc
@@ -31,7 +31,7 @@ public:
     void init_bufs(size_t min, size_t max)
     {
         size_t size = ucs_max(min, ucs_min(64ul, max));
-        lbuf = new mapped_buffer(size, 0, sender());
+        lbuf = new mapped_buffer(size, 0, sender(), 0, sender().md_attr().cap.mem_type);
         rbuf = new mapped_buffer(size, 0, receiver(), 0, sender().md_attr().cap.mem_type);
     }
 
@@ -284,43 +284,55 @@ UCS_TEST_P(test_uct_stats, get_zcopy)
                       lbuf->length());
 }
 
-#define TEST_STATS_ATOMIC_ADD(val) \
-UCS_TEST_P(test_uct_stats, atomic_add ## val) \
-{ \
-    ucs_status_t status; \
-\
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ADD ## val); \
-    init_bufs(sizeof(uint##val##_t), sizeof(uint##val##_t)); \
-    status = uct_ep_atomic_add ## val (sender_ep(), 1, rbuf->addr(), rbuf->rkey()); \
-    EXPECT_UCS_OK(status); \
-    check_atomic_counters(); \
+#define TEST_STATS_ATOMIC_POST(_op, _val)                                      \
+UCS_TEST_P(test_uct_stats, atomic_post_ ## _op ## _val)                        \
+{                                                                              \
+    ucs_status_t status;                                                       \
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ ## _op), OP ## _val);                 \
+    init_bufs(sizeof(uint##_val##_t), sizeof(uint##_val##_t));                 \
+    status = uct_ep_atomic ##_val##_post(sender_ep(), (UCT_ATOMIC_OP_ ## _op), \
+                                         1, rbuf->addr(), rbuf->rkey());       \
+    EXPECT_UCS_OK(status);                                                     \
+    check_atomic_counters();                                                   \
 }
 
-TEST_STATS_ATOMIC_ADD(32)
-
-TEST_STATS_ATOMIC_ADD(64)
-
-#define TEST_STATS_ATOMIC_FUNC(func, flag, val) \
-UCS_TEST_P(test_uct_stats, atomic_##func##val) \
-{ \
-    ucs_status_t status; \
-    uint##val##_t result; \
-\
-    check_caps(UCT_IFACE_FLAG_ATOMIC_ ## flag ## val); \
-    init_bufs(sizeof(result), sizeof(result)); \
-\
-    init_completion(); \
-    status = uct_ep_atomic_##func##val (sender_ep(), 1, rbuf->addr(), rbuf->rkey(), &result, &m_comp); \
-    wait_for_completion(status); \
-\
-    check_atomic_counters(); \
+TEST_STATS_ATOMIC_POST(ADD, 32)
+TEST_STATS_ATOMIC_POST(ADD, 64)
+TEST_STATS_ATOMIC_POST(AND, 32)
+TEST_STATS_ATOMIC_POST(AND, 64)
+TEST_STATS_ATOMIC_POST(OR,  32)
+TEST_STATS_ATOMIC_POST(OR,  64)
+TEST_STATS_ATOMIC_POST(XOR, 32)
+TEST_STATS_ATOMIC_POST(XOR, 64)
+
+
+#define TEST_STATS_ATOMIC_FETCH(_op, _val)                                              \
+UCS_TEST_P(test_uct_stats, atomic_fetch_## _op ## _val)                                 \
+{                                                                                       \
+    ucs_status_t status;                                                                \
+    uint##_val##_t result;                                                              \
+                                                                                        \
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_ ## _op), FOP ## _val);                         \
+    init_bufs(sizeof(result), sizeof(result));                                          \
+                                                                                        \
+    init_completion();                                                                  \
+    status = uct_ep_atomic##_val##_fetch(sender_ep(), (UCT_ATOMIC_OP_ ## _op), 1,       \
+                                         &result, rbuf->addr(), rbuf->rkey(), &m_comp); \
+    wait_for_completion(status);                                                        \
+                                                                                        \
+    check_atomic_counters();                                                            \
 }
 
-TEST_STATS_ATOMIC_FUNC(fadd, FADD, 32)
-TEST_STATS_ATOMIC_FUNC(fadd, FADD, 64)
-
-TEST_STATS_ATOMIC_FUNC(swap, SWAP, 32)
-TEST_STATS_ATOMIC_FUNC(swap, SWAP, 64)
+TEST_STATS_ATOMIC_FETCH(ADD,  32)
+TEST_STATS_ATOMIC_FETCH(ADD,  64)
+TEST_STATS_ATOMIC_FETCH(AND,  32)
+TEST_STATS_ATOMIC_FETCH(AND,  64)
+TEST_STATS_ATOMIC_FETCH(OR,   32)
+TEST_STATS_ATOMIC_FETCH(OR,   64)
+TEST_STATS_ATOMIC_FETCH(XOR,  32)
+TEST_STATS_ATOMIC_FETCH(XOR,  64)
+TEST_STATS_ATOMIC_FETCH(SWAP, 32)
+TEST_STATS_ATOMIC_FETCH(SWAP, 64)
 
 #define TEST_STATS_ATOMIC_CSWAP(val) \
 UCS_TEST_P(test_uct_stats, atomic_cswap##val) \
@@ -328,7 +340,7 @@ UCS_TEST_P(test_uct_stats, atomic_cswap##val) \
     ucs_status_t status; \
     uint##val##_t result; \
 \
-    check_caps(UCT_IFACE_FLAG_ATOMIC_CSWAP ## val); \
+    check_atomics(UCS_BIT(UCT_ATOMIC_OP_CSWAP), FOP ## val); \
     init_bufs(sizeof(result), sizeof(result)); \
 \
     init_completion(); \
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_tag.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_tag.cc
index dc0d4835c..b86218e86 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_tag.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_tag.cc
@@ -10,6 +10,11 @@ extern "C" {
 #include <common/test.h>
 #include "uct_test.h"
 
+#define UCT_TAG_INSTANTIATE_TEST_CASE(_test_case) \
+    _UCT_INSTANTIATE_TEST_CASE(_test_case, rc) \
+    _UCT_INSTANTIATE_TEST_CASE(_test_case, dc) \
+    _UCT_INSTANTIATE_TEST_CASE(_test_case, rc_mlx5) \
+    _UCT_INSTANTIATE_TEST_CASE(_test_case, dc_mlx5)
 
 class test_tag : public uct_test {
 public:
@@ -36,30 +41,28 @@ public:
     };
 
     struct send_ctx {
-        send_ctx(mapped_buffer *b, uct_tag_t t, uint64_t i) :
-                 mbuf(b), rndv_op(NULL), tag(t), imm_data(i) {
-
-            uct_comp.count = 2;
-            uct_comp.func  = NULL;
-            sw_rndv        = false;
-        }
         mapped_buffer    *mbuf;
         void             *rndv_op;
         uct_tag_t        tag;
         uint64_t         imm_data;
         uct_completion_t uct_comp;
-        bool sw_rndv;
+        ucs_status_t     status;
+        bool             sw_rndv;
+        bool             comp;
+        bool             unexp;
     };
 
     typedef ucs_status_t (test_tag::*send_func)(entity&, send_ctx&);
 
-    void init() {
+    void init()
+    {
         ucs_status_t status = uct_config_modify(m_iface_config, "RC_TM_ENABLE", "y");
         ASSERT_TRUE((status == UCS_OK) || (status == UCS_ERR_NO_ELEM));
 
         uct_test::init();
 
         uct_iface_params params;
+        memset(&params, 0, sizeof(params));
 
         // tl and dev names are taken from resources via GetParam, no need
         // to fill it here
@@ -86,8 +89,23 @@ public:
         }
     }
 
+    void init_send_ctx(send_ctx &s,mapped_buffer *b, uct_tag_t t, uint64_t i,
+                       bool unexp_flow = true)
+    {
+        s.mbuf           = b;
+        s.rndv_op        = NULL;
+        s.tag            = t;
+        s.imm_data       = i;
+        s.uct_comp.count = 1;
+        s.uct_comp.func  = send_completion;
+        s.sw_rndv        = s.comp = false;
+        s.unexp          = unexp_flow;
+        s.status         = UCS_ERR_NO_PROGRESS;
+    }
+
     void init_recv_ctx(recv_ctx &r,  mapped_buffer *b, uct_tag_t t,
-                       uct_tag_t m = MASK, bool uct_d = false) {
+                       uct_tag_t m = MASK, bool uct_d = false)
+    {
         r.mbuf                    = b;
         r.tag                     = t;
         r.tmask                   = m;
@@ -95,13 +113,17 @@ public:
         r.uct_ctx.tag_consumed_cb = tag_consumed;
         r.uct_ctx.rndv_cb         = sw_rndv_completed;
         r.take_uct_desc           = uct_d;
+        r.status                  = UCS_ERR_NO_PROGRESS;
         r.comp = r.unexp = r.consumed = r.sw_rndv = false;
     }
 
     ucs_status_t tag_eager_short(entity &e, send_ctx &ctx)
     {
-        return uct_ep_tag_eager_short(e.ep(0), ctx.tag, ctx.mbuf->ptr(),
-                                      ctx.mbuf->length());
+        ctx.status = uct_ep_tag_eager_short(e.ep(0), ctx.tag, ctx.mbuf->ptr(),
+                                            ctx.mbuf->length());
+        ctx.comp   = true;
+
+        return ctx.status;
     }
 
     ucs_status_t tag_eager_bcopy(entity &e, send_ctx &ctx)
@@ -110,8 +132,10 @@ public:
                                                 ctx.imm_data, mapped_buffer::pack,
                                                 reinterpret_cast<void*>(ctx.mbuf),
                                                 0);
+        ctx.status = (status >= 0) ? UCS_OK : static_cast<ucs_status_t>(status);
+        ctx.comp   = true;
 
-        return (status >= 0) ? UCS_OK : static_cast<ucs_status_t>(status);
+        return ctx.status;
     }
 
     ucs_status_t tag_eager_zcopy(entity &e, send_ctx &ctx)
@@ -155,14 +179,25 @@ public:
 
     ucs_status_t tag_rndv_request(entity &e, send_ctx &ctx)
     {
-        rndv_hdr hdr = {{ctx.imm_data,
-                         reinterpret_cast<uint64_t>(&ctx)
-                        },
-                        0xFAFA
-                       };
         ctx.sw_rndv = true;
 
-        return uct_ep_tag_rndv_request(e.ep(0), ctx.tag, &hdr, sizeof(hdr), 0);
+        if (ctx.unexp) {
+            // Unexpected flow, will need to analyze ctx data on the receiver
+            rndv_hdr hdr = {{ctx.imm_data,
+                             reinterpret_cast<uint64_t>(&ctx)
+                            },
+                            0xFAFA
+                           };
+            ctx.status = uct_ep_tag_rndv_request(e.ep(0), ctx.tag, &hdr,
+                                                 sizeof(hdr), 0);
+        } else {
+            // Expected flow, send just plain data (will be stored in rx buf by HCA)
+            ctx.status = uct_ep_tag_rndv_request(e.ep(0), ctx.tag, ctx.mbuf->ptr(),
+                                                 ctx.mbuf->length(), 0);
+        }
+        ctx.comp = true;
+
+        return ctx.status;
     }
 
     ucs_status_t tag_post(entity &e, recv_ctx &ctx)
@@ -184,20 +219,33 @@ public:
     // called). And it is vice versa if message arrives unexpectedly.
     // If expected SW RNDV request arrives tag_consumed and sw_rndv_cb
     // should be called.
-    void check_completion(recv_ctx &ctx, bool is_expected, uint64_t seed,
-                          ucs_status_t status = UCS_OK, bool is_sw_rndv = false) {
-      EXPECT_EQ(ctx.consumed, is_expected);
-      EXPECT_EQ(ctx.comp,     (is_expected && !is_sw_rndv));
-      EXPECT_EQ(ctx.unexp,    (!is_expected && !is_sw_rndv));
-      EXPECT_EQ(ctx.sw_rndv,  is_sw_rndv);
-      EXPECT_EQ(ctx.status,   status);
-      if (is_expected) {
-          ctx.mbuf->pattern_check(seed);
-      }
-    }
-
-    void test_tag_expected(send_func sfunc, size_t length = 75) {
-        uct_tag_t    tag    = 11;
+    void check_rx_completion(recv_ctx &ctx, bool is_expected, uint64_t seed,
+                             ucs_status_t status = UCS_OK, bool is_sw_rndv = false)
+    {
+        EXPECT_EQ(ctx.consumed, is_expected);
+        EXPECT_EQ(ctx.comp,     (is_expected && !is_sw_rndv));
+        EXPECT_EQ(ctx.unexp,    (!is_expected && !is_sw_rndv));
+        EXPECT_EQ(ctx.sw_rndv,  is_sw_rndv);
+        EXPECT_EQ(ctx.status,   status);
+        if (is_expected) {
+            ctx.mbuf->pattern_check(seed);
+        }
+    }
+
+    void check_tx_completion(send_ctx &ctx)
+    {
+        wait_for_flag(&ctx.comp);
+        EXPECT_TRUE(ctx.comp);
+        EXPECT_EQ(ctx.status, UCS_OK);
+    }
+
+    void test_tag_expected(send_func sfunc, size_t length = 75,
+                           bool is_sw_rndv = false) {
+        uct_tag_t tag = 11;
+
+        if (RUNNING_ON_VALGRIND) {
+            length = ucs_min(length, 128U);
+        }
 
         mapped_buffer recvbuf(length, RECV_SEED, receiver());
         recv_ctx r_ctx;
@@ -207,25 +255,36 @@ public:
         short_progress_loop();
 
         mapped_buffer sendbuf(length, SEND_SEED, sender());
-        send_ctx s_ctx(&sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
+        send_ctx s_ctx;
+        init_send_ctx(s_ctx, &sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx),
+                      false);
         ASSERT_UCS_OK((this->*sfunc)(sender(), s_ctx));
 
-        wait_for_flag(&r_ctx.comp);
+        wait_for_flag(is_sw_rndv ? &r_ctx.sw_rndv : &r_ctx.comp);
+
+        check_rx_completion(r_ctx, true, SEND_SEED, UCS_OK, is_sw_rndv);
 
-        check_completion(r_ctx, true, SEND_SEED);
+        // If it was RNDV send, need to wait send completion as well
+        check_tx_completion(s_ctx);
 
         flush();
     }
 
     void test_tag_unexpected(send_func sfunc, size_t length = 75,
-                             bool take_uct_desc = false) {
+                             bool take_uct_desc = false)
+    {
         uct_tag_t tag = 11;
 
+        if (RUNNING_ON_VALGRIND) {
+            length = ucs_min(length, 128U);
+        }
+
         mapped_buffer recvbuf(length, RECV_SEED, receiver());
         mapped_buffer sendbuf(length, SEND_SEED, sender());
         recv_ctx r_ctx;
         init_recv_ctx(r_ctx, &recvbuf, tag, MASK, take_uct_desc);
-        send_ctx s_ctx(&sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
+        send_ctx s_ctx;
+        init_send_ctx(s_ctx, &sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
         ASSERT_UCS_OK((this->*sfunc)(sender(), s_ctx));
 
         wait_for_flag(&r_ctx.unexp);
@@ -236,11 +295,12 @@ public:
             ASSERT_UCS_OK(tag_rndv_cancel(sender(), s_ctx.rndv_op));
         }
 
-        check_completion(r_ctx, false, SEND_SEED);
+        check_rx_completion(r_ctx, false, SEND_SEED);
         flush();
     }
 
-    void test_tag_wrong_tag(send_func sfunc) {
+    void test_tag_wrong_tag(send_func sfunc)
+    {
         const size_t length = 65;
         uct_tag_t    tag    = 11;
 
@@ -251,7 +311,8 @@ public:
         // and not to be macthed.
         recv_ctx r_ctx;
         init_recv_ctx(r_ctx, &recvbuf, tag + 1);
-        send_ctx s_ctx(&sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
+        send_ctx s_ctx;
+        init_send_ctx(s_ctx, &sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
 
         ASSERT_UCS_OK((this->*sfunc)(sender(), s_ctx));
 
@@ -263,11 +324,12 @@ public:
 
         // Message should be reported as unexpected and filled with
         // recv seed (unchanged), as the incoming tag does not match the expected
-        check_completion(r_ctx, false, RECV_SEED);
+        check_rx_completion(r_ctx, false, RECV_SEED);
         flush();
     }
 
-    void test_tag_mask(send_func sfunc) {
+    void test_tag_mask(send_func sfunc)
+    {
         const size_t length = 65;
 
         mapped_buffer recvbuf(length, RECV_SEED, receiver());
@@ -281,12 +343,16 @@ public:
         short_progress_loop();
 
         mapped_buffer sendbuf(length, SEND_SEED, sender());
-        send_ctx s_ctx(&sendbuf, 0xffff, reinterpret_cast<uint64_t>(&r_ctx));
+        send_ctx s_ctx;
+        init_send_ctx(s_ctx, &sendbuf, 0xffff, reinterpret_cast<uint64_t>(&r_ctx));
         ASSERT_UCS_OK((this->*sfunc)(sender(), s_ctx));
         wait_for_flag(&r_ctx.comp);
 
         // Should be matched because tags are equal with tag mask applied.
-        check_completion(r_ctx, true, SEND_SEED);
+        check_rx_completion(r_ctx, true, SEND_SEED);
+
+        // If it was RNDV send, need to wait send completion as well
+        check_tx_completion(s_ctx);
         flush();
     }
 
@@ -366,7 +432,8 @@ public:
     }
 
     static ucs_status_t am_handler(void *arg, void *data, size_t length,
-                                   unsigned flags) {
+                                   unsigned flags)
+    {
         is_am_received = true;
         return UCS_OK;
     }
@@ -382,6 +449,14 @@ public:
         return UCS_LOG_FUNC_RC_CONTINUE;
     }
 
+    static void send_completion(uct_completion_t *self, ucs_status_t status)
+    {
+        send_ctx *user_ctx = ucs_container_of(self, send_ctx, uct_comp);
+        user_ctx->comp     = true;
+        user_ctx->status   = status;
+    }
+
+
 protected:
     uct_test::entity& sender() {
         return **m_entities.begin();
@@ -509,8 +584,7 @@ UCS_TEST_P(test_tag, tag_send_no_tag)
 {
   check_caps(UCT_IFACE_FLAG_TAG_EAGER_BCOPY);
 
-  uct_iface_set_am_handler(receiver().iface(), 0, am_handler,
-                           NULL, UCT_CB_FLAG_SYNC);
+  uct_iface_set_am_handler(receiver().iface(), 0, am_handler, NULL, 0);
   mapped_buffer lbuf(200, SEND_SEED, sender());
   ssize_t len = uct_ep_am_bcopy(sender().ep(0), 0, mapped_buffer::pack,
                                 reinterpret_cast<void*>(&lbuf), 0);
@@ -535,13 +609,14 @@ UCS_TEST_P(test_tag, tag_cancel_force)
     short_progress_loop();
 
     mapped_buffer sendbuf(length, SEND_SEED, sender());
-    send_ctx s_ctx(&sendbuf, 1, reinterpret_cast<uint64_t>(&r_ctx));
+    send_ctx s_ctx;
+    init_send_ctx(s_ctx, &sendbuf, 1, reinterpret_cast<uint64_t>(&r_ctx));
     ASSERT_UCS_OK(tag_eager_bcopy(sender(), s_ctx));
 
     // Message should arrive unexpected, since tag was cancelled
     // on the receiver.
     wait_for_flag(&r_ctx.unexp);
-    check_completion(r_ctx, false, SEND_SEED);
+    check_rx_completion(r_ctx, false, SEND_SEED);
 }
 
 UCS_TEST_P(test_tag, tag_cancel_noforce)
@@ -557,7 +632,7 @@ UCS_TEST_P(test_tag, tag_cancel_noforce)
     short_progress_loop(200);
     ASSERT_UCS_OK(tag_cancel(receiver(), r_ctx, 0));
 
-    short_progress_loop();
+    wait_for_flag(&r_ctx.comp);
 
     // Check that completed callback has been called with CANCELED status
     // (because 0 was passed as force parameter to cancel).
@@ -604,28 +679,8 @@ UCS_TEST_P(test_tag, sw_rndv_expected)
 {
     check_caps(UCT_IFACE_FLAG_TAG_EAGER_BCOPY | UCT_IFACE_FLAG_TAG_RNDV_ZCOPY);
 
-    uct_tag_t    tag    = 11;
-    const size_t length = sender().iface_attr().cap.tag.rndv.max_hdr;
-
-    mapped_buffer recvbuf(length, RECV_SEED, receiver());
-    recv_ctx r_ctx;
-    init_recv_ctx(r_ctx, &recvbuf, tag);
-    ASSERT_UCS_OK(tag_post(receiver(), r_ctx));
-
-    short_progress_loop();
-
-    mapped_buffer sendbuf(length, SEND_SEED, sender());
-    send_ctx s_ctx(&sendbuf, tag, reinterpret_cast<uint64_t>(&r_ctx));
-
-    ASSERT_UCS_OK(uct_ep_tag_rndv_request(sender().ep(0), s_ctx.tag,
-                                          s_ctx.mbuf->ptr(),
-                                          s_ctx.mbuf->length(), 0));
-
-    wait_for_flag(&r_ctx.sw_rndv);
-
-    check_completion(r_ctx, true, SEND_SEED, UCS_OK, true);
-
-    flush();
+    test_tag_expected(static_cast<send_func>(&test_tag::tag_rndv_request),
+                      sender().iface_attr().cap.tag.rndv.max_hdr, true);
 }
 
 UCS_TEST_P(test_tag, rndv_limit)
@@ -639,7 +694,8 @@ UCS_TEST_P(test_tag, rndv_limit)
     void *op;
 
     do {
-        sctx_p = new send_ctx(&sendbuf, 0xffff, 0);
+        sctx_p = new send_ctx;
+        init_send_ctx(*sctx_p, &sendbuf, 0xffff, 0);
         status = tag_rndv_zcopy(sender(), *sctx_p);
         sctxs.push_back(sctx_p);
     } while (status == UCS_OK);
@@ -666,7 +722,171 @@ UCS_TEST_P(test_tag, sw_rndv_unexpected)
     test_tag_unexpected(static_cast<send_func>(&test_tag::tag_rndv_request));
 }
 
-_UCT_INSTANTIATE_TEST_CASE(test_tag, rc)
-_UCT_INSTANTIATE_TEST_CASE(test_tag, dc)
-_UCT_INSTANTIATE_TEST_CASE(test_tag, rc_mlx5)
-_UCT_INSTANTIATE_TEST_CASE(test_tag, dc_mlx5)
+UCT_TAG_INSTANTIATE_TEST_CASE(test_tag)
+
+
+#if ENABLE_STATS && IBV_EXP_HW_TM
+extern "C" {
+#include <uct/api/uct.h>
+#include <uct/ib/rc/base/rc_iface.h>
+#include <uct/ib/base/ib_verbs.h>
+}
+
+class test_tag_stats : public test_tag {
+public:
+    void init() {
+        stats_activate();
+        test_tag::init();
+    }
+
+    void cleanup() {
+        test_tag::cleanup();
+        stats_restore();
+    }
+
+    ucs_stats_node_t *ep_stats(const entity &e)
+    {
+        return ucs_derived_of(e.ep(0), uct_base_ep_t)->stats;
+    }
+
+    ucs_stats_node_t *iface_stats(const entity &e)
+    {
+        return ucs_derived_of(e.iface(), uct_rc_iface_t)->tm.stats;
+    }
+
+    void provoke_sync(const entity &e)
+    {
+        uct_rc_iface_t *iface = ucs_derived_of(e.iface(), uct_rc_iface_t);
+
+        // Counters are synced every IBV_DEVICE_MAX_UNEXP_COUNT ops, set
+        // it one op before, so that any following unexpected message would
+        // cause HW ans SW counters sync.
+        iface->tm.unexpected_cnt = IBV_DEVICE_MAX_UNEXP_COUNT - 1;
+    }
+
+    void check_tx_counters(int op, uint64_t op_val, int type, size_t len)
+    {
+        uint64_t v;
+
+        v = UCS_STATS_GET_COUNTER(ep_stats(sender()), op);
+        EXPECT_EQ(op_val, v);
+
+        // With valgrind reduced messages is sent
+        if (!RUNNING_ON_VALGRIND) {
+            v = UCS_STATS_GET_COUNTER(ep_stats(sender()), type);
+            EXPECT_EQ(len, v);
+        }
+    }
+
+    void check_rx_counter(int op, uint64_t val, entity &e)
+    {
+        EXPECT_EQ(val, UCS_STATS_GET_COUNTER(iface_stats(e), op));
+    }
+};
+
+UCS_TEST_P(test_tag_stats, tag_expected_eager)
+{
+    check_caps(UCT_IFACE_FLAG_TAG_EAGER_SHORT |
+               UCT_IFACE_FLAG_TAG_EAGER_BCOPY |
+               UCT_IFACE_FLAG_TAG_EAGER_ZCOPY);
+
+    std::pair<send_func, std::pair<size_t, int> > sfuncs[3] = {
+                std::make_pair(static_cast<send_func>(&test_tag::tag_eager_short),
+                std::make_pair(sender().iface_attr().cap.tag.eager.max_short,
+                static_cast<int>(UCT_EP_STAT_BYTES_SHORT))),
+
+                std::make_pair(static_cast<send_func>(&test_tag::tag_eager_bcopy),
+                std::make_pair(sender().iface_attr().cap.tag.eager.max_bcopy,
+                static_cast<int>(UCT_EP_STAT_BYTES_BCOPY))),
+
+                std::make_pair(static_cast<send_func>(&test_tag::tag_eager_zcopy),
+                std::make_pair(sender().iface_attr().cap.tag.eager.max_zcopy,
+                static_cast<int>(UCT_EP_STAT_BYTES_ZCOPY)))
+    };
+
+    for (int i = 0; i < 3; ++i) {
+        test_tag_expected(sfuncs[i].first, sfuncs[i].second.first);
+        check_tx_counters(UCT_EP_STAT_TAG, i + 1,
+                          sfuncs[i].second.second,
+                          sfuncs[i].second.first);
+        check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_EXP, i + 1, receiver());
+    }
+}
+
+UCS_TEST_P(test_tag_stats, tag_unexpected_eager)
+{
+    check_caps(UCT_IFACE_FLAG_TAG_EAGER_BCOPY | UCT_IFACE_FLAG_TAG_EAGER_ZCOPY);
+
+    std::pair<send_func, std::pair<size_t, int> > sfuncs[2] = {
+                std::make_pair(static_cast<send_func>(&test_tag::tag_eager_bcopy),
+                std::make_pair(sender().iface_attr().cap.tag.eager.max_bcopy,
+                static_cast<int>(UCT_EP_STAT_BYTES_BCOPY))),
+
+                std::make_pair(static_cast<send_func>(&test_tag::tag_eager_zcopy),
+                std::make_pair(sender().iface_attr().cap.tag.eager.max_zcopy,
+                static_cast<int>(UCT_EP_STAT_BYTES_ZCOPY)))
+    };
+
+    for (int i = 0; i < 2; ++i) {
+        test_tag_unexpected(sfuncs[i].first, sfuncs[i].second.first);
+        check_tx_counters(UCT_EP_STAT_TAG, i + 1,
+                          sfuncs[i].second.second,
+                          sfuncs[i].second.first);
+        check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_EAGER_UNEXP, i + 1, receiver());
+    }
+}
+
+UCS_TEST_P(test_tag_stats, tag_list_ops)
+{
+    check_caps(UCT_IFACE_FLAG_TAG_EAGER_BCOPY);
+    mapped_buffer recvbuf(32, RECV_SEED, receiver());
+    recv_ctx rctx;
+
+    init_recv_ctx(rctx, &recvbuf, 1);
+
+    ASSERT_UCS_OK(tag_post(receiver(), rctx));
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_LIST_ADD, 1ul, receiver());
+
+    ASSERT_UCS_OK(tag_cancel(receiver(), rctx, 1));
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_LIST_DEL, 1ul, receiver());
+
+    // Every ADD and DEL is paired with SYNC, but stats counter is increased
+    // when separate SYNC op is issued only. So, we expect it to be 0 after
+    // ADD and DEL operations.
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_LIST_SYNC, 0ul, receiver());
+
+    // Provoke real SYNC op and send a message unexpectedly
+    provoke_sync(receiver());
+    test_tag_unexpected(static_cast<send_func>(&test_tag::tag_eager_bcopy));
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_LIST_SYNC, 1ul, receiver());
+}
+
+
+UCS_TEST_P(test_tag_stats, tag_rndv)
+{
+    check_caps(UCT_IFACE_FLAG_TAG_RNDV_ZCOPY | UCT_IFACE_FLAG_TAG_EAGER_BCOPY);
+
+    size_t len = sender().iface_attr().cap.tag.rndv.max_zcopy / 8;
+
+    // Check UNEXP_RNDV on the receiver
+    test_tag_unexpected(static_cast<send_func>(&test_tag::tag_rndv_zcopy), len);
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_RNDV_UNEXP, 1ul, receiver());
+
+    // Check that sender receives RNDV_FIN in case of expected rndv message
+    test_tag_expected(static_cast<send_func>(&test_tag::tag_rndv_zcopy), len);
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_RNDV_FIN, 1ul, sender());
+
+
+    // Check UNEXP_RNDV_REQ on the receiver
+    test_tag_unexpected(static_cast<send_func>(&test_tag::tag_rndv_request));
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_UNEXP, 1ul, receiver());
+
+    // Check NEXP_RNDV_REQ on the receiver
+    test_tag_expected(static_cast<send_func>(&test_tag::tag_rndv_request),
+                     sender().iface_attr().cap.tag.rndv.max_hdr, true);
+    check_rx_counter(UCT_RC_IFACE_STAT_TAG_RX_RNDV_REQ_EXP, 1ul, receiver());
+}
+
+UCT_TAG_INSTANTIATE_TEST_CASE(test_tag_stats)
+
+#endif
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_ep.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_ep.cc
index baf4810fe..ce985a55c 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_ep.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_ep.cc
@@ -45,6 +45,12 @@ UCS_TEST_P(test_uct_ep, disconnect_after_send) {
     ucs_status_t status;
     unsigned count;
 
+#if HAVE_DC_DV
+    if (GetParam()->tl_name.compare("dc_mlx5") == 0) {
+        UCS_TEST_SKIP_R("DCI stuck bug");
+    }
+#endif
+
     check_caps(UCT_IFACE_FLAG_AM_ZCOPY);
 
     mapped_buffer buffer(256, 0, *m_sender);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_perf.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_perf.cc
index 17499532b..fdcb46f6e 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_perf.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_uct_perf.cc
@@ -132,7 +132,7 @@ test_perf::test_spec test_uct_perf::tests[] =
 UCS_TEST_P(test_uct_perf, envelope) {
     bool check_perf;
 
-    if (GetParam()->tl_name == "cm" || GetParam()->tl_name == "ugni_udt") {
+    if (GetParam()->tl_name == "cm" || GetParam()->tl_name == "ugni_udt" || GetParam()->tl_name == "cuda_ipc") {
         UCS_TEST_SKIP;
     }
 
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_zcopy_comp.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_zcopy_comp.cc
index b7f95a1e7..be8f901a8 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_zcopy_comp.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/test_zcopy_comp.cc
@@ -31,6 +31,12 @@ UCS_TEST_P(test_zcopy_comp, issue1440)
     size_t size_large = ucs_min(65536ul, sender->iface_attr().cap.put.max_zcopy);
     ucs_assert(size_large > size_small);
 
+    if (sender->md_attr().cap.mem_type != UCT_MD_MEM_TYPE_HOST) {
+        std::stringstream ss;
+        ss << "test_zcopy_comp is not supported by " << GetParam();
+        UCS_TEST_SKIP_R(ss.str());
+    }
+
     mapped_buffer sendbuf_small(size_small, 0, *sender);
     mapped_buffer sendbuf_large(size_large, 0, *sender);
     mapped_buffer recvbuf_small(size_small, 0, *receiver_small);
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_p2p_test.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_p2p_test.cc
index e40adb297..3c709779d 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_p2p_test.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_p2p_test.cc
@@ -149,8 +149,12 @@ void uct_p2p_test::test_xfer_multi(send_func_t send, size_t min_length,
 {
 
     for (int mem_type = 0; mem_type < UCT_MD_MEM_TYPE_LAST; mem_type++) {
+        /* test mem type if md supports mem type
+         * (or) if HOST MD can register mem type
+         */
         if (!((sender().md_attr().cap.mem_type == mem_type) ||
-            (sender().md_attr().cap.reg_mem_types & UCS_BIT(mem_type)))) {
+            (sender().md_attr().cap.mem_type == UCT_MD_MEM_TYPE_HOST &&
+		sender().md_attr().cap.reg_mem_types & UCS_BIT(mem_type)))) {
             continue;
         }
         if (mem_type == UCT_MD_MEM_TYPE_CUDA) {
@@ -162,6 +166,7 @@ void uct_p2p_test::test_xfer_multi(send_func_t send, size_t min_length,
                                  (uct_memory_type_t) mem_type);
     }
 }
+
 void uct_p2p_test::test_xfer_multi_mem_type(send_func_t send, size_t min_length,
                                             size_t max_length, unsigned flags,
                                             uct_memory_type_t mem_type) {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.cc b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.cc
index 00effe28d..002cf9cba 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.cc
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.cc
@@ -58,6 +58,65 @@ uct_test::~uct_test() {
     uct_config_release(m_md_config);
 }
 
+void uct_test::init_sockaddr_rsc(resource *rsc, struct sockaddr *listen_addr,
+                                 struct sockaddr *connect_addr, size_t size)
+{
+    memcpy(&rsc->listen_if_addr,  listen_addr,  size);
+    memcpy(&rsc->connect_if_addr, connect_addr, size);
+}
+
+void uct_test::set_interface_rscs(char *md_name, cpu_set_t local_cpus,
+                                  struct ifaddrs *ifa,
+                                  std::vector<resource>& all_resources)
+{
+    int i;
+
+    /* Create two resources on the same interface. the first one will have the
+     * ip of the interface and the second one will have INADDR_ANY */
+    for (i = 0; i < 2; i++) {
+        resource rsc;
+        rsc.md_name    = md_name,
+        rsc.local_cpus = local_cpus,
+        rsc.tl_name    = "sockaddr",
+        rsc.dev_name   = ifa->ifa_name;
+        rsc.dev_type   = UCT_DEVICE_TYPE_NET;
+
+        if (i == 0) {
+            /* first rsc */
+            if (ifa->ifa_addr->sa_family == AF_INET) {
+                uct_test::init_sockaddr_rsc(&rsc, ifa->ifa_addr, ifa->ifa_addr,
+                                            sizeof(struct sockaddr_in));
+            } else if (ifa->ifa_addr->sa_family == AF_INET6) {
+                uct_test::init_sockaddr_rsc(&rsc, ifa->ifa_addr, ifa->ifa_addr,
+                                            sizeof(struct sockaddr_in6));
+            } else {
+                UCS_TEST_ABORT("Unknown sa_family " << ifa->ifa_addr->sa_family);
+            }
+            all_resources.push_back(rsc);
+        } else {
+            /* second rsc */
+            if (ifa->ifa_addr->sa_family == AF_INET) {
+                struct sockaddr_in sin;
+                memset(&sin, 0, sizeof(struct sockaddr_in));
+                sin.sin_family      = AF_INET;
+                sin.sin_addr.s_addr = INADDR_ANY;
+                uct_test::init_sockaddr_rsc(&rsc, (struct sockaddr*)&sin,
+                                            ifa->ifa_addr, sizeof(struct sockaddr_in));
+            } else if (ifa->ifa_addr->sa_family == AF_INET6) {
+                struct sockaddr_in6 sin;
+                memset(&sin, 0, sizeof(struct sockaddr_in6));
+                sin.sin6_family     = AF_INET6;
+                sin.sin6_addr       = in6addr_any;
+                uct_test::init_sockaddr_rsc(&rsc, (struct sockaddr*)&sin,
+                                            ifa->ifa_addr, sizeof(struct sockaddr_in6));
+            } else {
+                UCS_TEST_ABORT("Unknown sa_family " << ifa->ifa_addr->sa_family);
+            }
+            all_resources.push_back(rsc);
+        }
+    }
+}
+
 void uct_test::set_sockaddr_resources(uct_md_h md, char *md_name, cpu_set_t local_cpus,
                                       std::vector<resource>& all_resources) {
 
@@ -69,24 +128,16 @@ void uct_test::set_sockaddr_resources(uct_md_h md, char *md_name, cpu_set_t loca
     for (ifa = ifaddr; ifa != NULL; ifa = ifa->ifa_next) {
         sock_addr.addr = ifa->ifa_addr;
 
+        /* If rdmacm is tested, make sure that this is an IPoIB or RoCE interface */
+        if (!strcmp(md_name, "rdmacm") && (!ucs::is_rdmacm_netdev(ifa->ifa_name))) {
+            continue;
+        }
+
         if (uct_md_is_sockaddr_accessible(md, &sock_addr, UCT_SOCKADDR_ACC_LOCAL) &&
             uct_md_is_sockaddr_accessible(md, &sock_addr, UCT_SOCKADDR_ACC_REMOTE) &&
             ucs_netif_is_active(ifa->ifa_name)) {
-            resource rsc;
-            rsc.md_name    = md_name,
-            rsc.local_cpus = local_cpus,
-            rsc.tl_name    = "sockaddr",
-            rsc.dev_name   = ifa->ifa_name;
-            rsc.dev_type   = UCT_DEVICE_TYPE_NET;
 
-            if (ifa->ifa_addr->sa_family == AF_INET) {
-                memcpy(&rsc.if_addr, ifa->ifa_addr, sizeof(struct sockaddr_in));
-            } else if (ifa->ifa_addr->sa_family == AF_INET6) {
-                memcpy(&rsc.if_addr, ifa->ifa_addr, sizeof(struct sockaddr_in6));
-            } else {
-                UCS_TEST_ABORT("Unknown sa_family " << ifa->ifa_addr->sa_family);
-            }
-            all_resources.push_back(rsc);
+            uct_test::set_interface_rscs(md_name, local_cpus, ifa, all_resources);
         }
     }
 
@@ -114,9 +165,14 @@ std::vector<const resource*> uct_test::enum_resources(const std::string& tl_name
                                         &md_config);
             ASSERT_UCS_OK(status);
 
-            status = uct_md_open(md_resources[i].md_name, md_config, &pd);
+            {
+                scoped_log_handler slh(hide_errors_logger);
+                status = uct_md_open(md_resources[i].md_name, md_config, &pd);
+            }
             uct_config_release(md_config);
-            ASSERT_UCS_OK(status);
+            if (status != UCS_OK) {
+                continue;
+            }
 
             uct_md_attr_t md_attr;
             status = uct_md_query(pd, &md_attr);
@@ -176,6 +232,12 @@ void uct_test::check_caps(uint64_t required_flags, uint64_t invalid_flags) {
     }
 }
 
+void uct_test::check_atomics(uint64_t required_ops, atomic_mode mode) {
+    FOR_EACH_ENTITY(iter) {
+        (*iter)->check_atomics(required_ops, mode);
+    }
+}
+
 void uct_test::modify_config(const std::string& name, const std::string& value,
                              bool optional) {
     ucs_status_t status;
@@ -235,9 +297,11 @@ uct_test::entity* uct_test::create_entity(size_t rx_headroom,
     uct_iface_params_t iface_params;
 
     memset(&iface_params, 0, sizeof(iface_params));
-    iface_params.rx_headroom = rx_headroom;
-    iface_params.open_mode   = UCT_IFACE_OPEN_MODE_DEVICE;
-    iface_params.err_handler = err_handler;
+    iface_params.rx_headroom       = rx_headroom;
+    iface_params.open_mode         = UCT_IFACE_OPEN_MODE_DEVICE;
+    iface_params.err_handler       = err_handler;
+    iface_params.err_handler_arg   = this;
+    iface_params.err_handler_flags = 0;
     entity *new_ent = new entity(*GetParam(), m_iface_config, &iface_params,
                                  m_md_config);
     return new_ent;
@@ -301,7 +365,17 @@ void uct_test::twait(int delta_ms) const {
     } while (now + ucs_time_from_msec(delta_ms) > ucs_get_time());
 }
 
-const std::string uct_test::entity::client_priv_data = "Client private data";
+int uct_test::max_connections()
+{
+    if (GetParam()->tl_name == "tcp") {
+        return ucs::max_tcp_connections();
+    } else {
+        return std::numeric_limits<int>::max();
+    }
+}
+
+std::string uct_test::entity::client_priv_data = "";
+size_t uct_test::entity::client_cb_arg = 0;
 
 uct_test::entity::entity(const resource& resource, uct_iface_config_t *iface_config,
                          uct_iface_params_t *params, uct_md_config_t *md_config) {
@@ -317,7 +391,7 @@ uct_test::entity::entity(const resource& resource, uct_iface_config_t *iface_con
     UCS_CPU_ZERO(&params->cpu_mask);
 
     UCS_TEST_CREATE_HANDLE(uct_worker_h, m_worker, uct_worker_destroy,
-                           uct_worker_create, &m_async.m_async, UCS_THREAD_MODE_MULTI /* TODO */);
+                           uct_worker_create, &m_async.m_async, UCS_THREAD_MODE_SINGLE);
 
     UCS_TEST_CREATE_HANDLE(uct_md_h, m_md, uct_md_close,
                            uct_md_open, resource.md_name.c_str(), md_config);
@@ -466,6 +540,33 @@ void uct_test::entity::check_caps(uint64_t required_flags,
     }
 }
 
+void uct_test::entity::check_atomics(uint64_t required_ops, atomic_mode mode)
+{
+    uint64_t amo;
+
+    switch (mode) {
+    case OP32:
+        amo = iface_attr().cap.atomic32.op_flags;
+        break;
+    case OP64:
+        amo = iface_attr().cap.atomic64.op_flags;
+        break;
+    case FOP32:
+        amo = iface_attr().cap.atomic32.fop_flags;
+        break;
+    case FOP64:
+        amo = iface_attr().cap.atomic64.fop_flags;
+        break;
+    default:
+        UCS_TEST_ABORT("Incorrect atomic mode: " << mode);
+        break;
+    }
+
+    if (!ucs_test_all_flags(amo, required_ops)) {
+        UCS_TEST_SKIP_R("unsupported");
+    }
+}
+
 uct_md_h uct_test::entity::md() const {
     return m_md;
 }
@@ -558,7 +659,23 @@ void uct_test::entity::destroy_eps() {
     }
 }
 
-void uct_test::entity::connect_to_sockaddr(unsigned index, entity& other)
+ssize_t uct_test::entity::client_priv_data_cb(void *arg, const char *dev_name,
+                                              void *priv_data)
+{
+    size_t *max_conn_priv = (size_t*)arg;
+    size_t priv_data_len;
+
+    client_priv_data = "Client private data";
+    priv_data_len = 1 + client_priv_data.length();
+
+    memcpy(priv_data, client_priv_data.c_str(), priv_data_len);
+    EXPECT_LE(priv_data_len, (*max_conn_priv));
+
+    return priv_data_len;
+}
+
+void uct_test::entity::connect_to_sockaddr(unsigned index, entity& other,
+                                           ucs_sock_addr_t *remote_addr)
 {
     uct_ep_h ep;
     ucs_status_t status;
@@ -568,13 +685,10 @@ void uct_test::entity::connect_to_sockaddr(unsigned index, entity& other)
         return; /* Already connected */
     }
 
-    ASSERT_TRUE(client_priv_data.length() <= other.iface_attr().max_conn_priv);
-
     /* Connect to the server */
-    status = uct_ep_create_sockaddr(iface(),
-                                    &other.iface_params().mode.sockaddr.listen_sockaddr,
-                                    client_priv_data.c_str(),
-                                    client_priv_data.length(), &ep);
+    status = uct_ep_create_sockaddr(iface(), remote_addr,
+                                    client_priv_data_cb, (void*)&client_cb_arg,
+                                    UCT_CB_FLAG_ASYNC, &ep);
     ASSERT_UCS_OK(status);
 
     m_eps[index].reset(ep, uct_ep_destroy);
@@ -639,19 +753,25 @@ void uct_test::entity::connect_to_iface(unsigned index, entity& other) {
 }
 
 void uct_test::entity::connect(unsigned index, entity& other,
-                               unsigned other_index)
+                               unsigned other_index,
+                               ucs_sock_addr_t *remote_addr)
 {
     if (iface_attr().cap.flags & UCT_IFACE_FLAG_CONNECT_TO_EP) {
         connect_to_ep(index, other, other_index);
     } else if (iface_attr().cap.flags & UCT_IFACE_FLAG_CONNECT_TO_IFACE) {
         connect_to_iface(index, other);
     } else if (iface_attr().cap.flags & UCT_IFACE_FLAG_CONNECT_TO_SOCKADDR) {
-        connect_to_sockaddr(index, other);
+        connect_to_sockaddr(index, other, remote_addr);
     } else {
         UCS_TEST_SKIP_R("cannot connect");
     }
 }
 
+void uct_test::entity::connect(unsigned index, entity& other, unsigned other_index)
+{
+    connect(index, other, other_index, NULL);
+}
+
 void uct_test::entity::flush() const {
     ucs_status_t status;
     do {
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.h b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.h
index 092b4aa28..78a92c8d5 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.h
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/gtest/uct/uct_test.h
@@ -35,7 +35,8 @@ struct resource {
     std::string             tl_name;
     std::string             dev_name;
     uct_device_type_t       dev_type;
-    struct sockaddr_storage if_addr;
+    struct sockaddr_storage listen_if_addr;     /* sockaddr to listen on */
+    struct sockaddr_storage connect_if_addr;    /* sockaddr to connect to */
 };
 
 
@@ -53,10 +54,19 @@ public:
     uct_test();
     virtual ~uct_test();
 
+    enum atomic_mode {
+        OP32,
+        OP64,
+        FOP32,
+        FOP64
+    };
+
 protected:
 
     class entity {
     public:
+        typedef uct_test::atomic_mode atomic_mode;
+
         entity(const resource& resource, uct_iface_config_t *iface_config,
                uct_iface_params_t *params, uct_md_config_t *md_config);
 
@@ -71,6 +81,7 @@ protected:
 
         bool is_caps_supported(uint64_t required_flags);
         void check_caps(uint64_t required_flags, uint64_t invalid_flags = 0);
+        void check_atomics(uint64_t required_ops, atomic_mode mode);
 
         uct_md_h md() const;
 
@@ -90,14 +101,17 @@ protected:
         void destroy_ep(unsigned index);
         void destroy_eps();
         void connect(unsigned index, entity& other, unsigned other_index);
+        void connect(unsigned index, entity& other, unsigned other_index,
+                     ucs_sock_addr_t *remote_addr);
         void connect_to_iface(unsigned index, entity& other);
         void connect_to_ep(unsigned index, entity& other,
                            unsigned other_index);
-        void connect_to_sockaddr(unsigned index, entity& other);
+        void connect_to_sockaddr(unsigned index, entity& other, ucs_sock_addr_t *remote_addr);
 
         void flush() const;
 
-        static const std::string client_priv_data;
+        static std::string client_priv_data;
+        static size_t      client_cb_arg;
 
     private:
         class async_wrapper {
@@ -118,6 +132,8 @@ protected:
         void connect_p2p_ep(uct_ep_h from, uct_ep_h to);
         void cuda_mem_alloc(size_t length, uct_allocated_memory_t *mem) const;
         void cuda_mem_free(const uct_allocated_memory_t *mem) const;
+        static ssize_t client_priv_data_cb(void *arg, const char *dev_name,
+                                           void *priv_data);
 
         ucs::handle<uct_md_h>      m_md;
         uct_md_attr_t              m_md_attr;
@@ -214,6 +230,7 @@ protected:
     bool is_caps_supported(uint64_t required_flags);
     void check_caps(uint64_t required_flags, uint64_t invalid_flags = 0);
     void check_caps(const entity& e, uint64_t required_flags, uint64_t invalid_flags = 0);
+    void check_atomics(uint64_t required_ops, atomic_mode mode);
     const entity& ent(unsigned index) const;
     unsigned progress() const;
     void flush(ucs_time_t deadline = ULONG_MAX) const;
@@ -221,11 +238,17 @@ protected:
     virtual void twait(int delta_ms = DEFAULT_DELAY_MS) const;
     static void set_sockaddr_resources(uct_md_h pd, char *md_name, cpu_set_t local_cpus,
                                        std::vector<resource>& all_resources);
+    static void set_interface_rscs(char *md_name, cpu_set_t local_cpus,
+                                   struct ifaddrs *ifa,
+                                   std::vector<resource>& all_resources);
+    static void init_sockaddr_rsc(resource *rsc, struct sockaddr *listen_addr,
+                                  struct sockaddr *connect_addr, size_t size);
     static const char *uct_mem_type_names[];
 
     uct_test::entity* create_entity(size_t rx_headroom,
                                     uct_error_handler_t err_handler = NULL);
     uct_test::entity* create_entity(uct_iface_params_t &params);
+    int max_connections();
 
     ucs::ptr_vector<entity> m_entities;
     uct_iface_config_t      *m_iface_config;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/test/mpi/test_memhooks.c b/src/mpid/ch4/netmod/ucx/ucx/test/mpi/test_memhooks.c
index c029c9b64..e6366b140 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/test/mpi/test_memhooks.c
+++ b/src/mpid/ch4/netmod/ucx/ucx/test/mpi/test_memhooks.c
@@ -9,10 +9,12 @@
 #include <ucs/sys/preprocessor.h>
 #include <ucm/api/ucm.h>
 #include <sys/mman.h>
+#include <sys/shm.h>
 #include <malloc.h>
 #include <dlfcn.h>
 #include <unistd.h>
 #include <string.h>
+#include <stdlib.h>
 
 #define CHKERR_JUMP(cond, msg, label) \
     do { \
@@ -35,9 +37,12 @@
         } \
     } while (0);
 
+#define SHMAT_FAILED ((void*)-1)
+
 void* open_dyn_lib(const char *lib_path);
 void* flag_no_install_init(const char *path);
-int malloc_hooks_run(void *dl);
+int malloc_hooks_run_all(void *dl);
+int malloc_hooks_run_unmapped(void *dl);
 int ext_event_run(void *dl);
 void *ext_event_init(const char *path);
 
@@ -48,9 +53,10 @@ typedef struct memtest_type {
 } memtest_type_t;
 
 memtest_type_t tests[] = {
-    {"malloc_hooks",    open_dyn_lib,         malloc_hooks_run},
-    {"external_events", ext_event_init,       ext_event_run},
-    {"flag_no_install", flag_no_install_init, ext_event_run},
+    {"malloc_hooks",          open_dyn_lib,         malloc_hooks_run_all},
+    {"malloc_hooks_unmapped", open_dyn_lib,         malloc_hooks_run_unmapped},
+    {"external_events",       ext_event_init,       ext_event_run},
+    {"flag_no_install",       flag_no_install_init, ext_event_run},
     {NULL}
 };
 
@@ -62,9 +68,10 @@ static void usage() {
     printf("Options are:\n");
     printf("  -h         Print this info.\n");
     printf("  -t <name>  Test name to execute (malloc_hooks)\n");
-    printf("                 malloc_hooks     : General UCM test.\n");
-    printf("                 external_events  : Test of ucm_set_external_event() API.\n");
-    printf("                 flag_no_install  : Test of UCM_EVENT_FLAG_NO_INSTALL flag.\n");
+    printf("                 malloc_hooks          : General UCM test for VM_MAPPED and VM_UNMAPPED\n");
+    printf("                 malloc_hooks_unmapped : Test VM_UNMAPPED event only\n");
+    printf("                 external_events       : Test of ucm_set_external_event() API\n");
+    printf("                 flag_no_install       : Test of UCM_EVENT_FLAG_NO_INSTALL flag\n");
     printf("\n");
 }
 
@@ -78,7 +85,7 @@ static void event_callback(ucm_event_type_t event_type, ucm_event_t *event,
     }
 }
 
-static ucs_status_t set_event_handler (void *dl, int events)
+static ucs_status_t set_event_handler(void *dl, int events)
 {
     ucs_status_t (*set_handler)(int events, int priority,
                                 ucm_event_callback_t cb, void *arg);
@@ -91,18 +98,9 @@ static ucs_status_t set_event_handler (void *dl, int events)
 
 static ucs_status_t disable_memory_hooks(void *dl)
 {
-    ucs_status_t (*modify_cfg)(const char *name, const char *val);
-    ucs_status_t status;
-
-    DL_FIND_FUNC(dl, "ucm_config_modify", modify_cfg,
-                 return UCS_ERR_UNSUPPORTED);
-
-    status = modify_cfg("MALLOC_HOOKS", "no");
-    if (status == UCS_OK) {
-        status = modify_cfg("MMAP_RELOC", "no");
-    }
-
-    return status;
+    setenv("UCX_MEM_MALLOC_HOOKS", "n", 1);
+    setenv("UCX_MEM_MMAP_RELOC",   "n", 1);
+    return UCS_OK;
 }
 
 void* open_dyn_lib(const char *lib_path)
@@ -171,59 +169,141 @@ fail:
     return NULL;
 }
 
-int malloc_hooks_run(void *dl)
+int malloc_hooks_run_flags(void *dl, ucm_event_type_t events)
 {
     ucs_status_t status;
     void *ptr_malloc_core = NULL;
     void *ptr_malloc_mmap = NULL;
     void *ptr_direct_mmap = MAP_FAILED;
+    int  shmid            = -1;
+    void *ptr_shmat       = SHMAT_FAILED;
     void *dl_test;
     const size_t size = 1024 * 1024;
     const char *lib_path = UCS_PP_MAKE_STRING(TEST_LIB_DIR) "/" "libtest_memhooks.so";
     const char *cust_mmap_name  = "memhook_test_lib_call_mmap";
     void * (*cust_mmap)(size_t size);
 
-    status = set_event_handler(dl, UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);
+    status = set_event_handler(dl, events);
     CHKERR_JUMP(status != UCS_OK, "Failed to set event handler", fail_close_ucm);
 
     printf("Allocating memory\n");
 
+    /* Create SysV segment */
+    shmid = shmget(IPC_PRIVATE, size, IPC_CREAT|SHM_R|SHM_W);
+    CHKERR_JUMP(shmid == -1, "Failed to create shared memory segment: %m",
+                fail_close_ucm);
+
+    /*
+     * Test shmat/shmdt before malloc() because shmat() add entires to an internal
+     * hash of pointers->size, which makes previous pointers un-releasable
+     */
+
+    /* Attach SysV segment */
+    total_mapped = 0;
+    ptr_shmat = shmat(shmid, NULL, 0);
+    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, "Failed to attach shared memory segment",
+                fail_close_ucm);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped < size, "No callback for shmat", fail_close_ucm);
+    }
+    printf("After shmat: reported mapped=%zu\n", total_mapped);
+
+    /* Detach SysV segment */
+    total_unmapped = 0;
+    shmdt(ptr_shmat);
+    ptr_shmat = SHMAT_FAILED;
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped < size, "No callback for shmdt", fail_close_ucm);
+    }
+    printf("After shmdt: reported unmapped=%zu\n", total_unmapped);
+
+    /* Attach SysV segment at fixed address */
+    total_mapped = 0;
+    total_unmapped = 0;
+    ptr_shmat = shmat(shmid, (void*)0xff000000, SHM_REMAP);
+    CHKERR_JUMP(ptr_shmat == SHMAT_FAILED, "Failed to attach shared memory segment",
+                fail_close_ucm);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped < size, "No map callback for shmat(REMAP)", fail_close_ucm);
+    }
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped < size, "No unmap callback for shmat(REMAP)",
+                    fail_close_ucm);
+    }
+    printf("After shmat(REMAP): reported mapped=%zu unmapped=%zu\n", total_mapped,
+           total_unmapped);
+
+    /* Detach SysV segment */
+    total_unmapped = 0;
+    shmdt(ptr_shmat);
+    ptr_shmat = SHMAT_FAILED;
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped < size, "No callback for shmdt", fail_close_ucm);
+    }
+    printf("After shmdt: reported unmapped=%zu\n", total_unmapped);
+
+    /* Destroy SysV segment */
+    shmctl(shmid, IPC_RMID, NULL);
+    shmid = -1;
+
     /* Allocate using morecore */
     mallopt(M_MMAP_THRESHOLD, size * 2);
     mallopt(M_TRIM_THRESHOLD, size / 2);
     total_mapped = 0;
     ptr_malloc_core = malloc(1024 * 1024);
-    CHKERR_JUMP(total_mapped == 0, "No callback for core malloc", fail_close_ucm);
-    printf("After core malloc: mapped=%zu\n", total_mapped);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped == 0, "No callback for core malloc",
+                    fail_close_ucm);
+    }
+    printf("After core malloc: reported mapped=%zu\n", total_mapped);
 
     /* Allocate using mmap */
     mallopt(M_MMAP_THRESHOLD, size / 2);
     total_mapped = 0;
     ptr_malloc_mmap = malloc(2 * 1024 * 1024);
-    CHKERR_JUMP(total_mapped == 0, "No callback for mmap malloc", fail_close_ucm);
-    printf("After mmap malloc: mapped=%zu\n", total_mapped);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped == 0, "No callback for mmap malloc",
+                    fail_close_ucm);
+    }
+    printf("After mmap malloc: reported mapped=%zu\n", total_mapped);
 
     /* Allocate directly with mmap */
     total_mapped = 0;
     ptr_direct_mmap = mmap(NULL, size, PROT_READ|PROT_WRITE,
                            MAP_PRIVATE|MAP_ANON, -1, 0);
-    CHKERR_JUMP(total_mapped == 0, "No callback for mmap", fail_close_ucm);
-    printf("After mmap: mapped=%zu\n", total_mapped);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped < size, "No callback for mmap", fail_close_ucm);
+    }
+    printf("After mmap: reported mapped=%zu\n", total_mapped);
+
+    /* Remap */
+    total_unmapped = 0;
+    ptr_direct_mmap = mmap(ptr_direct_mmap, size, PROT_READ|PROT_WRITE,
+                           MAP_PRIVATE|MAP_ANON|MAP_FIXED, -1, 0);
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped < size, "No unmap callback for mmap(FIXED)",
+                    fail_close_ucm);
+    }
+    printf("After mmap(FIXED): reported unmapped=%zu\n", total_unmapped);
 
     /* Call munmap directly */
     total_unmapped = 0;
     munmap(ptr_direct_mmap, size);
-    CHKERR_JUMP(total_unmapped == 0, "No callback for munmap", fail_close_ucm);
-    printf("After munmap: unmapped=%zu\n", total_unmapped);
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped == 0, "No callback for munmap", fail_close_ucm);
+    }
+    printf("After munmap: reported unmapped=%zu\n", total_unmapped);
 
     /* Release indirectly */
     total_unmapped = 0;
     free(ptr_malloc_mmap);
     ptr_malloc_mmap = NULL;
     malloc_trim(0);
-    CHKERR_JUMP(total_unmapped == 0, "No callback for munmap from malloc",
-                fail_close_ucm);
-    printf("After mmap free + trim: unmapped=%zu\n", total_unmapped);
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped == 0, "No callback for munmap from free",
+                    fail_close_ucm);
+    }
+    printf("After mmap free + trim: reported unmapped=%zu\n", total_unmapped);
 
     /* Call mmap from a library we load after hooks are installed */
     dl_test = open_dyn_lib(lib_path);
@@ -234,15 +314,17 @@ int malloc_hooks_run(void *dl)
     ptr_direct_mmap = cust_mmap(size);
     CHKERR_JUMP(ptr_direct_mmap == MAP_FAILED, "Failed to mmap from dynamic lib",
                 fail_close_all);
-    CHKERR_JUMP(total_mapped == 0,"No callback for mmap from dynamic lib",
-                fail_close_all);
-    printf("After another mmap from dynamic lib: mapped=%zu\n", total_mapped);
+    if (events & UCM_EVENT_VM_MAPPED) {
+        CHKERR_JUMP(total_mapped == 0,"No callback for mmap from dynamic lib",
+                    fail_close_all);
+    }
+    printf("After another mmap from dynamic lib: reported mapped=%zu\n", total_mapped);
     munmap(ptr_direct_mmap, size);
     ptr_direct_mmap = MAP_FAILED;
 
     /*
      * Test closing UCM.
-     * The library should not really be unloaded, because the meory hooks still
+     * The library should not really be unloaded, because the memory hooks still
      * point to functions inside it.
      */
     total_unmapped = 0;
@@ -250,8 +332,10 @@ int malloc_hooks_run(void *dl)
     dlclose(dl_test);
     free(ptr_malloc_core); /* This should still work */
     ptr_malloc_core = NULL;
-    CHKERR_JUMP(total_unmapped == 0, "No callback for munmap from malloc", fail);
-    printf("After core malloc free: unmapped=%zu\n", total_unmapped);
+    if (events & UCM_EVENT_VM_UNMAPPED) {
+        CHKERR_JUMP(total_unmapped == 0, "No callback for munmap from malloc", fail);
+    }
+    printf("After core malloc free: reported unmapped=%zu\n", total_unmapped);
 
     return 0;
 
@@ -260,6 +344,12 @@ fail_close_all:
 fail_close_ucm:
     dlclose(dl);
 fail:
+    if (ptr_shmat != SHMAT_FAILED) {
+        shmdt(ptr_shmat);
+    }
+    if (shmid != -1) {
+        shmctl(shmid, IPC_RMID, NULL);
+    }
     free(ptr_malloc_mmap);
     free(ptr_malloc_core);
     if (ptr_direct_mmap != MAP_FAILED) {
@@ -269,6 +359,16 @@ fail:
     return  -1;
 }
 
+int malloc_hooks_run_all(void *dl)
+{
+    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_MAPPED | UCM_EVENT_VM_UNMAPPED);
+}
+
+int malloc_hooks_run_unmapped(void *dl)
+{
+    return malloc_hooks_run_flags(dl, UCM_EVENT_VM_UNMAPPED);
+}
+
 int ext_event_run(void *dl)
 {
     void *ptr_direct_mmap;
diff --git a/src/mpid/ch4/netmod/ucx/ucx/ucx.spec.in b/src/mpid/ch4/netmod/ucx/ucx/ucx.spec.in
index e909619f2..422405660 100644
--- a/src/mpid/ch4/netmod/ucx/ucx/ucx.spec.in
+++ b/src/mpid/ch4/netmod/ucx/ucx/ucx.spec.in
@@ -16,15 +16,17 @@ BuildRoot: %(mktemp -ud %{_tmppath}/%{name}-%{version}-%{release}-XXXXXX)
 ExclusiveArch: aarch64 ppc64le x86_64
 
 BuildRequires: numactl-devel libibverbs-devel
-BuildRequires: automake autoconf libtool
+BuildRequires: automake autoconf libtool gcc-c++
 
 %description
-UCX stands for Unified Communication X. It requires either RDMA-capable device
-(InfiniBand, RoCE, etc), Cray Gemini or Aries, for inter-node communication.
-Future versions will support also TCP for inter-node, to lift that hardware
-dependency.
-In addition, the library can be used for intra-node communication by leveraging
-the following shared memory mechanisms: posix. sysv, cma, knem, xpmem.
+UCX stands for Unified Communication X. UCX provides an optimized communication
+layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric
+applications. UCX utilizes high-speed networks, such as RDMA (InfiniBand, RoCE,
+etc), Cray Gemini or Aries, for inter-node communication. If no such network is
+available, TCP is used instead. UCX supports efficient transfer of data in
+either main memory (RAM) or GPU memory (through CUDA and ROCm libraries).
+In addition, UCX provides efficient intra-node communication, by leveraging the
+following shared memory mechanisms: posix, sysv, cma, knem, and xpmem.
 
 %package devel
 Requires: %{name}%{?_isa} = %{version}-%{release}
@@ -51,7 +53,6 @@ Provides static libraries required for development with UCX.
            --disable-debug \
            --disable-assertions \
            --disable-params-check \
-           CXXFLAGS="%{optflags} -fno-exceptions" \
            %{?configure_options}
 make %{?_smp_mflags} V=1
 
@@ -81,6 +82,15 @@ rm -f %{buildroot}%{_libdir}/*.la
 %postun -p /sbin/ldconfig
 
 %changelog
+* Tue Nov 6 2018 Andrey Maslennikov <andreyma@mellanox.com> 1.5.0-1
+- Bump version to 1.5.0
+- See NEWS for details
+* Tue Oct 30 2018 Andrey Maslennikov <andreyma@mellanox.com> 1.4.0-1
+- See NEWS for details
+* Mon Aug 20 2018 Andrey Maslennikov <andreyma@mellanox.com> 1.3.1-1
+- See NEWS for details
+* Thu Aug 16 2018 Andrey Maslennikov <andreyma@mellanox.com> 1.3.0-1
+- Explicitly set gcc-c++ as requirements
 * Wed Mar 7 2018 Andrey Maslennikov <andreyma@mellanox.com> 1.3.0-1
 - See NEWS for details
 * Mon Aug 21 2017 Andrey Maslennikov <andreyma@mellanox.com> 1.2.1-1
